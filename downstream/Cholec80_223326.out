[2025-05-23 14:58:35,112] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-23 14:58:35,132] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
===========     Init Distribution     ===========
===========     Init Distribution     ===========
| distributed init (rank 1): env://, gpu 1
| distributed init (rank 0): env://, gpu 0
===========     Use Deepspeed     ===========
Namespace(batch_size=32, epochs=50, update_freq=1, save_ckpt_freq=10, model='unified_base_st', pretrained_data='settingE', pretrained_method='videomae-st', input_size=224, fc_drop_rate=0.5, drop=0.0, attn_drop_rate=0.0, drop_path=0.1, disable_eval_during_finetuning=False, opt='adamw', opt_eps=1e-08, opt_betas=(0.9, 0.999), clip_grad=None, momentum=0.9, weight_decay=0.05, weight_decay_end=None, lr=0.0005, layer_decay=0.75, warmup_lr=1e-06, min_lr=1e-06, warmup_epochs=5, warmup_steps=-1, color_jitter=0.4, aa='rand-m7-n4-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', crop_pct=None, short_side_size=224, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='', model_key='model|module', model_prefix='', data_path='/project/medimgfmod/syangcw/downstream/cholec80', eval_data_path='/project/medimgfmod/syangcw/downstream/cholec80', nb_classes=7, imagenet_default_mean_and_std=True, data_strategy='online', output_mode='key_frame', cut_black=True, num_frames=16, sampling_rate=4, data_set='Cholec80', data_fps='1fps', output_dir='/project/mmendoscope/Natural_Comparison/Cholec75', log_dir='/project/mmendoscope/Natural_Comparison/Cholec75', device='cuda', seed=42, resume='', auto_resume=False, save_ckpt=True, start_epoch=0, eval=False, qkv_bias=True, qkv_divided_bias=False, qkv_divided=False, dist_eval=True, num_workers=16, pin_mem=True, world_size=2, local_rank=0, dist_on_itp=False, dist_url='env://', enable_deepspeed=True, deepspeed=False, deepspeed_config='/project/mmendoscope/Natural_Comparison/Cholec75/deepspeed_config.json', deepscale=False, deepscale_config=None, rank=0, gpu=0, distributed=True, dist_backend='nccl')
train - 1fps : Number of the class = 7
val - 1fps : Number of the class = 7
test - 1fps : Number of the class = 7
Train Dataset Length:  86344
Val Dataset Length:  21445
Test Dataset Length:  88494
Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x1554a1329ff0>
Warning: Enabling distributed evaluation with an eval dataset not divisible by process number. This will slightly alter validation results as extra duplicate entries are added to achieve equal num of samples per-process.
Distribute Sampler For Val/Test
Log dir: /project/mmendoscope/Natural_Comparison/Cholec75/unified_base_st_settingE_videomae-st_Cholec80_0.0005_0.75_online_key_frame_frame16_Fixed_Stride_4/log
Mixup is activated!
Load ckpt from /project/mmendoscope/SurgSSL_output/[401]-[0.85]-[SurgeryMAEKD]-[pretrain_masked_video_student_base_patch16_224]-[surgery_teacher_vit_large_patch16]-[Cholec80_M2CAI16-tool_M2CAI16-workflow_HeiChole_PitVis_PSI-AVA_AutoLaparo_BernBypass70_StrasBypass70_GenSurgery]/checkpoint-199.pth
Removing keys from pretrained checkpoint: 
Weights of VisionTransformer not initialized from pretrained model: ['fc_norm.weight', 'fc_norm.bias', 'head.weight', 'head.bias']
Weights from pretrained model not used in VisionTransformer: ['pos_embed', 'mask_token_img', 'pos_embed_img', 'mask_token_vid', 'pos_embed_vid', 'down_img.weight', 'down_img.bias', 'decoder_img.blocks.0.norm1.weight', 'decoder_img.blocks.0.norm1.bias', 'decoder_img.blocks.0.attn.q_bias', 'decoder_img.blocks.0.attn.v_bias', 'decoder_img.blocks.0.attn.qkv.weight', 'decoder_img.blocks.0.attn.proj.weight', 'decoder_img.blocks.0.attn.proj.bias', 'decoder_img.blocks.0.norm2.weight', 'decoder_img.blocks.0.norm2.bias', 'decoder_img.blocks.0.mlp.fc1.weight', 'decoder_img.blocks.0.mlp.fc1.bias', 'decoder_img.blocks.0.mlp.fc2.weight', 'decoder_img.blocks.0.mlp.fc2.bias', 'decoder_img.blocks.1.norm1.weight', 'decoder_img.blocks.1.norm1.bias', 'decoder_img.blocks.1.attn.q_bias', 'decoder_img.blocks.1.attn.v_bias', 'decoder_img.blocks.1.attn.qkv.weight', 'decoder_img.blocks.1.attn.proj.weight', 'decoder_img.blocks.1.attn.proj.bias', 'decoder_img.blocks.1.norm2.weight', 'decoder_img.blocks.1.norm2.bias', 'decoder_img.blocks.1.mlp.fc1.weight', 'decoder_img.blocks.1.mlp.fc1.bias', 'decoder_img.blocks.1.mlp.fc2.weight', 'decoder_img.blocks.1.mlp.fc2.bias', 'decoder_img.norm.weight', 'decoder_img.norm.bias', 'decoder_img.head.weight', 'decoder_img.head.bias', 'down_vid.weight', 'down_vid.bias', 'decoder_vid.blocks.0.norm1.weight', 'decoder_vid.blocks.0.norm1.bias', 'decoder_vid.blocks.0.attn.q_bias', 'decoder_vid.blocks.0.attn.v_bias', 'decoder_vid.blocks.0.attn.qkv.weight', 'decoder_vid.blocks.0.attn.proj.weight', 'decoder_vid.blocks.0.attn.proj.bias', 'decoder_vid.blocks.0.norm2.weight', 'decoder_vid.blocks.0.norm2.bias', 'decoder_vid.blocks.0.mlp.fc1.weight', 'decoder_vid.blocks.0.mlp.fc1.bias', 'decoder_vid.blocks.0.mlp.fc2.weight', 'decoder_vid.blocks.0.mlp.fc2.bias', 'decoder_vid.blocks.1.norm1.weight', 'decoder_vid.blocks.1.norm1.bias', 'decoder_vid.blocks.1.attn.q_bias', 'decoder_vid.blocks.1.attn.v_bias', 'decoder_vid.blocks.1.attn.qkv.weight', 'decoder_vid.blocks.1.attn.proj.weight', 'decoder_vid.blocks.1.attn.proj.bias', 'decoder_vid.blocks.1.norm2.weight', 'decoder_vid.blocks.1.norm2.bias', 'decoder_vid.blocks.1.mlp.fc1.weight', 'decoder_vid.blocks.1.mlp.fc1.bias', 'decoder_vid.blocks.1.mlp.fc2.weight', 'decoder_vid.blocks.1.mlp.fc2.bias', 'decoder_vid.blocks.2.norm1.weight', 'decoder_vid.blocks.2.norm1.bias', 'decoder_vid.blocks.2.attn.q_bias', 'decoder_vid.blocks.2.attn.v_bias', 'decoder_vid.blocks.2.attn.qkv.weight', 'decoder_vid.blocks.2.attn.proj.weight', 'decoder_vid.blocks.2.attn.proj.bias', 'decoder_vid.blocks.2.norm2.weight', 'decoder_vid.blocks.2.norm2.bias', 'decoder_vid.blocks.2.mlp.fc1.weight', 'decoder_vid.blocks.2.mlp.fc1.bias', 'decoder_vid.blocks.2.mlp.fc2.weight', 'decoder_vid.blocks.2.mlp.fc2.bias', 'decoder_vid.blocks.3.norm1.weight', 'decoder_vid.blocks.3.norm1.bias', 'decoder_vid.blocks.3.attn.q_bias', 'decoder_vid.blocks.3.attn.v_bias', 'decoder_vid.blocks.3.attn.qkv.weight', 'decoder_vid.blocks.3.attn.proj.weight', 'decoder_vid.blocks.3.attn.proj.bias', 'decoder_vid.blocks.3.norm2.weight', 'decoder_vid.blocks.3.norm2.bias', 'decoder_vid.blocks.3.mlp.fc1.weight', 'decoder_vid.blocks.3.mlp.fc1.bias', 'decoder_vid.blocks.3.mlp.fc2.weight', 'decoder_vid.blocks.3.mlp.fc2.bias', 'decoder_vid.norm.weight', 'decoder_vid.norm.bias', 'decoder_vid.head.weight', 'decoder_vid.head.bias', 'norm.weight', 'norm.bias']
Patch size = (16, 16)
Window size: = (16, 14, 14)
number of params: 86232583
LR = 0.00050000
Batch size = 64
Update frequent = 1
Number of training examples = 86344
Number of training training per epoch = 1349
Assigned values = [0.023757264018058777, 0.03167635202407837, 0.04223513603210449, 0.056313514709472656, 0.07508468627929688, 0.1001129150390625, 0.13348388671875, 0.177978515625, 0.2373046875, 0.31640625, 0.421875, 0.5625, 0.75, 1.0]
Skip weight decay list:  {'cls_token', 'pos_embed'}
Param groups = {
  "layer_0_decay": {
    "weight_decay": 0.05,
    "params": [
      "patch_embed.proj.weight"
    ],
    "lr_scale": 0.023757264018058777
  },
  "layer_0_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "patch_embed.proj.bias"
    ],
    "lr_scale": 0.023757264018058777
  },
  "layer_1_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.0.norm1.weight",
      "blocks.0.norm1.bias",
      "blocks.0.attn.q_bias",
      "blocks.0.attn.v_bias",
      "blocks.0.attn.proj.bias",
      "blocks.0.norm2.weight",
      "blocks.0.norm2.bias",
      "blocks.0.mlp.fc1.bias",
      "blocks.0.mlp.fc2.bias"
    ],
    "lr_scale": 0.03167635202407837
  },
  "layer_1_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.0.attn.qkv.weight",
      "blocks.0.attn.proj.weight",
      "blocks.0.mlp.fc1.weight",
      "blocks.0.mlp.fc2.weight"
    ],
    "lr_scale": 0.03167635202407837
  },
  "layer_2_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.1.norm1.weight",
      "blocks.1.norm1.bias",
      "blocks.1.attn.q_bias",
      "blocks.1.attn.v_bias",
      "blocks.1.attn.proj.bias",
      "blocks.1.norm2.weight",
      "blocks.1.norm2.bias",
      "blocks.1.mlp.fc1.bias",
      "blocks.1.mlp.fc2.bias"
    ],
    "lr_scale": 0.04223513603210449
  },
  "layer_2_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.1.attn.qkv.weight",
      "blocks.1.attn.proj.weight",
      "blocks.1.mlp.fc1.weight",
      "blocks.1.mlp.fc2.weight"
    ],
    "lr_scale": 0.04223513603210449
  },
  "layer_3_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.2.norm1.weight",
      "blocks.2.norm1.bias",
      "blocks.2.attn.q_bias",
      "blocks.2.attn.v_bias",
      "blocks.2.attn.proj.bias",
      "blocks.2.norm2.weight",
      "blocks.2.norm2.bias",
      "blocks.2.mlp.fc1.bias",
      "blocks.2.mlp.fc2.bias"
    ],
    "lr_scale": 0.056313514709472656
  },
  "layer_3_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.2.attn.qkv.weight",
      "blocks.2.attn.proj.weight",
      "blocks.2.mlp.fc1.weight",
      "blocks.2.mlp.fc2.weight"
    ],
    "lr_scale": 0.056313514709472656
  },
  "layer_4_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.3.norm1.weight",
      "blocks.3.norm1.bias",
      "blocks.3.attn.q_bias",
      "blocks.3.attn.v_bias",
      "blocks.3.attn.proj.bias",
      "blocks.3.norm2.weight",
      "blocks.3.norm2.bias",
      "blocks.3.mlp.fc1.bias",
      "blocks.3.mlp.fc2.bias"
    ],
    "lr_scale": 0.07508468627929688
  },
  "layer_4_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.3.attn.qkv.weight",
      "blocks.3.attn.proj.weight",
      "blocks.3.mlp.fc1.weight",
      "blocks.3.mlp.fc2.weight"
    ],
    "lr_scale": 0.07508468627929688
  },
  "layer_5_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.4.norm1.weight",
      "blocks.4.norm1.bias",
      "blocks.4.attn.q_bias",
      "blocks.4.attn.v_bias",
      "blocks.4.attn.proj.bias",
      "blocks.4.norm2.weight",
      "blocks.4.norm2.bias",
      "blocks.4.mlp.fc1.bias",
      "blocks.4.mlp.fc2.bias"
    ],
    "lr_scale": 0.1001129150390625
  },
  "layer_5_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.4.attn.qkv.weight",
      "blocks.4.attn.proj.weight",
      "blocks.4.mlp.fc1.weight",
      "blocks.4.mlp.fc2.weight"
    ],
    "lr_scale": 0.1001129150390625
  },
  "layer_6_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.5.norm1.weight",
      "blocks.5.norm1.bias",
      "blocks.5.attn.q_bias",
      "blocks.5.attn.v_bias",
      "blocks.5.attn.proj.bias",
      "blocks.5.norm2.weight",
      "blocks.5.norm2.bias",
      "blocks.5.mlp.fc1.bias",
      "blocks.5.mlp.fc2.bias"
    ],
    "lr_scale": 0.13348388671875
  },
  "layer_6_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.5.attn.qkv.weight",
      "blocks.5.attn.proj.weight",
      "blocks.5.mlp.fc1.weight",
      "blocks.5.mlp.fc2.weight"
    ],
    "lr_scale": 0.13348388671875
  },
  "layer_7_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.6.norm1.weight",
      "blocks.6.norm1.bias",
      "blocks.6.attn.q_bias",
      "blocks.6.attn.v_bias",
      "blocks.6.attn.proj.bias",
      "blocks.6.norm2.weight",
      "blocks.6.norm2.bias",
      "blocks.6.mlp.fc1.bias",
      "blocks.6.mlp.fc2.bias"
    ],
    "lr_scale": 0.177978515625
  },
  "layer_7_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.6.attn.qkv.weight",
      "blocks.6.attn.proj.weight",
      "blocks.6.mlp.fc1.weight",
      "blocks.6.mlp.fc2.weight"
    ],
    "lr_scale": 0.177978515625
  },
  "layer_8_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.7.norm1.weight",
      "blocks.7.norm1.bias",
      "blocks.7.attn.q_bias",
      "blocks.7.attn.v_bias",
      "blocks.7.attn.proj.bias",
      "blocks.7.norm2.weight",
      "blocks.7.norm2.bias",
      "blocks.7.mlp.fc1.bias",
      "blocks.7.mlp.fc2.bias"
    ],
    "lr_scale": 0.2373046875
  },
  "layer_8_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.7.attn.qkv.weight",
      "blocks.7.attn.proj.weight",
      "blocks.7.mlp.fc1.weight",
      "blocks.7.mlp.fc2.weight"
    ],
    "lr_scale": 0.2373046875
  },
  "layer_9_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.8.norm1.weight",
      "blocks.8.norm1.bias",
      "blocks.8.attn.q_bias",
      "blocks.8.attn.v_bias",
      "blocks.8.attn.proj.bias",
      "blocks.8.norm2.weight",
      "blocks.8.norm2.bias",
      "blocks.8.mlp.fc1.bias",
      "blocks.8.mlp.fc2.bias"
    ],
    "lr_scale": 0.31640625
  },
  "layer_9_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.8.attn.qkv.weight",
      "blocks.8.attn.proj.weight",
      "blocks.8.mlp.fc1.weight",
      "blocks.8.mlp.fc2.weight"
    ],
    "lr_scale": 0.31640625
  },
  "layer_10_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.9.norm1.weight",
      "blocks.9.norm1.bias",
      "blocks.9.attn.q_bias",
      "blocks.9.attn.v_bias",
      "blocks.9.attn.proj.bias",
      "blocks.9.norm2.weight",
      "blocks.9.norm2.bias",
      "blocks.9.mlp.fc1.bias",
      "blocks.9.mlp.fc2.bias"
    ],
    "lr_scale": 0.421875
  },
  "layer_10_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.9.attn.qkv.weight",
      "blocks.9.attn.proj.weight",
      "blocks.9.mlp.fc1.weight",
      "blocks.9.mlp.fc2.weight"
    ],
    "lr_scale": 0.421875
  },
  "layer_11_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.10.norm1.weight",
      "blocks.10.norm1.bias",
      "blocks.10.attn.q_bias",
      "blocks.10.attn.v_bias",
      "blocks.10.attn.proj.bias",
      "blocks.10.norm2.weight",
      "blocks.10.norm2.bias",
      "blocks.10.mlp.fc1.bias",
      "blocks.10.mlp.fc2.bias"
    ],
    "lr_scale": 0.5625
  },
  "layer_11_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.10.attn.qkv.weight",
      "blocks.10.attn.proj.weight",
      "blocks.10.mlp.fc1.weight",
      "blocks.10.mlp.fc2.weight"
    ],
    "lr_scale": 0.5625
  },
  "layer_12_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.11.norm1.weight",
      "blocks.11.norm1.bias",
      "blocks.11.attn.q_bias",
      "blocks.11.attn.v_bias",
      "blocks.11.attn.proj.bias",
      "blocks.11.norm2.weight",
      "blocks.11.norm2.bias",
      "blocks.11.mlp.fc1.bias",
      "blocks.11.mlp.fc2.bias"
    ],
    "lr_scale": 0.75
  },
  "layer_12_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.11.attn.qkv.weight",
      "blocks.11.attn.proj.weight",
      "blocks.11.mlp.fc1.weight",
      "blocks.11.mlp.fc2.weight"
    ],
    "lr_scale": 0.75
  },
  "layer_13_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "fc_norm.weight",
      "fc_norm.bias",
      "head.bias"
    ],
    "lr_scale": 1.0
  },
  "layer_13_decay": {
    "weight_decay": 0.05,
    "params": [
      "head.weight"
    ],
    "lr_scale": 1.0
  }
}
[2025-05-23 14:58:51,293] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown
[2025-05-23 14:58:51,293] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown
[2025-05-23 14:58:51,293] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-05-23 14:58:51,293] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-05-23 14:58:51,598] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
ninja: no work to do.
Time to load fused_adam op: 0.20610713958740234 seconds
[2025-05-23 14:58:52,330] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adam as basic optimizer
[2025-05-23 14:58:52,330] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2025-05-23 14:58:52,332] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[2025-05-23 14:58:52,332] [INFO] [logging.py:96:log_dist] [Rank 0] Creating fp16 optimizer with dynamic loss scale
[2025-05-23 14:58:52,342] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = adam
[2025-05-23 14:58:52,342] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2025-05-23 14:58:52,342] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2025-05-23 14:58:52,342] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-05-23 14:58:52,342] [INFO] [config.py:996:print] DeepSpeedEngine configuration:
[2025-05-23 14:58:52,342] [INFO] [config.py:1000:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2025-05-23 14:58:52,342] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2025-05-23 14:58:52,342] [INFO] [config.py:1000:print]   amp_enabled .................. False
[2025-05-23 14:58:52,342] [INFO] [config.py:1000:print]   amp_params ................... False
[2025-05-23 14:58:52,343] [INFO] [config.py:1000:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2025-05-23 14:58:52,343] [INFO] [config.py:1000:print]   bfloat16_enabled ............. False
[2025-05-23 14:58:52,343] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False
[2025-05-23 14:58:52,343] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False
[2025-05-23 14:58:52,343] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True
[2025-05-23 14:58:52,343] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False
[2025-05-23 14:58:52,343] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x155235a12e90>
[2025-05-23 14:58:52,343] [INFO] [config.py:1000:print]   communication_data_type ...... None
[2025-05-23 14:58:52,343] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}
[2025-05-23 14:58:52,343] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2025-05-23 14:58:52,343] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False
[2025-05-23 14:58:52,343] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False
[2025-05-23 14:58:52,343] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2025-05-23 14:58:52,343] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False
[2025-05-23 14:58:52,343] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False
[2025-05-23 14:58:52,343] [INFO] [config.py:1000:print]   disable_allgather ............ False
[2025-05-23 14:58:52,343] [INFO] [config.py:1000:print]   dump_state ................... False
[2025-05-23 14:58:52,343] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... {'init_scale': 128, 'scale_window': 128, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}
[2025-05-23 14:58:52,343] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False
[2025-05-23 14:58:52,343] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1
[2025-05-23 14:58:52,343] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2025-05-23 14:58:52,343] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0
[2025-05-23 14:58:52,343] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100
[2025-05-23 14:58:52,343] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06
[2025-05-23 14:58:52,343] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01
[2025-05-23 14:58:52,343] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False
[2025-05-23 14:58:52,343] [INFO] [config.py:1000:print]   elasticity_enabled ........... False
[2025-05-23 14:58:52,343] [INFO] [config.py:1000:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2025-05-23 14:58:52,343] [INFO] [config.py:1000:print]   fp16_auto_cast ............... False
[2025-05-23 14:58:52,343] [INFO] [config.py:1000:print]   fp16_enabled ................. True
[2025-05-23 14:58:52,343] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False
[2025-05-23 14:58:52,343] [INFO] [config.py:1000:print]   global_rank .................. 0
[2025-05-23 14:58:52,343] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None
[2025-05-23 14:58:52,343] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1
[2025-05-23 14:58:52,343] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0
[2025-05-23 14:58:52,343] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0
[2025-05-23 14:58:52,343] [INFO] [config.py:1000:print]   graph_harvesting ............. False
[2025-05-23 14:58:52,343] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2025-05-23 14:58:52,343] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 128
[2025-05-23 14:58:52,343] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False
[2025-05-23 14:58:52,343] [INFO] [config.py:1000:print]   loss_scale ................... 0
[2025-05-23 14:58:52,343] [INFO] [config.py:1000:print]   memory_breakdown ............. False
[2025-05-23 14:58:52,343] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False
[2025-05-23 14:58:52,343] [INFO] [config.py:1000:print]   mics_shard_size .............. -1
[2025-05-23 14:58:52,343] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2025-05-23 14:58:52,344] [INFO] [config.py:1000:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2025-05-23 14:58:52,344] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False
[2025-05-23 14:58:52,344] [INFO] [config.py:1000:print]   optimizer_name ............... adam
[2025-05-23 14:58:52,344] [INFO] [config.py:1000:print]   optimizer_params ............. {'lr': 0.0005, 'weight_decay': 0.05, 'bias_correction': True, 'betas': [0.9, 0.999], 'eps': 1e-08}
[2025-05-23 14:58:52,344] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2025-05-23 14:58:52,344] [INFO] [config.py:1000:print]   pld_enabled .................. False
[2025-05-23 14:58:52,344] [INFO] [config.py:1000:print]   pld_params ................... False
[2025-05-23 14:58:52,344] [INFO] [config.py:1000:print]   prescale_gradients ........... False
[2025-05-23 14:58:52,344] [INFO] [config.py:1000:print]   scheduler_name ............... None
[2025-05-23 14:58:52,344] [INFO] [config.py:1000:print]   scheduler_params ............. None
[2025-05-23 14:58:52,344] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32
[2025-05-23 14:58:52,344] [INFO] [config.py:1000:print]   sparse_attention ............. None
[2025-05-23 14:58:52,344] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False
[2025-05-23 14:58:52,344] [INFO] [config.py:1000:print]   steps_per_print .............. 1000
[2025-05-23 14:58:52,344] [INFO] [config.py:1000:print]   train_batch_size ............. 64
[2025-05-23 14:58:52,344] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  32
[2025-05-23 14:58:52,344] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False
[2025-05-23 14:58:52,344] [INFO] [config.py:1000:print]   use_node_local_storage ....... False
[2025-05-23 14:58:52,344] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False
[2025-05-23 14:58:52,344] [INFO] [config.py:1000:print]   weight_quantization_config ... None
[2025-05-23 14:58:52,344] [INFO] [config.py:1000:print]   world_size ................... 2
[2025-05-23 14:58:52,344] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  False
[2025-05-23 14:58:52,344] [INFO] [config.py:1000:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2025-05-23 14:58:52,344] [INFO] [config.py:1000:print]   zero_enabled ................. False
[2025-05-23 14:58:52,344] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True
[2025-05-23 14:58:52,344] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 0
[2025-05-23 14:58:52,344] [INFO] [config.py:986:print_user_config]   json = {
    "train_batch_size": 64, 
    "train_micro_batch_size_per_gpu": 32, 
    "steps_per_print": 1000, 
    "optimizer": {
        "type": "Adam", 
        "adam_w_mode": true, 
        "params": {
            "lr": 0.0005, 
            "weight_decay": 0.05, 
            "bias_correction": true, 
            "betas": [0.9, 0.999], 
            "eps": 1e-08
        }
    }, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 7, 
        "loss_scale_window": 128
    }
}
model.gradient_accumulation_steps() = 1
Use step level LR scheduler!
Set warmup steps = 6745
Set warmup steps = 0
Max WD = 0.0500000, Min WD = 0.0500000
criterion = SoftTargetCrossEntropy()
Start training for 50 epochs
Epoch: [0]  [   0/1349]  eta: 3:53:01  lr: 0.000000  min_lr: 0.000000  loss: 1.9463 (1.9463)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 10.3640  data: 5.8426  max mem: 41808
Epoch: [0]  [  10/1349]  eta: 0:29:27  lr: 0.000001  min_lr: 0.000000  loss: 1.9459 (1.9460)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 1.3197  data: 0.5312  max mem: 41808
Epoch: [0]  [  20/1349]  eta: 0:19:09  lr: 0.000001  min_lr: 0.000000  loss: 1.9456 (1.9455)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3899  data: 0.0001  max mem: 41808
Epoch: [0]  [  30/1349]  eta: 0:15:04  lr: 0.000002  min_lr: 0.000000  loss: 1.9443 (1.9449)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3365  data: 0.0002  max mem: 41808
Epoch: [0]  [  40/1349]  eta: 0:12:57  lr: 0.000003  min_lr: 0.000000  loss: 1.9424 (1.9438)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3087  data: 0.0002  max mem: 41808
Epoch: [0]  [  50/1349]  eta: 0:11:38  lr: 0.000004  min_lr: 0.000000  loss: 1.9397 (1.9427)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3092  data: 0.0002  max mem: 41808
Epoch: [0]  [  60/1349]  eta: 0:10:45  lr: 0.000004  min_lr: 0.000000  loss: 1.9362 (1.9414)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3101  data: 0.0002  max mem: 41808
Epoch: [0]  [  70/1349]  eta: 0:10:05  lr: 0.000005  min_lr: 0.000000  loss: 1.9322 (1.9396)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3104  data: 0.0002  max mem: 41808
Epoch: [0]  [  80/1349]  eta: 0:09:35  lr: 0.000006  min_lr: 0.000000  loss: 1.9245 (1.9372)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3104  data: 0.0003  max mem: 41808
Epoch: [0]  [  90/1349]  eta: 0:09:11  lr: 0.000007  min_lr: 0.000000  loss: 1.9155 (1.9343)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3099  data: 0.0002  max mem: 41808
Epoch: [0]  [ 100/1349]  eta: 0:08:50  lr: 0.000007  min_lr: 0.000000  loss: 1.9080 (1.9311)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3090  data: 0.0001  max mem: 41808
Epoch: [0]  [ 110/1349]  eta: 0:08:33  lr: 0.000008  min_lr: 0.000000  loss: 1.8954 (1.9272)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3090  data: 0.0002  max mem: 41808
Epoch: [0]  [ 120/1349]  eta: 0:08:19  lr: 0.000009  min_lr: 0.000000  loss: 1.8829 (1.9228)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3112  data: 0.0002  max mem: 41808
[2025-05-23 14:59:44,101] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 14:59:44,102] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 128 to 256
[2025-05-23 14:59:44,101] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 14:59:44,102] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 128 to 256
Epoch: [0]  [ 130/1349]  eta: 0:08:06  lr: 0.000010  min_lr: 0.000000  loss: 1.8628 (1.9164)  loss_scale: 128.0000 (130.9313)  weight_decay: 0.0500 (0.0500)  time: 0.3106  data: 0.0001  max mem: 41808
Epoch: [0]  [ 140/1349]  eta: 0:07:54  lr: 0.000010  min_lr: 0.000000  loss: 1.8252 (1.9083)  loss_scale: 256.0000 (139.8014)  weight_decay: 0.0500 (0.0500)  time: 0.3086  data: 0.0002  max mem: 41808
Epoch: [0]  [ 150/1349]  eta: 0:07:43  lr: 0.000011  min_lr: 0.000000  loss: 1.7808 (1.8996)  loss_scale: 256.0000 (147.4967)  weight_decay: 0.0500 (0.0500)  time: 0.3094  data: 0.0001  max mem: 41808
Epoch: [0]  [ 160/1349]  eta: 0:07:35  lr: 0.000012  min_lr: 0.000000  loss: 1.7636 (1.8879)  loss_scale: 256.0000 (154.2360)  weight_decay: 0.0500 (0.0500)  time: 0.3171  data: 0.0001  max mem: 41808
Epoch: [0]  [ 170/1349]  eta: 0:07:26  lr: 0.000013  min_lr: 0.000000  loss: 1.6676 (1.8750)  loss_scale: 256.0000 (160.1871)  weight_decay: 0.0500 (0.0500)  time: 0.3167  data: 0.0002  max mem: 41808
Epoch: [0]  [ 180/1349]  eta: 0:07:18  lr: 0.000013  min_lr: 0.000000  loss: 1.6463 (1.8612)  loss_scale: 256.0000 (165.4807)  weight_decay: 0.0500 (0.0500)  time: 0.3088  data: 0.0002  max mem: 41808
Epoch: [0]  [ 190/1349]  eta: 0:07:10  lr: 0.000014  min_lr: 0.000000  loss: 1.5895 (1.8483)  loss_scale: 256.0000 (170.2199)  weight_decay: 0.0500 (0.0500)  time: 0.3080  data: 0.0002  max mem: 41808
Epoch: [0]  [ 200/1349]  eta: 0:07:03  lr: 0.000015  min_lr: 0.000000  loss: 1.5674 (1.8340)  loss_scale: 256.0000 (174.4876)  weight_decay: 0.0500 (0.0500)  time: 0.3085  data: 0.0002  max mem: 41808
Epoch: [0]  [ 210/1349]  eta: 0:06:56  lr: 0.000016  min_lr: 0.000000  loss: 1.5500 (1.8238)  loss_scale: 256.0000 (178.3507)  weight_decay: 0.0500 (0.0500)  time: 0.3093  data: 0.0001  max mem: 41808
Epoch: [0]  [ 220/1349]  eta: 0:06:49  lr: 0.000016  min_lr: 0.000000  loss: 1.6672 (1.8187)  loss_scale: 256.0000 (181.8643)  weight_decay: 0.0500 (0.0500)  time: 0.3085  data: 0.0001  max mem: 41808
Epoch: [0]  [ 230/1349]  eta: 0:06:43  lr: 0.000017  min_lr: 0.000000  loss: 1.6585 (1.8099)  loss_scale: 256.0000 (185.0736)  weight_decay: 0.0500 (0.0500)  time: 0.3085  data: 0.0001  max mem: 41808
Epoch: [0]  [ 240/1349]  eta: 0:06:37  lr: 0.000018  min_lr: 0.000000  loss: 1.6268 (1.8011)  loss_scale: 256.0000 (188.0166)  weight_decay: 0.0500 (0.0500)  time: 0.3083  data: 0.0001  max mem: 41808
Epoch: [0]  [ 250/1349]  eta: 0:06:31  lr: 0.000019  min_lr: 0.000000  loss: 1.5769 (1.7935)  loss_scale: 256.0000 (190.7251)  weight_decay: 0.0500 (0.0500)  time: 0.3084  data: 0.0001  max mem: 41808
[2025-05-23 15:00:23,794] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 15:00:23,794] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 256 to 512
[2025-05-23 15:00:23,794] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 15:00:23,794] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 256 to 512
Epoch: [0]  [ 260/1349]  eta: 0:06:26  lr: 0.000019  min_lr: 0.000000  loss: 1.5637 (1.7854)  loss_scale: 256.0000 (198.1303)  weight_decay: 0.0500 (0.0500)  time: 0.3086  data: 0.0002  max mem: 41808
Epoch: [0]  [ 270/1349]  eta: 0:06:20  lr: 0.000020  min_lr: 0.000000  loss: 1.5637 (1.7791)  loss_scale: 512.0000 (209.7122)  weight_decay: 0.0500 (0.0500)  time: 0.3088  data: 0.0002  max mem: 41808
Epoch: [0]  [ 280/1349]  eta: 0:06:15  lr: 0.000021  min_lr: 0.000000  loss: 1.5859 (1.7731)  loss_scale: 512.0000 (220.4698)  weight_decay: 0.0500 (0.0500)  time: 0.3098  data: 0.0002  max mem: 41808
Epoch: [0]  [ 290/1349]  eta: 0:06:10  lr: 0.000022  min_lr: 0.000001  loss: 1.5859 (1.7674)  loss_scale: 512.0000 (230.4880)  weight_decay: 0.0500 (0.0500)  time: 0.3092  data: 0.0003  max mem: 41808
Epoch: [0]  [ 300/1349]  eta: 0:06:05  lr: 0.000022  min_lr: 0.000001  loss: 1.5675 (1.7614)  loss_scale: 512.0000 (239.8405)  weight_decay: 0.0500 (0.0500)  time: 0.3079  data: 0.0002  max mem: 41808
Epoch: [0]  [ 310/1349]  eta: 0:06:00  lr: 0.000023  min_lr: 0.000001  loss: 1.5621 (1.7553)  loss_scale: 512.0000 (248.5916)  weight_decay: 0.0500 (0.0500)  time: 0.3080  data: 0.0003  max mem: 41808
Epoch: [0]  [ 320/1349]  eta: 0:05:55  lr: 0.000024  min_lr: 0.000001  loss: 1.6024 (1.7504)  loss_scale: 512.0000 (256.7975)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0002  max mem: 41808
Epoch: [0]  [ 330/1349]  eta: 0:05:51  lr: 0.000024  min_lr: 0.000001  loss: 1.6024 (1.7453)  loss_scale: 512.0000 (264.5076)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0002  max mem: 41808
Epoch: [0]  [ 340/1349]  eta: 0:05:46  lr: 0.000025  min_lr: 0.000001  loss: 1.5624 (1.7406)  loss_scale: 512.0000 (271.7654)  weight_decay: 0.0500 (0.0500)  time: 0.3098  data: 0.0002  max mem: 41808
Epoch: [0]  [ 350/1349]  eta: 0:05:42  lr: 0.000026  min_lr: 0.000001  loss: 1.5443 (1.7351)  loss_scale: 512.0000 (278.6097)  weight_decay: 0.0500 (0.0500)  time: 0.3100  data: 0.0003  max mem: 41808
Epoch: [0]  [ 360/1349]  eta: 0:05:37  lr: 0.000027  min_lr: 0.000001  loss: 1.5163 (1.7290)  loss_scale: 512.0000 (285.0748)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0002  max mem: 41808
Epoch: [0]  [ 370/1349]  eta: 0:05:33  lr: 0.000027  min_lr: 0.000001  loss: 1.5317 (1.7241)  loss_scale: 512.0000 (291.1914)  weight_decay: 0.0500 (0.0500)  time: 0.3093  data: 0.0001  max mem: 41808
Epoch: [0]  [ 380/1349]  eta: 0:05:29  lr: 0.000028  min_lr: 0.000001  loss: 1.5511 (1.7200)  loss_scale: 512.0000 (296.9869)  weight_decay: 0.0500 (0.0500)  time: 0.3127  data: 0.0002  max mem: 41808
[2025-05-23 15:01:03,383] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 15:01:03,383] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 15:01:03,383] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 512 to 1024
[2025-05-23 15:01:03,383] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 512 to 1024
Epoch: [0]  [ 390/1349]  eta: 0:05:25  lr: 0.000029  min_lr: 0.000001  loss: 1.5740 (1.7168)  loss_scale: 512.0000 (311.6522)  weight_decay: 0.0500 (0.0500)  time: 0.3112  data: 0.0002  max mem: 41808
Epoch: [0]  [ 400/1349]  eta: 0:05:21  lr: 0.000030  min_lr: 0.000001  loss: 1.6119 (1.7134)  loss_scale: 1024.0000 (329.4165)  weight_decay: 0.0500 (0.0500)  time: 0.3097  data: 0.0002  max mem: 41808
Epoch: [0]  [ 410/1349]  eta: 0:05:17  lr: 0.000030  min_lr: 0.000001  loss: 1.6077 (1.7091)  loss_scale: 1024.0000 (346.3163)  weight_decay: 0.0500 (0.0500)  time: 0.3088  data: 0.0002  max mem: 41808
Epoch: [0]  [ 420/1349]  eta: 0:05:13  lr: 0.000031  min_lr: 0.000001  loss: 1.5346 (1.7044)  loss_scale: 1024.0000 (362.4133)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0002  max mem: 41808
Epoch: [0]  [ 430/1349]  eta: 0:05:09  lr: 0.000032  min_lr: 0.000001  loss: 1.5322 (1.7016)  loss_scale: 1024.0000 (377.7633)  weight_decay: 0.0500 (0.0500)  time: 0.3060  data: 0.0002  max mem: 41808
Epoch: [0]  [ 440/1349]  eta: 0:05:05  lr: 0.000033  min_lr: 0.000001  loss: 1.5338 (1.6972)  loss_scale: 1024.0000 (392.4172)  weight_decay: 0.0500 (0.0500)  time: 0.3118  data: 0.0002  max mem: 41808
Epoch: [0]  [ 450/1349]  eta: 0:05:01  lr: 0.000033  min_lr: 0.000001  loss: 1.5338 (1.6940)  loss_scale: 1024.0000 (406.4213)  weight_decay: 0.0500 (0.0500)  time: 0.3197  data: 0.0003  max mem: 41808
Epoch: [0]  [ 460/1349]  eta: 0:04:58  lr: 0.000034  min_lr: 0.000001  loss: 1.5638 (1.6911)  loss_scale: 1024.0000 (419.8178)  weight_decay: 0.0500 (0.0500)  time: 0.3168  data: 0.0002  max mem: 41808
Epoch: [0]  [ 470/1349]  eta: 0:04:54  lr: 0.000035  min_lr: 0.000001  loss: 1.5815 (1.6874)  loss_scale: 1024.0000 (432.6454)  weight_decay: 0.0500 (0.0500)  time: 0.3105  data: 0.0002  max mem: 41808
Epoch: [0]  [ 480/1349]  eta: 0:04:50  lr: 0.000036  min_lr: 0.000001  loss: 1.5821 (1.6846)  loss_scale: 1024.0000 (444.9397)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0002  max mem: 41808
Epoch: [0]  [ 490/1349]  eta: 0:04:46  lr: 0.000036  min_lr: 0.000001  loss: 1.4747 (1.6787)  loss_scale: 1024.0000 (456.7332)  weight_decay: 0.0500 (0.0500)  time: 0.3081  data: 0.0002  max mem: 41808
Epoch: [0]  [ 500/1349]  eta: 0:04:42  lr: 0.000037  min_lr: 0.000001  loss: 1.4118 (1.6741)  loss_scale: 1024.0000 (468.0559)  weight_decay: 0.0500 (0.0500)  time: 0.3136  data: 0.0002  max mem: 41808
Epoch: [0]  [ 510/1349]  eta: 0:04:39  lr: 0.000038  min_lr: 0.000001  loss: 1.4421 (1.6705)  loss_scale: 1024.0000 (478.9354)  weight_decay: 0.0500 (0.0500)  time: 0.3135  data: 0.0002  max mem: 41808
[2025-05-23 15:01:43,209] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 15:01:43,209] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 1024 to 2048
[2025-05-23 15:01:43,209] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 15:01:43,209] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 1024 to 2048
Epoch: [0]  [ 520/1349]  eta: 0:04:35  lr: 0.000039  min_lr: 0.000001  loss: 1.4306 (1.6666)  loss_scale: 1024.0000 (507.0864)  weight_decay: 0.0500 (0.0500)  time: 0.3093  data: 0.0002  max mem: 41808
Epoch: [0]  [ 530/1349]  eta: 0:04:31  lr: 0.000039  min_lr: 0.000001  loss: 1.4253 (1.6617)  loss_scale: 2048.0000 (536.1055)  weight_decay: 0.0500 (0.0500)  time: 0.3057  data: 0.0001  max mem: 41808
Epoch: [0]  [ 540/1349]  eta: 0:04:28  lr: 0.000040  min_lr: 0.000001  loss: 1.4880 (1.6582)  loss_scale: 2048.0000 (564.0518)  weight_decay: 0.0500 (0.0500)  time: 0.3051  data: 0.0001  max mem: 41808
Epoch: [0]  [ 550/1349]  eta: 0:04:24  lr: 0.000041  min_lr: 0.000001  loss: 1.4626 (1.6540)  loss_scale: 2048.0000 (590.9837)  weight_decay: 0.0500 (0.0500)  time: 0.3147  data: 0.0002  max mem: 41808
Epoch: [0]  [ 560/1349]  eta: 0:04:21  lr: 0.000042  min_lr: 0.000001  loss: 1.3721 (1.6496)  loss_scale: 2048.0000 (616.9554)  weight_decay: 0.0500 (0.0500)  time: 0.3210  data: 0.0002  max mem: 41808
Epoch: [0]  [ 570/1349]  eta: 0:04:17  lr: 0.000042  min_lr: 0.000001  loss: 1.3721 (1.6454)  loss_scale: 2048.0000 (642.0175)  weight_decay: 0.0500 (0.0500)  time: 0.3143  data: 0.0002  max mem: 41808
Epoch: [0]  [ 580/1349]  eta: 0:04:13  lr: 0.000043  min_lr: 0.000001  loss: 1.3327 (1.6409)  loss_scale: 2048.0000 (666.2169)  weight_decay: 0.0500 (0.0500)  time: 0.3081  data: 0.0002  max mem: 41808
Epoch: [0]  [ 590/1349]  eta: 0:04:10  lr: 0.000044  min_lr: 0.000001  loss: 1.3671 (1.6371)  loss_scale: 2048.0000 (689.5973)  weight_decay: 0.0500 (0.0500)  time: 0.3109  data: 0.0002  max mem: 41808
Epoch: [0]  [ 600/1349]  eta: 0:04:07  lr: 0.000044  min_lr: 0.000001  loss: 1.3671 (1.6326)  loss_scale: 2048.0000 (712.1997)  weight_decay: 0.0500 (0.0500)  time: 0.3199  data: 0.0002  max mem: 41808
Epoch: [0]  [ 610/1349]  eta: 0:04:03  lr: 0.000045  min_lr: 0.000001  loss: 1.3596 (1.6293)  loss_scale: 2048.0000 (734.0622)  weight_decay: 0.0500 (0.0500)  time: 0.3198  data: 0.0002  max mem: 41808
Epoch: [0]  [ 620/1349]  eta: 0:03:59  lr: 0.000046  min_lr: 0.000001  loss: 1.4078 (1.6257)  loss_scale: 2048.0000 (755.2206)  weight_decay: 0.0500 (0.0500)  time: 0.3117  data: 0.0001  max mem: 41808
Epoch: [0]  [ 630/1349]  eta: 0:03:56  lr: 0.000047  min_lr: 0.000001  loss: 1.3963 (1.6216)  loss_scale: 2048.0000 (775.7084)  weight_decay: 0.0500 (0.0500)  time: 0.3057  data: 0.0001  max mem: 41808
[2025-05-23 15:02:23,220] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 15:02:23,220] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048 to 4096
[2025-05-23 15:02:23,222] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 15:02:23,222] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048 to 4096
Epoch: [0]  [ 640/1349]  eta: 0:03:53  lr: 0.000047  min_lr: 0.000001  loss: 1.3181 (1.6172)  loss_scale: 2048.0000 (798.7520)  weight_decay: 0.0500 (0.0500)  time: 0.3116  data: 0.0002  max mem: 41808
Epoch: [0]  [ 650/1349]  eta: 0:03:49  lr: 0.000048  min_lr: 0.000001  loss: 1.3552 (1.6145)  loss_scale: 4096.0000 (849.4009)  weight_decay: 0.0500 (0.0500)  time: 0.3221  data: 0.0003  max mem: 41808
Epoch: [0]  [ 660/1349]  eta: 0:03:46  lr: 0.000049  min_lr: 0.000001  loss: 1.3973 (1.6103)  loss_scale: 4096.0000 (898.5174)  weight_decay: 0.0500 (0.0500)  time: 0.3167  data: 0.0003  max mem: 41808
Epoch: [0]  [ 670/1349]  eta: 0:03:42  lr: 0.000050  min_lr: 0.000001  loss: 1.3338 (1.6071)  loss_scale: 4096.0000 (946.1699)  weight_decay: 0.0500 (0.0500)  time: 0.3102  data: 0.0005  max mem: 41808
Epoch: [0]  [ 680/1349]  eta: 0:03:39  lr: 0.000050  min_lr: 0.000001  loss: 1.3338 (1.6027)  loss_scale: 4096.0000 (992.4229)  weight_decay: 0.0500 (0.0500)  time: 0.3098  data: 0.0004  max mem: 41808
Epoch: [0]  [ 690/1349]  eta: 0:03:35  lr: 0.000051  min_lr: 0.000001  loss: 1.2367 (1.5981)  loss_scale: 4096.0000 (1037.3372)  weight_decay: 0.0500 (0.0500)  time: 0.3086  data: 0.0001  max mem: 41808
Epoch: [0]  [ 700/1349]  eta: 0:03:32  lr: 0.000052  min_lr: 0.000001  loss: 1.3259 (1.5953)  loss_scale: 4096.0000 (1080.9700)  weight_decay: 0.0500 (0.0500)  time: 0.3090  data: 0.0001  max mem: 41808
Epoch: [0]  [ 710/1349]  eta: 0:03:28  lr: 0.000053  min_lr: 0.000001  loss: 1.3491 (1.5914)  loss_scale: 4096.0000 (1123.3755)  weight_decay: 0.0500 (0.0500)  time: 0.3082  data: 0.0001  max mem: 41808
Epoch: [0]  [ 720/1349]  eta: 0:03:25  lr: 0.000053  min_lr: 0.000001  loss: 1.2821 (1.5890)  loss_scale: 4096.0000 (1164.6047)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
Epoch: [0]  [ 730/1349]  eta: 0:03:22  lr: 0.000054  min_lr: 0.000001  loss: 1.4223 (1.5860)  loss_scale: 4096.0000 (1204.7059)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0002  max mem: 41808
Epoch: [0]  [ 740/1349]  eta: 0:03:18  lr: 0.000055  min_lr: 0.000001  loss: 1.3863 (1.5834)  loss_scale: 4096.0000 (1243.7247)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [0]  [ 750/1349]  eta: 0:03:15  lr: 0.000056  min_lr: 0.000001  loss: 1.3322 (1.5792)  loss_scale: 4096.0000 (1281.7044)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [0]  [ 760/1349]  eta: 0:03:11  lr: 0.000056  min_lr: 0.000001  loss: 1.2204 (1.5745)  loss_scale: 4096.0000 (1318.6859)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
[2025-05-23 15:03:02,860] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 15:03:02,860] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 15:03:02,860] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096 to 8192
[2025-05-23 15:03:02,860] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096 to 8192
Epoch: [0]  [ 770/1349]  eta: 0:03:08  lr: 0.000057  min_lr: 0.000001  loss: 1.1981 (1.5709)  loss_scale: 4096.0000 (1370.6459)  weight_decay: 0.0500 (0.0500)  time: 0.3078  data: 0.0001  max mem: 41808
Epoch: [0]  [ 780/1349]  eta: 0:03:05  lr: 0.000058  min_lr: 0.000001  loss: 1.2568 (1.5679)  loss_scale: 8192.0000 (1457.9872)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [0]  [ 790/1349]  eta: 0:03:01  lr: 0.000059  min_lr: 0.000001  loss: 1.2758 (1.5642)  loss_scale: 8192.0000 (1543.1201)  weight_decay: 0.0500 (0.0500)  time: 0.3081  data: 0.0001  max mem: 41808
Epoch: [0]  [ 800/1349]  eta: 0:02:58  lr: 0.000059  min_lr: 0.000001  loss: 1.3184 (1.5621)  loss_scale: 8192.0000 (1626.1273)  weight_decay: 0.0500 (0.0500)  time: 0.3082  data: 0.0001  max mem: 41808
Epoch: [0]  [ 810/1349]  eta: 0:02:54  lr: 0.000060  min_lr: 0.000001  loss: 1.3133 (1.5587)  loss_scale: 8192.0000 (1707.0875)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
Epoch: [0]  [ 820/1349]  eta: 0:02:51  lr: 0.000061  min_lr: 0.000001  loss: 1.2695 (1.5549)  loss_scale: 8192.0000 (1786.0755)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [0]  [ 830/1349]  eta: 0:02:48  lr: 0.000062  min_lr: 0.000001  loss: 1.3195 (1.5521)  loss_scale: 8192.0000 (1863.1625)  weight_decay: 0.0500 (0.0500)  time: 0.3080  data: 0.0001  max mem: 41808
Epoch: [0]  [ 840/1349]  eta: 0:02:44  lr: 0.000062  min_lr: 0.000001  loss: 1.2964 (1.5486)  loss_scale: 8192.0000 (1938.4162)  weight_decay: 0.0500 (0.0500)  time: 0.3080  data: 0.0001  max mem: 41808
Epoch: [0]  [ 850/1349]  eta: 0:02:41  lr: 0.000063  min_lr: 0.000001  loss: 1.2883 (1.5456)  loss_scale: 8192.0000 (2011.9013)  weight_decay: 0.0500 (0.0500)  time: 0.3084  data: 0.0001  max mem: 41808
Epoch: [0]  [ 860/1349]  eta: 0:02:38  lr: 0.000064  min_lr: 0.000002  loss: 1.2696 (1.5425)  loss_scale: 8192.0000 (2083.6794)  weight_decay: 0.0500 (0.0500)  time: 0.3078  data: 0.0001  max mem: 41808
Epoch: [0]  [ 870/1349]  eta: 0:02:34  lr: 0.000065  min_lr: 0.000002  loss: 1.2828 (1.5397)  loss_scale: 8192.0000 (2153.8094)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [0]  [ 880/1349]  eta: 0:02:31  lr: 0.000065  min_lr: 0.000002  loss: 1.3176 (1.5370)  loss_scale: 8192.0000 (2222.3473)  weight_decay: 0.0500 (0.0500)  time: 0.3078  data: 0.0002  max mem: 41808
Epoch: [0]  [ 890/1349]  eta: 0:02:28  lr: 0.000066  min_lr: 0.000002  loss: 1.3176 (1.5345)  loss_scale: 8192.0000 (2289.3468)  weight_decay: 0.0500 (0.0500)  time: 0.3080  data: 0.0001  max mem: 41808
[2025-05-23 15:03:42,269] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 15:03:42,269] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 15:03:42,269] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192 to 16384
[2025-05-23 15:03:42,269] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192 to 16384
Epoch: [0]  [ 900/1349]  eta: 0:02:24  lr: 0.000067  min_lr: 0.000002  loss: 1.2973 (1.5316)  loss_scale: 8192.0000 (2400.3196)  weight_decay: 0.0500 (0.0500)  time: 0.3081  data: 0.0001  max mem: 41808
Epoch: [0]  [ 910/1349]  eta: 0:02:21  lr: 0.000067  min_lr: 0.000002  loss: 1.2147 (1.5284)  loss_scale: 16384.0000 (2553.8178)  weight_decay: 0.0500 (0.0500)  time: 0.3078  data: 0.0001  max mem: 41808
Epoch: [0]  [ 920/1349]  eta: 0:02:18  lr: 0.000068  min_lr: 0.000002  loss: 1.2137 (1.5252)  loss_scale: 16384.0000 (2703.9826)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [0]  [ 930/1349]  eta: 0:02:15  lr: 0.000069  min_lr: 0.000002  loss: 1.1622 (1.5211)  loss_scale: 16384.0000 (2850.9216)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
Epoch: [0]  [ 940/1349]  eta: 0:02:11  lr: 0.000070  min_lr: 0.000002  loss: 1.2587 (1.5188)  loss_scale: 16384.0000 (2994.7375)  weight_decay: 0.0500 (0.0500)  time: 0.3064  data: 0.0001  max mem: 41808
Epoch: [0]  [ 950/1349]  eta: 0:02:08  lr: 0.000070  min_lr: 0.000002  loss: 1.2587 (1.5158)  loss_scale: 16384.0000 (3135.5289)  weight_decay: 0.0500 (0.0500)  time: 0.3066  data: 0.0001  max mem: 41808
Epoch: [0]  [ 960/1349]  eta: 0:02:05  lr: 0.000071  min_lr: 0.000002  loss: 1.1032 (1.5113)  loss_scale: 16384.0000 (3273.3902)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
Epoch: [0]  [ 970/1349]  eta: 0:02:01  lr: 0.000072  min_lr: 0.000002  loss: 1.1032 (1.5077)  loss_scale: 16384.0000 (3408.4119)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [0]  [ 980/1349]  eta: 0:01:58  lr: 0.000073  min_lr: 0.000002  loss: 1.2449 (1.5050)  loss_scale: 16384.0000 (3540.6809)  weight_decay: 0.0500 (0.0500)  time: 0.3288  data: 0.0001  max mem: 41808
Epoch: [0]  [ 990/1349]  eta: 0:01:55  lr: 0.000073  min_lr: 0.000002  loss: 1.2212 (1.5017)  loss_scale: 16384.0000 (3670.2805)  weight_decay: 0.0500 (0.0500)  time: 0.3285  data: 0.0001  max mem: 41808
[2025-05-23 15:04:14,336] [INFO] [logging.py:96:log_dist] [Rank 0] step=1000, skipped=0, lr=[1.7596016276720579e-06, 1.7596016276720579e-06, 2.346135503562744e-06, 2.346135503562744e-06, 3.128180671416992e-06, 3.128180671416992e-06, 4.170907561889323e-06, 4.170907561889323e-06, 5.561210082519097e-06, 5.561210082519097e-06, 7.4149467766921295e-06, 7.4149467766921295e-06, 9.886595702256173e-06, 9.886595702256173e-06, 1.318212760300823e-05, 1.318212760300823e-05, 1.7576170137344306e-05, 1.7576170137344306e-05, 2.3434893516459074e-05, 2.3434893516459074e-05, 3.12465246886121e-05, 3.12465246886121e-05, 4.1662032918149464e-05, 4.1662032918149464e-05, 5.5549377224199285e-05, 5.5549377224199285e-05, 7.406583629893238e-05, 7.406583629893238e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-05-23 15:04:14,336] [INFO] [timer.py:260:stop] epoch=0/micro_step=1000/global_step=1000, RunningAvgSamplesPerSec=210.505656121198, CurrSamplesPerSec=213.54601531536835, MemAllocated=1.37GB, MaxMemAllocated=40.83GB
Epoch: [0]  [1000/1349]  eta: 0:01:52  lr: 0.000074  min_lr: 0.000002  loss: 1.2264 (1.4994)  loss_scale: 16384.0000 (3797.2907)  weight_decay: 0.0500 (0.0500)  time: 0.3090  data: 0.0001  max mem: 41808
Epoch: [0]  [1010/1349]  eta: 0:01:49  lr: 0.000075  min_lr: 0.000002  loss: 1.2666 (1.4975)  loss_scale: 16384.0000 (3921.7883)  weight_decay: 0.0500 (0.0500)  time: 0.3099  data: 0.0002  max mem: 41808
Epoch: [0]  [1020/1349]  eta: 0:01:45  lr: 0.000076  min_lr: 0.000002  loss: 1.2019 (1.4943)  loss_scale: 16384.0000 (4043.8472)  weight_decay: 0.0500 (0.0500)  time: 0.3081  data: 0.0002  max mem: 41808
[2025-05-23 15:04:22,072] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 15:04:22,072] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384 to 32768
[2025-05-23 15:04:22,072] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 15:04:22,072] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384 to 32768
Epoch: [0]  [1030/1349]  eta: 0:01:42  lr: 0.000076  min_lr: 0.000002  loss: 1.2019 (1.4920)  loss_scale: 16384.0000 (4274.7779)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0002  max mem: 41808
[2025-05-23 15:04:26,067] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 1037
[2025-05-23 15:04:26,068] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768 to 16384.0
[2025-05-23 15:04:26,068] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 1037
[2025-05-23 15:04:26,068] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768 to 16384.0
[2025-05-23 15:04:26,068] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768, reducing to 16384.0
Epoch: [0]  [1040/1349]  eta: 0:01:39  lr: 0.000077  min_lr: 0.000002  loss: 1.2203 (1.4894)  loss_scale: 32768.0000 (4485.5331)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [0]  [1050/1349]  eta: 0:01:36  lr: 0.000078  min_lr: 0.000002  loss: 1.1729 (1.4866)  loss_scale: 16384.0000 (4598.7441)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [0]  [1060/1349]  eta: 0:01:32  lr: 0.000079  min_lr: 0.000002  loss: 1.2165 (1.4842)  loss_scale: 16384.0000 (4709.8209)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [0]  [1070/1349]  eta: 0:01:29  lr: 0.000079  min_lr: 0.000002  loss: 1.2125 (1.4817)  loss_scale: 16384.0000 (4818.8235)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [0]  [1080/1349]  eta: 0:01:26  lr: 0.000080  min_lr: 0.000002  loss: 1.1664 (1.4785)  loss_scale: 16384.0000 (4925.8094)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [0]  [1090/1349]  eta: 0:01:23  lr: 0.000081  min_lr: 0.000002  loss: 1.1541 (1.4757)  loss_scale: 16384.0000 (5030.8341)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
Epoch: [0]  [1100/1349]  eta: 0:01:19  lr: 0.000082  min_lr: 0.000002  loss: 1.2114 (1.4739)  loss_scale: 16384.0000 (5133.9510)  weight_decay: 0.0500 (0.0500)  time: 0.3083  data: 0.0001  max mem: 41808
Epoch: [0]  [1110/1349]  eta: 0:01:16  lr: 0.000082  min_lr: 0.000002  loss: 1.1513 (1.4710)  loss_scale: 16384.0000 (5235.2115)  weight_decay: 0.0500 (0.0500)  time: 0.3084  data: 0.0001  max mem: 41808
Epoch: [0]  [1120/1349]  eta: 0:01:13  lr: 0.000083  min_lr: 0.000002  loss: 1.2231 (1.4693)  loss_scale: 16384.0000 (5334.6655)  weight_decay: 0.0500 (0.0500)  time: 0.3084  data: 0.0001  max mem: 41808
Epoch: [0]  [1130/1349]  eta: 0:01:10  lr: 0.000084  min_lr: 0.000002  loss: 1.1880 (1.4654)  loss_scale: 16384.0000 (5432.3607)  weight_decay: 0.0500 (0.0500)  time: 0.3095  data: 0.0001  max mem: 41808
Epoch: [0]  [1140/1349]  eta: 0:01:06  lr: 0.000085  min_lr: 0.000002  loss: 1.0237 (1.4619)  loss_scale: 16384.0000 (5528.3436)  weight_decay: 0.0500 (0.0500)  time: 0.3089  data: 0.0001  max mem: 41808
Epoch: [0]  [1150/1349]  eta: 0:01:03  lr: 0.000085  min_lr: 0.000002  loss: 1.0878 (1.4588)  loss_scale: 16384.0000 (5622.6586)  weight_decay: 0.0500 (0.0500)  time: 0.3081  data: 0.0001  max mem: 41808
Epoch: [0]  [1160/1349]  eta: 0:01:00  lr: 0.000086  min_lr: 0.000002  loss: 1.1373 (1.4563)  loss_scale: 16384.0000 (5715.3488)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0001  max mem: 41808
[2025-05-23 15:05:05,789] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 15:05:05,789] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-05-23 15:05:05,789] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 15:05:05,789] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [0]  [1170/1349]  eta: 0:00:57  lr: 0.000087  min_lr: 0.000002  loss: 1.1917 (1.4536)  loss_scale: 16384.0000 (5876.4133)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
[2025-05-23 15:05:07,934] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 1173
[2025-05-23 15:05:07,934] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-05-23 15:05:07,934] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2025-05-23 15:05:07,934] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 1173
[2025-05-23 15:05:07,934] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [0]  [1180/1349]  eta: 0:00:54  lr: 0.000087  min_lr: 0.000002  loss: 1.1917 (1.4517)  loss_scale: 16384.0000 (5993.1312)  weight_decay: 0.0500 (0.0500)  time: 0.3064  data: 0.0001  max mem: 41808
Epoch: [0]  [1190/1349]  eta: 0:00:50  lr: 0.000088  min_lr: 0.000002  loss: 1.1115 (1.4485)  loss_scale: 16384.0000 (6080.3762)  weight_decay: 0.0500 (0.0500)  time: 0.3061  data: 0.0001  max mem: 41808
Epoch: [0]  [1200/1349]  eta: 0:00:47  lr: 0.000089  min_lr: 0.000002  loss: 1.0333 (1.4455)  loss_scale: 16384.0000 (6166.1682)  weight_decay: 0.0500 (0.0500)  time: 0.3066  data: 0.0001  max mem: 41808
Epoch: [0]  [1210/1349]  eta: 0:00:44  lr: 0.000090  min_lr: 0.000002  loss: 1.1195 (1.4425)  loss_scale: 16384.0000 (6250.5434)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0001  max mem: 41808
Epoch: [0]  [1220/1349]  eta: 0:00:41  lr: 0.000090  min_lr: 0.000002  loss: 1.1292 (1.4400)  loss_scale: 16384.0000 (6333.5364)  weight_decay: 0.0500 (0.0500)  time: 0.3093  data: 0.0001  max mem: 41808
Epoch: [0]  [1230/1349]  eta: 0:00:37  lr: 0.000091  min_lr: 0.000002  loss: 1.1840 (1.4380)  loss_scale: 16384.0000 (6415.1812)  weight_decay: 0.0500 (0.0500)  time: 0.3084  data: 0.0002  max mem: 41808
Epoch: [0]  [1240/1349]  eta: 0:00:34  lr: 0.000092  min_lr: 0.000002  loss: 1.2447 (1.4360)  loss_scale: 16384.0000 (6495.5101)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0002  max mem: 41808
Epoch: [0]  [1250/1349]  eta: 0:00:31  lr: 0.000093  min_lr: 0.000002  loss: 1.2384 (1.4335)  loss_scale: 16384.0000 (6574.5548)  weight_decay: 0.0500 (0.0500)  time: 0.3079  data: 0.0002  max mem: 41808
Epoch: [0]  [1260/1349]  eta: 0:00:28  lr: 0.000093  min_lr: 0.000002  loss: 1.2261 (1.4317)  loss_scale: 16384.0000 (6652.3458)  weight_decay: 0.0500 (0.0500)  time: 0.3089  data: 0.0002  max mem: 41808
Epoch: [0]  [1270/1349]  eta: 0:00:25  lr: 0.000094  min_lr: 0.000002  loss: 1.1675 (1.4291)  loss_scale: 16384.0000 (6728.9127)  weight_decay: 0.0500 (0.0500)  time: 0.3097  data: 0.0002  max mem: 41808
Epoch: [0]  [1280/1349]  eta: 0:00:21  lr: 0.000095  min_lr: 0.000002  loss: 1.1595 (1.4276)  loss_scale: 16384.0000 (6804.2842)  weight_decay: 0.0500 (0.0500)  time: 0.3098  data: 0.0002  max mem: 41808
Epoch: [0]  [1290/1349]  eta: 0:00:18  lr: 0.000096  min_lr: 0.000002  loss: 1.1595 (1.4249)  loss_scale: 16384.0000 (6878.4880)  weight_decay: 0.0500 (0.0500)  time: 0.3090  data: 0.0001  max mem: 41808
Epoch: [0]  [1300/1349]  eta: 0:00:15  lr: 0.000096  min_lr: 0.000002  loss: 1.0495 (1.4220)  loss_scale: 16384.0000 (6951.5511)  weight_decay: 0.0500 (0.0500)  time: 0.3085  data: 0.0001  max mem: 41808
[2025-05-23 15:05:47,712] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 15:05:47,712] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-05-23 15:05:47,712] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 15:05:47,712] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [0]  [1310/1349]  eta: 0:00:12  lr: 0.000097  min_lr: 0.000002  loss: 1.0495 (1.4194)  loss_scale: 16384.0000 (7135.9756)  weight_decay: 0.0500 (0.0500)  time: 0.3092  data: 0.0002  max mem: 41808
Epoch: [0]  [1320/1349]  eta: 0:00:09  lr: 0.000098  min_lr: 0.000002  loss: 1.1637 (1.4178)  loss_scale: 32768.0000 (7330.0106)  weight_decay: 0.0500 (0.0500)  time: 0.3079  data: 0.0002  max mem: 41808
Epoch: [0]  [1330/1349]  eta: 0:00:06  lr: 0.000099  min_lr: 0.000002  loss: 1.2138 (1.4161)  loss_scale: 32768.0000 (7521.1300)  weight_decay: 0.0500 (0.0500)  time: 0.3042  data: 0.0001  max mem: 41808
Epoch: [0]  [1340/1349]  eta: 0:00:02  lr: 0.000099  min_lr: 0.000002  loss: 1.0442 (1.4129)  loss_scale: 32768.0000 (7709.3990)  weight_decay: 0.0500 (0.0500)  time: 0.3036  data: 0.0001  max mem: 41808
Epoch: [0]  [1348/1349]  eta: 0:00:00  lr: 0.000100  min_lr: 0.000002  loss: 0.9965 (1.4108)  loss_scale: 32768.0000 (7858.0044)  weight_decay: 0.0500 (0.0500)  time: 0.3033  data: 0.0001  max mem: 41808
Epoch: [0] Total time: 0:07:09 (0.3184 s / it)
Averaged stats: lr: 0.000100  min_lr: 0.000002  loss: 0.9965 (1.4102)  loss_scale: 32768.0000 (7858.0044)  weight_decay: 0.0500 (0.0500)  total_time: 429.5372 (429.5367)
Val:  [  0/346]  eta: 1:22:52  loss: 1.7525 (1.7525)  acc1: 19.5312 (19.5312)  acc5: 87.5000 (87.5000)  time: 14.3712  data: 8.4794  max mem: 41808
Val:  [ 10/346]  eta: 0:13:39  loss: 0.1854 (0.4079)  acc1: 100.0000 (88.4943)  acc5: 100.0000 (98.8636)  time: 2.4392  data: 1.2952  max mem: 41808
Val:  [ 20/346]  eta: 0:09:34  loss: 0.1814 (0.4092)  acc1: 100.0000 (89.2113)  acc5: 100.0000 (99.4048)  time: 1.1311  data: 0.4430  max mem: 41808
Val:  [ 30/346]  eta: 0:07:30  loss: 0.2394 (0.3810)  acc1: 97.6562 (90.7258)  acc5: 100.0000 (99.5968)  time: 0.8688  data: 0.1547  max mem: 41808
Val:  [ 40/346]  eta: 0:06:20  loss: 0.2373 (0.3891)  acc1: 97.6562 (90.3201)  acc5: 100.0000 (99.4665)  time: 0.6994  data: 0.0003  max mem: 41808
Val:  [ 50/346]  eta: 0:05:41  loss: 0.2371 (0.3730)  acc1: 95.3125 (91.3756)  acc5: 100.0000 (99.5711)  time: 0.7279  data: 0.0572  max mem: 41808
Val:  [ 60/346]  eta: 0:05:14  loss: 0.2536 (0.3763)  acc1: 99.2188 (91.1885)  acc5: 100.0000 (99.6414)  time: 0.8067  data: 0.1241  max mem: 41808
Val:  [ 70/346]  eta: 0:04:54  loss: 0.2870 (0.3923)  acc1: 94.5312 (90.3939)  acc5: 100.0000 (99.6919)  time: 0.8443  data: 0.1375  max mem: 41808
Val:  [ 80/346]  eta: 0:04:36  loss: 0.3892 (0.3938)  acc1: 92.9688 (90.3453)  acc5: 100.0000 (99.7203)  time: 0.8601  data: 0.1373  max mem: 41808
Val:  [ 90/346]  eta: 0:04:21  loss: 0.3558 (0.3953)  acc1: 91.4062 (90.2473)  acc5: 100.0000 (99.7510)  time: 0.8718  data: 0.1372  max mem: 41808
Val:  [100/346]  eta: 0:04:07  loss: 0.2980 (0.3798)  acc1: 96.0938 (91.0350)  acc5: 100.0000 (99.7757)  time: 0.8718  data: 0.1429  max mem: 41808
Val:  [110/346]  eta: 0:03:54  loss: 0.2454 (0.3963)  acc1: 98.4375 (90.3294)  acc5: 100.0000 (99.7959)  time: 0.8601  data: 0.1445  max mem: 41808
Val:  [120/346]  eta: 0:03:41  loss: 0.2498 (0.3931)  acc1: 94.5312 (90.3345)  acc5: 100.0000 (99.8128)  time: 0.8356  data: 0.1420  max mem: 41808
Val:  [130/346]  eta: 0:03:29  loss: 0.2035 (0.3890)  acc1: 98.4375 (90.5713)  acc5: 100.0000 (99.8271)  time: 0.8295  data: 0.1448  max mem: 41808
Val:  [140/346]  eta: 0:03:22  loss: 0.3660 (0.3981)  acc1: 94.5312 (90.1540)  acc5: 100.0000 (99.8393)  time: 0.9935  data: 0.3105  max mem: 41808
Val:  [150/346]  eta: 0:03:08  loss: 0.4065 (0.4003)  acc1: 87.5000 (90.1594)  acc5: 100.0000 (99.8500)  time: 0.9354  data: 0.2358  max mem: 41808
Val:  [160/346]  eta: 0:02:56  loss: 0.2711 (0.3969)  acc1: 95.3125 (90.3193)  acc5: 100.0000 (99.8593)  time: 0.7288  data: 0.0209  max mem: 41808
Val:  [170/346]  eta: 0:02:46  loss: 0.2702 (0.4038)  acc1: 96.8750 (89.8118)  acc5: 100.0000 (99.8675)  time: 0.7969  data: 0.1011  max mem: 41808
Val:  [180/346]  eta: 0:02:35  loss: 0.3436 (0.4124)  acc1: 92.1875 (89.3258)  acc5: 100.0000 (99.8748)  time: 0.8557  data: 0.1672  max mem: 41808
Val:  [190/346]  eta: 0:02:25  loss: 0.3436 (0.4112)  acc1: 92.1875 (89.5083)  acc5: 100.0000 (99.8814)  time: 0.8511  data: 0.1624  max mem: 41808
Val:  [200/346]  eta: 0:02:15  loss: 0.4059 (0.4219)  acc1: 89.8438 (89.0508)  acc5: 100.0000 (99.8873)  time: 0.8478  data: 0.1617  max mem: 41808
Val:  [210/346]  eta: 0:02:06  loss: 0.2688 (0.4158)  acc1: 92.9688 (89.3180)  acc5: 100.0000 (99.8926)  time: 0.8649  data: 0.1710  max mem: 41808
Val:  [220/346]  eta: 0:01:56  loss: 0.2384 (0.4154)  acc1: 100.0000 (89.3807)  acc5: 100.0000 (99.8621)  time: 0.8630  data: 0.1598  max mem: 41808
Val:  [230/346]  eta: 0:01:46  loss: 0.2292 (0.4095)  acc1: 99.2188 (89.6746)  acc5: 100.0000 (99.8681)  time: 0.8473  data: 0.1526  max mem: 41808
Val:  [240/346]  eta: 0:01:37  loss: 0.3522 (0.4163)  acc1: 96.0938 (89.3056)  acc5: 100.0000 (99.8736)  time: 0.8685  data: 0.1541  max mem: 41808
Val:  [250/346]  eta: 0:01:28  loss: 0.3401 (0.4129)  acc1: 91.4062 (89.4547)  acc5: 100.0000 (99.8786)  time: 0.8684  data: 0.1577  max mem: 41808
Val:  [260/346]  eta: 0:01:18  loss: 0.2280 (0.4111)  acc1: 94.5312 (89.5175)  acc5: 100.0000 (99.8833)  time: 0.8639  data: 0.1590  max mem: 41808
Val:  [270/346]  eta: 0:01:09  loss: 0.2068 (0.4070)  acc1: 97.6562 (89.6765)  acc5: 100.0000 (99.8876)  time: 0.8745  data: 0.1530  max mem: 41808
Val:  [280/346]  eta: 0:01:00  loss: 0.1826 (0.4023)  acc1: 100.0000 (89.8382)  acc5: 100.0000 (99.8916)  time: 0.8788  data: 0.1631  max mem: 41808
Val:  [290/346]  eta: 0:00:51  loss: 0.1716 (0.3950)  acc1: 100.0000 (90.1632)  acc5: 100.0000 (99.8953)  time: 0.8830  data: 0.1721  max mem: 41808
Val:  [300/346]  eta: 0:00:41  loss: 0.2107 (0.3967)  acc1: 98.4375 (90.1396)  acc5: 100.0000 (99.8884)  time: 0.8765  data: 0.1647  max mem: 41808
Val:  [310/346]  eta: 0:00:32  loss: 0.2295 (0.3965)  acc1: 96.8750 (90.1025)  acc5: 100.0000 (99.8920)  time: 0.8522  data: 0.1649  max mem: 41808
Val:  [320/346]  eta: 0:00:23  loss: 0.2181 (0.4010)  acc1: 99.2188 (89.8340)  acc5: 100.0000 (99.8953)  time: 0.8420  data: 0.1613  max mem: 41808
Val:  [330/346]  eta: 0:00:14  loss: 0.5524 (0.4099)  acc1: 78.9062 (89.4661)  acc5: 100.0000 (99.8985)  time: 0.8655  data: 0.1520  max mem: 41808
Val:  [340/346]  eta: 0:00:05  loss: 0.5325 (0.4150)  acc1: 78.1250 (89.2504)  acc5: 100.0000 (99.9015)  time: 0.8529  data: 0.1587  max mem: 41808
Val:  [345/346]  eta: 0:00:00  loss: 0.3789 (0.4130)  acc1: 92.9688 (89.3643)  acc5: 100.0000 (99.9028)  time: 0.8788  data: 0.1751  max mem: 41808
Val: Total time: 0:05:13 (0.9054 s / it)
* Acc@1 89.434 Acc@5 99.902 loss 0.412
Accuracy of the network on the 88494 val videos: 89.4%
[2025-05-23 15:11:15,292] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-05-23 15:11:15,304] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /project/mmendoscope/Natural_Comparison/Cholec75/unified_base_st_settingE_videomae-st_Cholec80_0.0005_0.75_online_key_frame_frame16_Fixed_Stride_4/checkpoint-best/mp_rank_00_model_states.pt
[2025-05-23 15:11:15,304] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2025-05-23 15:11:15,307] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /project/mmendoscope/Natural_Comparison/Cholec75/unified_base_st_settingE_videomae-st_Cholec80_0.0005_0.75_online_key_frame_frame16_Fixed_Stride_4/checkpoint-best/mp_rank_00_model_states.pt...
[2025-05-23 15:11:18,003] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /project/mmendoscope/Natural_Comparison/Cholec75/unified_base_st_settingE_videomae-st_Cholec80_0.0005_0.75_online_key_frame_frame16_Fixed_Stride_4/checkpoint-best/mp_rank_00_model_states.pt.
[2025-05-23 15:11:18,003] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 89.43%   Max Epoch: 0
Epoch: [1]  [   0/1349]  eta: 1:58:53  lr: 0.000100  min_lr: 0.000002  loss: 1.3012 (1.3012)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 5.2879  data: 4.9325  max mem: 41808
Epoch: [1]  [  10/1349]  eta: 0:17:06  lr: 0.000101  min_lr: 0.000002  loss: 1.2540 (1.1389)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7666  data: 0.4486  max mem: 41808
Epoch: [1]  [  20/1349]  eta: 0:12:10  lr: 0.000101  min_lr: 0.000002  loss: 1.2338 (1.1309)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3127  data: 0.0001  max mem: 41808
Epoch: [1]  [  30/1349]  eta: 0:10:21  lr: 0.000102  min_lr: 0.000002  loss: 1.1634 (1.1245)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3092  data: 0.0001  max mem: 41808
[2025-05-23 15:11:35,091] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 1387
[2025-05-23 15:11:35,091] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-05-23 15:11:35,091] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 1387
[2025-05-23 15:11:35,092] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-05-23 15:11:35,092] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [1]  [  40/1349]  eta: 0:09:24  lr: 0.000103  min_lr: 0.000002  loss: 1.0938 (1.1157)  loss_scale: 32768.0000 (31569.1707)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
Epoch: [1]  [  50/1349]  eta: 0:08:48  lr: 0.000104  min_lr: 0.000002  loss: 1.0938 (1.1113)  loss_scale: 16384.0000 (28591.6863)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
Epoch: [1]  [  60/1349]  eta: 0:08:23  lr: 0.000104  min_lr: 0.000002  loss: 1.1830 (1.1276)  loss_scale: 16384.0000 (26590.4262)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
Epoch: [1]  [  70/1349]  eta: 0:08:04  lr: 0.000105  min_lr: 0.000002  loss: 1.1318 (1.1217)  loss_scale: 16384.0000 (25152.9014)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0001  max mem: 41808
Epoch: [1]  [  80/1349]  eta: 0:07:49  lr: 0.000106  min_lr: 0.000003  loss: 1.1080 (1.1139)  loss_scale: 16384.0000 (24070.3210)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
Epoch: [1]  [  90/1349]  eta: 0:07:37  lr: 0.000107  min_lr: 0.000003  loss: 1.0671 (1.1097)  loss_scale: 16384.0000 (23225.6703)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [1]  [ 100/1349]  eta: 0:07:26  lr: 0.000107  min_lr: 0.000003  loss: 1.0671 (1.1059)  loss_scale: 16384.0000 (22548.2772)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
Epoch: [1]  [ 110/1349]  eta: 0:07:17  lr: 0.000108  min_lr: 0.000003  loss: 1.1634 (1.1173)  loss_scale: 16384.0000 (21992.9369)  weight_decay: 0.0500 (0.0500)  time: 0.3066  data: 0.0002  max mem: 41808
Epoch: [1]  [ 120/1349]  eta: 0:07:09  lr: 0.000109  min_lr: 0.000003  loss: 1.1634 (1.1129)  loss_scale: 16384.0000 (21529.3884)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0002  max mem: 41808
Epoch: [1]  [ 130/1349]  eta: 0:07:01  lr: 0.000110  min_lr: 0.000003  loss: 1.1845 (1.1196)  loss_scale: 16384.0000 (21136.6107)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0002  max mem: 41808
Epoch: [1]  [ 140/1349]  eta: 0:06:54  lr: 0.000110  min_lr: 0.000003  loss: 1.1850 (1.1216)  loss_scale: 16384.0000 (20799.5461)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
Epoch: [1]  [ 150/1349]  eta: 0:06:48  lr: 0.000111  min_lr: 0.000003  loss: 1.1455 (1.1219)  loss_scale: 16384.0000 (20507.1258)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [1]  [ 160/1349]  eta: 0:06:42  lr: 0.000112  min_lr: 0.000003  loss: 1.1092 (1.1227)  loss_scale: 16384.0000 (20251.0311)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
[2025-05-23 15:12:14,692] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 15:12:14,692] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 15:12:14,692] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-05-23 15:12:14,692] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [1]  [ 170/1349]  eta: 0:06:37  lr: 0.000113  min_lr: 0.000003  loss: 1.1092 (1.1229)  loss_scale: 16384.0000 (20408.1404)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0002  max mem: 41808
Epoch: [1]  [ 180/1349]  eta: 0:06:31  lr: 0.000113  min_lr: 0.000003  loss: 1.1001 (1.1205)  loss_scale: 32768.0000 (21091.0055)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0002  max mem: 41808
Epoch: [1]  [ 190/1349]  eta: 0:06:26  lr: 0.000114  min_lr: 0.000003  loss: 1.1001 (1.1193)  loss_scale: 32768.0000 (21702.3665)  weight_decay: 0.0500 (0.0500)  time: 0.3064  data: 0.0001  max mem: 41808
Epoch: [1]  [ 200/1349]  eta: 0:06:21  lr: 0.000115  min_lr: 0.000003  loss: 1.0600 (1.1175)  loss_scale: 32768.0000 (22252.8955)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
Epoch: [1]  [ 210/1349]  eta: 0:06:17  lr: 0.000116  min_lr: 0.000003  loss: 1.1105 (1.1175)  loss_scale: 32768.0000 (22751.2417)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
Epoch: [1]  [ 220/1349]  eta: 0:06:12  lr: 0.000116  min_lr: 0.000003  loss: 1.1533 (1.1185)  loss_scale: 32768.0000 (23204.4887)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0001  max mem: 41808
[2025-05-23 15:12:32,176] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 1573
[2025-05-23 15:12:32,176] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 1573
[2025-05-23 15:12:32,176] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-05-23 15:12:32,176] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-05-23 15:12:32,176] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [1]  [ 230/1349]  eta: 0:06:08  lr: 0.000117  min_lr: 0.000003  loss: 1.1533 (1.1173)  loss_scale: 32768.0000 (23122.0087)  weight_decay: 0.0500 (0.0500)  time: 0.3064  data: 0.0001  max mem: 41808
Epoch: [1]  [ 240/1349]  eta: 0:06:03  lr: 0.000118  min_lr: 0.000003  loss: 1.0921 (1.1163)  loss_scale: 16384.0000 (22842.4232)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
Epoch: [1]  [ 250/1349]  eta: 0:05:59  lr: 0.000119  min_lr: 0.000003  loss: 1.1668 (1.1217)  loss_scale: 16384.0000 (22585.1155)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0001  max mem: 41808
Epoch: [1]  [ 260/1349]  eta: 0:05:55  lr: 0.000119  min_lr: 0.000003  loss: 1.1177 (1.1204)  loss_scale: 16384.0000 (22347.5249)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [1]  [ 270/1349]  eta: 0:05:51  lr: 0.000120  min_lr: 0.000003  loss: 1.1061 (1.1202)  loss_scale: 16384.0000 (22127.4686)  weight_decay: 0.0500 (0.0500)  time: 0.3085  data: 0.0001  max mem: 41808
Epoch: [1]  [ 280/1349]  eta: 0:05:47  lr: 0.000121  min_lr: 0.000003  loss: 1.1462 (1.1217)  loss_scale: 16384.0000 (21923.0747)  weight_decay: 0.0500 (0.0500)  time: 0.3084  data: 0.0002  max mem: 41808
Epoch: [1]  [ 290/1349]  eta: 0:05:43  lr: 0.000122  min_lr: 0.000003  loss: 1.0756 (1.1203)  loss_scale: 16384.0000 (21732.7285)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [1]  [ 300/1349]  eta: 0:05:39  lr: 0.000122  min_lr: 0.000003  loss: 1.0075 (1.1163)  loss_scale: 16384.0000 (21555.0299)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0001  max mem: 41808
Epoch: [1]  [ 310/1349]  eta: 0:05:36  lr: 0.000123  min_lr: 0.000003  loss: 1.0676 (1.1147)  loss_scale: 16384.0000 (21388.7588)  weight_decay: 0.0500 (0.0500)  time: 0.3084  data: 0.0001  max mem: 41808
Epoch: [1]  [ 320/1349]  eta: 0:05:32  lr: 0.000124  min_lr: 0.000003  loss: 1.0972 (1.1134)  loss_scale: 16384.0000 (21232.8474)  weight_decay: 0.0500 (0.0500)  time: 0.3105  data: 0.0001  max mem: 41808
Epoch: [1]  [ 330/1349]  eta: 0:05:28  lr: 0.000124  min_lr: 0.000003  loss: 1.1596 (1.1152)  loss_scale: 16384.0000 (21086.3565)  weight_decay: 0.0500 (0.0500)  time: 0.3099  data: 0.0001  max mem: 41808
Epoch: [1]  [ 340/1349]  eta: 0:05:25  lr: 0.000125  min_lr: 0.000003  loss: 1.2159 (1.1139)  loss_scale: 16384.0000 (20948.4575)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [1]  [ 350/1349]  eta: 0:05:21  lr: 0.000126  min_lr: 0.000003  loss: 1.0867 (1.1126)  loss_scale: 16384.0000 (20818.4160)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
[2025-05-23 15:13:11,917] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 15:13:11,918] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-05-23 15:13:11,918] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 15:13:11,918] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-05-23 15:13:12,531] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 1704
[2025-05-23 15:13:12,532] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-05-23 15:13:12,532] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2025-05-23 15:13:12,532] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 1704
[2025-05-23 15:13:12,532] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [1]  [ 360/1349]  eta: 0:05:17  lr: 0.000127  min_lr: 0.000003  loss: 1.1147 (1.1149)  loss_scale: 16384.0000 (20786.3490)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
Epoch: [1]  [ 370/1349]  eta: 0:05:14  lr: 0.000127  min_lr: 0.000003  loss: 1.1560 (1.1166)  loss_scale: 16384.0000 (20667.6873)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0002  max mem: 41808
Epoch: [1]  [ 380/1349]  eta: 0:05:10  lr: 0.000128  min_lr: 0.000003  loss: 1.2530 (1.1191)  loss_scale: 16384.0000 (20555.2546)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0002  max mem: 41808
Epoch: [1]  [ 390/1349]  eta: 0:05:07  lr: 0.000129  min_lr: 0.000003  loss: 1.2078 (1.1181)  loss_scale: 16384.0000 (20448.5729)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [1]  [ 400/1349]  eta: 0:05:03  lr: 0.000130  min_lr: 0.000003  loss: 1.1137 (1.1173)  loss_scale: 16384.0000 (20347.2120)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0001  max mem: 41808
Epoch: [1]  [ 410/1349]  eta: 0:05:00  lr: 0.000130  min_lr: 0.000003  loss: 1.0225 (1.1151)  loss_scale: 16384.0000 (20250.7835)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
Epoch: [1]  [ 420/1349]  eta: 0:04:56  lr: 0.000131  min_lr: 0.000003  loss: 0.9300 (1.1105)  loss_scale: 16384.0000 (20158.9359)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [1]  [ 430/1349]  eta: 0:04:53  lr: 0.000132  min_lr: 0.000003  loss: 0.9098 (1.1100)  loss_scale: 16384.0000 (20071.3503)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [1]  [ 440/1349]  eta: 0:04:49  lr: 0.000133  min_lr: 0.000003  loss: 1.0763 (1.1094)  loss_scale: 16384.0000 (19987.7370)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0001  max mem: 41808
Epoch: [1]  [ 450/1349]  eta: 0:04:46  lr: 0.000133  min_lr: 0.000003  loss: 1.1215 (1.1068)  loss_scale: 16384.0000 (19907.8315)  weight_decay: 0.0500 (0.0500)  time: 0.3089  data: 0.0002  max mem: 41808
Epoch: [1]  [ 460/1349]  eta: 0:04:43  lr: 0.000134  min_lr: 0.000003  loss: 1.1473 (1.1075)  loss_scale: 16384.0000 (19831.3926)  weight_decay: 0.0500 (0.0500)  time: 0.3089  data: 0.0002  max mem: 41808
Epoch: [1]  [ 470/1349]  eta: 0:04:39  lr: 0.000135  min_lr: 0.000003  loss: 1.1473 (1.1045)  loss_scale: 16384.0000 (19758.1996)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0002  max mem: 41808
Epoch: [1]  [ 480/1349]  eta: 0:04:36  lr: 0.000136  min_lr: 0.000003  loss: 1.0574 (1.1054)  loss_scale: 16384.0000 (19688.0499)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
[2025-05-23 15:13:52,232] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 15:13:52,232] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-05-23 15:13:52,232] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 15:13:52,232] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [1]  [ 490/1349]  eta: 0:04:32  lr: 0.000136  min_lr: 0.000003  loss: 1.1818 (1.1042)  loss_scale: 16384.0000 (19854.3381)  weight_decay: 0.0500 (0.0500)  time: 0.3079  data: 0.0001  max mem: 41808
[2025-05-23 15:13:55,302] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 1843
[2025-05-23 15:13:55,302] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-05-23 15:13:55,302] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 1843
[2025-05-23 15:13:55,302] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-05-23 15:13:55,303] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [1]  [ 500/1349]  eta: 0:04:29  lr: 0.000137  min_lr: 0.000003  loss: 1.0805 (1.1031)  loss_scale: 16384.0000 (19883.1776)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
Epoch: [1]  [ 510/1349]  eta: 0:04:26  lr: 0.000138  min_lr: 0.000003  loss: 1.0624 (1.1016)  loss_scale: 16384.0000 (19814.7006)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
Epoch: [1]  [ 520/1349]  eta: 0:04:22  lr: 0.000139  min_lr: 0.000003  loss: 1.0468 (1.1014)  loss_scale: 16384.0000 (19748.8522)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0001  max mem: 41808
Epoch: [1]  [ 530/1349]  eta: 0:04:19  lr: 0.000139  min_lr: 0.000003  loss: 1.0278 (1.0994)  loss_scale: 16384.0000 (19685.4840)  weight_decay: 0.0500 (0.0500)  time: 0.3078  data: 0.0001  max mem: 41808
Epoch: [1]  [ 540/1349]  eta: 0:04:16  lr: 0.000140  min_lr: 0.000003  loss: 1.1033 (1.1007)  loss_scale: 16384.0000 (19624.4584)  weight_decay: 0.0500 (0.0500)  time: 0.3081  data: 0.0002  max mem: 41808
Epoch: [1]  [ 550/1349]  eta: 0:04:12  lr: 0.000141  min_lr: 0.000003  loss: 1.1026 (1.0995)  loss_scale: 16384.0000 (19565.6479)  weight_decay: 0.0500 (0.0500)  time: 0.3078  data: 0.0001  max mem: 41808
Epoch: [1]  [ 560/1349]  eta: 0:04:09  lr: 0.000142  min_lr: 0.000003  loss: 0.9794 (1.0976)  loss_scale: 16384.0000 (19508.9340)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [1]  [ 570/1349]  eta: 0:04:06  lr: 0.000142  min_lr: 0.000003  loss: 0.9953 (1.0974)  loss_scale: 16384.0000 (19454.2067)  weight_decay: 0.0500 (0.0500)  time: 0.3080  data: 0.0001  max mem: 41808
Epoch: [1]  [ 580/1349]  eta: 0:04:03  lr: 0.000143  min_lr: 0.000003  loss: 1.1012 (1.0978)  loss_scale: 16384.0000 (19401.3632)  weight_decay: 0.0500 (0.0500)  time: 0.3078  data: 0.0001  max mem: 41808
Epoch: [1]  [ 590/1349]  eta: 0:03:59  lr: 0.000144  min_lr: 0.000003  loss: 1.1775 (1.0977)  loss_scale: 16384.0000 (19350.3080)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [1]  [ 600/1349]  eta: 0:03:56  lr: 0.000144  min_lr: 0.000003  loss: 1.1036 (1.0960)  loss_scale: 16384.0000 (19300.9517)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0002  max mem: 41808
Epoch: [1]  [ 610/1349]  eta: 0:03:53  lr: 0.000145  min_lr: 0.000003  loss: 1.0866 (1.0964)  loss_scale: 16384.0000 (19253.2111)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0002  max mem: 41808
Epoch: [1]  [ 620/1349]  eta: 0:03:50  lr: 0.000146  min_lr: 0.000003  loss: 1.0830 (1.0962)  loss_scale: 16384.0000 (19207.0081)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
[2025-05-23 15:14:34,986] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 15:14:34,986] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-05-23 15:14:34,986] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 15:14:34,986] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [1]  [ 630/1349]  eta: 0:03:46  lr: 0.000147  min_lr: 0.000003  loss: 1.1285 (1.0978)  loss_scale: 16384.0000 (19369.9905)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0002  max mem: 41808
Epoch: [1]  [ 640/1349]  eta: 0:03:43  lr: 0.000147  min_lr: 0.000004  loss: 1.0854 (1.0962)  loss_scale: 32768.0000 (19579.0078)  weight_decay: 0.0500 (0.0500)  time: 0.3062  data: 0.0001  max mem: 41808
[2025-05-23 15:14:43,263] [INFO] [logging.py:96:log_dist] [Rank 0] step=2000, skipped=6, lr=[3.5209646183347784e-06, 3.5209646183347784e-06, 4.694619491113038e-06, 4.694619491113038e-06, 6.259492654817384e-06, 6.259492654817384e-06, 8.345990206423178e-06, 8.345990206423178e-06, 1.1127986941897572e-05, 1.1127986941897572e-05, 1.4837315922530095e-05, 1.4837315922530095e-05, 1.9783087896706793e-05, 1.9783087896706793e-05, 2.637745052894239e-05, 2.637745052894239e-05, 3.5169934038589855e-05, 3.5169934038589855e-05, 4.689324538478647e-05, 4.689324538478647e-05, 6.25243271797153e-05, 6.25243271797153e-05, 8.336576957295373e-05, 8.336576957295373e-05, 0.00011115435943060498, 0.00011115435943060498, 0.00014820581257413997, 0.00014820581257413997], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-05-23 15:14:43,263] [INFO] [timer.py:260:stop] epoch=0/micro_step=2000/global_step=2000, RunningAvgSamplesPerSec=211.83364278198547, CurrSamplesPerSec=213.7168816424409, MemAllocated=1.37GB, MaxMemAllocated=40.83GB
Epoch: [1]  [ 650/1349]  eta: 0:03:40  lr: 0.000148  min_lr: 0.000004  loss: 1.0349 (1.0959)  loss_scale: 32768.0000 (19781.6037)  weight_decay: 0.0500 (0.0500)  time: 0.3064  data: 0.0001  max mem: 41808
Epoch: [1]  [ 660/1349]  eta: 0:03:37  lr: 0.000149  min_lr: 0.000004  loss: 1.0295 (1.0945)  loss_scale: 32768.0000 (19978.0696)  weight_decay: 0.0500 (0.0500)  time: 0.3079  data: 0.0001  max mem: 41808
Epoch: [1]  [ 670/1349]  eta: 0:03:33  lr: 0.000150  min_lr: 0.000004  loss: 1.1307 (1.0955)  loss_scale: 32768.0000 (20168.6796)  weight_decay: 0.0500 (0.0500)  time: 0.3083  data: 0.0001  max mem: 41808
[2025-05-23 15:14:50,349] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 2022
[2025-05-23 15:14:50,349] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 2022
[2025-05-23 15:14:50,349] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-05-23 15:14:50,349] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-05-23 15:14:50,349] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [1]  [ 680/1349]  eta: 0:03:30  lr: 0.000150  min_lr: 0.000004  loss: 1.2368 (1.0971)  loss_scale: 32768.0000 (20161.2217)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0001  max mem: 41808
Epoch: [1]  [ 690/1349]  eta: 0:03:27  lr: 0.000151  min_lr: 0.000004  loss: 1.1691 (1.0972)  loss_scale: 16384.0000 (20106.5586)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
Epoch: [1]  [ 700/1349]  eta: 0:03:24  lr: 0.000152  min_lr: 0.000004  loss: 1.1089 (1.0974)  loss_scale: 16384.0000 (20053.4551)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
Epoch: [1]  [ 710/1349]  eta: 0:03:20  lr: 0.000153  min_lr: 0.000004  loss: 1.0897 (1.0967)  loss_scale: 16384.0000 (20001.8453)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0001  max mem: 41808
Epoch: [1]  [ 720/1349]  eta: 0:03:17  lr: 0.000153  min_lr: 0.000004  loss: 0.9848 (1.0957)  loss_scale: 16384.0000 (19951.6671)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0001  max mem: 41808
Epoch: [1]  [ 730/1349]  eta: 0:03:14  lr: 0.000154  min_lr: 0.000004  loss: 1.0458 (1.0965)  loss_scale: 16384.0000 (19902.8618)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [1]  [ 740/1349]  eta: 0:03:11  lr: 0.000155  min_lr: 0.000004  loss: 1.0847 (1.0954)  loss_scale: 16384.0000 (19855.3738)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0002  max mem: 41808
Epoch: [1]  [ 750/1349]  eta: 0:03:08  lr: 0.000156  min_lr: 0.000004  loss: 1.0120 (1.0948)  loss_scale: 16384.0000 (19809.1505)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
Epoch: [1]  [ 760/1349]  eta: 0:03:04  lr: 0.000156  min_lr: 0.000004  loss: 1.0146 (1.0940)  loss_scale: 16384.0000 (19764.1419)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0001  max mem: 41808
Epoch: [1]  [ 770/1349]  eta: 0:03:01  lr: 0.000157  min_lr: 0.000004  loss: 1.0629 (1.0938)  loss_scale: 16384.0000 (19720.3009)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0002  max mem: 41808
Epoch: [1]  [ 780/1349]  eta: 0:02:58  lr: 0.000158  min_lr: 0.000004  loss: 1.0635 (1.0936)  loss_scale: 16384.0000 (19677.5826)  weight_decay: 0.0500 (0.0500)  time: 0.3066  data: 0.0001  max mem: 41808
Epoch: [1]  [ 790/1349]  eta: 0:02:55  lr: 0.000159  min_lr: 0.000004  loss: 0.9588 (1.0907)  loss_scale: 16384.0000 (19635.9444)  weight_decay: 0.0500 (0.0500)  time: 0.3064  data: 0.0001  max mem: 41808
Epoch: [1]  [ 800/1349]  eta: 0:02:52  lr: 0.000159  min_lr: 0.000004  loss: 0.9894 (1.0909)  loss_scale: 16384.0000 (19595.3458)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
[2025-05-23 15:15:29,989] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 15:15:29,989] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 15:15:29,989] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-05-23 15:15:29,990] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [1]  [ 810/1349]  eta: 0:02:49  lr: 0.000160  min_lr: 0.000004  loss: 1.1117 (1.0906)  loss_scale: 16384.0000 (19737.5684)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
Epoch: [1]  [ 820/1349]  eta: 0:02:45  lr: 0.000161  min_lr: 0.000004  loss: 1.0763 (1.0908)  loss_scale: 32768.0000 (19896.2826)  weight_decay: 0.0500 (0.0500)  time: 0.3064  data: 0.0001  max mem: 41808
Epoch: [1]  [ 830/1349]  eta: 0:02:42  lr: 0.000162  min_lr: 0.000004  loss: 1.0882 (1.0905)  loss_scale: 32768.0000 (20051.1769)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
[2025-05-23 15:15:39,194] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 2181
[2025-05-23 15:15:39,194] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 2181
[2025-05-23 15:15:39,194] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-05-23 15:15:39,194] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-05-23 15:15:39,194] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [1]  [ 840/1349]  eta: 0:02:39  lr: 0.000162  min_lr: 0.000004  loss: 1.0882 (1.0910)  loss_scale: 32768.0000 (20027.0535)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
Epoch: [1]  [ 850/1349]  eta: 0:02:36  lr: 0.000163  min_lr: 0.000004  loss: 1.1164 (1.0905)  loss_scale: 16384.0000 (19984.2444)  weight_decay: 0.0500 (0.0500)  time: 0.3064  data: 0.0001  max mem: 41808
Epoch: [1]  [ 860/1349]  eta: 0:02:33  lr: 0.000164  min_lr: 0.000004  loss: 1.1214 (1.0917)  loss_scale: 16384.0000 (19942.4297)  weight_decay: 0.0500 (0.0500)  time: 0.3063  data: 0.0001  max mem: 41808
Epoch: [1]  [ 870/1349]  eta: 0:02:29  lr: 0.000165  min_lr: 0.000004  loss: 1.1087 (1.0910)  loss_scale: 16384.0000 (19901.5752)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [1]  [ 880/1349]  eta: 0:02:26  lr: 0.000165  min_lr: 0.000004  loss: 1.0842 (1.0908)  loss_scale: 16384.0000 (19861.6481)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [1]  [ 890/1349]  eta: 0:02:23  lr: 0.000166  min_lr: 0.000004  loss: 1.0420 (1.0899)  loss_scale: 16384.0000 (19822.6173)  weight_decay: 0.0500 (0.0500)  time: 0.3063  data: 0.0001  max mem: 41808
Epoch: [1]  [ 900/1349]  eta: 0:02:20  lr: 0.000167  min_lr: 0.000004  loss: 0.9716 (1.0886)  loss_scale: 16384.0000 (19784.4528)  weight_decay: 0.0500 (0.0500)  time: 0.3062  data: 0.0001  max mem: 41808
Epoch: [1]  [ 910/1349]  eta: 0:02:17  lr: 0.000167  min_lr: 0.000004  loss: 1.0556 (1.0882)  loss_scale: 16384.0000 (19747.1262)  weight_decay: 0.0500 (0.0500)  time: 0.3061  data: 0.0001  max mem: 41808
Epoch: [1]  [ 920/1349]  eta: 0:02:14  lr: 0.000168  min_lr: 0.000004  loss: 1.0926 (1.0879)  loss_scale: 16384.0000 (19710.6102)  weight_decay: 0.0500 (0.0500)  time: 0.3064  data: 0.0001  max mem: 41808
Epoch: [1]  [ 930/1349]  eta: 0:02:11  lr: 0.000169  min_lr: 0.000004  loss: 1.0573 (1.0882)  loss_scale: 16384.0000 (19674.8786)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
Epoch: [1]  [ 940/1349]  eta: 0:02:07  lr: 0.000170  min_lr: 0.000004  loss: 1.0491 (1.0878)  loss_scale: 16384.0000 (19639.9065)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
Epoch: [1]  [ 950/1349]  eta: 0:02:04  lr: 0.000170  min_lr: 0.000004  loss: 1.0205 (1.0867)  loss_scale: 16384.0000 (19605.6698)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [1]  [ 960/1349]  eta: 0:02:01  lr: 0.000171  min_lr: 0.000004  loss: 1.0477 (1.0866)  loss_scale: 16384.0000 (19572.1457)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
[2025-05-23 15:16:18,766] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 15:16:18,766] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 15:16:18,766] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-05-23 15:16:18,766] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [1]  [ 970/1349]  eta: 0:01:58  lr: 0.000172  min_lr: 0.000004  loss: 1.0760 (1.0865)  loss_scale: 16384.0000 (19708.0453)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [1]  [ 980/1349]  eta: 0:01:55  lr: 0.000173  min_lr: 0.000004  loss: 1.1178 (1.0869)  loss_scale: 32768.0000 (19841.1743)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
Epoch: [1]  [ 990/1349]  eta: 0:01:52  lr: 0.000173  min_lr: 0.000004  loss: 1.1616 (1.0872)  loss_scale: 32768.0000 (19971.6165)  weight_decay: 0.0500 (0.0500)  time: 0.3088  data: 0.0001  max mem: 41808
Epoch: [1]  [1000/1349]  eta: 0:01:49  lr: 0.000174  min_lr: 0.000004  loss: 1.0896 (1.0875)  loss_scale: 32768.0000 (20099.4525)  weight_decay: 0.0500 (0.0500)  time: 0.3091  data: 0.0001  max mem: 41808
Epoch: [1]  [1010/1349]  eta: 0:01:45  lr: 0.000175  min_lr: 0.000004  loss: 1.1542 (1.0887)  loss_scale: 32768.0000 (20224.7596)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
[2025-05-23 15:16:34,784] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 2362
[2025-05-23 15:16:34,784] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 2362
[2025-05-23 15:16:34,784] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-05-23 15:16:34,784] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-05-23 15:16:34,784] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [1]  [1020/1349]  eta: 0:01:42  lr: 0.000176  min_lr: 0.000004  loss: 1.1609 (1.0880)  loss_scale: 32768.0000 (20219.2360)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
Epoch: [1]  [1030/1349]  eta: 0:01:39  lr: 0.000176  min_lr: 0.000004  loss: 1.1021 (1.0878)  loss_scale: 16384.0000 (20182.0369)  weight_decay: 0.0500 (0.0500)  time: 0.3061  data: 0.0001  max mem: 41808
Epoch: [1]  [1040/1349]  eta: 0:01:36  lr: 0.000177  min_lr: 0.000004  loss: 1.1073 (1.0882)  loss_scale: 16384.0000 (20145.5524)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0002  max mem: 41808
Epoch: [1]  [1050/1349]  eta: 0:01:33  lr: 0.000178  min_lr: 0.000004  loss: 1.1311 (1.0886)  loss_scale: 16384.0000 (20109.7621)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0002  max mem: 41808
Epoch: [1]  [1060/1349]  eta: 0:01:30  lr: 0.000179  min_lr: 0.000004  loss: 1.1081 (1.0881)  loss_scale: 16384.0000 (20074.6466)  weight_decay: 0.0500 (0.0500)  time: 0.3066  data: 0.0001  max mem: 41808
Epoch: [1]  [1070/1349]  eta: 0:01:27  lr: 0.000179  min_lr: 0.000004  loss: 1.1105 (1.0886)  loss_scale: 16384.0000 (20040.1867)  weight_decay: 0.0500 (0.0500)  time: 0.3080  data: 0.0001  max mem: 41808
Epoch: [1]  [1080/1349]  eta: 0:01:23  lr: 0.000180  min_lr: 0.000004  loss: 1.1299 (1.0884)  loss_scale: 16384.0000 (20006.3645)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
Epoch: [1]  [1090/1349]  eta: 0:01:20  lr: 0.000181  min_lr: 0.000004  loss: 0.9646 (1.0869)  loss_scale: 16384.0000 (19973.1622)  weight_decay: 0.0500 (0.0500)  time: 0.3060  data: 0.0001  max mem: 41808
Epoch: [1]  [1100/1349]  eta: 0:01:17  lr: 0.000182  min_lr: 0.000004  loss: 0.8946 (1.0856)  loss_scale: 16384.0000 (19940.5631)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0002  max mem: 41808
Epoch: [1]  [1110/1349]  eta: 0:01:14  lr: 0.000182  min_lr: 0.000004  loss: 0.9998 (1.0850)  loss_scale: 16384.0000 (19908.5509)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
Epoch: [1]  [1120/1349]  eta: 0:01:11  lr: 0.000183  min_lr: 0.000004  loss: 1.0001 (1.0847)  loss_scale: 16384.0000 (19877.1097)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [1]  [1130/1349]  eta: 0:01:08  lr: 0.000184  min_lr: 0.000004  loss: 1.0318 (1.0839)  loss_scale: 16384.0000 (19846.2246)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
Epoch: [1]  [1140/1349]  eta: 0:01:05  lr: 0.000185  min_lr: 0.000004  loss: 1.0318 (1.0840)  loss_scale: 16384.0000 (19815.8808)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
[2025-05-23 15:17:14,418] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 15:17:14,418] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 15:17:14,418] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-05-23 15:17:14,418] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [1]  [1150/1349]  eta: 0:01:02  lr: 0.000185  min_lr: 0.000004  loss: 0.9507 (1.0828)  loss_scale: 16384.0000 (19914.1755)  weight_decay: 0.0500 (0.0500)  time: 0.3082  data: 0.0002  max mem: 41808
Epoch: [1]  [1160/1349]  eta: 0:00:58  lr: 0.000186  min_lr: 0.000004  loss: 0.9513 (1.0823)  loss_scale: 32768.0000 (20024.8889)  weight_decay: 0.0500 (0.0500)  time: 0.3083  data: 0.0002  max mem: 41808
Epoch: [1]  [1170/1349]  eta: 0:00:55  lr: 0.000187  min_lr: 0.000004  loss: 0.9873 (1.0816)  loss_scale: 32768.0000 (20133.7114)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
Epoch: [1]  [1180/1349]  eta: 0:00:52  lr: 0.000188  min_lr: 0.000004  loss: 0.9873 (1.0813)  loss_scale: 32768.0000 (20240.6909)  weight_decay: 0.0500 (0.0500)  time: 0.3081  data: 0.0001  max mem: 41808
Epoch: [1]  [1190/1349]  eta: 0:00:49  lr: 0.000188  min_lr: 0.000004  loss: 0.9754 (1.0800)  loss_scale: 32768.0000 (20345.8741)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0001  max mem: 41808
Epoch: [1]  [1200/1349]  eta: 0:00:46  lr: 0.000189  min_lr: 0.000004  loss: 0.8855 (1.0799)  loss_scale: 32768.0000 (20449.3056)  weight_decay: 0.0500 (0.0500)  time: 0.3063  data: 0.0001  max mem: 41808
[2025-05-23 15:17:33,484] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 2553
[2025-05-23 15:17:33,484] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 2553
[2025-05-23 15:17:33,484] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-05-23 15:17:33,484] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-05-23 15:17:33,484] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [1]  [1210/1349]  eta: 0:00:43  lr: 0.000190  min_lr: 0.000005  loss: 1.0078 (1.0799)  loss_scale: 32768.0000 (20456.3237)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
Epoch: [1]  [1220/1349]  eta: 0:00:40  lr: 0.000190  min_lr: 0.000005  loss: 1.0655 (1.0796)  loss_scale: 16384.0000 (20422.9713)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [1]  [1230/1349]  eta: 0:00:37  lr: 0.000191  min_lr: 0.000005  loss: 1.0325 (1.0796)  loss_scale: 16384.0000 (20390.1608)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0001  max mem: 41808
Epoch: [1]  [1240/1349]  eta: 0:00:33  lr: 0.000192  min_lr: 0.000005  loss: 0.9830 (1.0788)  loss_scale: 16384.0000 (20357.8791)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0001  max mem: 41808
Epoch: [1]  [1250/1349]  eta: 0:00:30  lr: 0.000193  min_lr: 0.000005  loss: 1.0133 (1.0783)  loss_scale: 16384.0000 (20326.1135)  weight_decay: 0.0500 (0.0500)  time: 0.3066  data: 0.0001  max mem: 41808
Epoch: [1]  [1260/1349]  eta: 0:00:27  lr: 0.000193  min_lr: 0.000005  loss: 1.0923 (1.0783)  loss_scale: 16384.0000 (20294.8517)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0002  max mem: 41808
Epoch: [1]  [1270/1349]  eta: 0:00:24  lr: 0.000194  min_lr: 0.000005  loss: 1.0923 (1.0788)  loss_scale: 16384.0000 (20264.0818)  weight_decay: 0.0500 (0.0500)  time: 0.3063  data: 0.0002  max mem: 41808
Epoch: [1]  [1280/1349]  eta: 0:00:21  lr: 0.000195  min_lr: 0.000005  loss: 1.1056 (1.0786)  loss_scale: 16384.0000 (20233.7923)  weight_decay: 0.0500 (0.0500)  time: 0.3063  data: 0.0001  max mem: 41808
Epoch: [1]  [1290/1349]  eta: 0:00:18  lr: 0.000196  min_lr: 0.000005  loss: 0.9648 (1.0774)  loss_scale: 16384.0000 (20203.9721)  weight_decay: 0.0500 (0.0500)  time: 0.3078  data: 0.0001  max mem: 41808
Epoch: [1]  [1300/1349]  eta: 0:00:15  lr: 0.000196  min_lr: 0.000005  loss: 0.9443 (1.0767)  loss_scale: 16384.0000 (20174.6103)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [1]  [1310/1349]  eta: 0:00:12  lr: 0.000197  min_lr: 0.000005  loss: 1.0285 (1.0775)  loss_scale: 16384.0000 (20145.6964)  weight_decay: 0.0500 (0.0500)  time: 0.3059  data: 0.0001  max mem: 41808
Epoch: [1]  [1320/1349]  eta: 0:00:09  lr: 0.000198  min_lr: 0.000005  loss: 0.9893 (1.0766)  loss_scale: 16384.0000 (20117.2203)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
Epoch: [1]  [1330/1349]  eta: 0:00:05  lr: 0.000199  min_lr: 0.000005  loss: 0.9190 (1.0754)  loss_scale: 16384.0000 (20089.1721)  weight_decay: 0.0500 (0.0500)  time: 0.3056  data: 0.0001  max mem: 41808
[2025-05-23 15:18:13,028] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 15:18:13,028] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 15:18:13,028] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-05-23 15:18:13,028] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-05-23 15:18:13,630] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 2684
[2025-05-23 15:18:13,630] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-05-23 15:18:13,630] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 2684
[2025-05-23 15:18:13,630] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-05-23 15:18:13,630] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [1]  [1340/1349]  eta: 0:00:02  lr: 0.000199  min_lr: 0.000005  loss: 1.0305 (1.0757)  loss_scale: 16384.0000 (20085.9776)  weight_decay: 0.0500 (0.0500)  time: 0.3014  data: 0.0001  max mem: 41808
Epoch: [1]  [1348/1349]  eta: 0:00:00  lr: 0.000200  min_lr: 0.000005  loss: 1.0962 (1.0757)  loss_scale: 16384.0000 (20064.0237)  weight_decay: 0.0500 (0.0500)  time: 0.3010  data: 0.0001  max mem: 41808
Epoch: [1] Total time: 0:06:59 (0.3112 s / it)
Averaged stats: lr: 0.000200  min_lr: 0.000005  loss: 1.0962 (1.0763)  loss_scale: 16384.0000 (20064.0237)  weight_decay: 0.0500 (0.0500)  total_time: 419.8380 (419.8318)
Val:  [  0/346]  eta: 0:44:34  loss: 1.5865 (1.5865)  acc1: 25.7812 (25.7812)  acc5: 91.4062 (91.4062)  time: 7.7294  data: 6.7732  max mem: 41808
Val:  [ 10/346]  eta: 0:10:48  loss: 0.2298 (0.4129)  acc1: 97.6562 (87.7131)  acc5: 100.0000 (99.2188)  time: 1.9290  data: 1.1780  max mem: 41808
Val:  [ 20/346]  eta: 0:08:17  loss: 0.2178 (0.3838)  acc1: 99.2188 (90.1786)  acc5: 100.0000 (99.5536)  time: 1.2155  data: 0.4894  max mem: 41808
Val:  [ 30/346]  eta: 0:06:34  loss: 0.1969 (0.3471)  acc1: 100.0000 (91.9355)  acc5: 100.0000 (99.6976)  time: 0.8727  data: 0.1803  max mem: 41808
Val:  [ 40/346]  eta: 0:05:40  loss: 0.1969 (0.3755)  acc1: 100.0000 (91.3491)  acc5: 100.0000 (99.3331)  time: 0.6787  data: 0.0003  max mem: 41808
Val:  [ 50/346]  eta: 0:05:11  loss: 0.2082 (0.3490)  acc1: 99.2188 (92.5705)  acc5: 100.0000 (99.4638)  time: 0.7510  data: 0.0368  max mem: 41808
Val:  [ 60/346]  eta: 0:04:51  loss: 0.2258 (0.3521)  acc1: 98.4375 (92.0978)  acc5: 100.0000 (99.5517)  time: 0.8294  data: 0.1125  max mem: 41808
Val:  [ 70/346]  eta: 0:04:34  loss: 0.3044 (0.3662)  acc1: 93.7500 (91.6043)  acc5: 100.0000 (99.6149)  time: 0.8469  data: 0.1438  max mem: 41808
Val:  [ 80/346]  eta: 0:04:20  loss: 0.3044 (0.3635)  acc1: 93.7500 (91.8403)  acc5: 100.0000 (99.6528)  time: 0.8556  data: 0.1386  max mem: 41808
Val:  [ 90/346]  eta: 0:04:06  loss: 0.2946 (0.3634)  acc1: 95.3125 (91.8698)  acc5: 100.0000 (99.6909)  time: 0.8579  data: 0.1348  max mem: 41808
Val:  [100/346]  eta: 0:03:55  loss: 0.2894 (0.3515)  acc1: 97.6562 (92.5743)  acc5: 100.0000 (99.7215)  time: 0.8609  data: 0.1351  max mem: 41808
Val:  [110/346]  eta: 0:03:42  loss: 0.2241 (0.3628)  acc1: 99.2188 (92.1593)  acc5: 100.0000 (99.7255)  time: 0.8519  data: 0.1426  max mem: 41808
Val:  [120/346]  eta: 0:03:32  loss: 0.2163 (0.3571)  acc1: 98.4375 (92.4780)  acc5: 100.0000 (99.7482)  time: 0.8662  data: 0.1543  max mem: 41808
Val:  [130/346]  eta: 0:03:22  loss: 0.2163 (0.3578)  acc1: 99.2188 (92.4380)  acc5: 100.0000 (99.7674)  time: 0.8925  data: 0.1649  max mem: 41808
Val:  [140/346]  eta: 0:03:11  loss: 0.2561 (0.3561)  acc1: 96.0938 (92.5255)  acc5: 100.0000 (99.7839)  time: 0.8561  data: 0.1543  max mem: 41808
Val:  [150/346]  eta: 0:03:00  loss: 0.3596 (0.3554)  acc1: 94.5312 (92.5704)  acc5: 100.0000 (99.7982)  time: 0.8344  data: 0.1381  max mem: 41808
Val:  [160/346]  eta: 0:02:50  loss: 0.3347 (0.3573)  acc1: 92.9688 (92.5611)  acc5: 100.0000 (99.8108)  time: 0.8465  data: 0.1427  max mem: 41808
Val:  [170/346]  eta: 0:02:41  loss: 0.2987 (0.3676)  acc1: 89.0625 (91.9042)  acc5: 100.0000 (99.8218)  time: 0.8541  data: 0.1499  max mem: 41808
Val:  [180/346]  eta: 0:02:31  loss: 0.2987 (0.3801)  acc1: 88.2812 (90.9142)  acc5: 100.0000 (99.8317)  time: 0.8621  data: 0.1594  max mem: 41808
Val:  [190/346]  eta: 0:02:22  loss: 0.3417 (0.3805)  acc1: 92.1875 (90.9563)  acc5: 100.0000 (99.8405)  time: 0.9060  data: 0.1712  max mem: 41808
Val:  [200/346]  eta: 0:02:12  loss: 0.5001 (0.3914)  acc1: 85.1562 (90.3451)  acc5: 100.0000 (99.8484)  time: 0.8958  data: 0.1663  max mem: 41808
Val:  [210/346]  eta: 0:02:03  loss: 0.2610 (0.3864)  acc1: 98.4375 (90.5621)  acc5: 100.0000 (99.8556)  time: 0.8607  data: 0.1563  max mem: 41808
Val:  [220/346]  eta: 0:01:54  loss: 0.2384 (0.3837)  acc1: 100.0000 (90.6957)  acc5: 100.0000 (99.8480)  time: 0.8675  data: 0.1541  max mem: 41808
Val:  [230/346]  eta: 0:01:44  loss: 0.2384 (0.3773)  acc1: 99.2188 (91.0038)  acc5: 100.0000 (99.8546)  time: 0.8576  data: 0.1564  max mem: 41808
Val:  [240/346]  eta: 0:01:35  loss: 0.2731 (0.3847)  acc1: 96.8750 (90.8487)  acc5: 100.0000 (99.8606)  time: 0.8775  data: 0.1657  max mem: 41808
Val:  [250/346]  eta: 0:01:26  loss: 0.2731 (0.3816)  acc1: 96.0938 (90.9798)  acc5: 100.0000 (99.8662)  time: 0.8733  data: 0.1565  max mem: 41808
Val:  [260/346]  eta: 0:01:17  loss: 0.2354 (0.3815)  acc1: 96.8750 (90.9782)  acc5: 100.0000 (99.8713)  time: 0.8559  data: 0.1484  max mem: 41808
Val:  [270/346]  eta: 0:01:08  loss: 0.1954 (0.3772)  acc1: 97.6562 (91.1410)  acc5: 100.0000 (99.8760)  time: 0.8989  data: 0.1683  max mem: 41808
Val:  [280/346]  eta: 0:00:59  loss: 0.1954 (0.3746)  acc1: 100.0000 (91.2367)  acc5: 100.0000 (99.8804)  time: 0.9039  data: 0.1646  max mem: 41808
Val:  [290/346]  eta: 0:00:50  loss: 0.1886 (0.3680)  acc1: 100.0000 (91.4948)  acc5: 100.0000 (99.8846)  time: 0.8913  data: 0.1478  max mem: 41808
Val:  [300/346]  eta: 0:00:41  loss: 0.1944 (0.3712)  acc1: 99.2188 (91.4582)  acc5: 100.0000 (99.7716)  time: 0.8937  data: 0.1445  max mem: 41808
Val:  [310/346]  eta: 0:00:32  loss: 0.2320 (0.3723)  acc1: 97.6562 (91.3862)  acc5: 100.0000 (99.7789)  time: 0.8521  data: 0.1376  max mem: 41808
Val:  [320/346]  eta: 0:00:23  loss: 0.2667 (0.3767)  acc1: 99.2188 (91.1896)  acc5: 100.0000 (99.7858)  time: 0.8287  data: 0.1376  max mem: 41808
Val:  [330/346]  eta: 0:00:14  loss: 0.4084 (0.3828)  acc1: 88.2812 (91.0026)  acc5: 100.0000 (99.7923)  time: 0.8352  data: 0.1581  max mem: 41808
Val:  [340/346]  eta: 0:00:05  loss: 0.4066 (0.3852)  acc1: 89.0625 (90.8541)  acc5: 100.0000 (99.7984)  time: 0.8399  data: 0.1641  max mem: 41808
Val:  [345/346]  eta: 0:00:00  loss: 0.2526 (0.3822)  acc1: 97.6562 (90.9734)  acc5: 100.0000 (99.8011)  time: 0.8333  data: 0.1746  max mem: 41808
Val: Total time: 0:05:08 (0.8928 s / it)
* Acc@1 90.996 Acc@5 99.784 loss 0.382
Accuracy of the network on the 88494 val videos: 91.0%
[2025-05-23 15:23:26,772] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-05-23 15:23:26,775] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /project/mmendoscope/Natural_Comparison/Cholec75/unified_base_st_settingE_videomae-st_Cholec80_0.0005_0.75_online_key_frame_frame16_Fixed_Stride_4/checkpoint-best/mp_rank_00_model_states.pt
[2025-05-23 15:23:26,775] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2025-05-23 15:23:26,775] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /project/mmendoscope/Natural_Comparison/Cholec75/unified_base_st_settingE_videomae-st_Cholec80_0.0005_0.75_online_key_frame_frame16_Fixed_Stride_4/checkpoint-best/mp_rank_00_model_states.pt...
[2025-05-23 15:23:29,716] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /project/mmendoscope/Natural_Comparison/Cholec75/unified_base_st_settingE_videomae-st_Cholec80_0.0005_0.75_online_key_frame_frame16_Fixed_Stride_4/checkpoint-best/mp_rank_00_model_states.pt.
[2025-05-23 15:23:29,717] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 91.00%   Max Epoch: 1
Epoch: [2]  [   0/1349]  eta: 2:05:50  lr: 0.000200  min_lr: 0.000005  loss: 0.7315 (0.7315)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 5.5967  data: 5.2556  max mem: 41808
Epoch: [2]  [  10/1349]  eta: 0:17:42  lr: 0.000201  min_lr: 0.000005  loss: 1.0666 (1.0162)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7935  data: 0.4779  max mem: 41808
Epoch: [2]  [  20/1349]  eta: 0:12:28  lr: 0.000202  min_lr: 0.000005  loss: 1.0868 (1.0510)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3114  data: 0.0001  max mem: 41808
Epoch: [2]  [  30/1349]  eta: 0:10:34  lr: 0.000202  min_lr: 0.000005  loss: 1.0868 (1.0342)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3087  data: 0.0001  max mem: 41808
Epoch: [2]  [  40/1349]  eta: 0:09:34  lr: 0.000203  min_lr: 0.000005  loss: 0.9362 (0.9995)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3078  data: 0.0002  max mem: 41808
Epoch: [2]  [  50/1349]  eta: 0:08:56  lr: 0.000204  min_lr: 0.000005  loss: 1.0096 (1.0150)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
Epoch: [2]  [  60/1349]  eta: 0:08:29  lr: 0.000204  min_lr: 0.000005  loss: 1.0847 (1.0220)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [2]  [  70/1349]  eta: 0:08:10  lr: 0.000205  min_lr: 0.000005  loss: 1.0847 (1.0320)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
Epoch: [2]  [  80/1349]  eta: 0:07:54  lr: 0.000206  min_lr: 0.000005  loss: 1.0065 (1.0195)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
Epoch: [2]  [  90/1349]  eta: 0:07:41  lr: 0.000207  min_lr: 0.000005  loss: 1.0019 (1.0212)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0001  max mem: 41808
Epoch: [2]  [ 100/1349]  eta: 0:07:30  lr: 0.000207  min_lr: 0.000005  loss: 1.0019 (1.0167)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3080  data: 0.0001  max mem: 41808
Epoch: [2]  [ 110/1349]  eta: 0:07:21  lr: 0.000208  min_lr: 0.000005  loss: 0.9122 (1.0126)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3084  data: 0.0001  max mem: 41808
[2025-05-23 15:24:10,807] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 15:24:10,808] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-05-23 15:24:10,807] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 15:24:10,808] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [2]  [ 120/1349]  eta: 0:07:12  lr: 0.000209  min_lr: 0.000005  loss: 0.9723 (1.0170)  loss_scale: 16384.0000 (17196.4298)  weight_decay: 0.0500 (0.0500)  time: 0.3089  data: 0.0001  max mem: 41808
[2025-05-23 15:24:12,964] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 2820
[2025-05-23 15:24:12,965] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-05-23 15:24:12,965] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 2820
[2025-05-23 15:24:12,965] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-05-23 15:24:12,965] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [2]  [ 130/1349]  eta: 0:07:05  lr: 0.000210  min_lr: 0.000005  loss: 1.1147 (1.0336)  loss_scale: 16384.0000 (17259.4809)  weight_decay: 0.0500 (0.0500)  time: 0.3081  data: 0.0002  max mem: 41808
Epoch: [2]  [ 140/1349]  eta: 0:06:58  lr: 0.000210  min_lr: 0.000005  loss: 1.1313 (1.0368)  loss_scale: 16384.0000 (17197.3901)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0002  max mem: 41808
Epoch: [2]  [ 150/1349]  eta: 0:06:51  lr: 0.000211  min_lr: 0.000005  loss: 1.0644 (1.0316)  loss_scale: 16384.0000 (17143.5232)  weight_decay: 0.0500 (0.0500)  time: 0.3079  data: 0.0001  max mem: 41808
Epoch: [2]  [ 160/1349]  eta: 0:06:45  lr: 0.000212  min_lr: 0.000005  loss: 0.9651 (1.0288)  loss_scale: 16384.0000 (17096.3478)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [2]  [ 170/1349]  eta: 0:06:39  lr: 0.000213  min_lr: 0.000005  loss: 1.0783 (1.0346)  loss_scale: 16384.0000 (17054.6901)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
Epoch: [2]  [ 180/1349]  eta: 0:06:34  lr: 0.000213  min_lr: 0.000005  loss: 1.0783 (1.0332)  loss_scale: 16384.0000 (17017.6354)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [2]  [ 190/1349]  eta: 0:06:29  lr: 0.000214  min_lr: 0.000005  loss: 1.0984 (1.0361)  loss_scale: 16384.0000 (16984.4607)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [2]  [ 200/1349]  eta: 0:06:24  lr: 0.000215  min_lr: 0.000005  loss: 1.0936 (1.0376)  loss_scale: 16384.0000 (16954.5871)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [2]  [ 210/1349]  eta: 0:06:19  lr: 0.000216  min_lr: 0.000005  loss: 1.0142 (1.0373)  loss_scale: 16384.0000 (16927.5450)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
Epoch: [2]  [ 220/1349]  eta: 0:06:14  lr: 0.000216  min_lr: 0.000005  loss: 1.0142 (1.0368)  loss_scale: 16384.0000 (16902.9502)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
Epoch: [2]  [ 230/1349]  eta: 0:06:10  lr: 0.000217  min_lr: 0.000005  loss: 1.1244 (1.0369)  loss_scale: 16384.0000 (16880.4848)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [2]  [ 240/1349]  eta: 0:06:05  lr: 0.000218  min_lr: 0.000005  loss: 1.0861 (1.0405)  loss_scale: 16384.0000 (16859.8838)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [2]  [ 250/1349]  eta: 0:06:01  lr: 0.000219  min_lr: 0.000005  loss: 1.0295 (1.0390)  loss_scale: 16384.0000 (16840.9243)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
[2025-05-23 15:24:52,613] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 15:24:52,613] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-05-23 15:24:52,613] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 15:24:52,613] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [2]  [ 260/1349]  eta: 0:05:57  lr: 0.000219  min_lr: 0.000005  loss: 1.0190 (1.0391)  loss_scale: 16384.0000 (17451.1571)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
Epoch: [2]  [ 270/1349]  eta: 0:05:53  lr: 0.000220  min_lr: 0.000005  loss: 1.0190 (1.0346)  loss_scale: 32768.0000 (18016.3542)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
Epoch: [2]  [ 280/1349]  eta: 0:05:49  lr: 0.000221  min_lr: 0.000005  loss: 0.9449 (1.0330)  loss_scale: 32768.0000 (18541.3238)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
Epoch: [2]  [ 290/1349]  eta: 0:05:45  lr: 0.000222  min_lr: 0.000005  loss: 1.0309 (1.0316)  loss_scale: 32768.0000 (19030.2131)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
Epoch: [2]  [ 300/1349]  eta: 0:05:41  lr: 0.000222  min_lr: 0.000005  loss: 1.0309 (1.0305)  loss_scale: 32768.0000 (19486.6179)  weight_decay: 0.0500 (0.0500)  time: 0.3078  data: 0.0001  max mem: 41808
[2025-05-23 15:25:08,045] [INFO] [logging.py:96:log_dist] [Rank 0] step=3000, skipped=12, lr=[5.2823276089974995e-06, 5.2823276089974995e-06, 7.043103478663332e-06, 7.043103478663332e-06, 9.390804638217776e-06, 9.390804638217776e-06, 1.2521072850957035e-05, 1.2521072850957035e-05, 1.6694763801276048e-05, 1.6694763801276048e-05, 2.2259685068368063e-05, 2.2259685068368063e-05, 2.9679580091157416e-05, 2.9679580091157416e-05, 3.9572773454876555e-05, 3.9572773454876555e-05, 5.276369793983541e-05, 5.276369793983541e-05, 7.035159725311387e-05, 7.035159725311387e-05, 9.380212967081851e-05, 9.380212967081851e-05, 0.00012506950622775801, 0.00012506950622775801, 0.00016675934163701067, 0.00016675934163701067, 0.00022234578884934757, 0.00022234578884934757], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-05-23 15:25:08,046] [INFO] [timer.py:260:stop] epoch=0/micro_step=3000/global_step=3000, RunningAvgSamplesPerSec=212.364943875027, CurrSamplesPerSec=192.99809687598633, MemAllocated=1.37GB, MaxMemAllocated=40.83GB
Epoch: [2]  [ 310/1349]  eta: 0:05:37  lr: 0.000223  min_lr: 0.000005  loss: 1.0517 (1.0300)  loss_scale: 32768.0000 (19913.6720)  weight_decay: 0.0500 (0.0500)  time: 0.3093  data: 0.0001  max mem: 41808
[2025-05-23 15:25:13,577] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 3017
[2025-05-23 15:25:13,577] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-05-23 15:25:13,577] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 3017
[2025-05-23 15:25:13,578] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-05-23 15:25:13,578] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [2]  [ 320/1349]  eta: 0:05:33  lr: 0.000224  min_lr: 0.000005  loss: 1.0078 (1.0276)  loss_scale: 32768.0000 (20212.0374)  weight_decay: 0.0500 (0.0500)  time: 0.3089  data: 0.0001  max mem: 41808
Epoch: [2]  [ 330/1349]  eta: 0:05:29  lr: 0.000224  min_lr: 0.000005  loss: 1.0395 (1.0290)  loss_scale: 16384.0000 (20096.3867)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [2]  [ 340/1349]  eta: 0:05:26  lr: 0.000225  min_lr: 0.000005  loss: 1.1073 (1.0321)  loss_scale: 16384.0000 (19987.5191)  weight_decay: 0.0500 (0.0500)  time: 0.3084  data: 0.0002  max mem: 41808
Epoch: [2]  [ 350/1349]  eta: 0:05:22  lr: 0.000226  min_lr: 0.000005  loss: 1.1381 (1.0345)  loss_scale: 16384.0000 (19884.8547)  weight_decay: 0.0500 (0.0500)  time: 0.3083  data: 0.0002  max mem: 41808
Epoch: [2]  [ 360/1349]  eta: 0:05:18  lr: 0.000227  min_lr: 0.000005  loss: 1.1049 (1.0354)  loss_scale: 16384.0000 (19787.8781)  weight_decay: 0.0500 (0.0500)  time: 0.3078  data: 0.0002  max mem: 41808
Epoch: [2]  [ 370/1349]  eta: 0:05:15  lr: 0.000227  min_lr: 0.000005  loss: 1.0786 (1.0336)  loss_scale: 16384.0000 (19696.1294)  weight_decay: 0.0500 (0.0500)  time: 0.3082  data: 0.0001  max mem: 41808
Epoch: [2]  [ 380/1349]  eta: 0:05:11  lr: 0.000228  min_lr: 0.000005  loss: 1.0062 (1.0342)  loss_scale: 16384.0000 (19609.1969)  weight_decay: 0.0500 (0.0500)  time: 0.3082  data: 0.0001  max mem: 41808
Epoch: [2]  [ 390/1349]  eta: 0:05:08  lr: 0.000229  min_lr: 0.000005  loss: 1.0724 (1.0354)  loss_scale: 16384.0000 (19526.7110)  weight_decay: 0.0500 (0.0500)  time: 0.3078  data: 0.0001  max mem: 41808
Epoch: [2]  [ 400/1349]  eta: 0:05:04  lr: 0.000230  min_lr: 0.000005  loss: 1.0730 (1.0344)  loss_scale: 16384.0000 (19448.3392)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [2]  [ 410/1349]  eta: 0:05:01  lr: 0.000230  min_lr: 0.000005  loss: 1.1408 (1.0372)  loss_scale: 16384.0000 (19373.7810)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
Epoch: [2]  [ 420/1349]  eta: 0:04:57  lr: 0.000231  min_lr: 0.000005  loss: 1.1307 (1.0356)  loss_scale: 16384.0000 (19302.7648)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [2]  [ 430/1349]  eta: 0:04:54  lr: 0.000232  min_lr: 0.000006  loss: 1.0724 (1.0376)  loss_scale: 16384.0000 (19235.0441)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
Epoch: [2]  [ 440/1349]  eta: 0:04:50  lr: 0.000233  min_lr: 0.000006  loss: 1.1569 (1.0392)  loss_scale: 16384.0000 (19170.3946)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0002  max mem: 41808
[2025-05-23 15:25:53,276] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 15:25:53,276] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-05-23 15:25:53,276] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 15:25:53,276] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [2]  [ 450/1349]  eta: 0:04:47  lr: 0.000233  min_lr: 0.000006  loss: 1.1238 (1.0387)  loss_scale: 16384.0000 (19217.5965)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0002  max mem: 41808
[2025-05-23 15:25:55,736] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 3154
[2025-05-23 15:25:55,736] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-05-23 15:25:55,736] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 3154
[2025-05-23 15:25:55,736] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-05-23 15:25:55,736] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [2]  [ 460/1349]  eta: 0:04:43  lr: 0.000234  min_lr: 0.000006  loss: 1.1539 (1.0409)  loss_scale: 16384.0000 (19333.8308)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0002  max mem: 41808
Epoch: [2]  [ 470/1349]  eta: 0:04:40  lr: 0.000235  min_lr: 0.000006  loss: 1.0653 (1.0406)  loss_scale: 16384.0000 (19271.2017)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0002  max mem: 41808
Epoch: [2]  [ 480/1349]  eta: 0:04:37  lr: 0.000236  min_lr: 0.000006  loss: 1.0411 (1.0419)  loss_scale: 16384.0000 (19211.1767)  weight_decay: 0.0500 (0.0500)  time: 0.3082  data: 0.0002  max mem: 41808
Epoch: [2]  [ 490/1349]  eta: 0:04:33  lr: 0.000236  min_lr: 0.000006  loss: 1.1143 (1.0438)  loss_scale: 16384.0000 (19153.5967)  weight_decay: 0.0500 (0.0500)  time: 0.3085  data: 0.0002  max mem: 41808
Epoch: [2]  [ 500/1349]  eta: 0:04:30  lr: 0.000237  min_lr: 0.000006  loss: 1.1123 (1.0434)  loss_scale: 16384.0000 (19098.3154)  weight_decay: 0.0500 (0.0500)  time: 0.3083  data: 0.0002  max mem: 41808
Epoch: [2]  [ 510/1349]  eta: 0:04:26  lr: 0.000238  min_lr: 0.000006  loss: 1.0663 (1.0435)  loss_scale: 16384.0000 (19045.1977)  weight_decay: 0.0500 (0.0500)  time: 0.3090  data: 0.0002  max mem: 41808
Epoch: [2]  [ 520/1349]  eta: 0:04:23  lr: 0.000239  min_lr: 0.000006  loss: 1.0002 (1.0424)  loss_scale: 16384.0000 (18994.1190)  weight_decay: 0.0500 (0.0500)  time: 0.3079  data: 0.0001  max mem: 41808
Epoch: [2]  [ 530/1349]  eta: 0:04:20  lr: 0.000239  min_lr: 0.000006  loss: 0.9864 (1.0416)  loss_scale: 16384.0000 (18944.9642)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [2]  [ 540/1349]  eta: 0:04:16  lr: 0.000240  min_lr: 0.000006  loss: 1.0159 (1.0415)  loss_scale: 16384.0000 (18897.6266)  weight_decay: 0.0500 (0.0500)  time: 0.3079  data: 0.0003  max mem: 41808
[2025-05-23 15:26:24,406] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 3247
[2025-05-23 15:26:24,406] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2025-05-23 15:26:24,407] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 3247
[2025-05-23 15:26:24,407] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2025-05-23 15:26:24,407] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [2]  [ 550/1349]  eta: 0:04:13  lr: 0.000241  min_lr: 0.000006  loss: 1.0231 (1.0421)  loss_scale: 16384.0000 (18822.2722)  weight_decay: 0.0500 (0.0500)  time: 0.3080  data: 0.0003  max mem: 41808
Epoch: [2]  [ 560/1349]  eta: 0:04:10  lr: 0.000242  min_lr: 0.000006  loss: 1.0618 (1.0413)  loss_scale: 8192.0000 (18632.7843)  weight_decay: 0.0500 (0.0500)  time: 0.3086  data: 0.0001  max mem: 41808
Epoch: [2]  [ 570/1349]  eta: 0:04:07  lr: 0.000242  min_lr: 0.000006  loss: 1.0618 (1.0410)  loss_scale: 8192.0000 (18449.9335)  weight_decay: 0.0500 (0.0500)  time: 0.3084  data: 0.0001  max mem: 41808
Epoch: [2]  [ 580/1349]  eta: 0:04:03  lr: 0.000243  min_lr: 0.000006  loss: 1.1122 (1.0417)  loss_scale: 8192.0000 (18273.3769)  weight_decay: 0.0500 (0.0500)  time: 0.3081  data: 0.0001  max mem: 41808
Epoch: [2]  [ 590/1349]  eta: 0:04:00  lr: 0.000244  min_lr: 0.000006  loss: 1.1139 (1.0418)  loss_scale: 8192.0000 (18102.7953)  weight_decay: 0.0500 (0.0500)  time: 0.3081  data: 0.0001  max mem: 41808
Epoch: [2]  [ 600/1349]  eta: 0:03:57  lr: 0.000245  min_lr: 0.000006  loss: 1.1063 (1.0408)  loss_scale: 8192.0000 (17937.8902)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [2]  [ 610/1349]  eta: 0:03:53  lr: 0.000245  min_lr: 0.000006  loss: 1.0273 (1.0406)  loss_scale: 8192.0000 (17778.3830)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0001  max mem: 41808
Epoch: [2]  [ 620/1349]  eta: 0:03:50  lr: 0.000246  min_lr: 0.000006  loss: 0.9821 (1.0396)  loss_scale: 8192.0000 (17624.0129)  weight_decay: 0.0500 (0.0500)  time: 0.3080  data: 0.0002  max mem: 41808
Epoch: [2]  [ 630/1349]  eta: 0:03:47  lr: 0.000247  min_lr: 0.000006  loss: 0.9315 (1.0386)  loss_scale: 8192.0000 (17474.5357)  weight_decay: 0.0500 (0.0500)  time: 0.3066  data: 0.0001  max mem: 41808
Epoch: [2]  [ 640/1349]  eta: 0:03:44  lr: 0.000247  min_lr: 0.000006  loss: 1.0080 (1.0393)  loss_scale: 8192.0000 (17329.7223)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
Epoch: [2]  [ 650/1349]  eta: 0:03:40  lr: 0.000248  min_lr: 0.000006  loss: 0.9947 (1.0379)  loss_scale: 8192.0000 (17189.3579)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [2]  [ 660/1349]  eta: 0:03:37  lr: 0.000249  min_lr: 0.000006  loss: 0.9834 (1.0370)  loss_scale: 8192.0000 (17053.2405)  weight_decay: 0.0500 (0.0500)  time: 0.3062  data: 0.0001  max mem: 41808
Epoch: [2]  [ 670/1349]  eta: 0:03:34  lr: 0.000250  min_lr: 0.000006  loss: 0.9924 (1.0360)  loss_scale: 8192.0000 (16921.1803)  weight_decay: 0.0500 (0.0500)  time: 0.3066  data: 0.0002  max mem: 41808
[2025-05-23 15:27:04,081] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 15:27:04,082] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2025-05-23 15:27:04,082] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 15:27:04,082] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [2]  [ 680/1349]  eta: 0:03:31  lr: 0.000250  min_lr: 0.000006  loss: 1.0116 (1.0352)  loss_scale: 8192.0000 (16829.0866)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0002  max mem: 41808
Epoch: [2]  [ 690/1349]  eta: 0:03:27  lr: 0.000251  min_lr: 0.000006  loss: 1.0344 (1.0355)  loss_scale: 16384.0000 (16822.6454)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0002  max mem: 41808
Epoch: [2]  [ 700/1349]  eta: 0:03:24  lr: 0.000252  min_lr: 0.000006  loss: 1.0404 (1.0347)  loss_scale: 16384.0000 (16816.3880)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
Epoch: [2]  [ 710/1349]  eta: 0:03:21  lr: 0.000253  min_lr: 0.000006  loss: 1.0359 (1.0351)  loss_scale: 16384.0000 (16810.3066)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
Epoch: [2]  [ 720/1349]  eta: 0:03:18  lr: 0.000253  min_lr: 0.000006  loss: 1.0799 (1.0347)  loss_scale: 16384.0000 (16804.3939)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
Epoch: [2]  [ 730/1349]  eta: 0:03:14  lr: 0.000254  min_lr: 0.000006  loss: 1.0005 (1.0340)  loss_scale: 16384.0000 (16798.6430)  weight_decay: 0.0500 (0.0500)  time: 0.3066  data: 0.0001  max mem: 41808
Epoch: [2]  [ 740/1349]  eta: 0:03:11  lr: 0.000255  min_lr: 0.000006  loss: 0.8881 (1.0322)  loss_scale: 16384.0000 (16793.0472)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
Epoch: [2]  [ 750/1349]  eta: 0:03:08  lr: 0.000256  min_lr: 0.000006  loss: 0.9164 (1.0319)  loss_scale: 16384.0000 (16787.6005)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
Epoch: [2]  [ 760/1349]  eta: 0:03:05  lr: 0.000256  min_lr: 0.000006  loss: 1.0809 (1.0320)  loss_scale: 16384.0000 (16782.2970)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [2]  [ 770/1349]  eta: 0:03:02  lr: 0.000257  min_lr: 0.000006  loss: 1.0809 (1.0322)  loss_scale: 16384.0000 (16777.1310)  weight_decay: 0.0500 (0.0500)  time: 0.3079  data: 0.0001  max mem: 41808
Epoch: [2]  [ 780/1349]  eta: 0:02:58  lr: 0.000258  min_lr: 0.000006  loss: 0.9458 (1.0303)  loss_scale: 16384.0000 (16772.0973)  weight_decay: 0.0500 (0.0500)  time: 0.3079  data: 0.0001  max mem: 41808
Epoch: [2]  [ 790/1349]  eta: 0:02:55  lr: 0.000259  min_lr: 0.000006  loss: 0.9316 (1.0295)  loss_scale: 16384.0000 (16767.1909)  weight_decay: 0.0500 (0.0500)  time: 0.3083  data: 0.0001  max mem: 41808
Epoch: [2]  [ 800/1349]  eta: 0:02:52  lr: 0.000259  min_lr: 0.000006  loss: 1.0080 (1.0290)  loss_scale: 16384.0000 (16762.4070)  weight_decay: 0.0500 (0.0500)  time: 0.3080  data: 0.0001  max mem: 41808
[2025-05-23 15:27:43,424] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 15:27:43,424] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-05-23 15:27:43,425] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 15:27:43,425] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-05-23 15:27:44,650] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 3508
[2025-05-23 15:27:44,650] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-05-23 15:27:44,650] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 3508
[2025-05-23 15:27:44,650] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-05-23 15:27:44,651] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [2]  [ 810/1349]  eta: 0:02:49  lr: 0.000260  min_lr: 0.000006  loss: 1.0644 (1.0297)  loss_scale: 16384.0000 (16838.5499)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
Epoch: [2]  [ 820/1349]  eta: 0:02:46  lr: 0.000261  min_lr: 0.000006  loss: 1.0644 (1.0298)  loss_scale: 16384.0000 (16833.0134)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
Epoch: [2]  [ 830/1349]  eta: 0:02:42  lr: 0.000262  min_lr: 0.000006  loss: 0.9829 (1.0297)  loss_scale: 16384.0000 (16827.6101)  weight_decay: 0.0500 (0.0500)  time: 0.3064  data: 0.0001  max mem: 41808
Epoch: [2]  [ 840/1349]  eta: 0:02:39  lr: 0.000262  min_lr: 0.000006  loss: 0.9829 (1.0305)  loss_scale: 16384.0000 (16822.3353)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
Epoch: [2]  [ 850/1349]  eta: 0:02:36  lr: 0.000263  min_lr: 0.000006  loss: 1.0278 (1.0308)  loss_scale: 16384.0000 (16817.1845)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
Epoch: [2]  [ 860/1349]  eta: 0:02:33  lr: 0.000264  min_lr: 0.000006  loss: 1.0460 (1.0308)  loss_scale: 16384.0000 (16812.1533)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
Epoch: [2]  [ 870/1349]  eta: 0:02:30  lr: 0.000265  min_lr: 0.000006  loss: 1.0460 (1.0296)  loss_scale: 16384.0000 (16807.2377)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
Epoch: [2]  [ 880/1349]  eta: 0:02:27  lr: 0.000265  min_lr: 0.000006  loss: 0.9631 (1.0298)  loss_scale: 16384.0000 (16802.4336)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [2]  [ 890/1349]  eta: 0:02:23  lr: 0.000266  min_lr: 0.000006  loss: 0.9634 (1.0285)  loss_scale: 16384.0000 (16797.7374)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [2]  [ 900/1349]  eta: 0:02:20  lr: 0.000267  min_lr: 0.000006  loss: 0.9877 (1.0284)  loss_scale: 16384.0000 (16793.1454)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0001  max mem: 41808
Epoch: [2]  [ 910/1349]  eta: 0:02:17  lr: 0.000267  min_lr: 0.000006  loss: 1.0387 (1.0282)  loss_scale: 16384.0000 (16788.6542)  weight_decay: 0.0500 (0.0500)  time: 0.3079  data: 0.0001  max mem: 41808
Epoch: [2]  [ 920/1349]  eta: 0:02:14  lr: 0.000268  min_lr: 0.000006  loss: 0.9851 (1.0276)  loss_scale: 16384.0000 (16784.2606)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0001  max mem: 41808
Epoch: [2]  [ 930/1349]  eta: 0:02:11  lr: 0.000269  min_lr: 0.000006  loss: 0.9755 (1.0272)  loss_scale: 16384.0000 (16779.9613)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0002  max mem: 41808
[2025-05-23 15:28:24,284] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 15:28:24,284] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-05-23 15:28:24,284] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 15:28:24,284] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [2]  [ 940/1349]  eta: 0:02:08  lr: 0.000270  min_lr: 0.000006  loss: 0.9755 (1.0268)  loss_scale: 16384.0000 (16810.5760)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [2]  [ 950/1349]  eta: 0:02:04  lr: 0.000270  min_lr: 0.000006  loss: 1.0283 (1.0268)  loss_scale: 32768.0000 (16978.3722)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
[2025-05-23 15:28:30,437] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 3657
[2025-05-23 15:28:30,437] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-05-23 15:28:30,437] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 3657
[2025-05-23 15:28:30,437] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-05-23 15:28:30,437] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [2]  [ 960/1349]  eta: 0:02:01  lr: 0.000271  min_lr: 0.000006  loss: 1.0283 (1.0269)  loss_scale: 32768.0000 (17108.5786)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [2]  [ 970/1349]  eta: 0:01:58  lr: 0.000272  min_lr: 0.000006  loss: 1.0024 (1.0270)  loss_scale: 16384.0000 (17101.1164)  weight_decay: 0.0500 (0.0500)  time: 0.3088  data: 0.0001  max mem: 41808
Epoch: [2]  [ 980/1349]  eta: 0:01:55  lr: 0.000273  min_lr: 0.000006  loss: 1.0465 (1.0267)  loss_scale: 16384.0000 (17093.8063)  weight_decay: 0.0500 (0.0500)  time: 0.3088  data: 0.0001  max mem: 41808
Epoch: [2]  [ 990/1349]  eta: 0:01:52  lr: 0.000273  min_lr: 0.000006  loss: 1.0821 (1.0275)  loss_scale: 16384.0000 (17086.6438)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [2]  [1000/1349]  eta: 0:01:49  lr: 0.000274  min_lr: 0.000007  loss: 1.0538 (1.0266)  loss_scale: 16384.0000 (17079.6244)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [2]  [1010/1349]  eta: 0:01:46  lr: 0.000275  min_lr: 0.000007  loss: 0.9150 (1.0250)  loss_scale: 16384.0000 (17072.7438)  weight_decay: 0.0500 (0.0500)  time: 0.3081  data: 0.0001  max mem: 41808
Epoch: [2]  [1020/1349]  eta: 0:01:42  lr: 0.000276  min_lr: 0.000007  loss: 0.9700 (1.0253)  loss_scale: 16384.0000 (17065.9980)  weight_decay: 0.0500 (0.0500)  time: 0.3087  data: 0.0001  max mem: 41808
Epoch: [2]  [1030/1349]  eta: 0:01:39  lr: 0.000276  min_lr: 0.000007  loss: 1.0755 (1.0263)  loss_scale: 16384.0000 (17059.3831)  weight_decay: 0.0500 (0.0500)  time: 0.3082  data: 0.0001  max mem: 41808
Epoch: [2]  [1040/1349]  eta: 0:01:36  lr: 0.000277  min_lr: 0.000007  loss: 1.1250 (1.0268)  loss_scale: 16384.0000 (17052.8953)  weight_decay: 0.0500 (0.0500)  time: 0.3088  data: 0.0001  max mem: 41808
Epoch: [2]  [1050/1349]  eta: 0:01:33  lr: 0.000278  min_lr: 0.000007  loss: 1.1052 (1.0272)  loss_scale: 16384.0000 (17046.5309)  weight_decay: 0.0500 (0.0500)  time: 0.3080  data: 0.0001  max mem: 41808
Epoch: [2]  [1060/1349]  eta: 0:01:30  lr: 0.000279  min_lr: 0.000007  loss: 1.0722 (1.0273)  loss_scale: 16384.0000 (17040.2865)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [2]  [1070/1349]  eta: 0:01:27  lr: 0.000279  min_lr: 0.000007  loss: 1.0168 (1.0263)  loss_scale: 16384.0000 (17034.1587)  weight_decay: 0.0500 (0.0500)  time: 0.3085  data: 0.0001  max mem: 41808
Epoch: [2]  [1080/1349]  eta: 0:01:24  lr: 0.000280  min_lr: 0.000007  loss: 0.9440 (1.0262)  loss_scale: 16384.0000 (17028.1443)  weight_decay: 0.0500 (0.0500)  time: 0.3082  data: 0.0002  max mem: 41808
[2025-05-23 15:29:10,191] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 15:29:10,191] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-05-23 15:29:10,191] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 15:29:10,191] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [2]  [1090/1349]  eta: 0:01:20  lr: 0.000281  min_lr: 0.000007  loss: 0.9909 (1.0258)  loss_scale: 16384.0000 (17067.2924)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0002  max mem: 41808
[2025-05-23 15:29:11,424] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 3790
[2025-05-23 15:29:11,425] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 3790
[2025-05-23 15:29:11,425] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-05-23 15:29:11,425] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-05-23 15:29:11,425] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [2]  [1100/1349]  eta: 0:01:17  lr: 0.000282  min_lr: 0.000007  loss: 0.8882 (1.0240)  loss_scale: 16384.0000 (17075.9673)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0002  max mem: 41808
Epoch: [2]  [1110/1349]  eta: 0:01:14  lr: 0.000282  min_lr: 0.000007  loss: 0.8882 (1.0236)  loss_scale: 16384.0000 (17069.7390)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0002  max mem: 41808
Epoch: [2]  [1120/1349]  eta: 0:01:11  lr: 0.000283  min_lr: 0.000007  loss: 0.9888 (1.0240)  loss_scale: 16384.0000 (17063.6218)  weight_decay: 0.0500 (0.0500)  time: 0.3086  data: 0.0001  max mem: 41808
Epoch: [2]  [1130/1349]  eta: 0:01:08  lr: 0.000284  min_lr: 0.000007  loss: 1.0242 (1.0235)  loss_scale: 16384.0000 (17057.6127)  weight_decay: 0.0500 (0.0500)  time: 0.3092  data: 0.0001  max mem: 41808
Epoch: [2]  [1140/1349]  eta: 0:01:05  lr: 0.000285  min_lr: 0.000007  loss: 1.0341 (1.0236)  loss_scale: 16384.0000 (17051.7090)  weight_decay: 0.0500 (0.0500)  time: 0.3089  data: 0.0001  max mem: 41808
Epoch: [2]  [1150/1349]  eta: 0:01:02  lr: 0.000285  min_lr: 0.000007  loss: 1.0537 (1.0235)  loss_scale: 16384.0000 (17045.9079)  weight_decay: 0.0500 (0.0500)  time: 0.3082  data: 0.0001  max mem: 41808
Epoch: [2]  [1160/1349]  eta: 0:00:59  lr: 0.000286  min_lr: 0.000007  loss: 0.9904 (1.0232)  loss_scale: 16384.0000 (17040.2067)  weight_decay: 0.0500 (0.0500)  time: 0.3083  data: 0.0001  max mem: 41808
Epoch: [2]  [1170/1349]  eta: 0:00:55  lr: 0.000287  min_lr: 0.000007  loss: 1.0109 (1.0226)  loss_scale: 16384.0000 (17034.6029)  weight_decay: 0.0500 (0.0500)  time: 0.3081  data: 0.0001  max mem: 41808
Epoch: [2]  [1180/1349]  eta: 0:00:52  lr: 0.000288  min_lr: 0.000007  loss: 1.0352 (1.0226)  loss_scale: 16384.0000 (17029.0940)  weight_decay: 0.0500 (0.0500)  time: 0.3084  data: 0.0001  max mem: 41808
Epoch: [2]  [1190/1349]  eta: 0:00:49  lr: 0.000288  min_lr: 0.000007  loss: 1.0454 (1.0222)  loss_scale: 16384.0000 (17023.6776)  weight_decay: 0.0500 (0.0500)  time: 0.3094  data: 0.0001  max mem: 41808
Epoch: [2]  [1200/1349]  eta: 0:00:46  lr: 0.000289  min_lr: 0.000007  loss: 0.9734 (1.0222)  loss_scale: 16384.0000 (17018.3514)  weight_decay: 0.0500 (0.0500)  time: 0.3084  data: 0.0001  max mem: 41808
Epoch: [2]  [1210/1349]  eta: 0:00:43  lr: 0.000290  min_lr: 0.000007  loss: 0.9384 (1.0217)  loss_scale: 16384.0000 (17013.1131)  weight_decay: 0.0500 (0.0500)  time: 0.3079  data: 0.0001  max mem: 41808
Epoch: [2]  [1220/1349]  eta: 0:00:40  lr: 0.000290  min_lr: 0.000007  loss: 0.8852 (1.0210)  loss_scale: 16384.0000 (17007.9607)  weight_decay: 0.0500 (0.0500)  time: 0.3081  data: 0.0001  max mem: 41808
[2025-05-23 15:29:51,219] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 15:29:51,219] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-05-23 15:29:51,219] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 15:29:51,219] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [2]  [1230/1349]  eta: 0:00:37  lr: 0.000291  min_lr: 0.000007  loss: 0.8887 (1.0200)  loss_scale: 16384.0000 (17135.9870)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0001  max mem: 41808
[2025-05-23 15:29:55,219] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 3932
[2025-05-23 15:29:55,219] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 3932
[2025-05-23 15:29:55,220] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-05-23 15:29:55,220] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-05-23 15:29:55,220] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [2]  [1240/1349]  eta: 0:00:34  lr: 0.000292  min_lr: 0.000007  loss: 0.8887 (1.0195)  loss_scale: 32768.0000 (17169.5342)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [2]  [1250/1349]  eta: 0:00:30  lr: 0.000293  min_lr: 0.000007  loss: 1.0229 (1.0195)  loss_scale: 16384.0000 (17163.2550)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [2]  [1260/1349]  eta: 0:00:27  lr: 0.000293  min_lr: 0.000007  loss: 0.9945 (1.0189)  loss_scale: 16384.0000 (17157.0753)  weight_decay: 0.0500 (0.0500)  time: 0.3081  data: 0.0002  max mem: 41808
Epoch: [2]  [1270/1349]  eta: 0:00:24  lr: 0.000294  min_lr: 0.000007  loss: 0.9465 (1.0188)  loss_scale: 16384.0000 (17150.9929)  weight_decay: 0.0500 (0.0500)  time: 0.3081  data: 0.0001  max mem: 41808
Epoch: [2]  [1280/1349]  eta: 0:00:21  lr: 0.000295  min_lr: 0.000007  loss: 1.0421 (1.0191)  loss_scale: 16384.0000 (17145.0055)  weight_decay: 0.0500 (0.0500)  time: 0.3080  data: 0.0001  max mem: 41808
Epoch: [2]  [1290/1349]  eta: 0:00:18  lr: 0.000296  min_lr: 0.000007  loss: 1.0688 (1.0185)  loss_scale: 16384.0000 (17139.1108)  weight_decay: 0.0500 (0.0500)  time: 0.3079  data: 0.0001  max mem: 41808
Epoch: [2]  [1300/1349]  eta: 0:00:15  lr: 0.000296  min_lr: 0.000007  loss: 1.0041 (1.0185)  loss_scale: 16384.0000 (17133.3067)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
[2025-05-23 15:30:15,886] [INFO] [logging.py:96:log_dist] [Rank 0] step=4000, skipped=19, lr=[7.04369059966022e-06, 7.04369059966022e-06, 9.391587466213627e-06, 9.391587466213627e-06, 1.252211662161817e-05, 1.252211662161817e-05, 1.669615549549089e-05, 1.669615549549089e-05, 2.2261540660654524e-05, 2.2261540660654524e-05, 2.968205421420603e-05, 2.968205421420603e-05, 3.957607228560804e-05, 3.957607228560804e-05, 5.276809638081072e-05, 5.276809638081072e-05, 7.035746184108097e-05, 7.035746184108097e-05, 9.380994912144128e-05, 9.380994912144128e-05, 0.0001250799321619217, 0.0001250799321619217, 0.00016677324288256229, 0.00016677324288256229, 0.00022236432384341637, 0.00022236432384341637, 0.00029648576512455516, 0.00029648576512455516], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-05-23 15:30:15,887] [INFO] [timer.py:260:stop] epoch=0/micro_step=4000/global_step=4000, RunningAvgSamplesPerSec=212.545973460194, CurrSamplesPerSec=195.86609023681052, MemAllocated=1.37GB, MaxMemAllocated=40.83GB
Epoch: [2]  [1310/1349]  eta: 0:00:12  lr: 0.000297  min_lr: 0.000007  loss: 0.9926 (1.0180)  loss_scale: 16384.0000 (17127.5912)  weight_decay: 0.0500 (0.0500)  time: 0.3090  data: 0.0001  max mem: 41808
Epoch: [2]  [1320/1349]  eta: 0:00:09  lr: 0.000298  min_lr: 0.000007  loss: 0.9926 (1.0175)  loss_scale: 16384.0000 (17121.9621)  weight_decay: 0.0500 (0.0500)  time: 0.3089  data: 0.0001  max mem: 41808
Epoch: [2]  [1330/1349]  eta: 0:00:05  lr: 0.000299  min_lr: 0.000007  loss: 0.9348 (1.0170)  loss_scale: 16384.0000 (17116.4177)  weight_decay: 0.0500 (0.0500)  time: 0.3051  data: 0.0001  max mem: 41808
Epoch: [2]  [1340/1349]  eta: 0:00:02  lr: 0.000299  min_lr: 0.000007  loss: 0.9404 (1.0171)  loss_scale: 16384.0000 (17110.9560)  weight_decay: 0.0500 (0.0500)  time: 0.3024  data: 0.0001  max mem: 41808
Epoch: [2]  [1348/1349]  eta: 0:00:00  lr: 0.000300  min_lr: 0.000007  loss: 0.9219 (1.0159)  loss_scale: 16384.0000 (17106.6449)  weight_decay: 0.0500 (0.0500)  time: 0.3017  data: 0.0001  max mem: 41808
Epoch: [2] Total time: 0:07:00 (0.3119 s / it)
Averaged stats: lr: 0.000300  min_lr: 0.000007  loss: 0.9219 (1.0151)  loss_scale: 16384.0000 (17106.6449)  weight_decay: 0.0500 (0.0500)  total_time: 420.7808 (420.7657)
Val:  [  0/346]  eta: 1:28:27  loss: 1.7335 (1.7335)  acc1: 26.5625 (26.5625)  acc5: 90.6250 (90.6250)  time: 15.3383  data: 14.0925  max mem: 41808
Val:  [ 10/346]  eta: 0:11:29  loss: 0.1264 (0.4304)  acc1: 100.0000 (86.7188)  acc5: 100.0000 (99.0057)  time: 2.0514  data: 1.2814  max mem: 41808
Val:  [ 20/346]  eta: 0:07:44  loss: 0.1127 (0.3530)  acc1: 100.0000 (90.5134)  acc5: 100.0000 (99.4792)  time: 0.7278  data: 0.0003  max mem: 41808
Val:  [ 30/346]  eta: 0:06:20  loss: 0.1340 (0.3158)  acc1: 100.0000 (91.9859)  acc5: 100.0000 (99.6472)  time: 0.7386  data: 0.0003  max mem: 41808
Val:  [ 40/346]  eta: 0:05:35  loss: 0.1599 (0.3542)  acc1: 98.4375 (91.3681)  acc5: 100.0000 (99.3521)  time: 0.7534  data: 0.0311  max mem: 41808
Val:  [ 50/346]  eta: 0:05:09  loss: 0.1364 (0.3188)  acc1: 98.4375 (92.5858)  acc5: 100.0000 (99.4792)  time: 0.7968  data: 0.0957  max mem: 41808
Val:  [ 60/346]  eta: 0:04:48  loss: 0.1521 (0.3206)  acc1: 98.4375 (92.1875)  acc5: 100.0000 (99.5645)  time: 0.8298  data: 0.1313  max mem: 41808
Val:  [ 70/346]  eta: 0:04:31  loss: 0.1790 (0.3288)  acc1: 95.3125 (91.6593)  acc5: 100.0000 (99.6259)  time: 0.8339  data: 0.1266  max mem: 41808
Val:  [ 80/346]  eta: 0:04:17  loss: 0.2055 (0.3244)  acc1: 95.3125 (91.9367)  acc5: 100.0000 (99.6528)  time: 0.8369  data: 0.1285  max mem: 41808
Val:  [ 90/346]  eta: 0:04:04  loss: 0.2314 (0.3253)  acc1: 95.3125 (91.8355)  acc5: 100.0000 (99.6909)  time: 0.8469  data: 0.1415  max mem: 41808
Val:  [100/346]  eta: 0:03:52  loss: 0.2314 (0.3105)  acc1: 97.6562 (92.4892)  acc5: 100.0000 (99.7215)  time: 0.8504  data: 0.1426  max mem: 41808
Val:  [110/346]  eta: 0:03:40  loss: 0.1650 (0.3337)  acc1: 98.4375 (91.7230)  acc5: 100.0000 (99.6481)  time: 0.8517  data: 0.1537  max mem: 41808
Val:  [120/346]  eta: 0:03:30  loss: 0.1650 (0.3282)  acc1: 98.4375 (91.7807)  acc5: 100.0000 (99.6772)  time: 0.8620  data: 0.1614  max mem: 41808
Val:  [130/346]  eta: 0:03:19  loss: 0.1431 (0.3345)  acc1: 99.2188 (91.4480)  acc5: 100.0000 (99.6899)  time: 0.8573  data: 0.1478  max mem: 41808
Val:  [140/346]  eta: 0:03:09  loss: 0.1904 (0.3290)  acc1: 96.8750 (91.7387)  acc5: 100.0000 (99.7119)  time: 0.8462  data: 0.1449  max mem: 41808
Val:  [150/346]  eta: 0:02:59  loss: 0.2432 (0.3260)  acc1: 96.0938 (91.7891)  acc5: 100.0000 (99.7310)  time: 0.8497  data: 0.1476  max mem: 41808
Val:  [160/346]  eta: 0:02:49  loss: 0.2343 (0.3259)  acc1: 96.0938 (91.8818)  acc5: 100.0000 (99.7428)  time: 0.8661  data: 0.1522  max mem: 41808
Val:  [170/346]  eta: 0:02:40  loss: 0.1975 (0.3298)  acc1: 98.4375 (91.4839)  acc5: 100.0000 (99.7579)  time: 0.8839  data: 0.1562  max mem: 41808
Val:  [180/346]  eta: 0:02:30  loss: 0.3048 (0.3433)  acc1: 90.6250 (90.5344)  acc5: 100.0000 (99.7712)  time: 0.8746  data: 0.1527  max mem: 41808
Val:  [190/346]  eta: 0:02:21  loss: 0.3048 (0.3393)  acc1: 95.3125 (90.7723)  acc5: 100.0000 (99.7832)  time: 0.8523  data: 0.1549  max mem: 41808
Val:  [200/346]  eta: 0:02:12  loss: 0.3128 (0.3521)  acc1: 91.4062 (90.3335)  acc5: 100.0000 (99.7435)  time: 0.8786  data: 0.1561  max mem: 41808
Val:  [210/346]  eta: 0:02:03  loss: 0.1896 (0.3456)  acc1: 97.6562 (90.5880)  acc5: 100.0000 (99.7556)  time: 0.9139  data: 0.1539  max mem: 41808
Val:  [220/346]  eta: 0:01:54  loss: 0.1307 (0.3424)  acc1: 100.0000 (90.7204)  acc5: 100.0000 (99.7384)  time: 0.9064  data: 0.1549  max mem: 41808
Val:  [230/346]  eta: 0:01:44  loss: 0.1388 (0.3355)  acc1: 100.0000 (91.0173)  acc5: 100.0000 (99.7497)  time: 0.8768  data: 0.1565  max mem: 41808
Val:  [240/346]  eta: 0:01:35  loss: 0.1898 (0.3379)  acc1: 97.6562 (90.9297)  acc5: 100.0000 (99.7536)  time: 0.8609  data: 0.1574  max mem: 41808
Val:  [250/346]  eta: 0:01:26  loss: 0.1898 (0.3373)  acc1: 96.8750 (90.9580)  acc5: 100.0000 (99.7603)  time: 0.8536  data: 0.1537  max mem: 41808
Val:  [260/346]  eta: 0:01:17  loss: 0.1425 (0.3366)  acc1: 97.6562 (90.9872)  acc5: 100.0000 (99.7635)  time: 0.8644  data: 0.1549  max mem: 41808
Val:  [270/346]  eta: 0:01:08  loss: 0.1294 (0.3312)  acc1: 97.6562 (91.1872)  acc5: 100.0000 (99.7723)  time: 0.8859  data: 0.1519  max mem: 41808
Val:  [280/346]  eta: 0:00:59  loss: 0.1037 (0.3276)  acc1: 100.0000 (91.3173)  acc5: 100.0000 (99.7804)  time: 0.8733  data: 0.1495  max mem: 41808
Val:  [290/346]  eta: 0:00:50  loss: 0.1037 (0.3213)  acc1: 100.0000 (91.5727)  acc5: 100.0000 (99.7879)  time: 0.8507  data: 0.1611  max mem: 41808
Val:  [300/346]  eta: 0:00:41  loss: 0.1159 (0.3237)  acc1: 100.0000 (91.6165)  acc5: 100.0000 (99.6600)  time: 0.8394  data: 0.1547  max mem: 41808
Val:  [310/346]  eta: 0:00:32  loss: 0.1451 (0.3250)  acc1: 98.4375 (91.4967)  acc5: 100.0000 (99.6709)  time: 0.8287  data: 0.1415  max mem: 41808
Val:  [320/346]  eta: 0:00:23  loss: 0.1278 (0.3273)  acc1: 100.0000 (91.4501)  acc5: 100.0000 (99.6812)  time: 0.8350  data: 0.1421  max mem: 41808
Val:  [330/346]  eta: 0:00:14  loss: 0.2461 (0.3359)  acc1: 94.5312 (91.1867)  acc5: 100.0000 (99.6578)  time: 0.8392  data: 0.1477  max mem: 41808
Val:  [340/346]  eta: 0:00:05  loss: 0.3031 (0.3419)  acc1: 92.9688 (90.9114)  acc5: 100.0000 (99.6632)  time: 0.8372  data: 0.1522  max mem: 41808
Val:  [345/346]  eta: 0:00:00  loss: 0.1944 (0.3390)  acc1: 96.5517 (91.0209)  acc5: 100.0000 (99.6678)  time: 0.8404  data: 0.1667  max mem: 41808
Val: Total time: 0:05:06 (0.8872 s / it)
* Acc@1 91.133 Acc@5 99.678 loss 0.337
Accuracy of the network on the 88494 val videos: 91.1%
[2025-05-23 15:35:37,483] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-05-23 15:35:37,486] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /project/mmendoscope/Natural_Comparison/Cholec75/unified_base_st_settingE_videomae-st_Cholec80_0.0005_0.75_online_key_frame_frame16_Fixed_Stride_4/checkpoint-best/mp_rank_00_model_states.pt
[2025-05-23 15:35:37,486] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2025-05-23 15:35:37,486] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /project/mmendoscope/Natural_Comparison/Cholec75/unified_base_st_settingE_videomae-st_Cholec80_0.0005_0.75_online_key_frame_frame16_Fixed_Stride_4/checkpoint-best/mp_rank_00_model_states.pt...
[2025-05-23 15:35:40,338] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /project/mmendoscope/Natural_Comparison/Cholec75/unified_base_st_settingE_videomae-st_Cholec80_0.0005_0.75_online_key_frame_frame16_Fixed_Stride_4/checkpoint-best/mp_rank_00_model_states.pt.
[2025-05-23 15:35:40,339] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 91.13%   Max Epoch: 2
Epoch: [3]  [   0/1349]  eta: 1:55:04  lr: 0.000300  min_lr: 0.000007  loss: 1.2326 (1.2326)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 5.1181  data: 4.7795  max mem: 41808
Epoch: [3]  [  10/1349]  eta: 0:16:47  lr: 0.000301  min_lr: 0.000007  loss: 0.9694 (1.0259)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7521  data: 0.4346  max mem: 41808
[2025-05-23 15:35:49,865] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 15:35:49,866] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-05-23 15:35:49,866] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 15:35:49,866] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-05-23 15:35:50,480] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 4063
[2025-05-23 15:35:50,480] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-05-23 15:35:50,480] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 4063
[2025-05-23 15:35:50,480] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-05-23 15:35:50,480] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [3]  [  20/1349]  eta: 0:11:58  lr: 0.000302  min_lr: 0.000007  loss: 1.0597 (1.0570)  loss_scale: 16384.0000 (17944.3810)  weight_decay: 0.0500 (0.0500)  time: 0.3121  data: 0.0002  max mem: 41808
Epoch: [3]  [  30/1349]  eta: 0:10:13  lr: 0.000302  min_lr: 0.000007  loss: 1.0470 (1.0418)  loss_scale: 16384.0000 (17441.0323)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [3]  [  40/1349]  eta: 0:09:18  lr: 0.000303  min_lr: 0.000007  loss: 1.0352 (1.0197)  loss_scale: 16384.0000 (17183.2195)  weight_decay: 0.0500 (0.0500)  time: 0.3061  data: 0.0001  max mem: 41808
Epoch: [3]  [  50/1349]  eta: 0:08:43  lr: 0.000304  min_lr: 0.000007  loss: 0.9928 (1.0237)  loss_scale: 16384.0000 (17026.5098)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
Epoch: [3]  [  60/1349]  eta: 0:08:19  lr: 0.000304  min_lr: 0.000007  loss: 0.9853 (1.0121)  loss_scale: 16384.0000 (16921.1803)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [3]  [  70/1349]  eta: 0:08:00  lr: 0.000305  min_lr: 0.000007  loss: 0.9415 (1.0000)  loss_scale: 16384.0000 (16845.5211)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
Epoch: [3]  [  80/1349]  eta: 0:07:46  lr: 0.000306  min_lr: 0.000007  loss: 0.8819 (0.9964)  loss_scale: 16384.0000 (16788.5432)  weight_decay: 0.0500 (0.0500)  time: 0.3066  data: 0.0001  max mem: 41808
Epoch: [3]  [  90/1349]  eta: 0:07:34  lr: 0.000307  min_lr: 0.000007  loss: 1.1292 (1.0200)  loss_scale: 16384.0000 (16744.0879)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
Epoch: [3]  [ 100/1349]  eta: 0:07:23  lr: 0.000307  min_lr: 0.000007  loss: 1.1063 (1.0150)  loss_scale: 16384.0000 (16708.4356)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
Epoch: [3]  [ 110/1349]  eta: 0:07:14  lr: 0.000308  min_lr: 0.000007  loss: 0.9170 (0.9998)  loss_scale: 16384.0000 (16679.2072)  weight_decay: 0.0500 (0.0500)  time: 0.3061  data: 0.0001  max mem: 41808
Epoch: [3]  [ 120/1349]  eta: 0:07:06  lr: 0.000309  min_lr: 0.000007  loss: 0.9421 (1.0040)  loss_scale: 16384.0000 (16654.8099)  weight_decay: 0.0500 (0.0500)  time: 0.3058  data: 0.0001  max mem: 41808
Epoch: [3]  [ 130/1349]  eta: 0:06:59  lr: 0.000310  min_lr: 0.000007  loss: 1.0546 (1.0060)  loss_scale: 16384.0000 (16634.1374)  weight_decay: 0.0500 (0.0500)  time: 0.3060  data: 0.0001  max mem: 41808
Epoch: [3]  [ 140/1349]  eta: 0:06:52  lr: 0.000310  min_lr: 0.000007  loss: 1.0953 (1.0108)  loss_scale: 16384.0000 (16616.3972)  weight_decay: 0.0500 (0.0500)  time: 0.3060  data: 0.0001  max mem: 41808
[2025-05-23 15:36:30,024] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 15:36:30,024] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-05-23 15:36:30,024] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 15:36:30,024] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [3]  [ 150/1349]  eta: 0:06:46  lr: 0.000311  min_lr: 0.000007  loss: 0.9862 (1.0006)  loss_scale: 16384.0000 (17252.0265)  weight_decay: 0.0500 (0.0500)  time: 0.3063  data: 0.0001  max mem: 41808
Epoch: [3]  [ 160/1349]  eta: 0:06:40  lr: 0.000312  min_lr: 0.000007  loss: 0.8632 (0.9978)  loss_scale: 32768.0000 (18215.7516)  weight_decay: 0.0500 (0.0500)  time: 0.3064  data: 0.0001  max mem: 41808
[2025-05-23 15:36:37,071] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 4215
[2025-05-23 15:36:37,072] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-05-23 15:36:37,072] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 4215
[2025-05-23 15:36:37,072] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-05-23 15:36:37,072] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [3]  [ 170/1349]  eta: 0:06:35  lr: 0.000313  min_lr: 0.000007  loss: 1.0453 (1.0049)  loss_scale: 32768.0000 (18779.3216)  weight_decay: 0.0500 (0.0500)  time: 0.3061  data: 0.0002  max mem: 41808
Epoch: [3]  [ 180/1349]  eta: 0:06:30  lr: 0.000313  min_lr: 0.000007  loss: 1.0448 (1.0004)  loss_scale: 16384.0000 (18646.9834)  weight_decay: 0.0500 (0.0500)  time: 0.3063  data: 0.0001  max mem: 41808
Epoch: [3]  [ 190/1349]  eta: 0:06:25  lr: 0.000314  min_lr: 0.000007  loss: 0.9151 (0.9986)  loss_scale: 16384.0000 (18528.5026)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
Epoch: [3]  [ 200/1349]  eta: 0:06:20  lr: 0.000315  min_lr: 0.000007  loss: 0.9151 (0.9964)  loss_scale: 16384.0000 (18421.8109)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0001  max mem: 41808
Epoch: [3]  [ 210/1349]  eta: 0:06:15  lr: 0.000316  min_lr: 0.000007  loss: 0.9914 (0.9983)  loss_scale: 16384.0000 (18325.2322)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [3]  [ 220/1349]  eta: 0:06:11  lr: 0.000316  min_lr: 0.000008  loss: 1.0003 (0.9990)  loss_scale: 16384.0000 (18237.3937)  weight_decay: 0.0500 (0.0500)  time: 0.3062  data: 0.0002  max mem: 41808
Epoch: [3]  [ 230/1349]  eta: 0:06:06  lr: 0.000317  min_lr: 0.000008  loss: 0.8877 (0.9919)  loss_scale: 16384.0000 (18157.1602)  weight_decay: 0.0500 (0.0500)  time: 0.3062  data: 0.0001  max mem: 41808
Epoch: [3]  [ 240/1349]  eta: 0:06:02  lr: 0.000318  min_lr: 0.000008  loss: 0.8266 (0.9872)  loss_scale: 16384.0000 (18083.5851)  weight_decay: 0.0500 (0.0500)  time: 0.3062  data: 0.0001  max mem: 41808
Epoch: [3]  [ 250/1349]  eta: 0:05:58  lr: 0.000319  min_lr: 0.000008  loss: 0.9792 (0.9862)  loss_scale: 16384.0000 (18015.8725)  weight_decay: 0.0500 (0.0500)  time: 0.3059  data: 0.0001  max mem: 41808
Epoch: [3]  [ 260/1349]  eta: 0:05:54  lr: 0.000319  min_lr: 0.000008  loss: 0.9690 (0.9861)  loss_scale: 16384.0000 (17953.3487)  weight_decay: 0.0500 (0.0500)  time: 0.3061  data: 0.0001  max mem: 41808
Epoch: [3]  [ 270/1349]  eta: 0:05:50  lr: 0.000320  min_lr: 0.000008  loss: 0.9690 (0.9861)  loss_scale: 16384.0000 (17895.4391)  weight_decay: 0.0500 (0.0500)  time: 0.3066  data: 0.0001  max mem: 41808
Epoch: [3]  [ 280/1349]  eta: 0:05:46  lr: 0.000321  min_lr: 0.000008  loss: 1.0062 (0.9870)  loss_scale: 16384.0000 (17841.6512)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
Epoch: [3]  [ 290/1349]  eta: 0:05:42  lr: 0.000322  min_lr: 0.000008  loss: 0.9519 (0.9854)  loss_scale: 16384.0000 (17791.5601)  weight_decay: 0.0500 (0.0500)  time: 0.3100  data: 0.0001  max mem: 41808
[2025-05-23 15:37:16,720] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 15:37:16,720] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-05-23 15:37:16,720] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 15:37:16,721] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-05-23 15:37:17,027] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 4345
[2025-05-23 15:37:17,027] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-05-23 15:37:17,027] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 4345
[2025-05-23 15:37:17,027] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2025-05-23 15:37:17,027] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [3]  [ 300/1349]  eta: 0:05:38  lr: 0.000322  min_lr: 0.000008  loss: 0.9516 (0.9858)  loss_scale: 16384.0000 (17799.2292)  weight_decay: 0.0500 (0.0500)  time: 0.3106  data: 0.0001  max mem: 41808
Epoch: [3]  [ 310/1349]  eta: 0:05:35  lr: 0.000323  min_lr: 0.000008  loss: 0.9304 (0.9850)  loss_scale: 16384.0000 (17753.7235)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
Epoch: [3]  [ 320/1349]  eta: 0:05:31  lr: 0.000324  min_lr: 0.000008  loss: 0.9344 (0.9831)  loss_scale: 16384.0000 (17711.0530)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [3]  [ 330/1349]  eta: 0:05:27  lr: 0.000325  min_lr: 0.000008  loss: 0.9251 (0.9795)  loss_scale: 16384.0000 (17670.9607)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [3]  [ 340/1349]  eta: 0:05:24  lr: 0.000325  min_lr: 0.000008  loss: 0.9066 (0.9785)  loss_scale: 16384.0000 (17633.2199)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
[2025-05-23 15:37:30,856] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 4390
[2025-05-23 15:37:30,856] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2025-05-23 15:37:30,856] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
[2025-05-23 15:37:30,856] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 4390
[2025-05-23 15:37:30,857] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
Epoch: [3]  [ 350/1349]  eta: 0:05:20  lr: 0.000326  min_lr: 0.000008  loss: 0.9581 (0.9790)  loss_scale: 16384.0000 (17410.9174)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [3]  [ 360/1349]  eta: 0:05:16  lr: 0.000327  min_lr: 0.000008  loss: 0.9615 (0.9777)  loss_scale: 8192.0000 (17155.5457)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0002  max mem: 41808
Epoch: [3]  [ 370/1349]  eta: 0:05:13  lr: 0.000327  min_lr: 0.000008  loss: 1.1025 (0.9821)  loss_scale: 8192.0000 (16913.9407)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0002  max mem: 41808
Epoch: [3]  [ 380/1349]  eta: 0:05:09  lr: 0.000328  min_lr: 0.000008  loss: 1.1283 (0.9839)  loss_scale: 8192.0000 (16685.0184)  weight_decay: 0.0500 (0.0500)  time: 0.3079  data: 0.0001  max mem: 41808
Epoch: [3]  [ 390/1349]  eta: 0:05:06  lr: 0.000329  min_lr: 0.000008  loss: 1.0358 (0.9823)  loss_scale: 8192.0000 (16467.8056)  weight_decay: 0.0500 (0.0500)  time: 0.3081  data: 0.0001  max mem: 41808
Epoch: [3]  [ 400/1349]  eta: 0:05:02  lr: 0.000330  min_lr: 0.000008  loss: 0.8672 (0.9807)  loss_scale: 8192.0000 (16261.4264)  weight_decay: 0.0500 (0.0500)  time: 0.3080  data: 0.0001  max mem: 41808
Epoch: [3]  [ 410/1349]  eta: 0:04:59  lr: 0.000330  min_lr: 0.000008  loss: 0.8672 (0.9812)  loss_scale: 8192.0000 (16065.0900)  weight_decay: 0.0500 (0.0500)  time: 0.3082  data: 0.0002  max mem: 41808
Epoch: [3]  [ 420/1349]  eta: 0:04:56  lr: 0.000331  min_lr: 0.000008  loss: 0.7954 (0.9784)  loss_scale: 8192.0000 (15878.0808)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0001  max mem: 41808
Epoch: [3]  [ 430/1349]  eta: 0:04:52  lr: 0.000332  min_lr: 0.000008  loss: 0.9119 (0.9800)  loss_scale: 8192.0000 (15699.7494)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [3]  [ 440/1349]  eta: 0:04:49  lr: 0.000333  min_lr: 0.000008  loss: 0.9119 (0.9778)  loss_scale: 8192.0000 (15529.5057)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [3]  [ 450/1349]  eta: 0:04:45  lr: 0.000333  min_lr: 0.000008  loss: 0.8737 (0.9770)  loss_scale: 8192.0000 (15366.8115)  weight_decay: 0.0500 (0.0500)  time: 0.3066  data: 0.0001  max mem: 41808
Epoch: [3]  [ 460/1349]  eta: 0:04:42  lr: 0.000334  min_lr: 0.000008  loss: 0.9955 (0.9794)  loss_scale: 8192.0000 (15211.1757)  weight_decay: 0.0500 (0.0500)  time: 0.3078  data: 0.0001  max mem: 41808
Epoch: [3]  [ 470/1349]  eta: 0:04:39  lr: 0.000335  min_lr: 0.000008  loss: 0.9356 (0.9791)  loss_scale: 8192.0000 (15062.1486)  weight_decay: 0.0500 (0.0500)  time: 0.3086  data: 0.0001  max mem: 41808
[2025-05-23 15:38:10,563] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 15:38:10,563] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 15:38:10,563] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2025-05-23 15:38:10,563] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [3]  [ 480/1349]  eta: 0:04:35  lr: 0.000336  min_lr: 0.000008  loss: 0.9348 (0.9787)  loss_scale: 8192.0000 (15072.5988)  weight_decay: 0.0500 (0.0500)  time: 0.3084  data: 0.0001  max mem: 41808
Epoch: [3]  [ 490/1349]  eta: 0:04:32  lr: 0.000336  min_lr: 0.000008  loss: 0.9848 (0.9791)  loss_scale: 16384.0000 (15099.3075)  weight_decay: 0.0500 (0.0500)  time: 0.3083  data: 0.0001  max mem: 41808
Epoch: [3]  [ 500/1349]  eta: 0:04:29  lr: 0.000337  min_lr: 0.000008  loss: 1.0238 (0.9788)  loss_scale: 16384.0000 (15124.9501)  weight_decay: 0.0500 (0.0500)  time: 0.3080  data: 0.0001  max mem: 41808
Epoch: [3]  [ 510/1349]  eta: 0:04:25  lr: 0.000338  min_lr: 0.000008  loss: 0.9998 (0.9796)  loss_scale: 16384.0000 (15149.5890)  weight_decay: 0.0500 (0.0500)  time: 0.3082  data: 0.0001  max mem: 41808
Epoch: [3]  [ 520/1349]  eta: 0:04:22  lr: 0.000339  min_lr: 0.000008  loss: 0.9877 (0.9781)  loss_scale: 16384.0000 (15173.2821)  weight_decay: 0.0500 (0.0500)  time: 0.3081  data: 0.0002  max mem: 41808
Epoch: [3]  [ 530/1349]  eta: 0:04:19  lr: 0.000339  min_lr: 0.000008  loss: 0.9920 (0.9790)  loss_scale: 16384.0000 (15196.0829)  weight_decay: 0.0500 (0.0500)  time: 0.3080  data: 0.0002  max mem: 41808
Epoch: [3]  [ 540/1349]  eta: 0:04:15  lr: 0.000340  min_lr: 0.000008  loss: 1.0201 (0.9799)  loss_scale: 16384.0000 (15218.0407)  weight_decay: 0.0500 (0.0500)  time: 0.3082  data: 0.0002  max mem: 41808
Epoch: [3]  [ 550/1349]  eta: 0:04:12  lr: 0.000341  min_lr: 0.000008  loss: 1.0147 (0.9794)  loss_scale: 16384.0000 (15239.2015)  weight_decay: 0.0500 (0.0500)  time: 0.3084  data: 0.0001  max mem: 41808
Epoch: [3]  [ 560/1349]  eta: 0:04:09  lr: 0.000342  min_lr: 0.000008  loss: 0.9509 (0.9773)  loss_scale: 16384.0000 (15259.6078)  weight_decay: 0.0500 (0.0500)  time: 0.3081  data: 0.0002  max mem: 41808
Epoch: [3]  [ 570/1349]  eta: 0:04:06  lr: 0.000342  min_lr: 0.000008  loss: 0.9437 (0.9772)  loss_scale: 16384.0000 (15279.2995)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [3]  [ 580/1349]  eta: 0:04:02  lr: 0.000343  min_lr: 0.000008  loss: 0.8695 (0.9764)  loss_scale: 16384.0000 (15298.3133)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [3]  [ 590/1349]  eta: 0:03:59  lr: 0.000344  min_lr: 0.000008  loss: 1.0104 (0.9768)  loss_scale: 16384.0000 (15316.6836)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
[2025-05-23 15:38:49,971] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 15:38:49,971] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 15:38:49,971] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-05-23 15:38:49,971] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [3]  [ 600/1349]  eta: 0:03:56  lr: 0.000345  min_lr: 0.000008  loss: 1.0527 (0.9760)  loss_scale: 16384.0000 (15361.7038)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
[2025-05-23 15:38:50,279] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 4648
[2025-05-23 15:38:50,279] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 4648
[2025-05-23 15:38:50,279] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-05-23 15:38:50,279] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-05-23 15:38:50,280] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [3]  [ 610/1349]  eta: 0:03:52  lr: 0.000345  min_lr: 0.000008  loss: 1.0650 (0.9770)  loss_scale: 16384.0000 (15378.4354)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [3]  [ 620/1349]  eta: 0:03:49  lr: 0.000346  min_lr: 0.000008  loss: 0.9859 (0.9771)  loss_scale: 16384.0000 (15394.6280)  weight_decay: 0.0500 (0.0500)  time: 0.3091  data: 0.0001  max mem: 41808
Epoch: [3]  [ 630/1349]  eta: 0:03:46  lr: 0.000347  min_lr: 0.000008  loss: 0.9859 (0.9784)  loss_scale: 16384.0000 (15410.3074)  weight_decay: 0.0500 (0.0500)  time: 0.3101  data: 0.0001  max mem: 41808
Epoch: [3]  [ 640/1349]  eta: 0:03:43  lr: 0.000347  min_lr: 0.000008  loss: 1.0760 (0.9792)  loss_scale: 16384.0000 (15425.4977)  weight_decay: 0.0500 (0.0500)  time: 0.3085  data: 0.0001  max mem: 41808
Epoch: [3]  [ 650/1349]  eta: 0:03:40  lr: 0.000348  min_lr: 0.000008  loss: 0.9362 (0.9775)  loss_scale: 16384.0000 (15440.2212)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [3]  [ 660/1349]  eta: 0:03:36  lr: 0.000349  min_lr: 0.000008  loss: 0.8860 (0.9779)  loss_scale: 16384.0000 (15454.4992)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
Epoch: [3]  [ 670/1349]  eta: 0:03:33  lr: 0.000350  min_lr: 0.000008  loss: 1.0391 (0.9775)  loss_scale: 16384.0000 (15468.3517)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
Epoch: [3]  [ 680/1349]  eta: 0:03:30  lr: 0.000350  min_lr: 0.000008  loss: 1.0391 (0.9779)  loss_scale: 16384.0000 (15481.7974)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
Epoch: [3]  [ 690/1349]  eta: 0:03:27  lr: 0.000351  min_lr: 0.000008  loss: 1.0670 (0.9793)  loss_scale: 16384.0000 (15494.8538)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
Epoch: [3]  [ 700/1349]  eta: 0:03:23  lr: 0.000352  min_lr: 0.000008  loss: 1.0670 (0.9802)  loss_scale: 16384.0000 (15507.5378)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0001  max mem: 41808
Epoch: [3]  [ 710/1349]  eta: 0:03:20  lr: 0.000353  min_lr: 0.000008  loss: 0.9918 (0.9794)  loss_scale: 16384.0000 (15519.8650)  weight_decay: 0.0500 (0.0500)  time: 0.3062  data: 0.0001  max mem: 41808
Epoch: [3]  [ 720/1349]  eta: 0:03:17  lr: 0.000353  min_lr: 0.000008  loss: 0.9918 (0.9800)  loss_scale: 16384.0000 (15531.8502)  weight_decay: 0.0500 (0.0500)  time: 0.3062  data: 0.0001  max mem: 41808
[2025-05-23 15:39:27,785] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 4770
[2025-05-23 15:39:27,786] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2025-05-23 15:39:27,786] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
[2025-05-23 15:39:27,786] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 4770
[2025-05-23 15:39:27,786] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
Epoch: [3]  [ 730/1349]  eta: 0:03:14  lr: 0.000354  min_lr: 0.000008  loss: 1.0270 (0.9804)  loss_scale: 16384.0000 (15453.8550)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0002  max mem: 41808
Epoch: [3]  [ 740/1349]  eta: 0:03:11  lr: 0.000355  min_lr: 0.000008  loss: 0.9138 (0.9794)  loss_scale: 8192.0000 (15355.8543)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0002  max mem: 41808
Epoch: [3]  [ 750/1349]  eta: 0:03:07  lr: 0.000356  min_lr: 0.000008  loss: 1.0296 (0.9805)  loss_scale: 8192.0000 (15260.4634)  weight_decay: 0.0500 (0.0500)  time: 0.3080  data: 0.0002  max mem: 41808
Epoch: [3]  [ 760/1349]  eta: 0:03:04  lr: 0.000356  min_lr: 0.000008  loss: 1.0297 (0.9810)  loss_scale: 8192.0000 (15167.5795)  weight_decay: 0.0500 (0.0500)  time: 0.3083  data: 0.0001  max mem: 41808
Epoch: [3]  [ 770/1349]  eta: 0:03:01  lr: 0.000357  min_lr: 0.000008  loss: 1.0077 (0.9812)  loss_scale: 8192.0000 (15077.1051)  weight_decay: 0.0500 (0.0500)  time: 0.3081  data: 0.0001  max mem: 41808
Epoch: [3]  [ 780/1349]  eta: 0:02:58  lr: 0.000358  min_lr: 0.000009  loss: 0.7995 (0.9788)  loss_scale: 8192.0000 (14988.9475)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
Epoch: [3]  [ 790/1349]  eta: 0:02:55  lr: 0.000359  min_lr: 0.000009  loss: 0.8157 (0.9792)  loss_scale: 8192.0000 (14903.0190)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
Epoch: [3]  [ 800/1349]  eta: 0:02:52  lr: 0.000359  min_lr: 0.000009  loss: 0.8965 (0.9786)  loss_scale: 8192.0000 (14819.2360)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
Epoch: [3]  [ 810/1349]  eta: 0:02:48  lr: 0.000360  min_lr: 0.000009  loss: 1.0431 (0.9790)  loss_scale: 8192.0000 (14737.5191)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [3]  [ 820/1349]  eta: 0:02:45  lr: 0.000361  min_lr: 0.000009  loss: 1.0303 (0.9786)  loss_scale: 8192.0000 (14657.7929)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
Epoch: [3]  [ 830/1349]  eta: 0:02:42  lr: 0.000362  min_lr: 0.000009  loss: 0.9073 (0.9775)  loss_scale: 8192.0000 (14579.9856)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [3]  [ 840/1349]  eta: 0:02:39  lr: 0.000362  min_lr: 0.000009  loss: 0.9348 (0.9773)  loss_scale: 8192.0000 (14504.0285)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
Epoch: [3]  [ 850/1349]  eta: 0:02:36  lr: 0.000363  min_lr: 0.000009  loss: 1.0409 (0.9778)  loss_scale: 8192.0000 (14429.8566)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
[2025-05-23 15:40:07,442] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 15:40:07,443] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2025-05-23 15:40:07,443] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 15:40:07,443] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [3]  [ 860/1349]  eta: 0:02:33  lr: 0.000364  min_lr: 0.000009  loss: 0.9631 (0.9772)  loss_scale: 8192.0000 (14443.0383)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
[2025-05-23 15:40:10,206] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 4908
[2025-05-23 15:40:10,206] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 4908
[2025-05-23 15:40:10,206] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2025-05-23 15:40:10,206] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2025-05-23 15:40:10,206] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [3]  [ 870/1349]  eta: 0:02:29  lr: 0.000365  min_lr: 0.000009  loss: 0.9429 (0.9771)  loss_scale: 8192.0000 (14371.2698)  weight_decay: 0.0500 (0.0500)  time: 0.3087  data: 0.0001  max mem: 41808
Epoch: [3]  [ 880/1349]  eta: 0:02:26  lr: 0.000365  min_lr: 0.000009  loss: 1.0242 (0.9770)  loss_scale: 8192.0000 (14301.1305)  weight_decay: 0.0500 (0.0500)  time: 0.3087  data: 0.0001  max mem: 41808
Epoch: [3]  [ 890/1349]  eta: 0:02:23  lr: 0.000366  min_lr: 0.000009  loss: 0.9279 (0.9764)  loss_scale: 8192.0000 (14232.5657)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [3]  [ 900/1349]  eta: 0:02:20  lr: 0.000367  min_lr: 0.000009  loss: 0.9269 (0.9757)  loss_scale: 8192.0000 (14165.5228)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
Epoch: [3]  [ 910/1349]  eta: 0:02:17  lr: 0.000368  min_lr: 0.000009  loss: 0.9298 (0.9754)  loss_scale: 8192.0000 (14099.9517)  weight_decay: 0.0500 (0.0500)  time: 0.3066  data: 0.0001  max mem: 41808
Epoch: [3]  [ 920/1349]  eta: 0:02:14  lr: 0.000368  min_lr: 0.000009  loss: 0.9537 (0.9752)  loss_scale: 8192.0000 (14035.8046)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
Epoch: [3]  [ 930/1349]  eta: 0:02:10  lr: 0.000369  min_lr: 0.000009  loss: 1.0754 (0.9764)  loss_scale: 8192.0000 (13973.0354)  weight_decay: 0.0500 (0.0500)  time: 0.3080  data: 0.0001  max mem: 41808
Epoch: [3]  [ 940/1349]  eta: 0:02:07  lr: 0.000370  min_lr: 0.000009  loss: 1.0927 (0.9769)  loss_scale: 8192.0000 (13911.6004)  weight_decay: 0.0500 (0.0500)  time: 0.3081  data: 0.0001  max mem: 41808
Epoch: [3]  [ 950/1349]  eta: 0:02:04  lr: 0.000370  min_lr: 0.000009  loss: 0.9851 (0.9770)  loss_scale: 8192.0000 (13851.4574)  weight_decay: 0.0500 (0.0500)  time: 0.3080  data: 0.0001  max mem: 41808
[2025-05-23 15:40:38,254] [INFO] [logging.py:96:log_dist] [Rank 0] step=5000, skipped=26, lr=[8.80505359032294e-06, 8.80505359032294e-06, 1.174007145376392e-05, 1.174007145376392e-05, 1.565342860501856e-05, 1.565342860501856e-05, 2.0871238140024746e-05, 2.0871238140024746e-05, 2.7828317520032997e-05, 2.7828317520032997e-05, 3.7104423360044e-05, 3.7104423360044e-05, 4.947256448005866e-05, 4.947256448005866e-05, 6.596341930674488e-05, 6.596341930674488e-05, 8.795122574232651e-05, 8.795122574232651e-05, 0.00011726830098976867, 0.00011726830098976867, 0.0001563577346530249, 0.0001563577346530249, 0.00020847697953736653, 0.00020847697953736653, 0.0002779693060498221, 0.0002779693060498221, 0.00037062574139976273, 0.00037062574139976273], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-05-23 15:40:38,255] [INFO] [timer.py:260:stop] epoch=0/micro_step=5000/global_step=5000, RunningAvgSamplesPerSec=212.70730871001564, CurrSamplesPerSec=197.4012194018886, MemAllocated=1.37GB, MaxMemAllocated=40.83GB
Epoch: [3]  [ 960/1349]  eta: 0:02:01  lr: 0.000371  min_lr: 0.000009  loss: 0.9851 (0.9771)  loss_scale: 8192.0000 (13792.5661)  weight_decay: 0.0500 (0.0500)  time: 0.3085  data: 0.0002  max mem: 41808
Epoch: [3]  [ 970/1349]  eta: 0:01:58  lr: 0.000372  min_lr: 0.000009  loss: 1.0104 (0.9769)  loss_scale: 8192.0000 (13734.8877)  weight_decay: 0.0500 (0.0500)  time: 0.3083  data: 0.0001  max mem: 41808
Epoch: [3]  [ 980/1349]  eta: 0:01:55  lr: 0.000373  min_lr: 0.000009  loss: 0.9191 (0.9764)  loss_scale: 8192.0000 (13678.3853)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
[2025-05-23 15:40:49,921] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 15:40:49,921] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2025-05-23 15:40:49,921] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 15:40:49,921] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [3]  [ 990/1349]  eta: 0:01:52  lr: 0.000373  min_lr: 0.000009  loss: 1.0077 (0.9768)  loss_scale: 8192.0000 (13631.2896)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [3]  [1000/1349]  eta: 0:01:48  lr: 0.000374  min_lr: 0.000009  loss: 1.0768 (0.9774)  loss_scale: 16384.0000 (13658.7892)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0002  max mem: 41808
[2025-05-23 15:40:53,605] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 5049
[2025-05-23 15:40:53,605] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 5049
[2025-05-23 15:40:53,605] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2025-05-23 15:40:53,605] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2025-05-23 15:40:53,605] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [3]  [1010/1349]  eta: 0:01:45  lr: 0.000375  min_lr: 0.000009  loss: 1.0366 (0.9772)  loss_scale: 16384.0000 (13612.8190)  weight_decay: 0.0500 (0.0500)  time: 0.3062  data: 0.0002  max mem: 41808
Epoch: [3]  [1020/1349]  eta: 0:01:42  lr: 0.000376  min_lr: 0.000009  loss: 1.0218 (0.9766)  loss_scale: 8192.0000 (13559.7258)  weight_decay: 0.0500 (0.0500)  time: 0.3058  data: 0.0001  max mem: 41808
Epoch: [3]  [1030/1349]  eta: 0:01:39  lr: 0.000376  min_lr: 0.000009  loss: 0.9516 (0.9760)  loss_scale: 8192.0000 (13507.6625)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0001  max mem: 41808
Epoch: [3]  [1040/1349]  eta: 0:01:36  lr: 0.000377  min_lr: 0.000009  loss: 0.9585 (0.9763)  loss_scale: 8192.0000 (13456.5994)  weight_decay: 0.0500 (0.0500)  time: 0.3064  data: 0.0001  max mem: 41808
Epoch: [3]  [1050/1349]  eta: 0:01:33  lr: 0.000378  min_lr: 0.000009  loss: 1.0164 (0.9762)  loss_scale: 8192.0000 (13406.5081)  weight_decay: 0.0500 (0.0500)  time: 0.3061  data: 0.0001  max mem: 41808
Epoch: [3]  [1060/1349]  eta: 0:01:30  lr: 0.000379  min_lr: 0.000009  loss: 0.9667 (0.9760)  loss_scale: 8192.0000 (13357.3610)  weight_decay: 0.0500 (0.0500)  time: 0.3060  data: 0.0001  max mem: 41808
Epoch: [3]  [1070/1349]  eta: 0:01:26  lr: 0.000379  min_lr: 0.000009  loss: 0.9898 (0.9763)  loss_scale: 8192.0000 (13309.1317)  weight_decay: 0.0500 (0.0500)  time: 0.3060  data: 0.0001  max mem: 41808
Epoch: [3]  [1080/1349]  eta: 0:01:23  lr: 0.000380  min_lr: 0.000009  loss: 0.9985 (0.9756)  loss_scale: 8192.0000 (13261.7946)  weight_decay: 0.0500 (0.0500)  time: 0.3061  data: 0.0001  max mem: 41808
Epoch: [3]  [1090/1349]  eta: 0:01:20  lr: 0.000381  min_lr: 0.000009  loss: 0.9192 (0.9752)  loss_scale: 8192.0000 (13215.3254)  weight_decay: 0.0500 (0.0500)  time: 0.3062  data: 0.0001  max mem: 41808
Epoch: [3]  [1100/1349]  eta: 0:01:17  lr: 0.000382  min_lr: 0.000009  loss: 1.0542 (0.9758)  loss_scale: 8192.0000 (13169.7003)  weight_decay: 0.0500 (0.0500)  time: 0.3058  data: 0.0001  max mem: 41808
Epoch: [3]  [1110/1349]  eta: 0:01:14  lr: 0.000382  min_lr: 0.000009  loss: 1.0842 (0.9770)  loss_scale: 8192.0000 (13124.8965)  weight_decay: 0.0500 (0.0500)  time: 0.3055  data: 0.0001  max mem: 41808
Epoch: [3]  [1120/1349]  eta: 0:01:11  lr: 0.000383  min_lr: 0.000009  loss: 0.9856 (0.9757)  loss_scale: 8192.0000 (13080.8921)  weight_decay: 0.0500 (0.0500)  time: 0.3056  data: 0.0001  max mem: 41808
Epoch: [3]  [1130/1349]  eta: 0:01:08  lr: 0.000384  min_lr: 0.000009  loss: 0.8671 (0.9751)  loss_scale: 8192.0000 (13037.6658)  weight_decay: 0.0500 (0.0500)  time: 0.3056  data: 0.0001  max mem: 41808
[2025-05-23 15:41:33,083] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 15:41:33,083] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2025-05-23 15:41:33,083] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 15:41:33,083] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [3]  [1140/1349]  eta: 0:01:05  lr: 0.000385  min_lr: 0.000009  loss: 0.8931 (0.9743)  loss_scale: 8192.0000 (13066.9939)  weight_decay: 0.0500 (0.0500)  time: 0.3063  data: 0.0001  max mem: 41808
Epoch: [3]  [1150/1349]  eta: 0:01:01  lr: 0.000385  min_lr: 0.000009  loss: 0.9209 (0.9742)  loss_scale: 16384.0000 (13095.8123)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0002  max mem: 41808
Epoch: [3]  [1160/1349]  eta: 0:00:58  lr: 0.000386  min_lr: 0.000009  loss: 0.9851 (0.9743)  loss_scale: 16384.0000 (13124.1344)  weight_decay: 0.0500 (0.0500)  time: 0.3088  data: 0.0002  max mem: 41808
Epoch: [3]  [1170/1349]  eta: 0:00:55  lr: 0.000387  min_lr: 0.000009  loss: 0.9278 (0.9737)  loss_scale: 16384.0000 (13151.9727)  weight_decay: 0.0500 (0.0500)  time: 0.3089  data: 0.0001  max mem: 41808
Epoch: [3]  [1180/1349]  eta: 0:00:52  lr: 0.000388  min_lr: 0.000009  loss: 0.9315 (0.9740)  loss_scale: 16384.0000 (13179.3395)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [3]  [1190/1349]  eta: 0:00:49  lr: 0.000388  min_lr: 0.000009  loss: 1.0294 (0.9736)  loss_scale: 16384.0000 (13206.2469)  weight_decay: 0.0500 (0.0500)  time: 0.3078  data: 0.0001  max mem: 41808
Epoch: [3]  [1200/1349]  eta: 0:00:46  lr: 0.000389  min_lr: 0.000009  loss: 0.9692 (0.9746)  loss_scale: 16384.0000 (13232.7061)  weight_decay: 0.0500 (0.0500)  time: 0.3111  data: 0.0001  max mem: 41808
Epoch: [3]  [1210/1349]  eta: 0:00:43  lr: 0.000390  min_lr: 0.000009  loss: 1.0220 (0.9747)  loss_scale: 16384.0000 (13258.7283)  weight_decay: 0.0500 (0.0500)  time: 0.3108  data: 0.0001  max mem: 41808
Epoch: [3]  [1220/1349]  eta: 0:00:40  lr: 0.000390  min_lr: 0.000009  loss: 1.0090 (0.9744)  loss_scale: 16384.0000 (13284.3243)  weight_decay: 0.0500 (0.0500)  time: 0.3078  data: 0.0001  max mem: 41808
Epoch: [3]  [1230/1349]  eta: 0:00:37  lr: 0.000391  min_lr: 0.000009  loss: 0.9405 (0.9739)  loss_scale: 16384.0000 (13309.5045)  weight_decay: 0.0500 (0.0500)  time: 0.3080  data: 0.0001  max mem: 41808
[2025-05-23 15:42:05,818] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 5284
[2025-05-23 15:42:05,818] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 5284
[2025-05-23 15:42:05,818] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2025-05-23 15:42:05,818] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2025-05-23 15:42:05,818] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [3]  [1240/1349]  eta: 0:00:33  lr: 0.000392  min_lr: 0.000009  loss: 0.9924 (0.9744)  loss_scale: 16384.0000 (13307.8743)  weight_decay: 0.0500 (0.0500)  time: 0.3095  data: 0.0001  max mem: 41808
Epoch: [3]  [1250/1349]  eta: 0:00:30  lr: 0.000393  min_lr: 0.000009  loss: 1.0164 (0.9744)  loss_scale: 8192.0000 (13266.9800)  weight_decay: 0.0500 (0.0500)  time: 0.3114  data: 0.0002  max mem: 41808
Epoch: [3]  [1260/1349]  eta: 0:00:27  lr: 0.000393  min_lr: 0.000009  loss: 1.0164 (0.9749)  loss_scale: 8192.0000 (13226.7343)  weight_decay: 0.0500 (0.0500)  time: 0.3097  data: 0.0002  max mem: 41808
Epoch: [3]  [1270/1349]  eta: 0:00:24  lr: 0.000394  min_lr: 0.000009  loss: 0.9754 (0.9748)  loss_scale: 8192.0000 (13187.1220)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [3]  [1280/1349]  eta: 0:00:21  lr: 0.000395  min_lr: 0.000009  loss: 1.0177 (0.9751)  loss_scale: 8192.0000 (13148.1280)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
Epoch: [3]  [1290/1349]  eta: 0:00:18  lr: 0.000396  min_lr: 0.000009  loss: 0.9927 (0.9752)  loss_scale: 8192.0000 (13109.7382)  weight_decay: 0.0500 (0.0500)  time: 0.3085  data: 0.0001  max mem: 41808
Epoch: [3]  [1300/1349]  eta: 0:00:15  lr: 0.000396  min_lr: 0.000009  loss: 0.9834 (0.9752)  loss_scale: 8192.0000 (13071.9385)  weight_decay: 0.0500 (0.0500)  time: 0.3085  data: 0.0001  max mem: 41808
Epoch: [3]  [1310/1349]  eta: 0:00:12  lr: 0.000397  min_lr: 0.000009  loss: 0.9896 (0.9751)  loss_scale: 8192.0000 (13034.7155)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [3]  [1320/1349]  eta: 0:00:09  lr: 0.000398  min_lr: 0.000009  loss: 0.8616 (0.9739)  loss_scale: 8192.0000 (12998.0560)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
Epoch: [3]  [1330/1349]  eta: 0:00:05  lr: 0.000399  min_lr: 0.000009  loss: 0.7917 (0.9733)  loss_scale: 8192.0000 (12961.9474)  weight_decay: 0.0500 (0.0500)  time: 0.3042  data: 0.0001  max mem: 41808
Epoch: [3]  [1340/1349]  eta: 0:00:02  lr: 0.000399  min_lr: 0.000009  loss: 0.9393 (0.9734)  loss_scale: 8192.0000 (12926.3773)  weight_decay: 0.0500 (0.0500)  time: 0.3021  data: 0.0001  max mem: 41808
Epoch: [3]  [1348/1349]  eta: 0:00:00  lr: 0.000400  min_lr: 0.000010  loss: 0.9970 (0.9735)  loss_scale: 8192.0000 (12898.3010)  weight_decay: 0.0500 (0.0500)  time: 0.3020  data: 0.0001  max mem: 41808
Epoch: [3] Total time: 0:06:59 (0.3112 s / it)
Averaged stats: lr: 0.000400  min_lr: 0.000010  loss: 0.9970 (0.9754)  loss_scale: 8192.0000 (12898.3010)  weight_decay: 0.0500 (0.0500)  total_time: 419.8297 (419.8148)
Val:  [  0/346]  eta: 1:21:29  loss: 1.1277 (1.1277)  acc1: 47.6562 (47.6562)  acc5: 100.0000 (100.0000)  time: 14.1315  data: 13.2861  max mem: 41808
Val:  [ 10/346]  eta: 0:11:09  loss: 0.2009 (0.3893)  acc1: 100.0000 (88.0682)  acc5: 100.0000 (99.9290)  time: 1.9939  data: 1.2918  max mem: 41808
Val:  [ 20/346]  eta: 0:08:12  loss: 0.1826 (0.3498)  acc1: 100.0000 (90.6250)  acc5: 100.0000 (99.9628)  time: 0.8801  data: 0.1814  max mem: 41808
Val:  [ 30/346]  eta: 0:06:35  loss: 0.1839 (0.3394)  acc1: 98.4375 (91.3558)  acc5: 100.0000 (99.9748)  time: 0.8435  data: 0.1353  max mem: 41808
Val:  [ 40/346]  eta: 0:05:42  loss: 0.1818 (0.3920)  acc1: 98.4375 (90.3011)  acc5: 100.0000 (98.9901)  time: 0.7058  data: 0.0003  max mem: 41808
Val:  [ 50/346]  eta: 0:05:12  loss: 0.1665 (0.3579)  acc1: 99.2188 (91.7433)  acc5: 100.0000 (99.1115)  time: 0.7515  data: 0.0473  max mem: 41808
Val:  [ 60/346]  eta: 0:04:53  loss: 0.1830 (0.3543)  acc1: 100.0000 (91.5599)  acc5: 100.0000 (99.2572)  time: 0.8401  data: 0.1240  max mem: 41808
Val:  [ 70/346]  eta: 0:04:35  loss: 0.2402 (0.3707)  acc1: 94.5312 (90.8011)  acc5: 100.0000 (99.3618)  time: 0.8564  data: 0.1408  max mem: 41808
Val:  [ 80/346]  eta: 0:04:20  loss: 0.2733 (0.3694)  acc1: 93.7500 (91.1169)  acc5: 100.0000 (99.3634)  time: 0.8407  data: 0.1354  max mem: 41808
Val:  [ 90/346]  eta: 0:04:08  loss: 0.2566 (0.3732)  acc1: 94.5312 (90.8997)  acc5: 100.0000 (99.4248)  time: 0.8607  data: 0.1433  max mem: 41808
Val:  [100/346]  eta: 0:03:56  loss: 0.2060 (0.3550)  acc1: 97.6562 (91.7311)  acc5: 100.0000 (99.4817)  time: 0.8751  data: 0.1400  max mem: 41808
Val:  [110/346]  eta: 0:03:44  loss: 0.1875 (0.3641)  acc1: 99.2188 (91.3851)  acc5: 100.0000 (99.4792)  time: 0.8635  data: 0.1345  max mem: 41808
Val:  [120/346]  eta: 0:03:32  loss: 0.1984 (0.3583)  acc1: 97.6562 (91.5677)  acc5: 100.0000 (99.5222)  time: 0.8502  data: 0.1455  max mem: 41808
Val:  [130/346]  eta: 0:03:22  loss: 0.1964 (0.3651)  acc1: 99.2188 (91.3764)  acc5: 100.0000 (99.4573)  time: 0.8536  data: 0.1545  max mem: 41808
Val:  [140/346]  eta: 0:03:11  loss: 0.2432 (0.3609)  acc1: 96.0938 (91.6057)  acc5: 100.0000 (99.4958)  time: 0.8452  data: 0.1471  max mem: 41808
Val:  [150/346]  eta: 0:03:01  loss: 0.3077 (0.3575)  acc1: 96.0938 (91.8046)  acc5: 100.0000 (99.5188)  time: 0.8554  data: 0.1497  max mem: 41808
Val:  [160/346]  eta: 0:02:51  loss: 0.2431 (0.3527)  acc1: 96.0938 (92.0468)  acc5: 100.0000 (99.5487)  time: 0.8775  data: 0.1578  max mem: 41808
Val:  [170/346]  eta: 0:02:41  loss: 0.2431 (0.3572)  acc1: 98.4375 (91.6530)  acc5: 100.0000 (99.5751)  time: 0.8530  data: 0.1536  max mem: 41808
Val:  [180/346]  eta: 0:02:31  loss: 0.3877 (0.3745)  acc1: 88.2812 (90.5559)  acc5: 100.0000 (99.5986)  time: 0.8366  data: 0.1449  max mem: 41808
Val:  [190/346]  eta: 0:02:22  loss: 0.3449 (0.3711)  acc1: 94.5312 (90.7968)  acc5: 100.0000 (99.6073)  time: 0.8588  data: 0.1456  max mem: 41808
Val:  [200/346]  eta: 0:02:12  loss: 0.3537 (0.3771)  acc1: 91.4062 (90.4851)  acc5: 100.0000 (99.6269)  time: 0.8793  data: 0.1583  max mem: 41808
Val:  [210/346]  eta: 0:02:03  loss: 0.2453 (0.3750)  acc1: 91.4062 (90.5806)  acc5: 100.0000 (99.6445)  time: 0.8655  data: 0.1559  max mem: 41808
Val:  [220/346]  eta: 0:01:54  loss: 0.2253 (0.3697)  acc1: 96.8750 (90.8725)  acc5: 100.0000 (99.6500)  time: 0.8659  data: 0.1498  max mem: 41808
Val:  [230/346]  eta: 0:01:44  loss: 0.2096 (0.3635)  acc1: 100.0000 (91.1627)  acc5: 100.0000 (99.6652)  time: 0.8816  data: 0.1563  max mem: 41808
Val:  [240/346]  eta: 0:01:35  loss: 0.2127 (0.3710)  acc1: 97.6562 (91.0043)  acc5: 100.0000 (99.6596)  time: 0.8705  data: 0.1584  max mem: 41808
Val:  [250/346]  eta: 0:01:26  loss: 0.2218 (0.3667)  acc1: 97.6562 (91.1479)  acc5: 100.0000 (99.6732)  time: 0.8844  data: 0.1575  max mem: 41808
Val:  [260/346]  eta: 0:01:17  loss: 0.1949 (0.3651)  acc1: 98.4375 (91.2446)  acc5: 100.0000 (99.6857)  time: 0.9021  data: 0.1557  max mem: 41808
Val:  [270/346]  eta: 0:01:08  loss: 0.1668 (0.3610)  acc1: 99.2188 (91.3745)  acc5: 100.0000 (99.6973)  time: 0.9025  data: 0.1548  max mem: 41808
Val:  [280/346]  eta: 0:00:59  loss: 0.1668 (0.3581)  acc1: 100.0000 (91.4758)  acc5: 100.0000 (99.7081)  time: 0.8868  data: 0.1523  max mem: 41808
Val:  [290/346]  eta: 0:00:50  loss: 0.1397 (0.3520)  acc1: 100.0000 (91.7096)  acc5: 100.0000 (99.7181)  time: 0.8441  data: 0.1511  max mem: 41808
Val:  [300/346]  eta: 0:00:41  loss: 0.1562 (0.3537)  acc1: 100.0000 (91.7437)  acc5: 100.0000 (99.6678)  time: 0.8520  data: 0.1576  max mem: 41808
Val:  [310/346]  eta: 0:00:32  loss: 0.1572 (0.3532)  acc1: 98.4375 (91.6750)  acc5: 100.0000 (99.6759)  time: 0.8652  data: 0.1605  max mem: 41808
Val:  [320/346]  eta: 0:00:23  loss: 0.2013 (0.3544)  acc1: 100.0000 (91.5717)  acc5: 100.0000 (99.6860)  time: 0.8462  data: 0.1498  max mem: 41808
Val:  [330/346]  eta: 0:00:14  loss: 0.2889 (0.3610)  acc1: 92.1875 (91.3425)  acc5: 100.0000 (99.6294)  time: 0.8304  data: 0.1436  max mem: 41808
Val:  [340/346]  eta: 0:00:05  loss: 0.3509 (0.3660)  acc1: 89.8438 (91.1153)  acc5: 100.0000 (99.6403)  time: 0.8621  data: 0.1578  max mem: 41808
Val:  [345/346]  eta: 0:00:00  loss: 0.2715 (0.3636)  acc1: 95.3125 (91.2197)  acc5: 100.0000 (99.6452)  time: 0.8886  data: 0.1696  max mem: 41808
Val: Total time: 0:05:09 (0.8944 s / it)
* Acc@1 91.247 Acc@5 99.643 loss 0.363
Accuracy of the network on the 88494 val videos: 91.2%
[2025-05-23 15:47:49,636] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-05-23 15:47:49,639] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /project/mmendoscope/Natural_Comparison/Cholec75/unified_base_st_settingE_videomae-st_Cholec80_0.0005_0.75_online_key_frame_frame16_Fixed_Stride_4/checkpoint-best/mp_rank_00_model_states.pt
[2025-05-23 15:47:49,639] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /project/mmendoscope/Natural_Comparison/Cholec75/unified_base_st_settingE_videomae-st_Cholec80_0.0005_0.75_online_key_frame_frame16_Fixed_Stride_4/checkpoint-best/mp_rank_00_model_states.pt...
[2025-05-23 15:47:49,639] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2025-05-23 15:47:52,264] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /project/mmendoscope/Natural_Comparison/Cholec75/unified_base_st_settingE_videomae-st_Cholec80_0.0005_0.75_online_key_frame_frame16_Fixed_Stride_4/checkpoint-best/mp_rank_00_model_states.pt.
[2025-05-23 15:47:52,265] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 91.25%   Max Epoch: 3
Epoch: [4]  [   0/1349]  eta: 2:02:51  lr: 0.000400  min_lr: 0.000010  loss: 0.6136 (0.6136)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 5.4648  data: 4.5938  max mem: 41808
Epoch: [4]  [  10/1349]  eta: 0:17:27  lr: 0.000401  min_lr: 0.000010  loss: 1.1744 (1.0694)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7827  data: 0.4177  max mem: 41808
[2025-05-23 15:48:03,055] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 15:48:03,055] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 15:48:03,055] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2025-05-23 15:48:03,055] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [4]  [  20/1349]  eta: 0:12:20  lr: 0.000402  min_lr: 0.000010  loss: 1.0454 (1.0506)  loss_scale: 8192.0000 (9752.3810)  weight_decay: 0.0500 (0.0500)  time: 0.3120  data: 0.0001  max mem: 41808
Epoch: [4]  [  30/1349]  eta: 0:10:29  lr: 0.000402  min_lr: 0.000010  loss: 1.0151 (1.0304)  loss_scale: 16384.0000 (11891.6129)  weight_decay: 0.0500 (0.0500)  time: 0.3088  data: 0.0001  max mem: 41808
Epoch: [4]  [  40/1349]  eta: 0:09:29  lr: 0.000403  min_lr: 0.000010  loss: 1.0019 (0.9951)  loss_scale: 16384.0000 (12987.3171)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0002  max mem: 41808
Epoch: [4]  [  50/1349]  eta: 0:08:52  lr: 0.000404  min_lr: 0.000010  loss: 1.0019 (0.9913)  loss_scale: 16384.0000 (13653.3333)  weight_decay: 0.0500 (0.0500)  time: 0.3066  data: 0.0001  max mem: 41808
Epoch: [4]  [  60/1349]  eta: 0:08:27  lr: 0.000405  min_lr: 0.000010  loss: 1.0199 (0.9853)  loss_scale: 16384.0000 (14100.9836)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [4]  [  70/1349]  eta: 0:08:07  lr: 0.000405  min_lr: 0.000010  loss: 0.9968 (0.9851)  loss_scale: 16384.0000 (14422.5352)  weight_decay: 0.0500 (0.0500)  time: 0.3078  data: 0.0001  max mem: 41808
Epoch: [4]  [  80/1349]  eta: 0:07:52  lr: 0.000406  min_lr: 0.000010  loss: 0.9968 (0.9896)  loss_scale: 16384.0000 (14664.6914)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
Epoch: [4]  [  90/1349]  eta: 0:07:39  lr: 0.000407  min_lr: 0.000010  loss: 0.9475 (0.9797)  loss_scale: 16384.0000 (14853.6264)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0001  max mem: 41808
Epoch: [4]  [ 100/1349]  eta: 0:07:28  lr: 0.000407  min_lr: 0.000010  loss: 0.9334 (0.9837)  loss_scale: 16384.0000 (15005.1485)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
Epoch: [4]  [ 110/1349]  eta: 0:07:19  lr: 0.000408  min_lr: 0.000010  loss: 0.9090 (0.9779)  loss_scale: 16384.0000 (15129.3694)  weight_decay: 0.0500 (0.0500)  time: 0.3081  data: 0.0001  max mem: 41808
Epoch: [4]  [ 120/1349]  eta: 0:07:11  lr: 0.000409  min_lr: 0.000010  loss: 0.8605 (0.9656)  loss_scale: 16384.0000 (15233.0579)  weight_decay: 0.0500 (0.0500)  time: 0.3082  data: 0.0001  max mem: 41808
Epoch: [4]  [ 130/1349]  eta: 0:07:03  lr: 0.000410  min_lr: 0.000010  loss: 0.9089 (0.9682)  loss_scale: 16384.0000 (15320.9160)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [4]  [ 140/1349]  eta: 0:06:56  lr: 0.000410  min_lr: 0.000010  loss: 1.0045 (0.9686)  loss_scale: 16384.0000 (15396.3121)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
[2025-05-23 15:48:42,397] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 15:48:42,397] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-05-23 15:48:42,397] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 15:48:42,397] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-05-23 15:48:43,936] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 5546
[2025-05-23 15:48:43,936] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-05-23 15:48:43,936] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 5546
[2025-05-23 15:48:43,936] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-05-23 15:48:43,936] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [4]  [ 150/1349]  eta: 0:06:50  lr: 0.000411  min_lr: 0.000010  loss: 1.0045 (0.9697)  loss_scale: 16384.0000 (16004.2384)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
Epoch: [4]  [ 160/1349]  eta: 0:06:44  lr: 0.000412  min_lr: 0.000010  loss: 1.0378 (0.9699)  loss_scale: 16384.0000 (16027.8261)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
Epoch: [4]  [ 170/1349]  eta: 0:06:38  lr: 0.000413  min_lr: 0.000010  loss: 1.0600 (0.9712)  loss_scale: 16384.0000 (16048.6550)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [4]  [ 180/1349]  eta: 0:06:33  lr: 0.000413  min_lr: 0.000010  loss: 1.0531 (0.9675)  loss_scale: 16384.0000 (16067.1823)  weight_decay: 0.0500 (0.0500)  time: 0.3082  data: 0.0001  max mem: 41808
Epoch: [4]  [ 190/1349]  eta: 0:06:28  lr: 0.000414  min_lr: 0.000010  loss: 1.0570 (0.9761)  loss_scale: 16384.0000 (16083.7696)  weight_decay: 0.0500 (0.0500)  time: 0.3081  data: 0.0001  max mem: 41808
Epoch: [4]  [ 200/1349]  eta: 0:06:23  lr: 0.000415  min_lr: 0.000010  loss: 1.0875 (0.9760)  loss_scale: 16384.0000 (16098.7065)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [4]  [ 210/1349]  eta: 0:06:18  lr: 0.000416  min_lr: 0.000010  loss: 0.9765 (0.9752)  loss_scale: 16384.0000 (16112.2275)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
Epoch: [4]  [ 220/1349]  eta: 0:06:14  lr: 0.000416  min_lr: 0.000010  loss: 0.9739 (0.9768)  loss_scale: 16384.0000 (16124.5249)  weight_decay: 0.0500 (0.0500)  time: 0.3103  data: 0.0001  max mem: 41808
Epoch: [4]  [ 230/1349]  eta: 0:06:09  lr: 0.000417  min_lr: 0.000010  loss: 1.0734 (0.9788)  loss_scale: 16384.0000 (16135.7576)  weight_decay: 0.0500 (0.0500)  time: 0.3102  data: 0.0001  max mem: 41808
Epoch: [4]  [ 240/1349]  eta: 0:06:05  lr: 0.000418  min_lr: 0.000010  loss: 1.0353 (0.9796)  loss_scale: 16384.0000 (16146.0581)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [4]  [ 250/1349]  eta: 0:06:01  lr: 0.000419  min_lr: 0.000010  loss: 0.9194 (0.9719)  loss_scale: 16384.0000 (16155.5378)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
[2025-05-23 15:49:15,364] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 5648
[2025-05-23 15:49:15,364] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2025-05-23 15:49:15,364] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 5648
[2025-05-23 15:49:15,364] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2025-05-23 15:49:15,364] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [4]  [ 260/1349]  eta: 0:05:56  lr: 0.000419  min_lr: 0.000010  loss: 0.7221 (0.9679)  loss_scale: 16384.0000 (15881.8084)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0002  max mem: 41808
Epoch: [4]  [ 270/1349]  eta: 0:05:52  lr: 0.000420  min_lr: 0.000010  loss: 0.9412 (0.9678)  loss_scale: 8192.0000 (15598.0517)  weight_decay: 0.0500 (0.0500)  time: 0.3089  data: 0.0002  max mem: 41808
Epoch: [4]  [ 280/1349]  eta: 0:05:48  lr: 0.000421  min_lr: 0.000010  loss: 0.9101 (0.9658)  loss_scale: 8192.0000 (15334.4911)  weight_decay: 0.0500 (0.0500)  time: 0.3097  data: 0.0002  max mem: 41808
Epoch: [4]  [ 290/1349]  eta: 0:05:44  lr: 0.000422  min_lr: 0.000010  loss: 0.8794 (0.9631)  loss_scale: 8192.0000 (15089.0447)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0002  max mem: 41808
Epoch: [4]  [ 300/1349]  eta: 0:05:41  lr: 0.000422  min_lr: 0.000010  loss: 0.8886 (0.9618)  loss_scale: 8192.0000 (14859.9070)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0002  max mem: 41808
Epoch: [4]  [ 310/1349]  eta: 0:05:37  lr: 0.000423  min_lr: 0.000010  loss: 0.9621 (0.9605)  loss_scale: 8192.0000 (14645.5048)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0002  max mem: 41808
Epoch: [4]  [ 320/1349]  eta: 0:05:33  lr: 0.000424  min_lr: 0.000010  loss: 0.8933 (0.9574)  loss_scale: 8192.0000 (14444.4611)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0002  max mem: 41808
Epoch: [4]  [ 330/1349]  eta: 0:05:29  lr: 0.000425  min_lr: 0.000010  loss: 0.8933 (0.9564)  loss_scale: 8192.0000 (14255.5650)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0001  max mem: 41808
Epoch: [4]  [ 340/1349]  eta: 0:05:25  lr: 0.000425  min_lr: 0.000010  loss: 0.8105 (0.9512)  loss_scale: 8192.0000 (14077.7478)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
Epoch: [4]  [ 350/1349]  eta: 0:05:22  lr: 0.000426  min_lr: 0.000010  loss: 1.0803 (0.9569)  loss_scale: 8192.0000 (13910.0627)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
Epoch: [4]  [ 360/1349]  eta: 0:05:18  lr: 0.000427  min_lr: 0.000010  loss: 1.0803 (0.9572)  loss_scale: 8192.0000 (13751.6676)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0002  max mem: 41808
Epoch: [4]  [ 370/1349]  eta: 0:05:14  lr: 0.000427  min_lr: 0.000010  loss: 0.9910 (0.9552)  loss_scale: 8192.0000 (13601.8113)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0002  max mem: 41808
Epoch: [4]  [ 380/1349]  eta: 0:05:11  lr: 0.000428  min_lr: 0.000010  loss: 1.0213 (0.9588)  loss_scale: 8192.0000 (13459.8215)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
[2025-05-23 15:49:55,047] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 15:49:55,047] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 15:49:55,047] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2025-05-23 15:49:55,047] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [4]  [ 390/1349]  eta: 0:05:07  lr: 0.000429  min_lr: 0.000010  loss: 0.9623 (0.9554)  loss_scale: 8192.0000 (13534.6087)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
[2025-05-23 15:49:59,346] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 5791
[2025-05-23 15:49:59,346] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2025-05-23 15:49:59,346] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
[2025-05-23 15:49:59,347] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 5791
[2025-05-23 15:49:59,347] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
Epoch: [4]  [ 400/1349]  eta: 0:05:04  lr: 0.000430  min_lr: 0.000010  loss: 0.8886 (0.9574)  loss_scale: 16384.0000 (13483.0923)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
Epoch: [4]  [ 410/1349]  eta: 0:05:00  lr: 0.000430  min_lr: 0.000010  loss: 0.9607 (0.9556)  loss_scale: 8192.0000 (13354.3552)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
Epoch: [4]  [ 420/1349]  eta: 0:04:57  lr: 0.000431  min_lr: 0.000010  loss: 0.9296 (0.9563)  loss_scale: 8192.0000 (13231.7340)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
Epoch: [4]  [ 430/1349]  eta: 0:04:53  lr: 0.000432  min_lr: 0.000010  loss: 0.9135 (0.9543)  loss_scale: 8192.0000 (13114.8028)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
Epoch: [4]  [ 440/1349]  eta: 0:04:50  lr: 0.000433  min_lr: 0.000010  loss: 0.9134 (0.9547)  loss_scale: 8192.0000 (13003.1746)  weight_decay: 0.0500 (0.0500)  time: 0.3091  data: 0.0001  max mem: 41808
Epoch: [4]  [ 450/1349]  eta: 0:04:46  lr: 0.000433  min_lr: 0.000010  loss: 0.9601 (0.9539)  loss_scale: 8192.0000 (12896.4967)  weight_decay: 0.0500 (0.0500)  time: 0.3091  data: 0.0001  max mem: 41808
Epoch: [4]  [ 460/1349]  eta: 0:04:43  lr: 0.000434  min_lr: 0.000010  loss: 0.9929 (0.9552)  loss_scale: 8192.0000 (12794.4469)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [4]  [ 470/1349]  eta: 0:04:40  lr: 0.000435  min_lr: 0.000010  loss: 1.0628 (0.9554)  loss_scale: 8192.0000 (12696.7304)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [4]  [ 480/1349]  eta: 0:04:36  lr: 0.000436  min_lr: 0.000010  loss: 0.9783 (0.9552)  loss_scale: 8192.0000 (12603.0769)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
Epoch: [4]  [ 490/1349]  eta: 0:04:33  lr: 0.000436  min_lr: 0.000010  loss: 0.9846 (0.9566)  loss_scale: 8192.0000 (12513.2383)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0001  max mem: 41808
Epoch: [4]  [ 500/1349]  eta: 0:04:30  lr: 0.000437  min_lr: 0.000010  loss: 0.9846 (0.9557)  loss_scale: 8192.0000 (12426.9860)  weight_decay: 0.0500 (0.0500)  time: 0.3078  data: 0.0001  max mem: 41808
Epoch: [4]  [ 510/1349]  eta: 0:04:26  lr: 0.000438  min_lr: 0.000010  loss: 0.9172 (0.9548)  loss_scale: 8192.0000 (12344.1096)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
Epoch: [4]  [ 520/1349]  eta: 0:04:23  lr: 0.000439  min_lr: 0.000010  loss: 0.9193 (0.9541)  loss_scale: 8192.0000 (12264.4146)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
[2025-05-23 15:50:39,035] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 15:50:39,035] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 15:50:39,036] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2025-05-23 15:50:39,036] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [4]  [ 530/1349]  eta: 0:04:19  lr: 0.000439  min_lr: 0.000010  loss: 0.9745 (0.9557)  loss_scale: 8192.0000 (12295.7137)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
[2025-05-23 15:50:43,950] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 5936
[2025-05-23 15:50:43,950] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2025-05-23 15:50:43,950] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 5936
[2025-05-23 15:50:43,950] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2025-05-23 15:50:43,950] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [4]  [ 540/1349]  eta: 0:04:16  lr: 0.000440  min_lr: 0.000010  loss: 1.0258 (0.9554)  loss_scale: 16384.0000 (12356.1405)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
Epoch: [4]  [ 550/1349]  eta: 0:04:13  lr: 0.000441  min_lr: 0.000010  loss: 0.9775 (0.9570)  loss_scale: 8192.0000 (12280.5662)  weight_decay: 0.0500 (0.0500)  time: 0.3066  data: 0.0001  max mem: 41808
Epoch: [4]  [ 560/1349]  eta: 0:04:09  lr: 0.000442  min_lr: 0.000010  loss: 0.9644 (0.9557)  loss_scale: 8192.0000 (12207.6863)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0001  max mem: 41808
Epoch: [4]  [ 570/1349]  eta: 0:04:06  lr: 0.000442  min_lr: 0.000011  loss: 0.9421 (0.9558)  loss_scale: 8192.0000 (12137.3590)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
Epoch: [4]  [ 580/1349]  eta: 0:04:03  lr: 0.000443  min_lr: 0.000011  loss: 0.9616 (0.9560)  loss_scale: 8192.0000 (12069.4527)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
Epoch: [4]  [ 590/1349]  eta: 0:04:00  lr: 0.000444  min_lr: 0.000011  loss: 0.9995 (0.9562)  loss_scale: 8192.0000 (12003.8443)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [4]  [ 600/1349]  eta: 0:03:56  lr: 0.000445  min_lr: 0.000011  loss: 0.9995 (0.9570)  loss_scale: 8192.0000 (11940.4193)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
[2025-05-23 15:51:03,341] [INFO] [logging.py:96:log_dist] [Rank 0] step=6000, skipped=32, lr=[1.056641658098566e-05, 1.056641658098566e-05, 1.4088555441314214e-05, 1.4088555441314214e-05, 1.878474058841895e-05, 1.878474058841895e-05, 2.50463207845586e-05, 2.50463207845586e-05, 3.339509437941147e-05, 3.339509437941147e-05, 4.452679250588196e-05, 4.452679250588196e-05, 5.9369056674509283e-05, 5.9369056674509283e-05, 7.915874223267904e-05, 7.915874223267904e-05, 0.00010554498964357205, 0.00010554498964357205, 0.00014072665285809607, 0.00014072665285809607, 0.0001876355371441281, 0.0001876355371441281, 0.0002501807161921708, 0.0002501807161921708, 0.0003335742882562277, 0.0003335742882562277, 0.0004447657176749703, 0.0004447657176749703], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-05-23 15:51:03,342] [INFO] [timer.py:260:stop] epoch=0/micro_step=6000/global_step=6000, RunningAvgSamplesPerSec=212.74291102554153, CurrSamplesPerSec=213.65258343010368, MemAllocated=1.37GB, MaxMemAllocated=40.83GB
Epoch: [4]  [ 610/1349]  eta: 0:03:53  lr: 0.000445  min_lr: 0.000011  loss: 0.9673 (0.9563)  loss_scale: 8192.0000 (11879.0704)  weight_decay: 0.0500 (0.0500)  time: 0.3093  data: 0.0001  max mem: 41808
Epoch: [4]  [ 620/1349]  eta: 0:03:50  lr: 0.000446  min_lr: 0.000011  loss: 0.9673 (0.9566)  loss_scale: 8192.0000 (11819.6973)  weight_decay: 0.0500 (0.0500)  time: 0.3099  data: 0.0001  max mem: 41808
Epoch: [4]  [ 630/1349]  eta: 0:03:47  lr: 0.000447  min_lr: 0.000011  loss: 1.0134 (0.9580)  loss_scale: 8192.0000 (11762.2060)  weight_decay: 0.0500 (0.0500)  time: 0.3080  data: 0.0001  max mem: 41808
Epoch: [4]  [ 640/1349]  eta: 0:03:43  lr: 0.000448  min_lr: 0.000011  loss: 0.9557 (0.9564)  loss_scale: 8192.0000 (11706.5086)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
Epoch: [4]  [ 650/1349]  eta: 0:03:40  lr: 0.000448  min_lr: 0.000011  loss: 0.9557 (0.9569)  loss_scale: 8192.0000 (11652.5223)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0001  max mem: 41808
Epoch: [4]  [ 660/1349]  eta: 0:03:37  lr: 0.000449  min_lr: 0.000011  loss: 0.9481 (0.9570)  loss_scale: 8192.0000 (11600.1694)  weight_decay: 0.0500 (0.0500)  time: 0.3082  data: 0.0001  max mem: 41808
[2025-05-23 15:51:23,660] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 15:51:23,660] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 15:51:23,660] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2025-05-23 15:51:23,660] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [4]  [ 670/1349]  eta: 0:03:34  lr: 0.000450  min_lr: 0.000011  loss: 0.9481 (0.9575)  loss_scale: 8192.0000 (11573.7943)  weight_decay: 0.0500 (0.0500)  time: 0.3079  data: 0.0001  max mem: 41808
Epoch: [4]  [ 680/1349]  eta: 0:03:30  lr: 0.000450  min_lr: 0.000011  loss: 0.8771 (0.9557)  loss_scale: 16384.0000 (11644.4288)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [4]  [ 690/1349]  eta: 0:03:27  lr: 0.000451  min_lr: 0.000011  loss: 0.8025 (0.9541)  loss_scale: 16384.0000 (11713.0188)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
Epoch: [4]  [ 700/1349]  eta: 0:03:24  lr: 0.000452  min_lr: 0.000011  loss: 0.9153 (0.9544)  loss_scale: 16384.0000 (11779.6519)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0001  max mem: 41808
Epoch: [4]  [ 710/1349]  eta: 0:03:21  lr: 0.000453  min_lr: 0.000011  loss: 1.0670 (0.9549)  loss_scale: 16384.0000 (11844.4107)  weight_decay: 0.0500 (0.0500)  time: 0.3083  data: 0.0002  max mem: 41808
Epoch: [4]  [ 720/1349]  eta: 0:03:18  lr: 0.000453  min_lr: 0.000011  loss: 0.9450 (0.9542)  loss_scale: 16384.0000 (11907.3731)  weight_decay: 0.0500 (0.0500)  time: 0.3086  data: 0.0001  max mem: 41808
Epoch: [4]  [ 730/1349]  eta: 0:03:14  lr: 0.000454  min_lr: 0.000011  loss: 0.9169 (0.9540)  loss_scale: 16384.0000 (11968.6129)  weight_decay: 0.0500 (0.0500)  time: 0.3082  data: 0.0002  max mem: 41808
Epoch: [4]  [ 740/1349]  eta: 0:03:11  lr: 0.000455  min_lr: 0.000011  loss: 1.0583 (0.9554)  loss_scale: 16384.0000 (12028.1997)  weight_decay: 0.0500 (0.0500)  time: 0.3084  data: 0.0002  max mem: 41808
Epoch: [4]  [ 750/1349]  eta: 0:03:08  lr: 0.000456  min_lr: 0.000011  loss: 1.0583 (0.9542)  loss_scale: 16384.0000 (12086.1997)  weight_decay: 0.0500 (0.0500)  time: 0.3078  data: 0.0001  max mem: 41808
Epoch: [4]  [ 760/1349]  eta: 0:03:05  lr: 0.000456  min_lr: 0.000011  loss: 0.8844 (0.9541)  loss_scale: 16384.0000 (12142.6754)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [4]  [ 770/1349]  eta: 0:03:02  lr: 0.000457  min_lr: 0.000011  loss: 0.9538 (0.9544)  loss_scale: 16384.0000 (12197.6861)  weight_decay: 0.0500 (0.0500)  time: 0.3089  data: 0.0001  max mem: 41808
Epoch: [4]  [ 780/1349]  eta: 0:02:58  lr: 0.000458  min_lr: 0.000011  loss: 0.9991 (0.9549)  loss_scale: 16384.0000 (12251.2881)  weight_decay: 0.0500 (0.0500)  time: 0.3091  data: 0.0002  max mem: 41808
Epoch: [4]  [ 790/1349]  eta: 0:02:55  lr: 0.000459  min_lr: 0.000011  loss: 0.9310 (0.9549)  loss_scale: 16384.0000 (12303.5348)  weight_decay: 0.0500 (0.0500)  time: 0.3080  data: 0.0002  max mem: 41808
[2025-05-23 15:52:03,135] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 15:52:03,135] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-05-23 15:52:03,136] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 15:52:03,136] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-05-23 15:52:04,061] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 6196
[2025-05-23 15:52:04,061] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-05-23 15:52:04,061] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 6196
[2025-05-23 15:52:04,061] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-05-23 15:52:04,061] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [4]  [ 800/1349]  eta: 0:02:52  lr: 0.000459  min_lr: 0.000011  loss: 0.8677 (0.9545)  loss_scale: 16384.0000 (12415.8402)  weight_decay: 0.0500 (0.0500)  time: 0.3089  data: 0.0001  max mem: 41808
Epoch: [4]  [ 810/1349]  eta: 0:02:49  lr: 0.000460  min_lr: 0.000011  loss: 0.9215 (0.9546)  loss_scale: 16384.0000 (12464.7694)  weight_decay: 0.0500 (0.0500)  time: 0.3093  data: 0.0001  max mem: 41808
Epoch: [4]  [ 820/1349]  eta: 0:02:46  lr: 0.000461  min_lr: 0.000011  loss: 0.9316 (0.9541)  loss_scale: 16384.0000 (12512.5067)  weight_decay: 0.0500 (0.0500)  time: 0.3085  data: 0.0001  max mem: 41808
Epoch: [4]  [ 830/1349]  eta: 0:02:42  lr: 0.000462  min_lr: 0.000011  loss: 0.9316 (0.9538)  loss_scale: 16384.0000 (12559.0951)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
Epoch: [4]  [ 840/1349]  eta: 0:02:39  lr: 0.000462  min_lr: 0.000011  loss: 1.0200 (0.9549)  loss_scale: 16384.0000 (12604.5755)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
Epoch: [4]  [ 850/1349]  eta: 0:02:36  lr: 0.000463  min_lr: 0.000011  loss: 1.0145 (0.9551)  loss_scale: 16384.0000 (12648.9871)  weight_decay: 0.0500 (0.0500)  time: 0.3078  data: 0.0001  max mem: 41808
Epoch: [4]  [ 860/1349]  eta: 0:02:33  lr: 0.000464  min_lr: 0.000011  loss: 0.9217 (0.9550)  loss_scale: 16384.0000 (12692.3670)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
Epoch: [4]  [ 870/1349]  eta: 0:02:30  lr: 0.000465  min_lr: 0.000011  loss: 0.8840 (0.9541)  loss_scale: 16384.0000 (12734.7509)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0001  max mem: 41808
Epoch: [4]  [ 880/1349]  eta: 0:02:27  lr: 0.000465  min_lr: 0.000011  loss: 0.9684 (0.9546)  loss_scale: 16384.0000 (12776.1725)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
Epoch: [4]  [ 890/1349]  eta: 0:02:23  lr: 0.000466  min_lr: 0.000011  loss: 0.9868 (0.9539)  loss_scale: 16384.0000 (12816.6644)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [4]  [ 900/1349]  eta: 0:02:20  lr: 0.000467  min_lr: 0.000011  loss: 0.9210 (0.9530)  loss_scale: 16384.0000 (12856.2575)  weight_decay: 0.0500 (0.0500)  time: 0.3079  data: 0.0001  max mem: 41808
Epoch: [4]  [ 910/1349]  eta: 0:02:17  lr: 0.000468  min_lr: 0.000011  loss: 0.9210 (0.9531)  loss_scale: 16384.0000 (12894.9813)  weight_decay: 0.0500 (0.0500)  time: 0.3080  data: 0.0001  max mem: 41808
Epoch: [4]  [ 920/1349]  eta: 0:02:14  lr: 0.000468  min_lr: 0.000011  loss: 0.9727 (0.9530)  loss_scale: 16384.0000 (12932.8643)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0002  max mem: 41808
[2025-05-23 15:52:43,753] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 15:52:43,753] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 15:52:43,753] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-05-23 15:52:43,753] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [4]  [ 930/1349]  eta: 0:02:11  lr: 0.000469  min_lr: 0.000011  loss: 0.9533 (0.9523)  loss_scale: 16384.0000 (13005.1300)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
[2025-05-23 15:52:46,254] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 6333
[2025-05-23 15:52:46,254] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-05-23 15:52:46,254] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2025-05-23 15:52:46,254] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 6333
[2025-05-23 15:52:46,254] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [4]  [ 940/1349]  eta: 0:02:08  lr: 0.000470  min_lr: 0.000011  loss: 0.9641 (0.9530)  loss_scale: 16384.0000 (13145.5048)  weight_decay: 0.0500 (0.0500)  time: 0.3089  data: 0.0001  max mem: 41808
Epoch: [4]  [ 950/1349]  eta: 0:02:04  lr: 0.000470  min_lr: 0.000011  loss: 1.0567 (0.9533)  loss_scale: 16384.0000 (13179.5584)  weight_decay: 0.0500 (0.0500)  time: 0.3095  data: 0.0001  max mem: 41808
Epoch: [4]  [ 960/1349]  eta: 0:02:01  lr: 0.000471  min_lr: 0.000011  loss: 0.9944 (0.9533)  loss_scale: 16384.0000 (13212.9032)  weight_decay: 0.0500 (0.0500)  time: 0.3088  data: 0.0002  max mem: 41808
Epoch: [4]  [ 970/1349]  eta: 0:01:58  lr: 0.000472  min_lr: 0.000011  loss: 0.9738 (0.9525)  loss_scale: 16384.0000 (13245.5613)  weight_decay: 0.0500 (0.0500)  time: 0.3087  data: 0.0001  max mem: 41808
Epoch: [4]  [ 980/1349]  eta: 0:01:55  lr: 0.000473  min_lr: 0.000011  loss: 0.9738 (0.9526)  loss_scale: 16384.0000 (13277.5535)  weight_decay: 0.0500 (0.0500)  time: 0.3087  data: 0.0001  max mem: 41808
Epoch: [4]  [ 990/1349]  eta: 0:01:52  lr: 0.000473  min_lr: 0.000011  loss: 1.0329 (0.9531)  loss_scale: 16384.0000 (13308.9001)  weight_decay: 0.0500 (0.0500)  time: 0.3085  data: 0.0001  max mem: 41808
Epoch: [4]  [1000/1349]  eta: 0:01:49  lr: 0.000474  min_lr: 0.000011  loss: 1.0151 (0.9532)  loss_scale: 16384.0000 (13339.6204)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
Epoch: [4]  [1010/1349]  eta: 0:01:46  lr: 0.000475  min_lr: 0.000011  loss: 0.9839 (0.9536)  loss_scale: 16384.0000 (13369.7329)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [4]  [1020/1349]  eta: 0:01:42  lr: 0.000476  min_lr: 0.000011  loss: 1.0058 (0.9543)  loss_scale: 16384.0000 (13399.2556)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [4]  [1030/1349]  eta: 0:01:39  lr: 0.000476  min_lr: 0.000011  loss: 0.9315 (0.9540)  loss_scale: 16384.0000 (13428.2056)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
Epoch: [4]  [1040/1349]  eta: 0:01:36  lr: 0.000477  min_lr: 0.000011  loss: 0.8912 (0.9538)  loss_scale: 16384.0000 (13456.5994)  weight_decay: 0.0500 (0.0500)  time: 0.3091  data: 0.0002  max mem: 41808
Epoch: [4]  [1050/1349]  eta: 0:01:33  lr: 0.000478  min_lr: 0.000011  loss: 0.9540 (0.9541)  loss_scale: 16384.0000 (13484.4529)  weight_decay: 0.0500 (0.0500)  time: 0.3090  data: 0.0002  max mem: 41808
Epoch: [4]  [1060/1349]  eta: 0:01:30  lr: 0.000479  min_lr: 0.000011  loss: 0.9733 (0.9546)  loss_scale: 16384.0000 (13511.7813)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
[2025-05-23 15:53:26,000] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 15:53:26,000] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 15:53:26,000] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-05-23 15:53:26,000] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [4]  [1070/1349]  eta: 0:01:27  lr: 0.000479  min_lr: 0.000011  loss: 0.9706 (0.9542)  loss_scale: 16384.0000 (13615.0887)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
[2025-05-23 15:53:28,768] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 6471
[2025-05-23 15:53:28,769] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-05-23 15:53:28,769] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2025-05-23 15:53:28,769] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 6471
[2025-05-23 15:53:28,769] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [4]  [1080/1349]  eta: 0:01:24  lr: 0.000480  min_lr: 0.000011  loss: 0.9392 (0.9538)  loss_scale: 16384.0000 (13701.3284)  weight_decay: 0.0500 (0.0500)  time: 0.3079  data: 0.0002  max mem: 41808
Epoch: [4]  [1090/1349]  eta: 0:01:20  lr: 0.000481  min_lr: 0.000011  loss: 0.9649 (0.9540)  loss_scale: 16384.0000 (13725.9175)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [4]  [1100/1349]  eta: 0:01:17  lr: 0.000482  min_lr: 0.000011  loss: 0.9812 (0.9535)  loss_scale: 16384.0000 (13750.0599)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
Epoch: [4]  [1110/1349]  eta: 0:01:14  lr: 0.000482  min_lr: 0.000011  loss: 1.0195 (0.9544)  loss_scale: 16384.0000 (13773.7678)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [4]  [1120/1349]  eta: 0:01:11  lr: 0.000483  min_lr: 0.000011  loss: 1.0211 (0.9538)  loss_scale: 16384.0000 (13797.0526)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0001  max mem: 41808
Epoch: [4]  [1130/1349]  eta: 0:01:08  lr: 0.000484  min_lr: 0.000011  loss: 0.8983 (0.9539)  loss_scale: 16384.0000 (13819.9257)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0001  max mem: 41808
Epoch: [4]  [1140/1349]  eta: 0:01:05  lr: 0.000485  min_lr: 0.000012  loss: 1.0059 (0.9551)  loss_scale: 16384.0000 (13842.3979)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
Epoch: [4]  [1150/1349]  eta: 0:01:02  lr: 0.000485  min_lr: 0.000012  loss: 0.9853 (0.9543)  loss_scale: 16384.0000 (13864.4796)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
Epoch: [4]  [1160/1349]  eta: 0:00:59  lr: 0.000486  min_lr: 0.000012  loss: 0.9753 (0.9541)  loss_scale: 16384.0000 (13886.1809)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [4]  [1170/1349]  eta: 0:00:55  lr: 0.000487  min_lr: 0.000012  loss: 0.9940 (0.9541)  loss_scale: 16384.0000 (13907.5115)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
Epoch: [4]  [1180/1349]  eta: 0:00:52  lr: 0.000488  min_lr: 0.000012  loss: 1.0042 (0.9546)  loss_scale: 16384.0000 (13928.4809)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
Epoch: [4]  [1190/1349]  eta: 0:00:49  lr: 0.000488  min_lr: 0.000012  loss: 1.0038 (0.9544)  loss_scale: 16384.0000 (13949.0982)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [4]  [1200/1349]  eta: 0:00:46  lr: 0.000489  min_lr: 0.000012  loss: 1.0038 (0.9544)  loss_scale: 16384.0000 (13969.3722)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
[2025-05-23 15:54:08,446] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 15:54:08,446] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-05-23 15:54:08,446] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 15:54:08,446] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-05-23 15:54:09,368] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 6603
[2025-05-23 15:54:09,368] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-05-23 15:54:09,368] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2025-05-23 15:54:09,368] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 6603
[2025-05-23 15:54:09,368] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [4]  [1210/1349]  eta: 0:00:43  lr: 0.000490  min_lr: 0.000012  loss: 1.0306 (0.9546)  loss_scale: 16384.0000 (14029.8993)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [4]  [1220/1349]  eta: 0:00:40  lr: 0.000491  min_lr: 0.000012  loss: 1.0257 (0.9548)  loss_scale: 16384.0000 (14049.1794)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
Epoch: [4]  [1230/1349]  eta: 0:00:37  lr: 0.000491  min_lr: 0.000012  loss: 0.9088 (0.9545)  loss_scale: 16384.0000 (14068.1462)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [4]  [1240/1349]  eta: 0:00:34  lr: 0.000492  min_lr: 0.000012  loss: 0.9088 (0.9549)  loss_scale: 16384.0000 (14086.8074)  weight_decay: 0.0500 (0.0500)  time: 0.3078  data: 0.0001  max mem: 41808
Epoch: [4]  [1250/1349]  eta: 0:00:30  lr: 0.000493  min_lr: 0.000012  loss: 0.9233 (0.9545)  loss_scale: 16384.0000 (14105.1703)  weight_decay: 0.0500 (0.0500)  time: 0.3078  data: 0.0001  max mem: 41808
Epoch: [4]  [1260/1349]  eta: 0:00:27  lr: 0.000493  min_lr: 0.000012  loss: 0.8684 (0.9542)  loss_scale: 16384.0000 (14123.2419)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [4]  [1270/1349]  eta: 0:00:24  lr: 0.000494  min_lr: 0.000012  loss: 0.8388 (0.9539)  loss_scale: 16384.0000 (14141.0291)  weight_decay: 0.0500 (0.0500)  time: 0.3085  data: 0.0001  max mem: 41808
Epoch: [4]  [1280/1349]  eta: 0:00:21  lr: 0.000495  min_lr: 0.000012  loss: 0.9237 (0.9533)  loss_scale: 16384.0000 (14158.5386)  weight_decay: 0.0500 (0.0500)  time: 0.3084  data: 0.0001  max mem: 41808
Epoch: [4]  [1290/1349]  eta: 0:00:18  lr: 0.000496  min_lr: 0.000012  loss: 0.9237 (0.9532)  loss_scale: 16384.0000 (14175.7769)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
Epoch: [4]  [1300/1349]  eta: 0:00:15  lr: 0.000496  min_lr: 0.000012  loss: 0.9381 (0.9527)  loss_scale: 16384.0000 (14192.7502)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
Epoch: [4]  [1310/1349]  eta: 0:00:12  lr: 0.000497  min_lr: 0.000012  loss: 0.9406 (0.9530)  loss_scale: 16384.0000 (14209.4645)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [4]  [1320/1349]  eta: 0:00:09  lr: 0.000498  min_lr: 0.000012  loss: 0.9406 (0.9526)  loss_scale: 16384.0000 (14225.9258)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [4]  [1330/1349]  eta: 0:00:05  lr: 0.000499  min_lr: 0.000012  loss: 0.9264 (0.9524)  loss_scale: 16384.0000 (14242.1397)  weight_decay: 0.0500 (0.0500)  time: 0.3048  data: 0.0001  max mem: 41808
[2025-05-23 15:54:48,973] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 15:54:48,973] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-05-23 15:54:48,973] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 15:54:48,973] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [4]  [1340/1349]  eta: 0:00:02  lr: 0.000499  min_lr: 0.000012  loss: 0.8686 (0.9514)  loss_scale: 16384.0000 (14319.2006)  weight_decay: 0.0500 (0.0500)  time: 0.3023  data: 0.0001  max mem: 41808
[2025-05-23 15:54:52,297] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 6743
[2025-05-23 15:54:52,297] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 6743
[2025-05-23 15:54:52,297] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-05-23 15:54:52,297] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-05-23 15:54:52,297] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [4]  [1348/1349]  eta: 0:00:00  lr: 0.000500  min_lr: 0.000012  loss: 0.7231 (0.9511)  loss_scale: 32768.0000 (14404.3173)  weight_decay: 0.0500 (0.0500)  time: 0.3018  data: 0.0001  max mem: 41808
Epoch: [4] Total time: 0:07:00 (0.3118 s / it)
Averaged stats: lr: 0.000500  min_lr: 0.000012  loss: 0.7231 (0.9525)  loss_scale: 32768.0000 (14404.3173)  weight_decay: 0.0500 (0.0500)  total_time: 420.6535 (420.6440)
Val:  [  0/346]  eta: 1:32:40  loss: 0.9141 (0.9141)  acc1: 64.0625 (64.0625)  acc5: 97.6562 (97.6562)  time: 16.0720  data: 14.7908  max mem: 41808
Val:  [ 10/346]  eta: 0:12:19  loss: 0.1628 (0.3717)  acc1: 100.0000 (89.8438)  acc5: 100.0000 (99.2188)  time: 2.1995  data: 1.3449  max mem: 41808
Val:  [ 20/346]  eta: 0:08:57  loss: 0.1450 (0.3450)  acc1: 100.0000 (91.3318)  acc5: 100.0000 (99.5536)  time: 0.9270  data: 0.1400  max mem: 41808
Val:  [ 30/346]  eta: 0:07:08  loss: 0.1650 (0.3206)  acc1: 99.2188 (92.6411)  acc5: 100.0000 (99.6976)  time: 0.8901  data: 0.1399  max mem: 41808
Val:  [ 40/346]  eta: 0:06:11  loss: 0.1821 (0.3624)  acc1: 98.4375 (91.3681)  acc5: 100.0000 (99.7142)  time: 0.7602  data: 0.0003  max mem: 41808
Val:  [ 50/346]  eta: 0:05:30  loss: 0.1392 (0.3260)  acc1: 100.0000 (92.8002)  acc5: 100.0000 (99.7702)  time: 0.7458  data: 0.0003  max mem: 41808
Val:  [ 60/346]  eta: 0:05:03  loss: 0.1392 (0.3251)  acc1: 99.2188 (92.5205)  acc5: 100.0000 (99.8079)  time: 0.7505  data: 0.0003  max mem: 41808
Val:  [ 70/346]  eta: 0:04:41  loss: 0.2615 (0.3352)  acc1: 95.3125 (92.0775)  acc5: 100.0000 (99.8239)  time: 0.7734  data: 0.0003  max mem: 41808
Val:  [ 80/346]  eta: 0:04:26  loss: 0.2224 (0.3374)  acc1: 95.3125 (92.3322)  acc5: 100.0000 (99.7975)  time: 0.8211  data: 0.0762  max mem: 41808
Val:  [ 90/346]  eta: 0:04:12  loss: 0.1895 (0.3414)  acc1: 97.6562 (92.2133)  acc5: 100.0000 (99.8025)  time: 0.8751  data: 0.1480  max mem: 41808
Val:  [100/346]  eta: 0:04:00  loss: 0.1772 (0.3271)  acc1: 99.2188 (92.8450)  acc5: 100.0000 (99.8221)  time: 0.8649  data: 0.1438  max mem: 41808
Val:  [110/346]  eta: 0:03:48  loss: 0.1820 (0.3509)  acc1: 98.4375 (91.9412)  acc5: 100.0000 (99.7607)  time: 0.8815  data: 0.1462  max mem: 41808
Val:  [120/346]  eta: 0:03:37  loss: 0.1864 (0.3510)  acc1: 98.4375 (91.7872)  acc5: 100.0000 (99.7740)  time: 0.8974  data: 0.1466  max mem: 41808
Val:  [130/346]  eta: 0:03:27  loss: 0.1748 (0.3651)  acc1: 99.2188 (91.3585)  acc5: 100.0000 (99.7137)  time: 0.9158  data: 0.1507  max mem: 41808
Val:  [140/346]  eta: 0:03:16  loss: 0.2184 (0.3748)  acc1: 94.5312 (90.9076)  acc5: 100.0000 (99.7340)  time: 0.9100  data: 0.1475  max mem: 41808
Val:  [150/346]  eta: 0:03:06  loss: 0.2988 (0.3681)  acc1: 94.5312 (91.2200)  acc5: 100.0000 (99.7413)  time: 0.8822  data: 0.1417  max mem: 41808
Val:  [160/346]  eta: 0:02:55  loss: 0.2988 (0.3721)  acc1: 95.3125 (90.9744)  acc5: 100.0000 (99.7380)  time: 0.8804  data: 0.1422  max mem: 41808
Val:  [170/346]  eta: 0:02:46  loss: 0.3519 (0.3787)  acc1: 87.5000 (90.5428)  acc5: 100.0000 (99.7533)  time: 0.9004  data: 0.1534  max mem: 41808
Val:  [180/346]  eta: 0:02:36  loss: 0.2625 (0.3833)  acc1: 92.9688 (90.2063)  acc5: 100.0000 (99.7669)  time: 0.9059  data: 0.1615  max mem: 41808
Val:  [190/346]  eta: 0:02:26  loss: 0.2625 (0.3793)  acc1: 96.0938 (90.4164)  acc5: 100.0000 (99.7750)  time: 0.8932  data: 0.1508  max mem: 41808
Val:  [200/346]  eta: 0:02:16  loss: 0.2777 (0.3858)  acc1: 94.5312 (90.1508)  acc5: 100.0000 (99.7668)  time: 0.8841  data: 0.1418  max mem: 41808
Val:  [210/346]  eta: 0:02:07  loss: 0.2777 (0.3821)  acc1: 92.9688 (90.2399)  acc5: 100.0000 (99.7778)  time: 0.8924  data: 0.1493  max mem: 41808
Val:  [220/346]  eta: 0:01:57  loss: 0.1917 (0.3787)  acc1: 96.8750 (90.4270)  acc5: 100.0000 (99.7490)  time: 0.9040  data: 0.1525  max mem: 41808
Val:  [230/346]  eta: 0:01:48  loss: 0.1774 (0.3709)  acc1: 99.2188 (90.7704)  acc5: 100.0000 (99.7463)  time: 0.8940  data: 0.1457  max mem: 41808
Val:  [240/346]  eta: 0:01:38  loss: 0.1960 (0.3781)  acc1: 97.6562 (90.5958)  acc5: 100.0000 (99.7374)  time: 0.8972  data: 0.1594  max mem: 41808
Val:  [250/346]  eta: 0:01:29  loss: 0.1960 (0.3729)  acc1: 96.8750 (90.7620)  acc5: 100.0000 (99.7323)  time: 0.8926  data: 0.1560  max mem: 41808
Val:  [260/346]  eta: 0:01:19  loss: 0.1565 (0.3704)  acc1: 97.6562 (90.8615)  acc5: 100.0000 (99.7336)  time: 0.8788  data: 0.1449  max mem: 41808
Val:  [270/346]  eta: 0:01:10  loss: 0.1539 (0.3655)  acc1: 99.2188 (91.0574)  acc5: 100.0000 (99.7405)  time: 0.8876  data: 0.1577  max mem: 41808
Val:  [280/346]  eta: 0:01:01  loss: 0.1436 (0.3613)  acc1: 100.0000 (91.2033)  acc5: 100.0000 (99.7498)  time: 0.9097  data: 0.1611  max mem: 41808
Val:  [290/346]  eta: 0:00:51  loss: 0.1297 (0.3539)  acc1: 100.0000 (91.4734)  acc5: 100.0000 (99.7584)  time: 0.9227  data: 0.1554  max mem: 41808
Val:  [300/346]  eta: 0:00:42  loss: 0.1297 (0.3560)  acc1: 100.0000 (91.4867)  acc5: 100.0000 (99.5977)  time: 0.9175  data: 0.1528  max mem: 41808
Val:  [310/346]  eta: 0:00:33  loss: 0.1567 (0.3574)  acc1: 97.6562 (91.4289)  acc5: 100.0000 (99.6031)  time: 0.9089  data: 0.1486  max mem: 41808
Val:  [320/346]  eta: 0:00:23  loss: 0.1884 (0.3608)  acc1: 99.2188 (91.2018)  acc5: 100.0000 (99.6130)  time: 0.8891  data: 0.1447  max mem: 41808
Val:  [330/346]  eta: 0:00:14  loss: 0.3199 (0.3677)  acc1: 92.1875 (90.9885)  acc5: 100.0000 (99.5822)  time: 0.8882  data: 0.1485  max mem: 41808
Val:  [340/346]  eta: 0:00:05  loss: 0.3240 (0.3771)  acc1: 91.4062 (90.6433)  acc5: 100.0000 (99.5922)  time: 0.8847  data: 0.1585  max mem: 41808
Val:  [345/346]  eta: 0:00:00  loss: 0.1965 (0.3739)  acc1: 96.8750 (90.7564)  acc5: 100.0000 (99.5977)  time: 0.8783  data: 0.1749  max mem: 41808
Val: Total time: 0:05:18 (0.9208 s / it)
* Acc@1 90.765 Acc@5 99.602 loss 0.372
Accuracy of the network on the 88494 val videos: 90.8%
Max accuracy: 91.25%   Max Epoch: 3
Epoch: [5]  [   0/1349]  eta: 1:45:23  lr: 0.000500  min_lr: 0.000012  loss: 1.2207 (1.2207)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 4.6876  data: 4.3267  max mem: 41808
Epoch: [5]  [  10/1349]  eta: 0:16:16  lr: 0.000500  min_lr: 0.000012  loss: 0.9917 (0.9752)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7294  data: 0.4074  max mem: 41808
Epoch: [5]  [  20/1349]  eta: 0:11:43  lr: 0.000500  min_lr: 0.000012  loss: 0.9638 (0.9850)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3217  data: 0.0078  max mem: 41808
Epoch: [5]  [  30/1349]  eta: 0:10:04  lr: 0.000500  min_lr: 0.000012  loss: 0.9638 (0.9861)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3095  data: 0.0001  max mem: 41808
Epoch: [5]  [  40/1349]  eta: 0:09:12  lr: 0.000500  min_lr: 0.000012  loss: 0.9366 (0.9885)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3087  data: 0.0001  max mem: 41808
Epoch: [5]  [  50/1349]  eta: 0:08:38  lr: 0.000500  min_lr: 0.000012  loss: 0.9608 (0.9810)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3078  data: 0.0001  max mem: 41808
Epoch: [5]  [  60/1349]  eta: 0:08:15  lr: 0.000500  min_lr: 0.000012  loss: 0.8513 (0.9550)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
Epoch: [5]  [  70/1349]  eta: 0:07:57  lr: 0.000500  min_lr: 0.000012  loss: 0.8488 (0.9461)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
Epoch: [5]  [  80/1349]  eta: 0:07:43  lr: 0.000500  min_lr: 0.000012  loss: 0.8246 (0.9316)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
Epoch: [5]  [  90/1349]  eta: 0:07:31  lr: 0.000500  min_lr: 0.000012  loss: 0.9365 (0.9364)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3059  data: 0.0001  max mem: 41808
Epoch: [5]  [ 100/1349]  eta: 0:07:21  lr: 0.000500  min_lr: 0.000012  loss: 0.9578 (0.9385)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3058  data: 0.0001  max mem: 41808
Epoch: [5]  [ 110/1349]  eta: 0:07:12  lr: 0.000500  min_lr: 0.000012  loss: 0.9578 (0.9411)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3059  data: 0.0001  max mem: 41808
Epoch: [5]  [ 120/1349]  eta: 0:07:04  lr: 0.000500  min_lr: 0.000012  loss: 0.9411 (0.9379)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3056  data: 0.0001  max mem: 41808
[2025-05-23 16:00:55,483] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 16:00:55,484] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-05-23 16:00:55,484] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 16:00:55,484] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [5]  [ 130/1349]  eta: 0:06:57  lr: 0.000500  min_lr: 0.000012  loss: 0.8859 (0.9342)  loss_scale: 16384.0000 (16884.2748)  weight_decay: 0.0500 (0.0500)  time: 0.3057  data: 0.0001  max mem: 41808
[2025-05-23 16:00:57,321] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 6878
[2025-05-23 16:00:57,321] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-05-23 16:00:57,321] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 6878
[2025-05-23 16:00:57,321] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-05-23 16:00:57,321] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [5]  [ 140/1349]  eta: 0:06:50  lr: 0.000500  min_lr: 0.000012  loss: 0.9296 (0.9378)  loss_scale: 16384.0000 (17081.1915)  weight_decay: 0.0500 (0.0500)  time: 0.3057  data: 0.0001  max mem: 41808
Epoch: [5]  [ 150/1349]  eta: 0:06:44  lr: 0.000500  min_lr: 0.000012  loss: 0.9046 (0.9320)  loss_scale: 16384.0000 (17035.0199)  weight_decay: 0.0500 (0.0500)  time: 0.3054  data: 0.0001  max mem: 41808
Epoch: [5]  [ 160/1349]  eta: 0:06:38  lr: 0.000500  min_lr: 0.000012  loss: 0.9046 (0.9336)  loss_scale: 16384.0000 (16994.5839)  weight_decay: 0.0500 (0.0500)  time: 0.3056  data: 0.0001  max mem: 41808
Epoch: [5]  [ 170/1349]  eta: 0:06:33  lr: 0.000500  min_lr: 0.000012  loss: 0.9717 (0.9344)  loss_scale: 16384.0000 (16958.8772)  weight_decay: 0.0500 (0.0500)  time: 0.3060  data: 0.0001  max mem: 41808
Epoch: [5]  [ 180/1349]  eta: 0:06:28  lr: 0.000500  min_lr: 0.000012  loss: 0.9186 (0.9322)  loss_scale: 16384.0000 (16927.1160)  weight_decay: 0.0500 (0.0500)  time: 0.3058  data: 0.0001  max mem: 41808
Epoch: [5]  [ 190/1349]  eta: 0:06:23  lr: 0.000500  min_lr: 0.000012  loss: 0.9186 (0.9327)  loss_scale: 16384.0000 (16898.6806)  weight_decay: 0.0500 (0.0500)  time: 0.3058  data: 0.0001  max mem: 41808
Epoch: [5]  [ 200/1349]  eta: 0:06:18  lr: 0.000500  min_lr: 0.000012  loss: 0.8566 (0.9264)  loss_scale: 16384.0000 (16873.0746)  weight_decay: 0.0500 (0.0500)  time: 0.3062  data: 0.0001  max mem: 41808
Epoch: [5]  [ 210/1349]  eta: 0:06:14  lr: 0.000500  min_lr: 0.000012  loss: 0.8184 (0.9252)  loss_scale: 16384.0000 (16849.8957)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
Epoch: [5]  [ 220/1349]  eta: 0:06:09  lr: 0.000500  min_lr: 0.000012  loss: 1.0260 (0.9315)  loss_scale: 16384.0000 (16828.8145)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0002  max mem: 41808
Epoch: [5]  [ 230/1349]  eta: 0:06:05  lr: 0.000500  min_lr: 0.000012  loss: 1.0080 (0.9333)  loss_scale: 16384.0000 (16809.5584)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
Epoch: [5]  [ 240/1349]  eta: 0:06:01  lr: 0.000500  min_lr: 0.000012  loss: 0.9415 (0.9347)  loss_scale: 16384.0000 (16791.9004)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [5]  [ 250/1349]  eta: 0:05:57  lr: 0.000500  min_lr: 0.000012  loss: 0.9380 (0.9335)  loss_scale: 16384.0000 (16775.6494)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
[2025-05-23 16:01:34,470] [INFO] [logging.py:96:log_dist] [Rank 0] step=7000, skipped=38, lr=[1.1878119915867487e-05, 1.1878119915867487e-05, 1.583749322115665e-05, 1.583749322115665e-05, 2.1116657628208866e-05, 2.1116657628208866e-05, 2.8155543504278486e-05, 2.8155543504278486e-05, 3.7540724672371315e-05, 3.7540724672371315e-05, 5.005429956316175e-05, 5.005429956316175e-05, 6.673906608421567e-05, 6.673906608421567e-05, 8.898542144562089e-05, 8.898542144562089e-05, 0.0001186472285941612, 0.0001186472285941612, 0.00015819630479221492, 0.00015819630479221492, 0.0002109284063896199, 0.0002109284063896199, 0.00028123787518615986, 0.00028123787518615986, 0.0003749838335815465, 0.0003749838335815465, 0.0004999784447753953, 0.0004999784447753953], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-05-23 16:01:34,471] [INFO] [timer.py:260:stop] epoch=0/micro_step=7000/global_step=7000, RunningAvgSamplesPerSec=212.80647791893037, CurrSamplesPerSec=214.2660091601786, MemAllocated=1.37GB, MaxMemAllocated=40.83GB
Epoch: [5]  [ 260/1349]  eta: 0:05:53  lr: 0.000500  min_lr: 0.000012  loss: 0.9339 (0.9344)  loss_scale: 16384.0000 (16760.6437)  weight_decay: 0.0500 (0.0500)  time: 0.3100  data: 0.0016  max mem: 41808
[2025-05-23 16:01:36,925] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 16:01:36,925] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-05-23 16:01:36,925] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 16:01:36,925] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [5]  [ 270/1349]  eta: 0:05:49  lr: 0.000500  min_lr: 0.000012  loss: 0.9339 (0.9340)  loss_scale: 16384.0000 (17290.8635)  weight_decay: 0.0500 (0.0500)  time: 0.3099  data: 0.0016  max mem: 41808
Epoch: [5]  [ 280/1349]  eta: 0:05:45  lr: 0.000500  min_lr: 0.000012  loss: 0.9573 (0.9341)  loss_scale: 32768.0000 (17841.6512)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
[2025-05-23 16:01:44,911] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 7033
[2025-05-23 16:01:44,911] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-05-23 16:01:44,911] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 7033
[2025-05-23 16:01:44,911] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-05-23 16:01:44,911] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [5]  [ 290/1349]  eta: 0:05:41  lr: 0.000500  min_lr: 0.000012  loss: 1.0172 (0.9365)  loss_scale: 32768.0000 (18185.6770)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0002  max mem: 41808
Epoch: [5]  [ 300/1349]  eta: 0:05:38  lr: 0.000500  min_lr: 0.000012  loss: 0.9627 (0.9339)  loss_scale: 16384.0000 (18125.8206)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
Epoch: [5]  [ 310/1349]  eta: 0:05:34  lr: 0.000500  min_lr: 0.000012  loss: 0.8987 (0.9343)  loss_scale: 16384.0000 (18069.8135)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0001  max mem: 41808
Epoch: [5]  [ 320/1349]  eta: 0:05:30  lr: 0.000500  min_lr: 0.000012  loss: 0.9724 (0.9372)  loss_scale: 16384.0000 (18017.2960)  weight_decay: 0.0500 (0.0500)  time: 0.3060  data: 0.0001  max mem: 41808
Epoch: [5]  [ 330/1349]  eta: 0:05:26  lr: 0.000500  min_lr: 0.000012  loss: 1.0189 (0.9387)  loss_scale: 16384.0000 (17967.9517)  weight_decay: 0.0500 (0.0500)  time: 0.3060  data: 0.0001  max mem: 41808
Epoch: [5]  [ 340/1349]  eta: 0:05:23  lr: 0.000500  min_lr: 0.000012  loss: 1.0189 (0.9420)  loss_scale: 16384.0000 (17921.5015)  weight_decay: 0.0500 (0.0500)  time: 0.3066  data: 0.0001  max mem: 41808
Epoch: [5]  [ 350/1349]  eta: 0:05:19  lr: 0.000500  min_lr: 0.000012  loss: 1.0149 (0.9435)  loss_scale: 16384.0000 (17877.6980)  weight_decay: 0.0500 (0.0500)  time: 0.3064  data: 0.0001  max mem: 41808
Epoch: [5]  [ 360/1349]  eta: 0:05:16  lr: 0.000500  min_lr: 0.000012  loss: 0.9298 (0.9420)  loss_scale: 16384.0000 (17836.3213)  weight_decay: 0.0500 (0.0500)  time: 0.3057  data: 0.0001  max mem: 41808
Epoch: [5]  [ 370/1349]  eta: 0:05:12  lr: 0.000500  min_lr: 0.000012  loss: 0.8948 (0.9405)  loss_scale: 16384.0000 (17797.1752)  weight_decay: 0.0500 (0.0500)  time: 0.3066  data: 0.0001  max mem: 41808
Epoch: [5]  [ 380/1349]  eta: 0:05:09  lr: 0.000500  min_lr: 0.000012  loss: 0.9896 (0.9417)  loss_scale: 16384.0000 (17760.0840)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [5]  [ 390/1349]  eta: 0:05:05  lr: 0.000500  min_lr: 0.000012  loss: 1.0219 (0.9431)  loss_scale: 16384.0000 (17724.8900)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
Epoch: [5]  [ 400/1349]  eta: 0:05:02  lr: 0.000500  min_lr: 0.000012  loss: 1.0098 (0.9430)  loss_scale: 16384.0000 (17691.4514)  weight_decay: 0.0500 (0.0500)  time: 0.3064  data: 0.0001  max mem: 41808
Epoch: [5]  [ 410/1349]  eta: 0:04:58  lr: 0.000500  min_lr: 0.000012  loss: 0.9136 (0.9431)  loss_scale: 16384.0000 (17659.6399)  weight_decay: 0.0500 (0.0500)  time: 0.3061  data: 0.0001  max mem: 41808
[2025-05-23 16:02:24,459] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 16:02:24,460] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-05-23 16:02:24,460] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 16:02:24,460] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [5]  [ 420/1349]  eta: 0:04:55  lr: 0.000500  min_lr: 0.000012  loss: 0.8838 (0.9412)  loss_scale: 16384.0000 (17785.0071)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [5]  [ 430/1349]  eta: 0:04:51  lr: 0.000500  min_lr: 0.000012  loss: 0.9365 (0.9433)  loss_scale: 32768.0000 (18132.6404)  weight_decay: 0.0500 (0.0500)  time: 0.3081  data: 0.0001  max mem: 41808
[2025-05-23 16:02:29,393] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 7178
[2025-05-23 16:02:29,393] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 7178
[2025-05-23 16:02:29,393] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-05-23 16:02:29,393] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-05-23 16:02:29,393] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [5]  [ 440/1349]  eta: 0:04:48  lr: 0.000500  min_lr: 0.000012  loss: 1.0148 (0.9427)  loss_scale: 32768.0000 (18167.2925)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
Epoch: [5]  [ 450/1349]  eta: 0:04:45  lr: 0.000500  min_lr: 0.000012  loss: 1.0025 (0.9428)  loss_scale: 16384.0000 (18127.7517)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [5]  [ 460/1349]  eta: 0:04:41  lr: 0.000500  min_lr: 0.000012  loss: 0.9881 (0.9429)  loss_scale: 16384.0000 (18089.9262)  weight_decay: 0.0500 (0.0500)  time: 0.3082  data: 0.0001  max mem: 41808
Epoch: [5]  [ 470/1349]  eta: 0:04:38  lr: 0.000500  min_lr: 0.000012  loss: 0.9845 (0.9438)  loss_scale: 16384.0000 (18053.7070)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [5]  [ 480/1349]  eta: 0:04:35  lr: 0.000500  min_lr: 0.000012  loss: 0.9845 (0.9449)  loss_scale: 16384.0000 (18018.9938)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
Epoch: [5]  [ 490/1349]  eta: 0:04:31  lr: 0.000500  min_lr: 0.000012  loss: 1.0557 (0.9457)  loss_scale: 16384.0000 (17985.6945)  weight_decay: 0.0500 (0.0500)  time: 0.3085  data: 0.0001  max mem: 41808
Epoch: [5]  [ 500/1349]  eta: 0:04:28  lr: 0.000500  min_lr: 0.000012  loss: 0.9763 (0.9450)  loss_scale: 16384.0000 (17953.7246)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [5]  [ 510/1349]  eta: 0:04:25  lr: 0.000500  min_lr: 0.000012  loss: 0.9763 (0.9463)  loss_scale: 16384.0000 (17923.0059)  weight_decay: 0.0500 (0.0500)  time: 0.3066  data: 0.0001  max mem: 41808
Epoch: [5]  [ 520/1349]  eta: 0:04:21  lr: 0.000500  min_lr: 0.000012  loss: 0.9992 (0.9464)  loss_scale: 16384.0000 (17893.4664)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
Epoch: [5]  [ 530/1349]  eta: 0:04:18  lr: 0.000500  min_lr: 0.000012  loss: 0.9412 (0.9465)  loss_scale: 16384.0000 (17865.0395)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [5]  [ 540/1349]  eta: 0:04:15  lr: 0.000500  min_lr: 0.000012  loss: 0.9223 (0.9459)  loss_scale: 16384.0000 (17837.6636)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
Epoch: [5]  [ 550/1349]  eta: 0:04:11  lr: 0.000500  min_lr: 0.000012  loss: 0.8912 (0.9450)  loss_scale: 16384.0000 (17811.2813)  weight_decay: 0.0500 (0.0500)  time: 0.3066  data: 0.0001  max mem: 41808
Epoch: [5]  [ 560/1349]  eta: 0:04:08  lr: 0.000500  min_lr: 0.000012  loss: 0.8623 (0.9431)  loss_scale: 16384.0000 (17785.8396)  weight_decay: 0.0500 (0.0500)  time: 0.3066  data: 0.0001  max mem: 41808
[2025-05-23 16:03:09,035] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 16:03:09,035] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-05-23 16:03:09,035] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 16:03:09,035] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-05-23 16:03:11,185] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 7314
[2025-05-23 16:03:11,185] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-05-23 16:03:11,185] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 7314
[2025-05-23 16:03:11,185] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-05-23 16:03:11,185] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [5]  [ 570/1349]  eta: 0:04:05  lr: 0.000500  min_lr: 0.000012  loss: 0.9811 (0.9453)  loss_scale: 16384.0000 (17962.1436)  weight_decay: 0.0500 (0.0500)  time: 0.3064  data: 0.0001  max mem: 41808
Epoch: [5]  [ 580/1349]  eta: 0:04:02  lr: 0.000500  min_lr: 0.000012  loss: 1.0054 (0.9462)  loss_scale: 16384.0000 (17934.9811)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [5]  [ 590/1349]  eta: 0:03:58  lr: 0.000500  min_lr: 0.000012  loss: 0.9685 (0.9455)  loss_scale: 16384.0000 (17908.7377)  weight_decay: 0.0500 (0.0500)  time: 0.3086  data: 0.0001  max mem: 41808
Epoch: [5]  [ 600/1349]  eta: 0:03:55  lr: 0.000500  min_lr: 0.000012  loss: 0.8711 (0.9435)  loss_scale: 16384.0000 (17883.3677)  weight_decay: 0.0500 (0.0500)  time: 0.3086  data: 0.0001  max mem: 41808
Epoch: [5]  [ 610/1349]  eta: 0:03:52  lr: 0.000500  min_lr: 0.000012  loss: 0.7887 (0.9420)  loss_scale: 16384.0000 (17858.8282)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
Epoch: [5]  [ 620/1349]  eta: 0:03:49  lr: 0.000500  min_lr: 0.000012  loss: 0.8386 (0.9407)  loss_scale: 16384.0000 (17835.0789)  weight_decay: 0.0500 (0.0500)  time: 0.3085  data: 0.0001  max mem: 41808
Epoch: [5]  [ 630/1349]  eta: 0:03:46  lr: 0.000500  min_lr: 0.000012  loss: 0.8359 (0.9393)  loss_scale: 16384.0000 (17812.0824)  weight_decay: 0.0500 (0.0500)  time: 0.3087  data: 0.0001  max mem: 41808
Epoch: [5]  [ 640/1349]  eta: 0:03:42  lr: 0.000500  min_lr: 0.000012  loss: 0.9677 (0.9389)  loss_scale: 16384.0000 (17789.8034)  weight_decay: 0.0500 (0.0500)  time: 0.3084  data: 0.0001  max mem: 41808
Epoch: [5]  [ 650/1349]  eta: 0:03:39  lr: 0.000500  min_lr: 0.000012  loss: 0.9869 (0.9392)  loss_scale: 16384.0000 (17768.2089)  weight_decay: 0.0500 (0.0500)  time: 0.3089  data: 0.0001  max mem: 41808
Epoch: [5]  [ 660/1349]  eta: 0:03:36  lr: 0.000500  min_lr: 0.000012  loss: 0.9374 (0.9391)  loss_scale: 16384.0000 (17747.2678)  weight_decay: 0.0500 (0.0500)  time: 0.3078  data: 0.0001  max mem: 41808
Epoch: [5]  [ 670/1349]  eta: 0:03:33  lr: 0.000500  min_lr: 0.000012  loss: 0.9374 (0.9386)  loss_scale: 16384.0000 (17726.9508)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [5]  [ 680/1349]  eta: 0:03:30  lr: 0.000500  min_lr: 0.000012  loss: 0.8918 (0.9381)  loss_scale: 16384.0000 (17707.2305)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
Epoch: [5]  [ 690/1349]  eta: 0:03:26  lr: 0.000500  min_lr: 0.000012  loss: 0.8918 (0.9374)  loss_scale: 16384.0000 (17688.0810)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
[2025-05-23 16:03:50,906] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 16:03:50,906] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-05-23 16:03:50,906] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 16:03:50,906] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [5]  [ 700/1349]  eta: 0:03:23  lr: 0.000500  min_lr: 0.000012  loss: 0.9492 (0.9370)  loss_scale: 16384.0000 (17739.5949)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
[2025-05-23 16:03:51,824] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 7446
[2025-05-23 16:03:51,825] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 7446
[2025-05-23 16:03:51,825] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-05-23 16:03:51,825] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-05-23 16:03:51,825] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [5]  [ 710/1349]  eta: 0:03:20  lr: 0.000500  min_lr: 0.000012  loss: 0.9460 (0.9366)  loss_scale: 16384.0000 (17720.5288)  weight_decay: 0.0500 (0.0500)  time: 0.3062  data: 0.0001  max mem: 41808
Epoch: [5]  [ 720/1349]  eta: 0:03:17  lr: 0.000500  min_lr: 0.000012  loss: 0.9199 (0.9370)  loss_scale: 16384.0000 (17701.9917)  weight_decay: 0.0500 (0.0500)  time: 0.3066  data: 0.0001  max mem: 41808
Epoch: [5]  [ 730/1349]  eta: 0:03:14  lr: 0.000500  min_lr: 0.000012  loss: 0.9199 (0.9367)  loss_scale: 16384.0000 (17683.9617)  weight_decay: 0.0500 (0.0500)  time: 0.3079  data: 0.0002  max mem: 41808
Epoch: [5]  [ 740/1349]  eta: 0:03:10  lr: 0.000500  min_lr: 0.000012  loss: 0.9761 (0.9372)  loss_scale: 16384.0000 (17666.4184)  weight_decay: 0.0500 (0.0500)  time: 0.3084  data: 0.0002  max mem: 41808
Epoch: [5]  [ 750/1349]  eta: 0:03:07  lr: 0.000500  min_lr: 0.000012  loss: 0.9305 (0.9364)  loss_scale: 16384.0000 (17649.3422)  weight_decay: 0.0500 (0.0500)  time: 0.3079  data: 0.0001  max mem: 41808
Epoch: [5]  [ 760/1349]  eta: 0:03:04  lr: 0.000500  min_lr: 0.000012  loss: 0.9305 (0.9374)  loss_scale: 16384.0000 (17632.7148)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [5]  [ 770/1349]  eta: 0:03:01  lr: 0.000500  min_lr: 0.000012  loss: 0.9503 (0.9379)  loss_scale: 16384.0000 (17616.5188)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
Epoch: [5]  [ 780/1349]  eta: 0:02:58  lr: 0.000500  min_lr: 0.000012  loss: 0.9188 (0.9364)  loss_scale: 16384.0000 (17600.7375)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [5]  [ 790/1349]  eta: 0:02:54  lr: 0.000500  min_lr: 0.000012  loss: 0.9020 (0.9366)  loss_scale: 16384.0000 (17585.3552)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [5]  [ 800/1349]  eta: 0:02:51  lr: 0.000500  min_lr: 0.000012  loss: 0.9803 (0.9367)  loss_scale: 16384.0000 (17570.3571)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [5]  [ 810/1349]  eta: 0:02:48  lr: 0.000500  min_lr: 0.000012  loss: 0.9423 (0.9362)  loss_scale: 16384.0000 (17555.7287)  weight_decay: 0.0500 (0.0500)  time: 0.3078  data: 0.0001  max mem: 41808
Epoch: [5]  [ 820/1349]  eta: 0:02:45  lr: 0.000500  min_lr: 0.000012  loss: 0.9149 (0.9349)  loss_scale: 16384.0000 (17541.4568)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
[2025-05-23 16:04:31,511] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 16:04:31,511] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-05-23 16:04:31,511] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 16:04:31,511] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [5]  [ 830/1349]  eta: 0:02:42  lr: 0.000500  min_lr: 0.000012  loss: 0.8561 (0.9349)  loss_scale: 16384.0000 (17547.2443)  weight_decay: 0.0500 (0.0500)  time: 0.3082  data: 0.0001  max mem: 41808
[2025-05-23 16:04:32,742] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 7579
[2025-05-23 16:04:32,742] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 7579
[2025-05-23 16:04:32,742] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-05-23 16:04:32,742] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-05-23 16:04:32,743] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [5]  [ 840/1349]  eta: 0:02:39  lr: 0.000500  min_lr: 0.000012  loss: 0.9838 (0.9357)  loss_scale: 16384.0000 (17591.8573)  weight_decay: 0.0500 (0.0500)  time: 0.3098  data: 0.0001  max mem: 41808
Epoch: [5]  [ 850/1349]  eta: 0:02:36  lr: 0.000500  min_lr: 0.000012  loss: 1.0514 (0.9360)  loss_scale: 16384.0000 (17577.6639)  weight_decay: 0.0500 (0.0500)  time: 0.3096  data: 0.0001  max mem: 41808
Epoch: [5]  [ 860/1349]  eta: 0:02:32  lr: 0.000500  min_lr: 0.000012  loss: 1.0083 (0.9362)  loss_scale: 16384.0000 (17563.8002)  weight_decay: 0.0500 (0.0500)  time: 0.3094  data: 0.0001  max mem: 41808
Epoch: [5]  [ 870/1349]  eta: 0:02:29  lr: 0.000500  min_lr: 0.000012  loss: 0.9763 (0.9365)  loss_scale: 16384.0000 (17550.2549)  weight_decay: 0.0500 (0.0500)  time: 0.3098  data: 0.0001  max mem: 41808
Epoch: [5]  [ 880/1349]  eta: 0:02:26  lr: 0.000500  min_lr: 0.000012  loss: 0.9406 (0.9356)  loss_scale: 16384.0000 (17537.0170)  weight_decay: 0.0500 (0.0500)  time: 0.3084  data: 0.0001  max mem: 41808
Epoch: [5]  [ 890/1349]  eta: 0:02:23  lr: 0.000500  min_lr: 0.000012  loss: 0.9018 (0.9349)  loss_scale: 16384.0000 (17524.0763)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
Epoch: [5]  [ 900/1349]  eta: 0:02:20  lr: 0.000500  min_lr: 0.000012  loss: 0.9180 (0.9347)  loss_scale: 16384.0000 (17511.4229)  weight_decay: 0.0500 (0.0500)  time: 0.3063  data: 0.0001  max mem: 41808
Epoch: [5]  [ 910/1349]  eta: 0:02:17  lr: 0.000500  min_lr: 0.000012  loss: 0.9249 (0.9344)  loss_scale: 16384.0000 (17499.0472)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
Epoch: [5]  [ 920/1349]  eta: 0:02:13  lr: 0.000500  min_lr: 0.000012  loss: 0.9628 (0.9350)  loss_scale: 16384.0000 (17486.9403)  weight_decay: 0.0500 (0.0500)  time: 0.3100  data: 0.0002  max mem: 41808
Epoch: [5]  [ 930/1349]  eta: 0:02:10  lr: 0.000500  min_lr: 0.000012  loss: 0.8376 (0.9334)  loss_scale: 16384.0000 (17475.0934)  weight_decay: 0.0500 (0.0500)  time: 0.3091  data: 0.0001  max mem: 41808
Epoch: [5]  [ 940/1349]  eta: 0:02:07  lr: 0.000500  min_lr: 0.000012  loss: 0.7786 (0.9322)  loss_scale: 16384.0000 (17463.4984)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
Epoch: [5]  [ 950/1349]  eta: 0:02:04  lr: 0.000500  min_lr: 0.000012  loss: 0.9243 (0.9329)  loss_scale: 16384.0000 (17452.1472)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
Epoch: [5]  [ 960/1349]  eta: 0:02:01  lr: 0.000500  min_lr: 0.000012  loss: 0.9777 (0.9331)  loss_scale: 16384.0000 (17441.0323)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
[2025-05-23 16:05:12,532] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 16:05:12,532] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 16:05:12,532] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-05-23 16:05:12,532] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [5]  [ 970/1349]  eta: 0:01:58  lr: 0.000500  min_lr: 0.000012  loss: 0.9444 (0.9325)  loss_scale: 16384.0000 (17565.1329)  weight_decay: 0.0500 (0.0500)  time: 0.3083  data: 0.0001  max mem: 41808
[2025-05-23 16:05:17,750] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 7725
[2025-05-23 16:05:17,750] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-05-23 16:05:17,750] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2025-05-23 16:05:17,750] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 7725
[2025-05-23 16:05:17,750] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [5]  [ 980/1349]  eta: 0:01:55  lr: 0.000500  min_lr: 0.000012  loss: 0.9491 (0.9327)  loss_scale: 32768.0000 (17703.4047)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [5]  [ 990/1349]  eta: 0:01:51  lr: 0.000500  min_lr: 0.000012  loss: 0.8750 (0.9330)  loss_scale: 16384.0000 (17690.0908)  weight_decay: 0.0500 (0.0500)  time: 0.3066  data: 0.0001  max mem: 41808
Epoch: [5]  [1000/1349]  eta: 0:01:48  lr: 0.000500  min_lr: 0.000012  loss: 0.8951 (0.9334)  loss_scale: 16384.0000 (17677.0430)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [5]  [1010/1349]  eta: 0:01:45  lr: 0.000500  min_lr: 0.000012  loss: 0.9108 (0.9328)  loss_scale: 16384.0000 (17664.2532)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [5]  [1020/1349]  eta: 0:01:42  lr: 0.000500  min_lr: 0.000012  loss: 0.8922 (0.9319)  loss_scale: 16384.0000 (17651.7140)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [5]  [1030/1349]  eta: 0:01:39  lr: 0.000500  min_lr: 0.000012  loss: 0.9457 (0.9326)  loss_scale: 16384.0000 (17639.4180)  weight_decay: 0.0500 (0.0500)  time: 0.3081  data: 0.0001  max mem: 41808
Epoch: [5]  [1040/1349]  eta: 0:01:36  lr: 0.000500  min_lr: 0.000012  loss: 0.9948 (0.9328)  loss_scale: 16384.0000 (17627.3583)  weight_decay: 0.0500 (0.0500)  time: 0.3081  data: 0.0001  max mem: 41808
Epoch: [5]  [1050/1349]  eta: 0:01:33  lr: 0.000500  min_lr: 0.000012  loss: 0.9482 (0.9324)  loss_scale: 16384.0000 (17615.5281)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [5]  [1060/1349]  eta: 0:01:30  lr: 0.000500  min_lr: 0.000012  loss: 0.8547 (0.9310)  loss_scale: 16384.0000 (17603.9208)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
Epoch: [5]  [1070/1349]  eta: 0:01:26  lr: 0.000500  min_lr: 0.000012  loss: 0.9360 (0.9316)  loss_scale: 16384.0000 (17592.5303)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0002  max mem: 41808
Epoch: [5]  [1080/1349]  eta: 0:01:23  lr: 0.000500  min_lr: 0.000012  loss: 0.9298 (0.9310)  loss_scale: 16384.0000 (17581.3506)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [5]  [1090/1349]  eta: 0:01:20  lr: 0.000500  min_lr: 0.000012  loss: 0.9298 (0.9314)  loss_scale: 16384.0000 (17570.3758)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
Epoch: [5]  [1100/1349]  eta: 0:01:17  lr: 0.000500  min_lr: 0.000012  loss: 0.9177 (0.9306)  loss_scale: 16384.0000 (17559.6004)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
[2025-05-23 16:05:57,388] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 16:05:57,389] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-05-23 16:05:57,389] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 16:05:57,389] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [5]  [1110/1349]  eta: 0:01:14  lr: 0.000500  min_lr: 0.000012  loss: 0.9177 (0.9312)  loss_scale: 16384.0000 (17578.5131)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
Epoch: [5]  [1120/1349]  eta: 0:01:11  lr: 0.000500  min_lr: 0.000012  loss: 0.9299 (0.9315)  loss_scale: 32768.0000 (17714.0125)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
[2025-05-23 16:06:02,616] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 7871
[2025-05-23 16:06:02,616] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-05-23 16:06:02,616] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 7871
[2025-05-23 16:06:02,617] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-05-23 16:06:02,617] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [5]  [1130/1349]  eta: 0:01:08  lr: 0.000500  min_lr: 0.000012  loss: 0.9904 (0.9324)  loss_scale: 32768.0000 (17774.6844)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
Epoch: [5]  [1140/1349]  eta: 0:01:05  lr: 0.000500  min_lr: 0.000012  loss: 0.9904 (0.9326)  loss_scale: 16384.0000 (17762.4961)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [5]  [1150/1349]  eta: 0:01:01  lr: 0.000500  min_lr: 0.000012  loss: 0.9532 (0.9327)  loss_scale: 16384.0000 (17750.5195)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0001  max mem: 41808
Epoch: [5]  [1160/1349]  eta: 0:00:58  lr: 0.000500  min_lr: 0.000012  loss: 0.9118 (0.9324)  loss_scale: 16384.0000 (17738.7494)  weight_decay: 0.0500 (0.0500)  time: 0.3080  data: 0.0001  max mem: 41808
Epoch: [5]  [1170/1349]  eta: 0:00:55  lr: 0.000500  min_lr: 0.000012  loss: 0.8804 (0.9322)  loss_scale: 16384.0000 (17727.1802)  weight_decay: 0.0500 (0.0500)  time: 0.3079  data: 0.0001  max mem: 41808
Epoch: [5]  [1180/1349]  eta: 0:00:52  lr: 0.000500  min_lr: 0.000012  loss: 0.7573 (0.9307)  loss_scale: 16384.0000 (17715.8069)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [5]  [1190/1349]  eta: 0:00:49  lr: 0.000500  min_lr: 0.000012  loss: 0.7667 (0.9300)  loss_scale: 16384.0000 (17704.6247)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0002  max mem: 41808
Epoch: [5]  [1200/1349]  eta: 0:00:46  lr: 0.000500  min_lr: 0.000012  loss: 0.8521 (0.9293)  loss_scale: 16384.0000 (17693.6286)  weight_decay: 0.0500 (0.0500)  time: 0.3084  data: 0.0001  max mem: 41808
Epoch: [5]  [1210/1349]  eta: 0:00:43  lr: 0.000500  min_lr: 0.000012  loss: 0.8552 (0.9290)  loss_scale: 16384.0000 (17682.8142)  weight_decay: 0.0500 (0.0500)  time: 0.3095  data: 0.0001  max mem: 41808
Epoch: [5]  [1220/1349]  eta: 0:00:40  lr: 0.000500  min_lr: 0.000012  loss: 0.8858 (0.9292)  loss_scale: 16384.0000 (17672.1769)  weight_decay: 0.0500 (0.0500)  time: 0.3093  data: 0.0001  max mem: 41808
Epoch: [5]  [1230/1349]  eta: 0:00:37  lr: 0.000499  min_lr: 0.000012  loss: 0.9638 (0.9295)  loss_scale: 16384.0000 (17661.7124)  weight_decay: 0.0500 (0.0500)  time: 0.3097  data: 0.0001  max mem: 41808
Epoch: [5]  [1240/1349]  eta: 0:00:33  lr: 0.000499  min_lr: 0.000012  loss: 1.0115 (0.9300)  loss_scale: 16384.0000 (17651.4166)  weight_decay: 0.0500 (0.0500)  time: 0.3096  data: 0.0001  max mem: 41808
Epoch: [5]  [1250/1349]  eta: 0:00:30  lr: 0.000499  min_lr: 0.000012  loss: 0.9257 (0.9290)  loss_scale: 16384.0000 (17641.2854)  weight_decay: 0.0500 (0.0500)  time: 0.3085  data: 0.0002  max mem: 41808
[2025-05-23 16:06:42,129] [INFO] [logging.py:96:log_dist] [Rank 0] step=8000, skipped=45, lr=[1.1866154426758566e-05, 1.1866154426758566e-05, 1.5821539235678086e-05, 1.5821539235678086e-05, 2.1095385647570783e-05, 2.1095385647570783e-05, 2.812718086342771e-05, 2.812718086342771e-05, 3.7502907817903615e-05, 3.7502907817903615e-05, 5.0003877090538154e-05, 5.0003877090538154e-05, 6.667183612071754e-05, 6.667183612071754e-05, 8.889578149429005e-05, 8.889578149429005e-05, 0.0001185277086590534, 0.0001185277086590534, 0.00015803694487873785, 0.00015803694487873785, 0.0002107159265049838, 0.0002107159265049838, 0.00028095456867331176, 0.00028095456867331176, 0.00037460609156441565, 0.00037460609156441565, 0.0004994747887525542, 0.0004994747887525542], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-05-23 16:06:42,130] [INFO] [timer.py:260:stop] epoch=0/micro_step=8000/global_step=8000, RunningAvgSamplesPerSec=212.85676136253775, CurrSamplesPerSec=214.80197937405177, MemAllocated=1.37GB, MaxMemAllocated=40.83GB
[2025-05-23 16:06:42,440] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 16:06:42,440] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 16:06:42,440] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-05-23 16:06:42,440] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [5]  [1260/1349]  eta: 0:00:27  lr: 0.000499  min_lr: 0.000012  loss: 0.8966 (0.9290)  loss_scale: 16384.0000 (17709.2720)  weight_decay: 0.0500 (0.0500)  time: 0.3101  data: 0.0001  max mem: 41808
Epoch: [5]  [1270/1349]  eta: 0:00:24  lr: 0.000499  min_lr: 0.000012  loss: 0.9640 (0.9288)  loss_scale: 32768.0000 (17827.7514)  weight_decay: 0.0500 (0.0500)  time: 0.3101  data: 0.0002  max mem: 41808
Epoch: [5]  [1280/1349]  eta: 0:00:21  lr: 0.000499  min_lr: 0.000012  loss: 0.9050 (0.9282)  loss_scale: 32768.0000 (17944.3810)  weight_decay: 0.0500 (0.0500)  time: 0.3081  data: 0.0001  max mem: 41808
Epoch: [5]  [1290/1349]  eta: 0:00:18  lr: 0.000499  min_lr: 0.000012  loss: 0.8524 (0.9278)  loss_scale: 32768.0000 (18059.2037)  weight_decay: 0.0500 (0.0500)  time: 0.3084  data: 0.0001  max mem: 41808
Epoch: [5]  [1300/1349]  eta: 0:00:15  lr: 0.000499  min_lr: 0.000012  loss: 0.9476 (0.9283)  loss_scale: 32768.0000 (18172.2613)  weight_decay: 0.0500 (0.0500)  time: 0.3079  data: 0.0001  max mem: 41808
Epoch: [5]  [1310/1349]  eta: 0:00:12  lr: 0.000499  min_lr: 0.000012  loss: 0.9581 (0.9289)  loss_scale: 32768.0000 (18283.5942)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0001  max mem: 41808
Epoch: [5]  [1320/1349]  eta: 0:00:09  lr: 0.000499  min_lr: 0.000012  loss: 0.9619 (0.9288)  loss_scale: 32768.0000 (18393.2415)  weight_decay: 0.0500 (0.0500)  time: 0.3079  data: 0.0002  max mem: 41808
Epoch: [5]  [1330/1349]  eta: 0:00:05  lr: 0.000499  min_lr: 0.000012  loss: 0.8839 (0.9283)  loss_scale: 32768.0000 (18501.2412)  weight_decay: 0.0500 (0.0500)  time: 0.3052  data: 0.0001  max mem: 41808
Epoch: [5]  [1340/1349]  eta: 0:00:02  lr: 0.000499  min_lr: 0.000012  loss: 0.8623 (0.9280)  loss_scale: 32768.0000 (18607.6301)  weight_decay: 0.0500 (0.0500)  time: 0.3027  data: 0.0001  max mem: 41808
Epoch: [5]  [1348/1349]  eta: 0:00:00  lr: 0.000499  min_lr: 0.000012  loss: 0.8539 (0.9281)  loss_scale: 32768.0000 (18691.6056)  weight_decay: 0.0500 (0.0500)  time: 0.3022  data: 0.0001  max mem: 41808
Epoch: [5] Total time: 0:06:59 (0.3111 s / it)
Averaged stats: lr: 0.000499  min_lr: 0.000012  loss: 0.8539 (0.9286)  loss_scale: 32768.0000 (18691.6056)  weight_decay: 0.0500 (0.0500)  total_time: 419.7287 (419.7111)
Val:  [  0/346]  eta: 1:14:19  loss: 1.6860 (1.6860)  acc1: 35.1562 (35.1562)  acc5: 89.0625 (89.0625)  time: 12.8879  data: 11.8592  max mem: 41808
Val:  [ 10/346]  eta: 0:11:00  loss: 0.1370 (0.4605)  acc1: 100.0000 (85.5824)  acc5: 100.0000 (98.8636)  time: 1.9644  data: 1.1610  max mem: 41808
Val:  [ 20/346]  eta: 0:08:03  loss: 0.1278 (0.3643)  acc1: 100.0000 (89.8065)  acc5: 100.0000 (99.4048)  time: 0.9142  data: 0.1265  max mem: 41808
Val:  [ 30/346]  eta: 0:06:37  loss: 0.1193 (0.3253)  acc1: 99.2188 (91.3558)  acc5: 100.0000 (99.5968)  time: 0.8701  data: 0.0811  max mem: 41808
Val:  [ 40/346]  eta: 0:05:47  loss: 0.1316 (0.3764)  acc1: 99.2188 (90.3773)  acc5: 100.0000 (99.1425)  time: 0.7677  data: 0.0003  max mem: 41808
Val:  [ 50/346]  eta: 0:05:13  loss: 0.1212 (0.3403)  acc1: 99.2188 (91.5441)  acc5: 100.0000 (99.2953)  time: 0.7502  data: 0.0003  max mem: 41808
Val:  [ 60/346]  eta: 0:04:54  loss: 0.1342 (0.3432)  acc1: 98.4375 (91.5215)  acc5: 100.0000 (99.3852)  time: 0.8184  data: 0.0640  max mem: 41808
Val:  [ 70/346]  eta: 0:04:39  loss: 0.2667 (0.3724)  acc1: 92.9688 (90.7680)  acc5: 100.0000 (99.4498)  time: 0.8908  data: 0.1366  max mem: 41808
Val:  [ 80/346]  eta: 0:04:25  loss: 0.2260 (0.3693)  acc1: 92.9688 (91.1073)  acc5: 100.0000 (99.4502)  time: 0.9002  data: 0.1401  max mem: 41808
Val:  [ 90/346]  eta: 0:04:13  loss: 0.2260 (0.3760)  acc1: 92.1875 (90.8053)  acc5: 100.0000 (99.5021)  time: 0.9185  data: 0.1444  max mem: 41808
Val:  [100/346]  eta: 0:04:00  loss: 0.1851 (0.3545)  acc1: 98.4375 (91.6074)  acc5: 100.0000 (99.5514)  time: 0.9017  data: 0.1510  max mem: 41808
Val:  [110/346]  eta: 0:03:50  loss: 0.1768 (0.3715)  acc1: 98.4375 (91.0473)  acc5: 100.0000 (99.5355)  time: 0.8992  data: 0.1520  max mem: 41808
Val:  [120/346]  eta: 0:03:38  loss: 0.1859 (0.3732)  acc1: 97.6562 (90.8316)  acc5: 100.0000 (99.5739)  time: 0.9053  data: 0.1512  max mem: 41808
Val:  [130/346]  eta: 0:03:27  loss: 0.1692 (0.3926)  acc1: 99.2188 (90.2016)  acc5: 100.0000 (99.3142)  time: 0.8797  data: 0.1463  max mem: 41808
Val:  [140/346]  eta: 0:03:16  loss: 0.2108 (0.3877)  acc1: 94.5312 (90.3646)  acc5: 100.0000 (99.3573)  time: 0.8767  data: 0.1444  max mem: 41808
Val:  [150/346]  eta: 0:03:06  loss: 0.3123 (0.3859)  acc1: 92.9688 (90.3146)  acc5: 100.0000 (99.3998)  time: 0.8897  data: 0.1575  max mem: 41808
Val:  [160/346]  eta: 0:02:56  loss: 0.2543 (0.3827)  acc1: 92.9688 (90.2853)  acc5: 100.0000 (99.4371)  time: 0.9203  data: 0.1609  max mem: 41808
Val:  [170/346]  eta: 0:02:46  loss: 0.2543 (0.3911)  acc1: 91.4062 (89.7752)  acc5: 100.0000 (99.4700)  time: 0.9266  data: 0.1622  max mem: 41808
Val:  [180/346]  eta: 0:02:36  loss: 0.3206 (0.4119)  acc1: 88.2812 (88.8898)  acc5: 100.0000 (99.4993)  time: 0.9060  data: 0.1648  max mem: 41808
Val:  [190/346]  eta: 0:02:27  loss: 0.2611 (0.4117)  acc1: 94.5312 (88.9766)  acc5: 100.0000 (99.5255)  time: 0.9178  data: 0.1607  max mem: 41808
Val:  [200/346]  eta: 0:02:17  loss: 0.3077 (0.4200)  acc1: 92.9688 (88.5572)  acc5: 100.0000 (99.5452)  time: 0.9265  data: 0.1709  max mem: 41808
Val:  [210/346]  eta: 0:02:07  loss: 0.1999 (0.4121)  acc1: 96.8750 (88.8440)  acc5: 100.0000 (99.5668)  time: 0.9015  data: 0.1626  max mem: 41808
Val:  [220/346]  eta: 0:01:58  loss: 0.1916 (0.4084)  acc1: 96.8750 (88.9706)  acc5: 100.0000 (99.5687)  time: 0.9018  data: 0.1526  max mem: 41808
Val:  [230/346]  eta: 0:01:48  loss: 0.2065 (0.3992)  acc1: 97.6562 (89.3229)  acc5: 100.0000 (99.5874)  time: 0.9115  data: 0.1576  max mem: 41808
Val:  [240/346]  eta: 0:01:39  loss: 0.2136 (0.4112)  acc1: 96.8750 (88.9426)  acc5: 100.0000 (99.5721)  time: 0.9037  data: 0.1586  max mem: 41808
Val:  [250/346]  eta: 0:01:29  loss: 0.2265 (0.4072)  acc1: 96.8750 (89.0376)  acc5: 100.0000 (99.5891)  time: 0.8872  data: 0.1518  max mem: 41808
Val:  [260/346]  eta: 0:01:20  loss: 0.1621 (0.4052)  acc1: 96.8750 (89.0745)  acc5: 100.0000 (99.5779)  time: 0.8953  data: 0.1472  max mem: 41808
Val:  [270/346]  eta: 0:01:10  loss: 0.1377 (0.4000)  acc1: 99.2188 (89.2557)  acc5: 100.0000 (99.5935)  time: 0.9333  data: 0.1500  max mem: 41808
Val:  [280/346]  eta: 0:01:01  loss: 0.1148 (0.3965)  acc1: 100.0000 (89.3767)  acc5: 100.0000 (99.6052)  time: 0.9452  data: 0.1594  max mem: 41808
Val:  [290/346]  eta: 0:00:52  loss: 0.1103 (0.3873)  acc1: 100.0000 (89.7095)  acc5: 100.0000 (99.6188)  time: 0.9116  data: 0.1616  max mem: 41808
Val:  [300/346]  eta: 0:00:42  loss: 0.1186 (0.3884)  acc1: 100.0000 (89.7996)  acc5: 100.0000 (99.4627)  time: 0.8760  data: 0.1529  max mem: 41808
Val:  [310/346]  eta: 0:00:33  loss: 0.1306 (0.3893)  acc1: 98.4375 (89.7508)  acc5: 100.0000 (99.4574)  time: 0.9037  data: 0.1476  max mem: 41808
Val:  [320/346]  eta: 0:00:24  loss: 0.1808 (0.3902)  acc1: 98.4375 (89.7878)  acc5: 100.0000 (99.4743)  time: 0.9056  data: 0.1507  max mem: 41808
Val:  [330/346]  eta: 0:00:14  loss: 0.2507 (0.3948)  acc1: 92.1875 (89.6030)  acc5: 100.0000 (99.4760)  time: 0.8910  data: 0.1582  max mem: 41808
Val:  [340/346]  eta: 0:00:05  loss: 0.2823 (0.4048)  acc1: 90.6250 (89.3008)  acc5: 100.0000 (99.4845)  time: 0.9086  data: 0.1671  max mem: 41808
Val:  [345/346]  eta: 0:00:00  loss: 0.1860 (0.4012)  acc1: 95.4023 (89.4275)  acc5: 100.0000 (99.4915)  time: 0.8979  data: 0.1849  max mem: 41808
Val: Total time: 0:05:21 (0.9282 s / it)
* Acc@1 89.477 Acc@5 99.505 loss 0.398
Accuracy of the network on the 88494 val videos: 89.5%
Max accuracy: 91.25%   Max Epoch: 3
Epoch: [6]  [   0/1349]  eta: 1:59:56  lr: 0.000499  min_lr: 0.000012  loss: 0.8848 (0.8848)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 5.3346  data: 3.5811  max mem: 41808
Epoch: [6]  [  10/1349]  eta: 0:17:15  lr: 0.000499  min_lr: 0.000012  loss: 0.8848 (0.9037)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7735  data: 0.3257  max mem: 41808
Epoch: [6]  [  20/1349]  eta: 0:12:13  lr: 0.000499  min_lr: 0.000012  loss: 0.8584 (0.8784)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3131  data: 0.0001  max mem: 41808
Epoch: [6]  [  30/1349]  eta: 0:10:24  lr: 0.000499  min_lr: 0.000012  loss: 0.9034 (0.8939)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3085  data: 0.0001  max mem: 41808
[2025-05-23 16:12:48,356] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 16:12:48,356] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-05-23 16:12:48,356] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 16:12:48,356] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-05-23 16:12:48,972] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 8130
[2025-05-23 16:12:48,972] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-05-23 16:12:48,972] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-05-23 16:12:48,972] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 8130
[2025-05-23 16:12:48,972] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [6]  [  40/1349]  eta: 0:09:27  lr: 0.000499  min_lr: 0.000012  loss: 0.9387 (0.9017)  loss_scale: 32768.0000 (34366.4390)  weight_decay: 0.0500 (0.0500)  time: 0.3093  data: 0.0001  max mem: 41808
Epoch: [6]  [  50/1349]  eta: 0:08:51  lr: 0.000499  min_lr: 0.000012  loss: 0.8210 (0.8807)  loss_scale: 32768.0000 (34053.0196)  weight_decay: 0.0500 (0.0500)  time: 0.3102  data: 0.0001  max mem: 41808
[2025-05-23 16:12:54,245] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 8147
[2025-05-23 16:12:54,245] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-05-23 16:12:54,245] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 8147
[2025-05-23 16:12:54,245] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2025-05-23 16:12:54,245] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [6]  [  60/1349]  eta: 0:08:26  lr: 0.000499  min_lr: 0.000012  loss: 0.8210 (0.8840)  loss_scale: 32768.0000 (31693.6393)  weight_decay: 0.0500 (0.0500)  time: 0.3090  data: 0.0001  max mem: 41808
Epoch: [6]  [  70/1349]  eta: 0:08:07  lr: 0.000499  min_lr: 0.000012  loss: 0.9646 (0.8933)  loss_scale: 16384.0000 (29537.3521)  weight_decay: 0.0500 (0.0500)  time: 0.3088  data: 0.0001  max mem: 41808
Epoch: [6]  [  80/1349]  eta: 0:07:52  lr: 0.000499  min_lr: 0.000012  loss: 0.9763 (0.8951)  loss_scale: 16384.0000 (27913.4815)  weight_decay: 0.0500 (0.0500)  time: 0.3092  data: 0.0001  max mem: 41808
Epoch: [6]  [  90/1349]  eta: 0:07:39  lr: 0.000499  min_lr: 0.000012  loss: 0.9138 (0.8921)  loss_scale: 16384.0000 (26646.5055)  weight_decay: 0.0500 (0.0500)  time: 0.3085  data: 0.0001  max mem: 41808
Epoch: [6]  [ 100/1349]  eta: 0:07:29  lr: 0.000499  min_lr: 0.000012  loss: 0.8689 (0.8968)  loss_scale: 16384.0000 (25630.4158)  weight_decay: 0.0500 (0.0500)  time: 0.3084  data: 0.0001  max mem: 41808
Epoch: [6]  [ 110/1349]  eta: 0:07:19  lr: 0.000499  min_lr: 0.000012  loss: 0.9243 (0.8948)  loss_scale: 16384.0000 (24797.4054)  weight_decay: 0.0500 (0.0500)  time: 0.3082  data: 0.0001  max mem: 41808
Epoch: [6]  [ 120/1349]  eta: 0:07:11  lr: 0.000499  min_lr: 0.000012  loss: 0.9559 (0.9025)  loss_scale: 16384.0000 (24102.0826)  weight_decay: 0.0500 (0.0500)  time: 0.3082  data: 0.0001  max mem: 41808
Epoch: [6]  [ 130/1349]  eta: 0:07:03  lr: 0.000499  min_lr: 0.000012  loss: 0.9550 (0.8984)  loss_scale: 16384.0000 (23512.9160)  weight_decay: 0.0500 (0.0500)  time: 0.3082  data: 0.0001  max mem: 41808
Epoch: [6]  [ 140/1349]  eta: 0:06:56  lr: 0.000499  min_lr: 0.000012  loss: 0.9223 (0.9013)  loss_scale: 16384.0000 (23007.3191)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0002  max mem: 41808
Epoch: [6]  [ 150/1349]  eta: 0:06:50  lr: 0.000499  min_lr: 0.000012  loss: 0.9404 (0.9016)  loss_scale: 16384.0000 (22568.6887)  weight_decay: 0.0500 (0.0500)  time: 0.3080  data: 0.0002  max mem: 41808
Epoch: [6]  [ 160/1349]  eta: 0:06:44  lr: 0.000499  min_lr: 0.000012  loss: 0.9002 (0.9015)  loss_scale: 16384.0000 (22184.5466)  weight_decay: 0.0500 (0.0500)  time: 0.3082  data: 0.0001  max mem: 41808
Epoch: [6]  [ 170/1349]  eta: 0:06:39  lr: 0.000499  min_lr: 0.000012  loss: 0.8971 (0.9021)  loss_scale: 16384.0000 (21845.3333)  weight_decay: 0.0500 (0.0500)  time: 0.3079  data: 0.0001  max mem: 41808
Epoch: [6]  [ 180/1349]  eta: 0:06:33  lr: 0.000499  min_lr: 0.000012  loss: 0.8602 (0.8968)  loss_scale: 16384.0000 (21543.6022)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
[2025-05-23 16:13:34,007] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 16:13:34,007] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2025-05-23 16:13:34,007] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 16:13:34,007] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [6]  [ 190/1349]  eta: 0:06:28  lr: 0.000499  min_lr: 0.000012  loss: 0.9575 (0.9019)  loss_scale: 16384.0000 (22045.4869)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
Epoch: [6]  [ 200/1349]  eta: 0:06:23  lr: 0.000499  min_lr: 0.000012  loss: 0.9758 (0.9015)  loss_scale: 32768.0000 (22578.9453)  weight_decay: 0.0500 (0.0500)  time: 0.3079  data: 0.0001  max mem: 41808
Epoch: [6]  [ 210/1349]  eta: 0:06:18  lr: 0.000499  min_lr: 0.000012  loss: 0.9429 (0.9025)  loss_scale: 32768.0000 (23061.8389)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0001  max mem: 41808
Epoch: [6]  [ 220/1349]  eta: 0:06:14  lr: 0.000499  min_lr: 0.000012  loss: 0.9400 (0.9018)  loss_scale: 32768.0000 (23501.0317)  weight_decay: 0.0500 (0.0500)  time: 0.3087  data: 0.0001  max mem: 41808
Epoch: [6]  [ 230/1349]  eta: 0:06:09  lr: 0.000499  min_lr: 0.000012  loss: 0.9400 (0.9041)  loss_scale: 32768.0000 (23902.1991)  weight_decay: 0.0500 (0.0500)  time: 0.3093  data: 0.0001  max mem: 41808
Epoch: [6]  [ 240/1349]  eta: 0:06:05  lr: 0.000499  min_lr: 0.000012  loss: 1.0389 (0.9081)  loss_scale: 32768.0000 (24270.0747)  weight_decay: 0.0500 (0.0500)  time: 0.3103  data: 0.0001  max mem: 41808
Epoch: [6]  [ 250/1349]  eta: 0:06:01  lr: 0.000499  min_lr: 0.000012  loss: 0.9289 (0.9064)  loss_scale: 32768.0000 (24608.6375)  weight_decay: 0.0500 (0.0500)  time: 0.3101  data: 0.0001  max mem: 41808
Epoch: [6]  [ 260/1349]  eta: 0:05:57  lr: 0.000499  min_lr: 0.000012  loss: 0.8881 (0.9053)  loss_scale: 32768.0000 (24921.2567)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
Epoch: [6]  [ 270/1349]  eta: 0:05:53  lr: 0.000499  min_lr: 0.000012  loss: 0.8550 (0.9055)  loss_scale: 32768.0000 (25210.8044)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0001  max mem: 41808
Epoch: [6]  [ 280/1349]  eta: 0:05:49  lr: 0.000499  min_lr: 0.000012  loss: 0.9228 (0.9065)  loss_scale: 32768.0000 (25479.7438)  weight_decay: 0.0500 (0.0500)  time: 0.3079  data: 0.0001  max mem: 41808
Epoch: [6]  [ 290/1349]  eta: 0:05:45  lr: 0.000499  min_lr: 0.000012  loss: 0.9945 (0.9073)  loss_scale: 32768.0000 (25730.1993)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0001  max mem: 41808
Epoch: [6]  [ 300/1349]  eta: 0:05:41  lr: 0.000499  min_lr: 0.000012  loss: 0.8836 (0.9049)  loss_scale: 32768.0000 (25964.0133)  weight_decay: 0.0500 (0.0500)  time: 0.3082  data: 0.0001  max mem: 41808
[2025-05-23 16:14:13,520] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 16:14:13,520] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-05-23 16:14:13,520] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 16:14:13,521] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [6]  [ 310/1349]  eta: 0:05:37  lr: 0.000499  min_lr: 0.000012  loss: 0.8728 (0.9055)  loss_scale: 32768.0000 (26288.1543)  weight_decay: 0.0500 (0.0500)  time: 0.3095  data: 0.0001  max mem: 41808
[2025-05-23 16:14:13,831] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 8405
[2025-05-23 16:14:13,831] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 8405
[2025-05-23 16:14:13,831] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-05-23 16:14:13,831] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-05-23 16:14:13,831] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [6]  [ 320/1349]  eta: 0:05:33  lr: 0.000499  min_lr: 0.000012  loss: 0.8728 (0.9041)  loss_scale: 32768.0000 (26490.0187)  weight_decay: 0.0500 (0.0500)  time: 0.3103  data: 0.0001  max mem: 41808
Epoch: [6]  [ 330/1349]  eta: 0:05:30  lr: 0.000499  min_lr: 0.000012  loss: 0.8608 (0.9030)  loss_scale: 32768.0000 (26679.6858)  weight_decay: 0.0500 (0.0500)  time: 0.3108  data: 0.0001  max mem: 41808
Epoch: [6]  [ 340/1349]  eta: 0:05:26  lr: 0.000499  min_lr: 0.000012  loss: 0.9397 (0.9026)  loss_scale: 32768.0000 (26858.2287)  weight_decay: 0.0500 (0.0500)  time: 0.3102  data: 0.0001  max mem: 41808
Epoch: [6]  [ 350/1349]  eta: 0:05:22  lr: 0.000499  min_lr: 0.000012  loss: 0.9663 (0.9034)  loss_scale: 32768.0000 (27026.5983)  weight_decay: 0.0500 (0.0500)  time: 0.3086  data: 0.0001  max mem: 41808
Epoch: [6]  [ 360/1349]  eta: 0:05:19  lr: 0.000499  min_lr: 0.000012  loss: 0.9152 (0.9026)  loss_scale: 32768.0000 (27185.6399)  weight_decay: 0.0500 (0.0500)  time: 0.3078  data: 0.0001  max mem: 41808
Epoch: [6]  [ 370/1349]  eta: 0:05:15  lr: 0.000499  min_lr: 0.000012  loss: 0.9152 (0.9028)  loss_scale: 32768.0000 (27336.1078)  weight_decay: 0.0500 (0.0500)  time: 0.3088  data: 0.0001  max mem: 41808
Epoch: [6]  [ 380/1349]  eta: 0:05:12  lr: 0.000499  min_lr: 0.000012  loss: 0.9426 (0.9048)  loss_scale: 32768.0000 (27478.6772)  weight_decay: 0.0500 (0.0500)  time: 0.3089  data: 0.0001  max mem: 41808
Epoch: [6]  [ 390/1349]  eta: 0:05:08  lr: 0.000499  min_lr: 0.000012  loss: 1.0308 (0.9083)  loss_scale: 32768.0000 (27613.9540)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [6]  [ 400/1349]  eta: 0:05:04  lr: 0.000499  min_lr: 0.000012  loss: 0.9720 (0.9084)  loss_scale: 32768.0000 (27742.4838)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0002  max mem: 41808
Epoch: [6]  [ 410/1349]  eta: 0:05:01  lr: 0.000499  min_lr: 0.000012  loss: 0.9102 (0.9081)  loss_scale: 32768.0000 (27864.7591)  weight_decay: 0.0500 (0.0500)  time: 0.3082  data: 0.0001  max mem: 41808
Epoch: [6]  [ 420/1349]  eta: 0:04:57  lr: 0.000499  min_lr: 0.000012  loss: 0.9361 (0.9090)  loss_scale: 32768.0000 (27981.2257)  weight_decay: 0.0500 (0.0500)  time: 0.3082  data: 0.0001  max mem: 41808
Epoch: [6]  [ 430/1349]  eta: 0:04:54  lr: 0.000499  min_lr: 0.000012  loss: 0.9870 (0.9105)  loss_scale: 32768.0000 (28092.2877)  weight_decay: 0.0500 (0.0500)  time: 0.3080  data: 0.0002  max mem: 41808
[2025-05-23 16:14:53,649] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 16:14:53,649] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 16:14:53,649] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-05-23 16:14:53,649] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [6]  [ 440/1349]  eta: 0:04:50  lr: 0.000499  min_lr: 0.000012  loss: 0.9498 (0.9117)  loss_scale: 32768.0000 (28272.6168)  weight_decay: 0.0500 (0.0500)  time: 0.3080  data: 0.0002  max mem: 41808
[2025-05-23 16:14:54,279] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 8536
[2025-05-23 16:14:54,279] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-05-23 16:14:54,279] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 8536
[2025-05-23 16:14:54,279] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-05-23 16:14:54,279] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [6]  [ 450/1349]  eta: 0:04:47  lr: 0.000499  min_lr: 0.000012  loss: 0.9088 (0.9106)  loss_scale: 32768.0000 (28444.9490)  weight_decay: 0.0500 (0.0500)  time: 0.3090  data: 0.0002  max mem: 41808
Epoch: [6]  [ 460/1349]  eta: 0:04:44  lr: 0.000499  min_lr: 0.000012  loss: 0.8768 (0.9094)  loss_scale: 32768.0000 (28538.7245)  weight_decay: 0.0500 (0.0500)  time: 0.3089  data: 0.0001  max mem: 41808
Epoch: [6]  [ 470/1349]  eta: 0:04:40  lr: 0.000499  min_lr: 0.000012  loss: 0.9759 (0.9107)  loss_scale: 32768.0000 (28628.5180)  weight_decay: 0.0500 (0.0500)  time: 0.3079  data: 0.0001  max mem: 41808
Epoch: [6]  [ 480/1349]  eta: 0:04:37  lr: 0.000499  min_lr: 0.000012  loss: 0.9421 (0.9095)  loss_scale: 32768.0000 (28714.5780)  weight_decay: 0.0500 (0.0500)  time: 0.3095  data: 0.0001  max mem: 41808
Epoch: [6]  [ 490/1349]  eta: 0:04:34  lr: 0.000499  min_lr: 0.000012  loss: 0.9210 (0.9104)  loss_scale: 32768.0000 (28797.1324)  weight_decay: 0.0500 (0.0500)  time: 0.3095  data: 0.0001  max mem: 41808
Epoch: [6]  [ 500/1349]  eta: 0:04:30  lr: 0.000499  min_lr: 0.000012  loss: 0.9366 (0.9118)  loss_scale: 32768.0000 (28876.3912)  weight_decay: 0.0500 (0.0500)  time: 0.3082  data: 0.0001  max mem: 41808
Epoch: [6]  [ 510/1349]  eta: 0:04:27  lr: 0.000499  min_lr: 0.000012  loss: 1.0004 (0.9138)  loss_scale: 32768.0000 (28952.5479)  weight_decay: 0.0500 (0.0500)  time: 0.3080  data: 0.0001  max mem: 41808
Epoch: [6]  [ 520/1349]  eta: 0:04:23  lr: 0.000499  min_lr: 0.000012  loss: 0.9844 (0.9139)  loss_scale: 32768.0000 (29025.7812)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
Epoch: [6]  [ 530/1349]  eta: 0:04:20  lr: 0.000499  min_lr: 0.000012  loss: 0.9328 (0.9135)  loss_scale: 32768.0000 (29096.2561)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [6]  [ 540/1349]  eta: 0:04:17  lr: 0.000499  min_lr: 0.000012  loss: 0.9017 (0.9140)  loss_scale: 32768.0000 (29164.1257)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
Epoch: [6]  [ 550/1349]  eta: 0:04:13  lr: 0.000499  min_lr: 0.000012  loss: 0.9134 (0.9146)  loss_scale: 32768.0000 (29229.5318)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
Epoch: [6]  [ 560/1349]  eta: 0:04:10  lr: 0.000499  min_lr: 0.000012  loss: 0.9973 (0.9156)  loss_scale: 32768.0000 (29292.6061)  weight_decay: 0.0500 (0.0500)  time: 0.3078  data: 0.0001  max mem: 41808
Epoch: [6]  [ 570/1349]  eta: 0:04:07  lr: 0.000499  min_lr: 0.000012  loss: 0.9948 (0.9170)  loss_scale: 32768.0000 (29353.4711)  weight_decay: 0.0500 (0.0500)  time: 0.3095  data: 0.0001  max mem: 41808
[2025-05-23 16:15:34,071] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 16:15:34,072] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-05-23 16:15:34,072] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 16:15:34,072] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [6]  [ 580/1349]  eta: 0:04:04  lr: 0.000499  min_lr: 0.000012  loss: 0.9798 (0.9170)  loss_scale: 32768.0000 (29976.2341)  weight_decay: 0.0500 (0.0500)  time: 0.3101  data: 0.0001  max mem: 41808
[2025-05-23 16:15:37,161] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 8675
[2025-05-23 16:15:37,162] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 8675
[2025-05-23 16:15:37,162] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-05-23 16:15:37,162] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-05-23 16:15:37,162] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [6]  [ 590/1349]  eta: 0:04:00  lr: 0.000499  min_lr: 0.000012  loss: 0.9798 (0.9178)  loss_scale: 32768.0000 (30023.4721)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
Epoch: [6]  [ 600/1349]  eta: 0:03:57  lr: 0.000499  min_lr: 0.000012  loss: 0.9521 (0.9160)  loss_scale: 32768.0000 (30069.1381)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
Epoch: [6]  [ 610/1349]  eta: 0:03:54  lr: 0.000499  min_lr: 0.000012  loss: 0.9123 (0.9176)  loss_scale: 32768.0000 (30113.3093)  weight_decay: 0.0500 (0.0500)  time: 0.3090  data: 0.0002  max mem: 41808
Epoch: [6]  [ 620/1349]  eta: 0:03:50  lr: 0.000499  min_lr: 0.000012  loss: 1.0234 (0.9181)  loss_scale: 32768.0000 (30156.0580)  weight_decay: 0.0500 (0.0500)  time: 0.3081  data: 0.0001  max mem: 41808
Epoch: [6]  [ 630/1349]  eta: 0:03:47  lr: 0.000499  min_lr: 0.000012  loss: 0.9680 (0.9179)  loss_scale: 32768.0000 (30197.4517)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0002  max mem: 41808
Epoch: [6]  [ 640/1349]  eta: 0:03:44  lr: 0.000499  min_lr: 0.000012  loss: 0.9911 (0.9196)  loss_scale: 32768.0000 (30237.5538)  weight_decay: 0.0500 (0.0500)  time: 0.3084  data: 0.0002  max mem: 41808
Epoch: [6]  [ 650/1349]  eta: 0:03:41  lr: 0.000499  min_lr: 0.000012  loss: 1.0035 (0.9192)  loss_scale: 32768.0000 (30276.4240)  weight_decay: 0.0500 (0.0500)  time: 0.3105  data: 0.0001  max mem: 41808
Epoch: [6]  [ 660/1349]  eta: 0:03:37  lr: 0.000499  min_lr: 0.000012  loss: 0.8993 (0.9189)  loss_scale: 32768.0000 (30314.1180)  weight_decay: 0.0500 (0.0500)  time: 0.3087  data: 0.0002  max mem: 41808
Epoch: [6]  [ 670/1349]  eta: 0:03:34  lr: 0.000499  min_lr: 0.000012  loss: 0.9160 (0.9189)  loss_scale: 32768.0000 (30350.6885)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0002  max mem: 41808
Epoch: [6]  [ 680/1349]  eta: 0:03:31  lr: 0.000499  min_lr: 0.000012  loss: 0.9308 (0.9189)  loss_scale: 32768.0000 (30386.1850)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
Epoch: [6]  [ 690/1349]  eta: 0:03:28  lr: 0.000499  min_lr: 0.000012  loss: 0.9308 (0.9190)  loss_scale: 32768.0000 (30420.6541)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [6]  [ 700/1349]  eta: 0:03:24  lr: 0.000499  min_lr: 0.000012  loss: 0.9434 (0.9187)  loss_scale: 32768.0000 (30454.1398)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
[2025-05-23 16:16:16,882] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 16:16:16,882] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-05-23 16:16:16,882] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 16:16:16,882] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [6]  [ 710/1349]  eta: 0:03:21  lr: 0.000499  min_lr: 0.000012  loss: 0.9352 (0.9191)  loss_scale: 32768.0000 (30532.7707)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0002  max mem: 41808
[2025-05-23 16:16:17,804] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 8807
[2025-05-23 16:16:17,804] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-05-23 16:16:17,804] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 8807
[2025-05-23 16:16:17,804] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-05-23 16:16:17,804] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [6]  [ 720/1349]  eta: 0:03:18  lr: 0.000499  min_lr: 0.000012  loss: 0.9352 (0.9193)  loss_scale: 32768.0000 (30654.6685)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0002  max mem: 41808
Epoch: [6]  [ 730/1349]  eta: 0:03:15  lr: 0.000499  min_lr: 0.000012  loss: 0.9738 (0.9194)  loss_scale: 32768.0000 (30683.5787)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0002  max mem: 41808
Epoch: [6]  [ 740/1349]  eta: 0:03:11  lr: 0.000499  min_lr: 0.000012  loss: 0.9360 (0.9194)  loss_scale: 32768.0000 (30711.7085)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [6]  [ 750/1349]  eta: 0:03:08  lr: 0.000499  min_lr: 0.000012  loss: 0.9360 (0.9192)  loss_scale: 32768.0000 (30739.0892)  weight_decay: 0.0500 (0.0500)  time: 0.3062  data: 0.0001  max mem: 41808
Epoch: [6]  [ 760/1349]  eta: 0:03:05  lr: 0.000499  min_lr: 0.000012  loss: 0.9630 (0.9191)  loss_scale: 32768.0000 (30765.7503)  weight_decay: 0.0500 (0.0500)  time: 0.3058  data: 0.0001  max mem: 41808
Epoch: [6]  [ 770/1349]  eta: 0:03:02  lr: 0.000499  min_lr: 0.000012  loss: 0.8699 (0.9182)  loss_scale: 32768.0000 (30791.7198)  weight_decay: 0.0500 (0.0500)  time: 0.3057  data: 0.0001  max mem: 41808
Epoch: [6]  [ 780/1349]  eta: 0:02:59  lr: 0.000498  min_lr: 0.000012  loss: 0.8712 (0.9185)  loss_scale: 32768.0000 (30817.0243)  weight_decay: 0.0500 (0.0500)  time: 0.3057  data: 0.0001  max mem: 41808
Epoch: [6]  [ 790/1349]  eta: 0:02:55  lr: 0.000498  min_lr: 0.000012  loss: 0.9352 (0.9187)  loss_scale: 32768.0000 (30841.6890)  weight_decay: 0.0500 (0.0500)  time: 0.3057  data: 0.0001  max mem: 41808
Epoch: [6]  [ 800/1349]  eta: 0:02:52  lr: 0.000498  min_lr: 0.000012  loss: 0.9134 (0.9181)  loss_scale: 32768.0000 (30865.7378)  weight_decay: 0.0500 (0.0500)  time: 0.3059  data: 0.0001  max mem: 41808
Epoch: [6]  [ 810/1349]  eta: 0:02:49  lr: 0.000498  min_lr: 0.000012  loss: 0.9095 (0.9184)  loss_scale: 32768.0000 (30889.1936)  weight_decay: 0.0500 (0.0500)  time: 0.3060  data: 0.0001  max mem: 41808
Epoch: [6]  [ 820/1349]  eta: 0:02:46  lr: 0.000498  min_lr: 0.000012  loss: 0.9309 (0.9186)  loss_scale: 32768.0000 (30912.0780)  weight_decay: 0.0500 (0.0500)  time: 0.3058  data: 0.0002  max mem: 41808
Epoch: [6]  [ 830/1349]  eta: 0:02:43  lr: 0.000498  min_lr: 0.000012  loss: 0.9212 (0.9192)  loss_scale: 32768.0000 (30934.4116)  weight_decay: 0.0500 (0.0500)  time: 0.3061  data: 0.0002  max mem: 41808
Epoch: [6]  [ 840/1349]  eta: 0:02:39  lr: 0.000498  min_lr: 0.000012  loss: 0.8995 (0.9186)  loss_scale: 32768.0000 (30956.2140)  weight_decay: 0.0500 (0.0500)  time: 0.3060  data: 0.0002  max mem: 41808
[2025-05-23 16:16:57,304] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 16:16:57,304] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-05-23 16:16:57,304] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 16:16:57,304] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [6]  [ 850/1349]  eta: 0:02:36  lr: 0.000498  min_lr: 0.000012  loss: 0.8874 (0.9177)  loss_scale: 32768.0000 (31324.0517)  weight_decay: 0.0500 (0.0500)  time: 0.3060  data: 0.0001  max mem: 41808
[2025-05-23 16:17:02,511] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 8953
[2025-05-23 16:17:02,511] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-05-23 16:17:02,511] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2025-05-23 16:17:02,511] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 8953
[2025-05-23 16:17:02,511] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [6]  [ 860/1349]  eta: 0:02:33  lr: 0.000498  min_lr: 0.000012  loss: 0.8874 (0.9172)  loss_scale: 65536.0000 (31645.2869)  weight_decay: 0.0500 (0.0500)  time: 0.3060  data: 0.0002  max mem: 41808
Epoch: [6]  [ 870/1349]  eta: 0:02:30  lr: 0.000498  min_lr: 0.000012  loss: 0.8364 (0.9156)  loss_scale: 32768.0000 (31658.1768)  weight_decay: 0.0500 (0.0500)  time: 0.3060  data: 0.0001  max mem: 41808
Epoch: [6]  [ 880/1349]  eta: 0:02:27  lr: 0.000498  min_lr: 0.000012  loss: 0.8845 (0.9165)  loss_scale: 32768.0000 (31670.7741)  weight_decay: 0.0500 (0.0500)  time: 0.3063  data: 0.0001  max mem: 41808
Epoch: [6]  [ 890/1349]  eta: 0:02:23  lr: 0.000498  min_lr: 0.000012  loss: 0.9479 (0.9163)  loss_scale: 32768.0000 (31683.0887)  weight_decay: 0.0500 (0.0500)  time: 0.3061  data: 0.0001  max mem: 41808
Epoch: [6]  [ 900/1349]  eta: 0:02:20  lr: 0.000498  min_lr: 0.000012  loss: 0.9018 (0.9158)  loss_scale: 32768.0000 (31695.1299)  weight_decay: 0.0500 (0.0500)  time: 0.3059  data: 0.0001  max mem: 41808
[2025-05-23 16:17:16,633] [INFO] [logging.py:96:log_dist] [Rank 0] step=9000, skipped=52, lr=[1.1838350742713598e-05, 1.1838350742713598e-05, 1.5784467656951463e-05, 1.5784467656951463e-05, 2.1045956875935285e-05, 2.1045956875935285e-05, 2.806127583458038e-05, 2.806127583458038e-05, 3.7415034446107174e-05, 3.7415034446107174e-05, 4.9886712594809565e-05, 4.9886712594809565e-05, 6.651561679307942e-05, 6.651561679307942e-05, 8.868748905743923e-05, 8.868748905743923e-05, 0.00011824998540991897, 0.00011824998540991897, 0.00015766664721322528, 0.00015766664721322528, 0.0002102221962843004, 0.0002102221962843004, 0.00028029626171240053, 0.00028029626171240053, 0.00037372834894986736, 0.00037372834894986736, 0.0004983044652664898, 0.0004983044652664898], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-05-23 16:17:16,634] [INFO] [timer.py:260:stop] epoch=0/micro_step=9000/global_step=9000, RunningAvgSamplesPerSec=212.75720062350965, CurrSamplesPerSec=214.29594320470622, MemAllocated=1.37GB, MaxMemAllocated=40.83GB
Epoch: [6]  [ 910/1349]  eta: 0:02:17  lr: 0.000498  min_lr: 0.000012  loss: 0.9264 (0.9160)  loss_scale: 32768.0000 (31706.9067)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0001  max mem: 41808
Epoch: [6]  [ 920/1349]  eta: 0:02:14  lr: 0.000498  min_lr: 0.000012  loss: 0.9521 (0.9171)  loss_scale: 32768.0000 (31718.4278)  weight_decay: 0.0500 (0.0500)  time: 0.3081  data: 0.0001  max mem: 41808
Epoch: [6]  [ 930/1349]  eta: 0:02:11  lr: 0.000498  min_lr: 0.000012  loss: 0.8924 (0.9168)  loss_scale: 32768.0000 (31729.7014)  weight_decay: 0.0500 (0.0500)  time: 0.3063  data: 0.0001  max mem: 41808
Epoch: [6]  [ 940/1349]  eta: 0:02:08  lr: 0.000498  min_lr: 0.000012  loss: 0.8377 (0.9163)  loss_scale: 32768.0000 (31740.7354)  weight_decay: 0.0500 (0.0500)  time: 0.3060  data: 0.0001  max mem: 41808
Epoch: [6]  [ 950/1349]  eta: 0:02:04  lr: 0.000498  min_lr: 0.000012  loss: 0.8996 (0.9162)  loss_scale: 32768.0000 (31751.5373)  weight_decay: 0.0500 (0.0500)  time: 0.3056  data: 0.0001  max mem: 41808
Epoch: [6]  [ 960/1349]  eta: 0:02:01  lr: 0.000498  min_lr: 0.000012  loss: 0.8843 (0.9159)  loss_scale: 32768.0000 (31762.1145)  weight_decay: 0.0500 (0.0500)  time: 0.3058  data: 0.0001  max mem: 41808
Epoch: [6]  [ 970/1349]  eta: 0:01:58  lr: 0.000498  min_lr: 0.000012  loss: 0.8818 (0.9153)  loss_scale: 32768.0000 (31772.4737)  weight_decay: 0.0500 (0.0500)  time: 0.3057  data: 0.0001  max mem: 41808
Epoch: [6]  [ 980/1349]  eta: 0:01:55  lr: 0.000498  min_lr: 0.000012  loss: 0.8813 (0.9146)  loss_scale: 32768.0000 (31782.6218)  weight_decay: 0.0500 (0.0500)  time: 0.3058  data: 0.0001  max mem: 41808
[2025-05-23 16:17:42,028] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 16:17:42,028] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 16:17:42,028] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-05-23 16:17:42,028] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [6]  [ 990/1349]  eta: 0:01:52  lr: 0.000498  min_lr: 0.000012  loss: 0.8956 (0.9146)  loss_scale: 32768.0000 (31891.7619)  weight_decay: 0.0500 (0.0500)  time: 0.3060  data: 0.0002  max mem: 41808
[2025-05-23 16:17:45,701] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 9094
[2025-05-23 16:17:45,701] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-05-23 16:17:45,701] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 9094
[2025-05-23 16:17:45,701] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-05-23 16:17:45,702] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [6]  [1000/1349]  eta: 0:01:49  lr: 0.000498  min_lr: 0.000012  loss: 1.0049 (0.9142)  loss_scale: 65536.0000 (32195.1329)  weight_decay: 0.0500 (0.0500)  time: 0.3057  data: 0.0002  max mem: 41808
Epoch: [6]  [1010/1349]  eta: 0:01:46  lr: 0.000498  min_lr: 0.000012  loss: 0.9368 (0.9141)  loss_scale: 32768.0000 (32200.7992)  weight_decay: 0.0500 (0.0500)  time: 0.3059  data: 0.0001  max mem: 41808
Epoch: [6]  [1020/1349]  eta: 0:01:42  lr: 0.000498  min_lr: 0.000012  loss: 0.8991 (0.9139)  loss_scale: 32768.0000 (32206.3546)  weight_decay: 0.0500 (0.0500)  time: 0.3061  data: 0.0001  max mem: 41808
Epoch: [6]  [1030/1349]  eta: 0:01:39  lr: 0.000498  min_lr: 0.000012  loss: 0.8991 (0.9141)  loss_scale: 32768.0000 (32211.8021)  weight_decay: 0.0500 (0.0500)  time: 0.3062  data: 0.0001  max mem: 41808
Epoch: [6]  [1040/1349]  eta: 0:01:36  lr: 0.000498  min_lr: 0.000012  loss: 0.8913 (0.9143)  loss_scale: 32768.0000 (32217.1451)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0001  max mem: 41808
Epoch: [6]  [1050/1349]  eta: 0:01:33  lr: 0.000498  min_lr: 0.000012  loss: 0.8102 (0.9131)  loss_scale: 32768.0000 (32222.3863)  weight_decay: 0.0500 (0.0500)  time: 0.3063  data: 0.0001  max mem: 41808
Epoch: [6]  [1060/1349]  eta: 0:01:30  lr: 0.000498  min_lr: 0.000012  loss: 0.8102 (0.9126)  loss_scale: 32768.0000 (32227.5287)  weight_decay: 0.0500 (0.0500)  time: 0.3060  data: 0.0001  max mem: 41808
Epoch: [6]  [1070/1349]  eta: 0:01:27  lr: 0.000498  min_lr: 0.000012  loss: 0.8194 (0.9123)  loss_scale: 32768.0000 (32232.5752)  weight_decay: 0.0500 (0.0500)  time: 0.3060  data: 0.0001  max mem: 41808
Epoch: [6]  [1080/1349]  eta: 0:01:24  lr: 0.000498  min_lr: 0.000012  loss: 0.8221 (0.9112)  loss_scale: 32768.0000 (32237.5282)  weight_decay: 0.0500 (0.0500)  time: 0.3062  data: 0.0001  max mem: 41808
Epoch: [6]  [1090/1349]  eta: 0:01:20  lr: 0.000498  min_lr: 0.000012  loss: 0.8785 (0.9117)  loss_scale: 32768.0000 (32242.3905)  weight_decay: 0.0500 (0.0500)  time: 0.3062  data: 0.0001  max mem: 41808
Epoch: [6]  [1100/1349]  eta: 0:01:17  lr: 0.000498  min_lr: 0.000012  loss: 0.9180 (0.9115)  loss_scale: 32768.0000 (32247.1644)  weight_decay: 0.0500 (0.0500)  time: 0.3060  data: 0.0001  max mem: 41808
Epoch: [6]  [1110/1349]  eta: 0:01:14  lr: 0.000498  min_lr: 0.000012  loss: 0.9360 (0.9121)  loss_scale: 32768.0000 (32251.8524)  weight_decay: 0.0500 (0.0500)  time: 0.3058  data: 0.0001  max mem: 41808
Epoch: [6]  [1120/1349]  eta: 0:01:11  lr: 0.000498  min_lr: 0.000012  loss: 0.9765 (0.9119)  loss_scale: 32768.0000 (32256.4567)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0001  max mem: 41808
[2025-05-23 16:18:25,223] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 16:18:25,223] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-05-23 16:18:25,223] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 16:18:25,223] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [6]  [1130/1349]  eta: 0:01:08  lr: 0.000498  min_lr: 0.000012  loss: 0.9765 (0.9122)  loss_scale: 32768.0000 (32318.9248)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [6]  [1140/1349]  eta: 0:01:05  lr: 0.000498  min_lr: 0.000012  loss: 1.0332 (0.9137)  loss_scale: 65536.0000 (32610.0473)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0002  max mem: 41808
Epoch: [6]  [1150/1349]  eta: 0:01:02  lr: 0.000498  min_lr: 0.000012  loss: 0.9852 (0.9134)  loss_scale: 65536.0000 (32896.1112)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0001  max mem: 41808
Epoch: [6]  [1160/1349]  eta: 0:00:58  lr: 0.000498  min_lr: 0.000012  loss: 0.9598 (0.9142)  loss_scale: 65536.0000 (33177.2472)  weight_decay: 0.0500 (0.0500)  time: 0.3078  data: 0.0001  max mem: 41808
Epoch: [6]  [1170/1349]  eta: 0:00:55  lr: 0.000498  min_lr: 0.000012  loss: 0.9880 (0.9137)  loss_scale: 65536.0000 (33453.5816)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
[2025-05-23 16:18:39,983] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 9271
[2025-05-23 16:18:39,983] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 9271
[2025-05-23 16:18:39,983] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-05-23 16:18:39,983] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-05-23 16:18:39,983] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [6]  [1180/1349]  eta: 0:00:52  lr: 0.000498  min_lr: 0.000012  loss: 0.9812 (0.9148)  loss_scale: 65536.0000 (33614.2523)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
Epoch: [6]  [1190/1349]  eta: 0:00:49  lr: 0.000498  min_lr: 0.000012  loss: 0.9225 (0.9144)  loss_scale: 32768.0000 (33607.1469)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [6]  [1200/1349]  eta: 0:00:46  lr: 0.000498  min_lr: 0.000012  loss: 0.8811 (0.9143)  loss_scale: 32768.0000 (33600.1599)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
Epoch: [6]  [1210/1349]  eta: 0:00:43  lr: 0.000498  min_lr: 0.000012  loss: 0.8693 (0.9135)  loss_scale: 32768.0000 (33593.2882)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
Epoch: [6]  [1220/1349]  eta: 0:00:40  lr: 0.000498  min_lr: 0.000012  loss: 0.8749 (0.9136)  loss_scale: 32768.0000 (33586.5291)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
Epoch: [6]  [1230/1349]  eta: 0:00:37  lr: 0.000498  min_lr: 0.000012  loss: 0.9399 (0.9137)  loss_scale: 32768.0000 (33579.8798)  weight_decay: 0.0500 (0.0500)  time: 0.3078  data: 0.0001  max mem: 41808
Epoch: [6]  [1240/1349]  eta: 0:00:33  lr: 0.000498  min_lr: 0.000012  loss: 0.8235 (0.9126)  loss_scale: 32768.0000 (33573.3376)  weight_decay: 0.0500 (0.0500)  time: 0.3095  data: 0.0005  max mem: 41808
Epoch: [6]  [1250/1349]  eta: 0:00:30  lr: 0.000498  min_lr: 0.000012  loss: 0.8456 (0.9128)  loss_scale: 32768.0000 (33566.9001)  weight_decay: 0.0500 (0.0500)  time: 0.3099  data: 0.0005  max mem: 41808
Epoch: [6]  [1260/1349]  eta: 0:00:27  lr: 0.000498  min_lr: 0.000012  loss: 0.9366 (0.9130)  loss_scale: 32768.0000 (33560.5646)  weight_decay: 0.0500 (0.0500)  time: 0.3084  data: 0.0001  max mem: 41808
Epoch: [6]  [1270/1349]  eta: 0:00:24  lr: 0.000498  min_lr: 0.000012  loss: 0.9357 (0.9131)  loss_scale: 32768.0000 (33554.3289)  weight_decay: 0.0500 (0.0500)  time: 0.3081  data: 0.0001  max mem: 41808
Epoch: [6]  [1280/1349]  eta: 0:00:21  lr: 0.000498  min_lr: 0.000012  loss: 0.8710 (0.9127)  loss_scale: 32768.0000 (33548.1905)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
Epoch: [6]  [1290/1349]  eta: 0:00:18  lr: 0.000498  min_lr: 0.000012  loss: 0.8650 (0.9125)  loss_scale: 32768.0000 (33542.1472)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [6]  [1300/1349]  eta: 0:00:15  lr: 0.000498  min_lr: 0.000012  loss: 0.9500 (0.9131)  loss_scale: 32768.0000 (33536.1968)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
[2025-05-23 16:19:19,711] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 16:19:19,711] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 16:19:19,711] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-05-23 16:19:19,711] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [6]  [1310/1349]  eta: 0:00:12  lr: 0.000498  min_lr: 0.000012  loss: 0.9804 (0.9133)  loss_scale: 32768.0000 (33655.3105)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
[2025-05-23 16:19:22,167] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 9408
[2025-05-23 16:19:22,167] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-05-23 16:19:22,167] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 9408
[2025-05-23 16:19:22,167] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-05-23 16:19:22,167] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [6]  [1320/1349]  eta: 0:00:09  lr: 0.000498  min_lr: 0.000012  loss: 1.0063 (0.9142)  loss_scale: 32768.0000 (33723.0098)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
Epoch: [6]  [1330/1349]  eta: 0:00:05  lr: 0.000498  min_lr: 0.000012  loss: 1.0018 (0.9142)  loss_scale: 32768.0000 (33715.8347)  weight_decay: 0.0500 (0.0500)  time: 0.3048  data: 0.0001  max mem: 41808
Epoch: [6]  [1340/1349]  eta: 0:00:02  lr: 0.000498  min_lr: 0.000012  loss: 0.8774 (0.9137)  loss_scale: 32768.0000 (33708.7666)  weight_decay: 0.0500 (0.0500)  time: 0.3027  data: 0.0001  max mem: 41808
Epoch: [6]  [1348/1349]  eta: 0:00:00  lr: 0.000498  min_lr: 0.000012  loss: 0.9163 (0.9139)  loss_scale: 32768.0000 (33703.1875)  weight_decay: 0.0500 (0.0500)  time: 0.3022  data: 0.0001  max mem: 41808
Epoch: [6] Total time: 0:07:00 (0.3116 s / it)
Averaged stats: lr: 0.000498  min_lr: 0.000012  loss: 0.9163 (0.9121)  loss_scale: 32768.0000 (33703.1875)  weight_decay: 0.0500 (0.0500)  total_time: 420.3510 (420.3466)
Val:  [  0/346]  eta: 1:26:26  loss: 2.4803 (2.4803)  acc1: 11.7188 (11.7188)  acc5: 86.7188 (86.7188)  time: 14.9890  data: 14.2272  max mem: 41808
Val:  [ 10/346]  eta: 0:11:36  loss: 0.1738 (0.5595)  acc1: 100.0000 (83.3807)  acc5: 100.0000 (97.7273)  time: 2.0718  data: 1.2936  max mem: 41808
Val:  [ 20/346]  eta: 0:08:32  loss: 0.1601 (0.4204)  acc1: 99.2188 (88.9137)  acc5: 100.0000 (98.8095)  time: 0.9010  data: 0.1008  max mem: 41808
Val:  [ 30/346]  eta: 0:06:54  loss: 0.1413 (0.3729)  acc1: 99.2188 (90.6502)  acc5: 100.0000 (98.9667)  time: 0.8943  data: 0.1009  max mem: 41808
Val:  [ 40/346]  eta: 0:06:01  loss: 0.1507 (0.4094)  acc1: 99.2188 (89.5198)  acc5: 100.0000 (99.1044)  time: 0.7717  data: 0.0003  max mem: 41808
Val:  [ 50/346]  eta: 0:05:23  loss: 0.1507 (0.3610)  acc1: 99.2188 (91.2684)  acc5: 100.0000 (99.2800)  time: 0.7558  data: 0.0003  max mem: 41808
Val:  [ 60/346]  eta: 0:04:55  loss: 0.1544 (0.3371)  acc1: 98.4375 (91.8801)  acc5: 100.0000 (99.3981)  time: 0.7281  data: 0.0003  max mem: 41808
Val:  [ 70/346]  eta: 0:04:37  loss: 0.2871 (0.3645)  acc1: 92.9688 (91.0871)  acc5: 100.0000 (99.4278)  time: 0.7830  data: 0.0588  max mem: 41808
Val:  [ 80/346]  eta: 0:04:24  loss: 0.2810 (0.3594)  acc1: 91.4062 (91.4062)  acc5: 100.0000 (99.3827)  time: 0.8816  data: 0.1360  max mem: 41808
Val:  [ 90/346]  eta: 0:04:12  loss: 0.2172 (0.3540)  acc1: 94.5312 (91.4663)  acc5: 100.0000 (99.4505)  time: 0.9085  data: 0.1495  max mem: 41808
Val:  [100/346]  eta: 0:03:59  loss: 0.1984 (0.3371)  acc1: 97.6562 (92.1643)  acc5: 100.0000 (99.5050)  time: 0.8872  data: 0.1409  max mem: 41808
Val:  [110/346]  eta: 0:03:47  loss: 0.1960 (0.3651)  acc1: 97.6562 (91.3077)  acc5: 100.0000 (99.1906)  time: 0.8709  data: 0.1400  max mem: 41808
Val:  [120/346]  eta: 0:03:36  loss: 0.1786 (0.3642)  acc1: 96.0938 (91.2642)  acc5: 100.0000 (99.2575)  time: 0.8811  data: 0.1483  max mem: 41808
Val:  [130/346]  eta: 0:03:26  loss: 0.1503 (0.3806)  acc1: 98.4375 (90.7502)  acc5: 100.0000 (99.2068)  time: 0.9224  data: 0.1586  max mem: 41808
Val:  [140/346]  eta: 0:03:16  loss: 0.2400 (0.3783)  acc1: 94.5312 (90.8134)  acc5: 100.0000 (99.2298)  time: 0.9103  data: 0.1534  max mem: 41808
Val:  [150/346]  eta: 0:03:05  loss: 0.2451 (0.3749)  acc1: 96.0938 (90.9665)  acc5: 100.0000 (99.2808)  time: 0.8823  data: 0.1486  max mem: 41808
Val:  [160/346]  eta: 0:02:55  loss: 0.2420 (0.3731)  acc1: 96.0938 (90.8967)  acc5: 100.0000 (99.3109)  time: 0.9063  data: 0.1511  max mem: 41808
Val:  [170/346]  eta: 0:02:46  loss: 0.2565 (0.3859)  acc1: 90.6250 (90.3874)  acc5: 100.0000 (99.3512)  time: 0.9302  data: 0.1541  max mem: 41808
Val:  [180/346]  eta: 0:02:36  loss: 0.4160 (0.4000)  acc1: 82.8125 (89.6150)  acc5: 100.0000 (99.3785)  time: 0.9414  data: 0.1553  max mem: 41808
Val:  [190/346]  eta: 0:02:27  loss: 0.3352 (0.4020)  acc1: 89.8438 (89.4388)  acc5: 100.0000 (99.4069)  time: 0.9235  data: 0.1589  max mem: 41808
Val:  [200/346]  eta: 0:02:17  loss: 0.3586 (0.4135)  acc1: 89.8438 (89.0508)  acc5: 100.0000 (99.4248)  time: 0.9075  data: 0.1570  max mem: 41808
Val:  [210/346]  eta: 0:02:07  loss: 0.2947 (0.4049)  acc1: 93.7500 (89.3624)  acc5: 100.0000 (99.4483)  time: 0.9100  data: 0.1532  max mem: 41808
Val:  [220/346]  eta: 0:01:58  loss: 0.1668 (0.4036)  acc1: 96.8750 (89.4690)  acc5: 100.0000 (99.4556)  time: 0.9128  data: 0.1587  max mem: 41808
Val:  [230/346]  eta: 0:01:48  loss: 0.1685 (0.3950)  acc1: 96.8750 (89.7896)  acc5: 100.0000 (99.4792)  time: 0.8989  data: 0.1505  max mem: 41808
Val:  [240/346]  eta: 0:01:39  loss: 0.2132 (0.3994)  acc1: 96.0938 (89.6201)  acc5: 100.0000 (99.5008)  time: 0.9002  data: 0.1527  max mem: 41808
Val:  [250/346]  eta: 0:01:29  loss: 0.2330 (0.3949)  acc1: 95.3125 (89.7597)  acc5: 100.0000 (99.5207)  time: 0.9217  data: 0.1580  max mem: 41808
Val:  [260/346]  eta: 0:01:20  loss: 0.1712 (0.3902)  acc1: 96.8750 (89.9066)  acc5: 100.0000 (99.5360)  time: 0.9082  data: 0.1568  max mem: 41808
Val:  [270/346]  eta: 0:01:10  loss: 0.1421 (0.3867)  acc1: 99.2188 (90.0398)  acc5: 100.0000 (99.5416)  time: 0.8933  data: 0.1556  max mem: 41808
Val:  [280/346]  eta: 0:01:01  loss: 0.1421 (0.3819)  acc1: 100.0000 (90.1885)  acc5: 100.0000 (99.5579)  time: 0.9038  data: 0.1552  max mem: 41808
Val:  [290/346]  eta: 0:00:52  loss: 0.1320 (0.3745)  acc1: 100.0000 (90.4451)  acc5: 100.0000 (99.5731)  time: 0.9275  data: 0.1636  max mem: 41808
Val:  [300/346]  eta: 0:00:42  loss: 0.1410 (0.3750)  acc1: 100.0000 (90.5160)  acc5: 100.0000 (99.4809)  time: 0.9134  data: 0.1436  max mem: 41808
Val:  [310/346]  eta: 0:00:33  loss: 0.1765 (0.3747)  acc1: 96.0938 (90.4919)  acc5: 100.0000 (99.4976)  time: 0.9027  data: 0.1306  max mem: 41808
Val:  [320/346]  eta: 0:00:24  loss: 0.1795 (0.3749)  acc1: 96.0938 (90.4717)  acc5: 100.0000 (99.4962)  time: 0.9103  data: 0.1466  max mem: 41808
Val:  [330/346]  eta: 0:00:14  loss: 0.2869 (0.3809)  acc1: 92.1875 (90.2733)  acc5: 100.0000 (99.4949)  time: 0.8806  data: 0.1542  max mem: 41808
Val:  [340/346]  eta: 0:00:05  loss: 0.3200 (0.3899)  acc1: 92.1875 (89.9973)  acc5: 100.0000 (99.5051)  time: 0.8971  data: 0.1631  max mem: 41808
Val:  [345/346]  eta: 0:00:00  loss: 0.2339 (0.3869)  acc1: 93.7500 (90.0942)  acc5: 100.0000 (99.5118)  time: 0.8864  data: 0.1684  max mem: 41808
Val: Total time: 0:05:20 (0.9274 s / it)
* Acc@1 90.107 Acc@5 99.537 loss 0.385
Accuracy of the network on the 88494 val videos: 90.1%
Max accuracy: 91.25%   Max Epoch: 3
Epoch: [7]  [   0/1349]  eta: 1:30:46  lr: 0.000498  min_lr: 0.000012  loss: 0.8795 (0.8795)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 4.0377  data: 3.7033  max mem: 41808
Epoch: [7]  [  10/1349]  eta: 0:16:40  lr: 0.000498  min_lr: 0.000012  loss: 0.8129 (0.8409)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7474  data: 0.4100  max mem: 41808
Epoch: [7]  [  20/1349]  eta: 0:11:56  lr: 0.000498  min_lr: 0.000012  loss: 0.8216 (0.8669)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3642  data: 0.0404  max mem: 41808
Epoch: [7]  [  30/1349]  eta: 0:10:12  lr: 0.000498  min_lr: 0.000012  loss: 0.9897 (0.9187)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3091  data: 0.0001  max mem: 41808
Epoch: [7]  [  40/1349]  eta: 0:09:18  lr: 0.000497  min_lr: 0.000012  loss: 1.0058 (0.9073)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3082  data: 0.0001  max mem: 41808
Epoch: [7]  [  50/1349]  eta: 0:08:44  lr: 0.000497  min_lr: 0.000012  loss: 0.8764 (0.9048)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3085  data: 0.0001  max mem: 41808
Epoch: [7]  [  60/1349]  eta: 0:08:19  lr: 0.000497  min_lr: 0.000012  loss: 0.8764 (0.9050)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3082  data: 0.0002  max mem: 41808
Epoch: [7]  [  70/1349]  eta: 0:08:01  lr: 0.000497  min_lr: 0.000012  loss: 0.9763 (0.9153)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0001  max mem: 41808
Epoch: [7]  [  80/1349]  eta: 0:07:47  lr: 0.000497  min_lr: 0.000012  loss: 0.9763 (0.9068)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3083  data: 0.0001  max mem: 41808
Epoch: [7]  [  90/1349]  eta: 0:07:35  lr: 0.000497  min_lr: 0.000012  loss: 0.7954 (0.8985)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3082  data: 0.0001  max mem: 41808
[2025-05-23 16:25:27,806] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 16:25:27,806] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-05-23 16:25:27,806] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 16:25:27,806] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [7]  [ 100/1349]  eta: 0:07:24  lr: 0.000497  min_lr: 0.000012  loss: 0.9139 (0.9027)  loss_scale: 32768.0000 (35039.0495)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0001  max mem: 41808
Epoch: [7]  [ 110/1349]  eta: 0:07:16  lr: 0.000497  min_lr: 0.000012  loss: 0.9859 (0.9114)  loss_scale: 65536.0000 (37786.5225)  weight_decay: 0.0500 (0.0500)  time: 0.3084  data: 0.0001  max mem: 41808
Epoch: [7]  [ 120/1349]  eta: 0:07:07  lr: 0.000497  min_lr: 0.000012  loss: 0.9840 (0.9136)  loss_scale: 65536.0000 (40079.8678)  weight_decay: 0.0500 (0.0500)  time: 0.3085  data: 0.0001  max mem: 41808
Epoch: [7]  [ 130/1349]  eta: 0:07:00  lr: 0.000497  min_lr: 0.000012  loss: 0.9120 (0.9163)  loss_scale: 65536.0000 (42023.0840)  weight_decay: 0.0500 (0.0500)  time: 0.3081  data: 0.0001  max mem: 41808
[2025-05-23 16:25:41,380] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 9581
[2025-05-23 16:25:41,380] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-05-23 16:25:41,381] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 9581
[2025-05-23 16:25:41,381] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-05-23 16:25:41,381] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [7]  [ 140/1349]  eta: 0:06:54  lr: 0.000497  min_lr: 0.000012  loss: 0.9104 (0.9130)  loss_scale: 65536.0000 (42993.4752)  weight_decay: 0.0500 (0.0500)  time: 0.3081  data: 0.0002  max mem: 41808
Epoch: [7]  [ 150/1349]  eta: 0:06:48  lr: 0.000497  min_lr: 0.000012  loss: 0.9101 (0.9134)  loss_scale: 32768.0000 (42316.2914)  weight_decay: 0.0500 (0.0500)  time: 0.3081  data: 0.0002  max mem: 41808
Epoch: [7]  [ 160/1349]  eta: 0:06:42  lr: 0.000497  min_lr: 0.000012  loss: 0.9215 (0.9105)  loss_scale: 32768.0000 (41723.2298)  weight_decay: 0.0500 (0.0500)  time: 0.3083  data: 0.0002  max mem: 41808
Epoch: [7]  [ 170/1349]  eta: 0:06:36  lr: 0.000497  min_lr: 0.000012  loss: 0.8779 (0.9078)  loss_scale: 32768.0000 (41199.5322)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [7]  [ 180/1349]  eta: 0:06:31  lr: 0.000497  min_lr: 0.000012  loss: 0.8703 (0.9100)  loss_scale: 32768.0000 (40733.7017)  weight_decay: 0.0500 (0.0500)  time: 0.3061  data: 0.0001  max mem: 41808
Epoch: [7]  [ 190/1349]  eta: 0:06:26  lr: 0.000497  min_lr: 0.000012  loss: 0.9727 (0.9107)  loss_scale: 32768.0000 (40316.6492)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0002  max mem: 41808
Epoch: [7]  [ 200/1349]  eta: 0:06:21  lr: 0.000497  min_lr: 0.000012  loss: 0.9294 (0.9069)  loss_scale: 32768.0000 (39941.0945)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0002  max mem: 41808
Epoch: [7]  [ 210/1349]  eta: 0:06:16  lr: 0.000497  min_lr: 0.000012  loss: 0.8252 (0.9027)  loss_scale: 32768.0000 (39601.1374)  weight_decay: 0.0500 (0.0500)  time: 0.3066  data: 0.0001  max mem: 41808
Epoch: [7]  [ 220/1349]  eta: 0:06:12  lr: 0.000497  min_lr: 0.000012  loss: 0.8665 (0.9041)  loss_scale: 32768.0000 (39291.9457)  weight_decay: 0.0500 (0.0500)  time: 0.3091  data: 0.0001  max mem: 41808
Epoch: [7]  [ 230/1349]  eta: 0:06:08  lr: 0.000497  min_lr: 0.000012  loss: 0.9356 (0.9045)  loss_scale: 32768.0000 (39009.5238)  weight_decay: 0.0500 (0.0500)  time: 0.3098  data: 0.0001  max mem: 41808
Epoch: [7]  [ 240/1349]  eta: 0:06:03  lr: 0.000497  min_lr: 0.000012  loss: 0.9416 (0.9044)  loss_scale: 32768.0000 (38750.5394)  weight_decay: 0.0500 (0.0500)  time: 0.3082  data: 0.0001  max mem: 41808
Epoch: [7]  [ 250/1349]  eta: 0:05:59  lr: 0.000497  min_lr: 0.000012  loss: 0.9630 (0.9038)  loss_scale: 32768.0000 (38512.1912)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [7]  [ 260/1349]  eta: 0:05:55  lr: 0.000497  min_lr: 0.000012  loss: 1.0065 (0.9087)  loss_scale: 32768.0000 (38292.1073)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
[2025-05-23 16:26:21,103] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 16:26:21,103] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-05-23 16:26:21,103] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 16:26:21,103] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [7]  [ 270/1349]  eta: 0:05:51  lr: 0.000497  min_lr: 0.000012  loss: 0.9982 (0.9073)  loss_scale: 32768.0000 (38571.9262)  weight_decay: 0.0500 (0.0500)  time: 0.3091  data: 0.0001  max mem: 41808
[2025-05-23 16:26:22,959] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 9716
[2025-05-23 16:26:22,959] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 9716
[2025-05-23 16:26:22,959] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-05-23 16:26:22,959] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-05-23 16:26:22,960] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [7]  [ 280/1349]  eta: 0:05:47  lr: 0.000497  min_lr: 0.000012  loss: 0.8886 (0.9071)  loss_scale: 32768.0000 (38598.6050)  weight_decay: 0.0500 (0.0500)  time: 0.3080  data: 0.0001  max mem: 41808
Epoch: [7]  [ 290/1349]  eta: 0:05:43  lr: 0.000497  min_lr: 0.000012  loss: 0.9460 (0.9081)  loss_scale: 32768.0000 (38398.2405)  weight_decay: 0.0500 (0.0500)  time: 0.3059  data: 0.0001  max mem: 41808
Epoch: [7]  [ 300/1349]  eta: 0:05:39  lr: 0.000497  min_lr: 0.000012  loss: 0.9523 (0.9079)  loss_scale: 32768.0000 (38211.1894)  weight_decay: 0.0500 (0.0500)  time: 0.3059  data: 0.0001  max mem: 41808
Epoch: [7]  [ 310/1349]  eta: 0:05:36  lr: 0.000497  min_lr: 0.000012  loss: 0.8523 (0.9053)  loss_scale: 32768.0000 (38036.1672)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
Epoch: [7]  [ 320/1349]  eta: 0:05:32  lr: 0.000497  min_lr: 0.000012  loss: 0.7932 (0.9020)  loss_scale: 32768.0000 (37872.0498)  weight_decay: 0.0500 (0.0500)  time: 0.3078  data: 0.0001  max mem: 41808
Epoch: [7]  [ 330/1349]  eta: 0:05:28  lr: 0.000497  min_lr: 0.000012  loss: 0.8435 (0.9031)  loss_scale: 32768.0000 (37717.8489)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
Epoch: [7]  [ 340/1349]  eta: 0:05:24  lr: 0.000497  min_lr: 0.000012  loss: 0.8626 (0.9023)  loss_scale: 32768.0000 (37572.6921)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
Epoch: [7]  [ 350/1349]  eta: 0:05:21  lr: 0.000497  min_lr: 0.000012  loss: 0.8506 (0.9035)  loss_scale: 32768.0000 (37435.8063)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [7]  [ 360/1349]  eta: 0:05:17  lr: 0.000497  min_lr: 0.000012  loss: 0.9442 (0.9042)  loss_scale: 32768.0000 (37306.5042)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
Epoch: [7]  [ 370/1349]  eta: 0:05:14  lr: 0.000497  min_lr: 0.000012  loss: 0.9442 (0.9062)  loss_scale: 32768.0000 (37184.1725)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [7]  [ 380/1349]  eta: 0:05:10  lr: 0.000497  min_lr: 0.000012  loss: 0.9241 (0.9040)  loss_scale: 32768.0000 (37068.2625)  weight_decay: 0.0500 (0.0500)  time: 0.3080  data: 0.0001  max mem: 41808
Epoch: [7]  [ 390/1349]  eta: 0:05:06  lr: 0.000497  min_lr: 0.000012  loss: 0.8762 (0.9017)  loss_scale: 32768.0000 (36958.2813)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
Epoch: [7]  [ 400/1349]  eta: 0:05:03  lr: 0.000497  min_lr: 0.000012  loss: 0.9486 (0.9038)  loss_scale: 32768.0000 (36853.7855)  weight_decay: 0.0500 (0.0500)  time: 0.3063  data: 0.0001  max mem: 41808
[2025-05-23 16:27:02,571] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 16:27:02,571] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-05-23 16:27:02,571] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 16:27:02,572] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-05-23 16:27:04,730] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 9852
[2025-05-23 16:27:04,730] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-05-23 16:27:04,730] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 9852
[2025-05-23 16:27:04,730] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-05-23 16:27:04,730] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [7]  [ 410/1349]  eta: 0:04:59  lr: 0.000497  min_lr: 0.000012  loss: 0.9880 (0.9048)  loss_scale: 32768.0000 (37312.4672)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [7]  [ 420/1349]  eta: 0:04:56  lr: 0.000497  min_lr: 0.000012  loss: 0.8416 (0.9035)  loss_scale: 32768.0000 (37204.5226)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
Epoch: [7]  [ 430/1349]  eta: 0:04:53  lr: 0.000497  min_lr: 0.000012  loss: 0.8158 (0.9044)  loss_scale: 32768.0000 (37101.5870)  weight_decay: 0.0500 (0.0500)  time: 0.3059  data: 0.0001  max mem: 41808
Epoch: [7]  [ 440/1349]  eta: 0:04:49  lr: 0.000497  min_lr: 0.000012  loss: 0.9504 (0.9044)  loss_scale: 32768.0000 (37003.3197)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [7]  [ 450/1349]  eta: 0:04:46  lr: 0.000497  min_lr: 0.000012  loss: 0.8911 (0.9035)  loss_scale: 32768.0000 (36909.4102)  weight_decay: 0.0500 (0.0500)  time: 0.3081  data: 0.0001  max mem: 41808
Epoch: [7]  [ 460/1349]  eta: 0:04:42  lr: 0.000497  min_lr: 0.000012  loss: 0.8391 (0.9020)  loss_scale: 32768.0000 (36819.5748)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
Epoch: [7]  [ 470/1349]  eta: 0:04:39  lr: 0.000497  min_lr: 0.000012  loss: 0.8845 (0.9027)  loss_scale: 32768.0000 (36733.5541)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
Epoch: [7]  [ 480/1349]  eta: 0:04:36  lr: 0.000497  min_lr: 0.000012  loss: 0.9557 (0.9031)  loss_scale: 32768.0000 (36651.1102)  weight_decay: 0.0500 (0.0500)  time: 0.3080  data: 0.0001  max mem: 41808
Epoch: [7]  [ 490/1349]  eta: 0:04:32  lr: 0.000497  min_lr: 0.000012  loss: 0.9368 (0.9036)  loss_scale: 32768.0000 (36572.0244)  weight_decay: 0.0500 (0.0500)  time: 0.3080  data: 0.0001  max mem: 41808
Epoch: [7]  [ 500/1349]  eta: 0:04:29  lr: 0.000497  min_lr: 0.000012  loss: 0.9817 (0.9051)  loss_scale: 32768.0000 (36496.0958)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0002  max mem: 41808
Epoch: [7]  [ 510/1349]  eta: 0:04:26  lr: 0.000497  min_lr: 0.000012  loss: 0.9443 (0.9033)  loss_scale: 32768.0000 (36423.1389)  weight_decay: 0.0500 (0.0500)  time: 0.3090  data: 0.0002  max mem: 41808
Epoch: [7]  [ 520/1349]  eta: 0:04:22  lr: 0.000497  min_lr: 0.000012  loss: 0.9013 (0.9044)  loss_scale: 32768.0000 (36352.9827)  weight_decay: 0.0500 (0.0500)  time: 0.3082  data: 0.0001  max mem: 41808
Epoch: [7]  [ 530/1349]  eta: 0:04:19  lr: 0.000497  min_lr: 0.000012  loss: 0.9291 (0.9043)  loss_scale: 32768.0000 (36285.4689)  weight_decay: 0.0500 (0.0500)  time: 0.3064  data: 0.0001  max mem: 41808
[2025-05-23 16:27:44,420] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 16:27:44,420] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-05-23 16:27:44,420] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 16:27:44,420] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [7]  [ 540/1349]  eta: 0:04:16  lr: 0.000497  min_lr: 0.000012  loss: 0.8928 (0.9047)  loss_scale: 32768.0000 (36402.1590)  weight_decay: 0.0500 (0.0500)  time: 0.3081  data: 0.0002  max mem: 41808
Epoch: [7]  [ 550/1349]  eta: 0:04:12  lr: 0.000496  min_lr: 0.000012  loss: 0.9714 (0.9050)  loss_scale: 65536.0000 (36930.9038)  weight_decay: 0.0500 (0.0500)  time: 0.3080  data: 0.0002  max mem: 41808
[2025-05-23 16:27:49,987] [INFO] [logging.py:96:log_dist] [Rank 0] step=10000, skipped=58, lr=[1.1794783312280586e-05, 1.1794783312280586e-05, 1.5726377749707448e-05, 1.5726377749707448e-05, 2.0968503666276597e-05, 2.0968503666276597e-05, 2.7958004888368795e-05, 2.7958004888368795e-05, 3.7277339851158394e-05, 3.7277339851158394e-05, 4.970311980154453e-05, 4.970311980154453e-05, 6.627082640205937e-05, 6.627082640205937e-05, 8.836110186941249e-05, 8.836110186941249e-05, 0.00011781480249254999, 0.00011781480249254999, 0.00015708640332339998, 0.00015708640332339998, 0.0002094485377645333, 0.0002094485377645333, 0.0002792647170193778, 0.0002792647170193778, 0.000372352956025837, 0.000372352956025837, 0.0004964706080344493, 0.0004964706080344493], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-05-23 16:27:49,988] [INFO] [timer.py:260:stop] epoch=0/micro_step=10000/global_step=10000, RunningAvgSamplesPerSec=212.80341376152174, CurrSamplesPerSec=213.9577481291133, MemAllocated=1.37GB, MaxMemAllocated=40.83GB
Epoch: [7]  [ 560/1349]  eta: 0:04:09  lr: 0.000496  min_lr: 0.000012  loss: 0.8680 (0.9037)  loss_scale: 65536.0000 (37440.7986)  weight_decay: 0.0500 (0.0500)  time: 0.3082  data: 0.0001  max mem: 41808
Epoch: [7]  [ 570/1349]  eta: 0:04:06  lr: 0.000496  min_lr: 0.000012  loss: 0.8361 (0.9036)  loss_scale: 65536.0000 (37932.8336)  weight_decay: 0.0500 (0.0500)  time: 0.3087  data: 0.0001  max mem: 41808
Epoch: [7]  [ 580/1349]  eta: 0:04:03  lr: 0.000496  min_lr: 0.000012  loss: 0.9202 (0.9034)  loss_scale: 65536.0000 (38407.9312)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [7]  [ 590/1349]  eta: 0:03:59  lr: 0.000496  min_lr: 0.000012  loss: 0.8981 (0.9030)  loss_scale: 65536.0000 (38866.9509)  weight_decay: 0.0500 (0.0500)  time: 0.3084  data: 0.0001  max mem: 41808
Epoch: [7]  [ 600/1349]  eta: 0:03:56  lr: 0.000496  min_lr: 0.000012  loss: 0.7975 (0.9012)  loss_scale: 65536.0000 (39310.6955)  weight_decay: 0.0500 (0.0500)  time: 0.3090  data: 0.0001  max mem: 41808
Epoch: [7]  [ 610/1349]  eta: 0:03:53  lr: 0.000496  min_lr: 0.000012  loss: 0.8903 (0.9024)  loss_scale: 65536.0000 (39739.9149)  weight_decay: 0.0500 (0.0500)  time: 0.3079  data: 0.0001  max mem: 41808
Epoch: [7]  [ 620/1349]  eta: 0:03:50  lr: 0.000496  min_lr: 0.000012  loss: 0.9235 (0.9020)  loss_scale: 65536.0000 (40155.3108)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [7]  [ 630/1349]  eta: 0:03:46  lr: 0.000496  min_lr: 0.000012  loss: 0.9235 (0.9015)  loss_scale: 65536.0000 (40557.5404)  weight_decay: 0.0500 (0.0500)  time: 0.3066  data: 0.0001  max mem: 41808
Epoch: [7]  [ 640/1349]  eta: 0:03:43  lr: 0.000496  min_lr: 0.000012  loss: 0.9178 (0.9017)  loss_scale: 65536.0000 (40947.2200)  weight_decay: 0.0500 (0.0500)  time: 0.3063  data: 0.0001  max mem: 41808
Epoch: [7]  [ 650/1349]  eta: 0:03:40  lr: 0.000496  min_lr: 0.000012  loss: 0.9219 (0.9024)  loss_scale: 65536.0000 (41324.9278)  weight_decay: 0.0500 (0.0500)  time: 0.3066  data: 0.0001  max mem: 41808
Epoch: [7]  [ 660/1349]  eta: 0:03:37  lr: 0.000496  min_lr: 0.000012  loss: 0.9259 (0.9028)  loss_scale: 65536.0000 (41691.2073)  weight_decay: 0.0500 (0.0500)  time: 0.3066  data: 0.0001  max mem: 41808
[2025-05-23 16:28:22,552] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 10105
[2025-05-23 16:28:22,552] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 10105
[2025-05-23 16:28:22,553] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-05-23 16:28:22,553] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-05-23 16:28:22,553] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [7]  [ 670/1349]  eta: 0:03:33  lr: 0.000496  min_lr: 0.000012  loss: 0.8205 (0.9008)  loss_scale: 65536.0000 (41607.0581)  weight_decay: 0.0500 (0.0500)  time: 0.3063  data: 0.0002  max mem: 41808
Epoch: [7]  [ 680/1349]  eta: 0:03:30  lr: 0.000496  min_lr: 0.000012  loss: 0.8763 (0.9018)  loss_scale: 32768.0000 (41477.2628)  weight_decay: 0.0500 (0.0500)  time: 0.3063  data: 0.0001  max mem: 41808
Epoch: [7]  [ 690/1349]  eta: 0:03:27  lr: 0.000496  min_lr: 0.000012  loss: 0.9550 (0.9021)  loss_scale: 32768.0000 (41351.2243)  weight_decay: 0.0500 (0.0500)  time: 0.3066  data: 0.0001  max mem: 41808
Epoch: [7]  [ 700/1349]  eta: 0:03:24  lr: 0.000496  min_lr: 0.000012  loss: 0.9820 (0.9027)  loss_scale: 32768.0000 (41228.7817)  weight_decay: 0.0500 (0.0500)  time: 0.3080  data: 0.0001  max mem: 41808
Epoch: [7]  [ 710/1349]  eta: 0:03:20  lr: 0.000496  min_lr: 0.000012  loss: 0.9618 (0.9030)  loss_scale: 32768.0000 (41109.7834)  weight_decay: 0.0500 (0.0500)  time: 0.3083  data: 0.0002  max mem: 41808
Epoch: [7]  [ 720/1349]  eta: 0:03:17  lr: 0.000496  min_lr: 0.000012  loss: 0.9308 (0.9032)  loss_scale: 32768.0000 (40994.0860)  weight_decay: 0.0500 (0.0500)  time: 0.3081  data: 0.0002  max mem: 41808
Epoch: [7]  [ 730/1349]  eta: 0:03:14  lr: 0.000496  min_lr: 0.000012  loss: 0.9152 (0.9038)  loss_scale: 32768.0000 (40881.5540)  weight_decay: 0.0500 (0.0500)  time: 0.3086  data: 0.0001  max mem: 41808
Epoch: [7]  [ 740/1349]  eta: 0:03:11  lr: 0.000496  min_lr: 0.000012  loss: 0.9080 (0.9032)  loss_scale: 32768.0000 (40772.0594)  weight_decay: 0.0500 (0.0500)  time: 0.3085  data: 0.0001  max mem: 41808
Epoch: [7]  [ 750/1349]  eta: 0:03:08  lr: 0.000496  min_lr: 0.000012  loss: 0.9070 (0.9041)  loss_scale: 32768.0000 (40665.4807)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [7]  [ 760/1349]  eta: 0:03:04  lr: 0.000496  min_lr: 0.000012  loss: 0.9070 (0.9039)  loss_scale: 32768.0000 (40561.7030)  weight_decay: 0.0500 (0.0500)  time: 0.3063  data: 0.0001  max mem: 41808
Epoch: [7]  [ 770/1349]  eta: 0:03:01  lr: 0.000496  min_lr: 0.000012  loss: 0.8895 (0.9036)  loss_scale: 32768.0000 (40460.6174)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
Epoch: [7]  [ 780/1349]  eta: 0:02:58  lr: 0.000496  min_lr: 0.000012  loss: 0.9167 (0.9034)  loss_scale: 32768.0000 (40362.1204)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
Epoch: [7]  [ 790/1349]  eta: 0:02:55  lr: 0.000496  min_lr: 0.000012  loss: 0.8668 (0.9034)  loss_scale: 32768.0000 (40266.1138)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
[2025-05-23 16:29:02,225] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 16:29:02,225] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-05-23 16:29:02,225] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 16:29:02,225] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [7]  [ 800/1349]  eta: 0:02:52  lr: 0.000496  min_lr: 0.000012  loss: 0.9038 (0.9042)  loss_scale: 32768.0000 (40581.5930)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [7]  [ 810/1349]  eta: 0:02:48  lr: 0.000496  min_lr: 0.000012  loss: 0.8727 (0.9020)  loss_scale: 65536.0000 (40889.2922)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0001  max mem: 41808
Epoch: [7]  [ 820/1349]  eta: 0:02:45  lr: 0.000496  min_lr: 0.000012  loss: 0.8022 (0.9015)  loss_scale: 65536.0000 (41189.4957)  weight_decay: 0.0500 (0.0500)  time: 0.3060  data: 0.0001  max mem: 41808
Epoch: [7]  [ 830/1349]  eta: 0:02:42  lr: 0.000496  min_lr: 0.000012  loss: 0.9266 (0.9028)  loss_scale: 65536.0000 (41482.4741)  weight_decay: 0.0500 (0.0500)  time: 0.3061  data: 0.0001  max mem: 41808
Epoch: [7]  [ 840/1349]  eta: 0:02:39  lr: 0.000496  min_lr: 0.000012  loss: 0.9583 (0.9027)  loss_scale: 65536.0000 (41768.4851)  weight_decay: 0.0500 (0.0500)  time: 0.3063  data: 0.0001  max mem: 41808
Epoch: [7]  [ 850/1349]  eta: 0:02:36  lr: 0.000496  min_lr: 0.000012  loss: 0.9244 (0.9038)  loss_scale: 65536.0000 (42047.7744)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [7]  [ 860/1349]  eta: 0:02:33  lr: 0.000496  min_lr: 0.000012  loss: 0.9407 (0.9046)  loss_scale: 65536.0000 (42320.5761)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [7]  [ 870/1349]  eta: 0:02:29  lr: 0.000496  min_lr: 0.000012  loss: 0.9407 (0.9044)  loss_scale: 65536.0000 (42587.1137)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0001  max mem: 41808
Epoch: [7]  [ 880/1349]  eta: 0:02:26  lr: 0.000496  min_lr: 0.000012  loss: 0.9768 (0.9055)  loss_scale: 65536.0000 (42847.6005)  weight_decay: 0.0500 (0.0500)  time: 0.3066  data: 0.0001  max mem: 41808
Epoch: [7]  [ 890/1349]  eta: 0:02:23  lr: 0.000496  min_lr: 0.000012  loss: 0.9541 (0.9050)  loss_scale: 65536.0000 (43102.2402)  weight_decay: 0.0500 (0.0500)  time: 0.3082  data: 0.0001  max mem: 41808
Epoch: [7]  [ 900/1349]  eta: 0:02:20  lr: 0.000496  min_lr: 0.000012  loss: 0.8372 (0.9047)  loss_scale: 65536.0000 (43351.2275)  weight_decay: 0.0500 (0.0500)  time: 0.3084  data: 0.0001  max mem: 41808
Epoch: [7]  [ 910/1349]  eta: 0:02:17  lr: 0.000496  min_lr: 0.000012  loss: 0.8397 (0.9037)  loss_scale: 65536.0000 (43594.7486)  weight_decay: 0.0500 (0.0500)  time: 0.3062  data: 0.0001  max mem: 41808
[2025-05-23 16:29:41,499] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 16:29:41,499] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-05-23 16:29:41,499] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 16:29:41,499] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [7]  [ 920/1349]  eta: 0:02:14  lr: 0.000496  min_lr: 0.000012  loss: 0.9156 (0.9044)  loss_scale: 65536.0000 (43975.2964)  weight_decay: 0.0500 (0.0500)  time: 0.3061  data: 0.0001  max mem: 41808
[2025-05-23 16:29:42,112] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 10364
[2025-05-23 16:29:42,113] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 16:29:42,113] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 10364
[2025-05-23 16:29:42,113] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
[2025-05-23 16:29:42,113] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
Epoch: [7]  [ 930/1349]  eta: 0:02:11  lr: 0.000496  min_lr: 0.000012  loss: 0.8165 (0.9028)  loss_scale: 65536.0000 (44206.8829)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0001  max mem: 41808
Epoch: [7]  [ 940/1349]  eta: 0:02:07  lr: 0.000496  min_lr: 0.000012  loss: 0.8165 (0.9029)  loss_scale: 65536.0000 (44433.5473)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0002  max mem: 41808
Epoch: [7]  [ 950/1349]  eta: 0:02:04  lr: 0.000496  min_lr: 0.000012  loss: 0.9282 (0.9033)  loss_scale: 65536.0000 (44655.4448)  weight_decay: 0.0500 (0.0500)  time: 0.3064  data: 0.0002  max mem: 41808
Epoch: [7]  [ 960/1349]  eta: 0:02:01  lr: 0.000496  min_lr: 0.000012  loss: 0.9282 (0.9031)  loss_scale: 65536.0000 (44872.7242)  weight_decay: 0.0500 (0.0500)  time: 0.3060  data: 0.0001  max mem: 41808
Epoch: [7]  [ 970/1349]  eta: 0:01:58  lr: 0.000496  min_lr: 0.000012  loss: 0.8958 (0.9025)  loss_scale: 65536.0000 (45085.5283)  weight_decay: 0.0500 (0.0500)  time: 0.3059  data: 0.0001  max mem: 41808
Epoch: [7]  [ 980/1349]  eta: 0:01:55  lr: 0.000495  min_lr: 0.000012  loss: 0.9129 (0.9031)  loss_scale: 65536.0000 (45293.9939)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
Epoch: [7]  [ 990/1349]  eta: 0:01:52  lr: 0.000495  min_lr: 0.000012  loss: 0.9978 (0.9032)  loss_scale: 65536.0000 (45498.2523)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [7]  [1000/1349]  eta: 0:01:48  lr: 0.000495  min_lr: 0.000012  loss: 0.9197 (0.9032)  loss_scale: 65536.0000 (45698.4296)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [7]  [1010/1349]  eta: 0:01:45  lr: 0.000495  min_lr: 0.000012  loss: 0.9197 (0.9033)  loss_scale: 65536.0000 (45894.6469)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
Epoch: [7]  [1020/1349]  eta: 0:01:42  lr: 0.000495  min_lr: 0.000012  loss: 0.9591 (0.9042)  loss_scale: 65536.0000 (46087.0206)  weight_decay: 0.0500 (0.0500)  time: 0.3082  data: 0.0001  max mem: 41808
Epoch: [7]  [1030/1349]  eta: 0:01:39  lr: 0.000495  min_lr: 0.000012  loss: 0.9851 (0.9050)  loss_scale: 65536.0000 (46275.6625)  weight_decay: 0.0500 (0.0500)  time: 0.3078  data: 0.0001  max mem: 41808
Epoch: [7]  [1040/1349]  eta: 0:01:36  lr: 0.000495  min_lr: 0.000012  loss: 0.9240 (0.9040)  loss_scale: 65536.0000 (46460.6801)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
[2025-05-23 16:30:21,729] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 16:30:21,729] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-05-23 16:30:21,729] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 16:30:21,729] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [7]  [1050/1349]  eta: 0:01:33  lr: 0.000495  min_lr: 0.000012  loss: 0.8380 (0.9036)  loss_scale: 65536.0000 (46704.5328)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
[2025-05-23 16:30:24,188] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 10501
[2025-05-23 16:30:24,188] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 16:30:24,188] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 10501
[2025-05-23 16:30:24,188] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 16:30:24,189] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [7]  [1060/1349]  eta: 0:01:30  lr: 0.000495  min_lr: 0.000012  loss: 0.8999 (0.9031)  loss_scale: 65536.0000 (47314.3977)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
Epoch: [7]  [1070/1349]  eta: 0:01:27  lr: 0.000495  min_lr: 0.000012  loss: 0.8569 (0.9026)  loss_scale: 65536.0000 (47484.5341)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
Epoch: [7]  [1080/1349]  eta: 0:01:23  lr: 0.000495  min_lr: 0.000012  loss: 0.8240 (0.9023)  loss_scale: 65536.0000 (47651.5227)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
Epoch: [7]  [1090/1349]  eta: 0:01:20  lr: 0.000495  min_lr: 0.000012  loss: 0.8715 (0.9023)  loss_scale: 65536.0000 (47815.4500)  weight_decay: 0.0500 (0.0500)  time: 0.3064  data: 0.0001  max mem: 41808
Epoch: [7]  [1100/1349]  eta: 0:01:17  lr: 0.000495  min_lr: 0.000012  loss: 0.9248 (0.9031)  loss_scale: 65536.0000 (47976.3996)  weight_decay: 0.0500 (0.0500)  time: 0.3060  data: 0.0001  max mem: 41808
Epoch: [7]  [1110/1349]  eta: 0:01:14  lr: 0.000495  min_lr: 0.000012  loss: 0.9313 (0.9031)  loss_scale: 65536.0000 (48134.4518)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0001  max mem: 41808
Epoch: [7]  [1120/1349]  eta: 0:01:11  lr: 0.000495  min_lr: 0.000012  loss: 0.9696 (0.9039)  loss_scale: 65536.0000 (48289.6842)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [7]  [1130/1349]  eta: 0:01:08  lr: 0.000495  min_lr: 0.000012  loss: 0.9696 (0.9036)  loss_scale: 65536.0000 (48442.1715)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
Epoch: [7]  [1140/1349]  eta: 0:01:05  lr: 0.000495  min_lr: 0.000012  loss: 0.8761 (0.9033)  loss_scale: 65536.0000 (48591.9860)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [7]  [1150/1349]  eta: 0:01:02  lr: 0.000495  min_lr: 0.000012  loss: 0.8805 (0.9031)  loss_scale: 65536.0000 (48739.1972)  weight_decay: 0.0500 (0.0500)  time: 0.3080  data: 0.0001  max mem: 41808
Epoch: [7]  [1160/1349]  eta: 0:00:58  lr: 0.000495  min_lr: 0.000012  loss: 0.8398 (0.9024)  loss_scale: 65536.0000 (48883.8725)  weight_decay: 0.0500 (0.0500)  time: 0.3082  data: 0.0001  max mem: 41808
Epoch: [7]  [1170/1349]  eta: 0:00:55  lr: 0.000495  min_lr: 0.000012  loss: 0.8653 (0.9026)  loss_scale: 65536.0000 (49026.0769)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0002  max mem: 41808
Epoch: [7]  [1180/1349]  eta: 0:00:52  lr: 0.000495  min_lr: 0.000012  loss: 0.8653 (0.9020)  loss_scale: 65536.0000 (49165.8730)  weight_decay: 0.0500 (0.0500)  time: 0.3084  data: 0.0002  max mem: 41808
[2025-05-23 16:31:03,842] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 16:31:03,842] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-05-23 16:31:03,842] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 16:31:03,842] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [7]  [1190/1349]  eta: 0:00:49  lr: 0.000495  min_lr: 0.000012  loss: 0.8012 (0.9018)  loss_scale: 65536.0000 (49523.4257)  weight_decay: 0.0500 (0.0500)  time: 0.3088  data: 0.0001  max mem: 41808
[2025-05-23 16:31:06,316] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 10638
[2025-05-23 16:31:06,316] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 16:31:06,316] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
[2025-05-23 16:31:06,316] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 10638
[2025-05-23 16:31:06,316] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
Epoch: [7]  [1200/1349]  eta: 0:00:46  lr: 0.000495  min_lr: 0.000012  loss: 0.8766 (0.9015)  loss_scale: 65536.0000 (49875.0241)  weight_decay: 0.0500 (0.0500)  time: 0.3082  data: 0.0001  max mem: 41808
Epoch: [7]  [1210/1349]  eta: 0:00:43  lr: 0.000495  min_lr: 0.000012  loss: 0.9764 (0.9020)  loss_scale: 65536.0000 (50004.3468)  weight_decay: 0.0500 (0.0500)  time: 0.3080  data: 0.0001  max mem: 41808
Epoch: [7]  [1220/1349]  eta: 0:00:40  lr: 0.000495  min_lr: 0.000012  loss: 0.8625 (0.9012)  loss_scale: 65536.0000 (50131.5512)  weight_decay: 0.0500 (0.0500)  time: 0.3100  data: 0.0001  max mem: 41808
Epoch: [7]  [1230/1349]  eta: 0:00:37  lr: 0.000495  min_lr: 0.000012  loss: 0.8236 (0.9009)  loss_scale: 65536.0000 (50256.6889)  weight_decay: 0.0500 (0.0500)  time: 0.3095  data: 0.0001  max mem: 41808
Epoch: [7]  [1240/1349]  eta: 0:00:33  lr: 0.000495  min_lr: 0.000012  loss: 0.8644 (0.9011)  loss_scale: 65536.0000 (50379.8098)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
Epoch: [7]  [1250/1349]  eta: 0:00:30  lr: 0.000495  min_lr: 0.000012  loss: 0.9825 (0.9019)  loss_scale: 65536.0000 (50500.9624)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [7]  [1260/1349]  eta: 0:00:27  lr: 0.000495  min_lr: 0.000012  loss: 0.9405 (0.9020)  loss_scale: 65536.0000 (50620.1935)  weight_decay: 0.0500 (0.0500)  time: 0.3066  data: 0.0001  max mem: 41808
Epoch: [7]  [1270/1349]  eta: 0:00:24  lr: 0.000495  min_lr: 0.000012  loss: 0.8957 (0.9017)  loss_scale: 65536.0000 (50737.5484)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
Epoch: [7]  [1280/1349]  eta: 0:00:21  lr: 0.000495  min_lr: 0.000012  loss: 0.8957 (0.9017)  loss_scale: 65536.0000 (50853.0710)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [7]  [1290/1349]  eta: 0:00:18  lr: 0.000495  min_lr: 0.000012  loss: 0.9775 (0.9022)  loss_scale: 65536.0000 (50966.8040)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [7]  [1300/1349]  eta: 0:00:15  lr: 0.000495  min_lr: 0.000012  loss: 0.9766 (0.9026)  loss_scale: 65536.0000 (51078.7886)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [7]  [1310/1349]  eta: 0:00:12  lr: 0.000495  min_lr: 0.000012  loss: 0.9492 (0.9027)  loss_scale: 65536.0000 (51189.0648)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [7]  [1320/1349]  eta: 0:00:09  lr: 0.000495  min_lr: 0.000012  loss: 0.9010 (0.9021)  loss_scale: 65536.0000 (51297.6715)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
[2025-05-23 16:31:45,984] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 16:31:45,984] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-05-23 16:31:45,984] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 16:31:45,984] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [7]  [1330/1349]  eta: 0:00:05  lr: 0.000495  min_lr: 0.000012  loss: 0.9010 (0.9021)  loss_scale: 65536.0000 (51749.3133)  weight_decay: 0.0500 (0.0500)  time: 0.3045  data: 0.0001  max mem: 41808
[2025-05-23 16:31:49,300] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 10778
[2025-05-23 16:31:49,300] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 16:31:49,300] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 10778
[2025-05-23 16:31:49,300] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 16:31:49,300] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [7]  [1340/1349]  eta: 0:00:02  lr: 0.000495  min_lr: 0.000012  loss: 0.9321 (0.9014)  loss_scale: 131072.0000 (52047.6063)  weight_decay: 0.0500 (0.0500)  time: 0.3022  data: 0.0001  max mem: 41808
Epoch: [7]  [1348/1349]  eta: 0:00:00  lr: 0.000495  min_lr: 0.000012  loss: 0.9301 (0.9015)  loss_scale: 65536.0000 (52127.5967)  weight_decay: 0.0500 (0.0500)  time: 0.3021  data: 0.0001  max mem: 41808
Epoch: [7] Total time: 0:06:59 (0.3112 s / it)
Averaged stats: lr: 0.000495  min_lr: 0.000012  loss: 0.9301 (0.8972)  loss_scale: 65536.0000 (52127.5967)  weight_decay: 0.0500 (0.0500)  total_time: 419.8753 (419.8685)
Val:  [  0/346]  eta: 1:15:25  loss: 2.2904 (2.2904)  acc1: 12.5000 (12.5000)  acc5: 93.7500 (93.7500)  time: 13.0788  data: 12.0593  max mem: 41808
Val:  [ 10/346]  eta: 0:11:43  loss: 0.1349 (0.5051)  acc1: 100.0000 (84.4460)  acc5: 100.0000 (99.3608)  time: 2.0952  data: 1.2978  max mem: 41808
Val:  [ 20/346]  eta: 0:08:22  loss: 0.1303 (0.3746)  acc1: 100.0000 (89.2857)  acc5: 100.0000 (99.6652)  time: 0.9644  data: 0.2069  max mem: 41808
Val:  [ 30/346]  eta: 0:06:48  loss: 0.0935 (0.3233)  acc1: 100.0000 (91.4567)  acc5: 100.0000 (99.7732)  time: 0.8498  data: 0.0962  max mem: 41808
Val:  [ 40/346]  eta: 0:05:54  loss: 0.0895 (0.3790)  acc1: 100.0000 (89.9009)  acc5: 100.0000 (98.4375)  time: 0.7541  data: 0.0003  max mem: 41808
Val:  [ 50/346]  eta: 0:05:21  loss: 0.1058 (0.3387)  acc1: 99.2188 (91.3603)  acc5: 100.0000 (98.7132)  time: 0.7692  data: 0.0003  max mem: 41808
Val:  [ 60/346]  eta: 0:04:57  loss: 0.1070 (0.3292)  acc1: 99.2188 (91.5215)  acc5: 100.0000 (98.9242)  time: 0.8004  data: 0.0003  max mem: 41808
Val:  [ 70/346]  eta: 0:04:38  loss: 0.2315 (0.3482)  acc1: 92.1875 (90.7680)  acc5: 100.0000 (99.0757)  time: 0.8123  data: 0.0320  max mem: 41808
Val:  [ 80/346]  eta: 0:04:23  loss: 0.2315 (0.3414)  acc1: 92.9688 (91.1458)  acc5: 100.0000 (99.0741)  time: 0.8425  data: 0.1052  max mem: 41808
Val:  [ 90/346]  eta: 0:04:11  loss: 0.2368 (0.3442)  acc1: 95.3125 (91.0714)  acc5: 100.0000 (99.1672)  time: 0.8922  data: 0.1453  max mem: 41808
Val:  [100/346]  eta: 0:03:59  loss: 0.1744 (0.3240)  acc1: 97.6562 (91.8858)  acc5: 100.0000 (99.2497)  time: 0.9016  data: 0.1423  max mem: 41808
Val:  [110/346]  eta: 0:03:48  loss: 0.1444 (0.3384)  acc1: 98.4375 (91.4133)  acc5: 100.0000 (99.2821)  time: 0.8930  data: 0.1467  max mem: 41808
Val:  [120/346]  eta: 0:03:37  loss: 0.1515 (0.3378)  acc1: 97.6562 (91.3546)  acc5: 100.0000 (99.3414)  time: 0.9116  data: 0.1491  max mem: 41808
Val:  [130/346]  eta: 0:03:26  loss: 0.1190 (0.3498)  acc1: 99.2188 (90.9470)  acc5: 100.0000 (99.2844)  time: 0.8989  data: 0.1417  max mem: 41808
Val:  [140/346]  eta: 0:03:15  loss: 0.1190 (0.3386)  acc1: 99.2188 (91.3287)  acc5: 100.0000 (99.3351)  time: 0.8707  data: 0.1371  max mem: 41808
Val:  [150/346]  eta: 0:03:05  loss: 0.1979 (0.3371)  acc1: 96.8750 (91.4114)  acc5: 100.0000 (99.3791)  time: 0.8745  data: 0.1464  max mem: 41808
Val:  [160/346]  eta: 0:02:55  loss: 0.3071 (0.3365)  acc1: 91.4062 (91.3820)  acc5: 100.0000 (99.4080)  time: 0.8758  data: 0.1473  max mem: 41808
Val:  [170/346]  eta: 0:02:45  loss: 0.3269 (0.3481)  acc1: 85.9375 (90.7027)  acc5: 100.0000 (99.4426)  time: 0.8962  data: 0.1444  max mem: 41808
Val:  [180/346]  eta: 0:02:35  loss: 0.3529 (0.3674)  acc1: 85.9375 (89.7272)  acc5: 100.0000 (99.4734)  time: 0.9232  data: 0.1533  max mem: 41808
Val:  [190/346]  eta: 0:02:26  loss: 0.2431 (0.3623)  acc1: 92.9688 (89.9460)  acc5: 100.0000 (99.4887)  time: 0.9068  data: 0.1572  max mem: 41808
Val:  [200/346]  eta: 0:02:16  loss: 0.3530 (0.3860)  acc1: 87.5000 (89.0120)  acc5: 100.0000 (99.5064)  time: 0.9071  data: 0.1424  max mem: 41808
Val:  [210/346]  eta: 0:02:07  loss: 0.1473 (0.3786)  acc1: 86.7188 (89.3217)  acc5: 100.0000 (99.5298)  time: 0.9170  data: 0.1374  max mem: 41808
Val:  [220/346]  eta: 0:01:57  loss: 0.1473 (0.3752)  acc1: 98.4375 (89.4089)  acc5: 100.0000 (99.5369)  time: 0.8849  data: 0.1526  max mem: 41808
Val:  [230/346]  eta: 0:01:47  loss: 0.1526 (0.3651)  acc1: 98.4375 (89.7998)  acc5: 100.0000 (99.5570)  time: 0.8440  data: 0.1514  max mem: 41808
Val:  [240/346]  eta: 0:01:38  loss: 0.1559 (0.3760)  acc1: 97.6562 (89.4774)  acc5: 100.0000 (99.3841)  time: 0.8629  data: 0.1460  max mem: 41808
Val:  [250/346]  eta: 0:01:28  loss: 0.1788 (0.3714)  acc1: 94.5312 (89.6228)  acc5: 100.0000 (99.4086)  time: 0.9024  data: 0.1522  max mem: 41808
Val:  [260/346]  eta: 0:01:19  loss: 0.1287 (0.3698)  acc1: 98.4375 (89.6252)  acc5: 100.0000 (99.4313)  time: 0.9106  data: 0.1590  max mem: 41808
Val:  [270/346]  eta: 0:01:10  loss: 0.1287 (0.3671)  acc1: 96.8750 (89.7515)  acc5: 100.0000 (99.4436)  time: 0.9179  data: 0.1493  max mem: 41808
Val:  [280/346]  eta: 0:01:01  loss: 0.1533 (0.3641)  acc1: 96.8750 (89.8827)  acc5: 100.0000 (99.4412)  time: 0.9245  data: 0.1521  max mem: 41808
Val:  [290/346]  eta: 0:00:51  loss: 0.0899 (0.3563)  acc1: 100.0000 (90.1659)  acc5: 100.0000 (99.4604)  time: 0.9383  data: 0.1574  max mem: 41808
Val:  [300/346]  eta: 0:00:42  loss: 0.0964 (0.3563)  acc1: 100.0000 (90.2487)  acc5: 100.0000 (99.4082)  time: 0.9215  data: 0.1514  max mem: 41808
Val:  [310/346]  eta: 0:00:33  loss: 0.1122 (0.3554)  acc1: 99.2188 (90.2758)  acc5: 100.0000 (99.4273)  time: 0.8761  data: 0.1459  max mem: 41808
Val:  [320/346]  eta: 0:00:23  loss: 0.1200 (0.3530)  acc1: 96.8750 (90.2964)  acc5: 100.0000 (99.4451)  time: 0.8815  data: 0.1413  max mem: 41808
Val:  [330/346]  eta: 0:00:14  loss: 0.2989 (0.3589)  acc1: 88.2812 (90.0939)  acc5: 100.0000 (99.4194)  time: 0.8873  data: 0.1305  max mem: 41808
Val:  [340/346]  eta: 0:00:05  loss: 0.4081 (0.3668)  acc1: 87.5000 (89.9216)  acc5: 100.0000 (99.4341)  time: 0.8924  data: 0.1444  max mem: 41808
Val:  [345/346]  eta: 0:00:00  loss: 0.1538 (0.3633)  acc1: 96.8750 (90.0400)  acc5: 100.0000 (99.4418)  time: 0.8849  data: 0.1601  max mem: 41808
Val: Total time: 0:05:18 (0.9204 s / it)
* Acc@1 90.055 Acc@5 99.481 loss 0.362
Accuracy of the network on the 88494 val videos: 90.1%
Max accuracy: 91.25%   Max Epoch: 3
Epoch: [8]  [   0/1349]  eta: 1:45:13  lr: 0.000495  min_lr: 0.000012  loss: 1.0079 (1.0079)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 4.6805  data: 3.3888  max mem: 41808
Epoch: [8]  [  10/1349]  eta: 0:16:02  lr: 0.000495  min_lr: 0.000012  loss: 0.8108 (0.8590)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7190  data: 0.3119  max mem: 41808
Epoch: [8]  [  20/1349]  eta: 0:11:38  lr: 0.000494  min_lr: 0.000012  loss: 0.8108 (0.8185)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3181  data: 0.0022  max mem: 41808
Epoch: [8]  [  30/1349]  eta: 0:10:00  lr: 0.000494  min_lr: 0.000012  loss: 0.8909 (0.8498)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3103  data: 0.0001  max mem: 41808
Epoch: [8]  [  40/1349]  eta: 0:09:09  lr: 0.000494  min_lr: 0.000012  loss: 0.9603 (0.8719)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3085  data: 0.0001  max mem: 41808
Epoch: [8]  [  50/1349]  eta: 0:08:36  lr: 0.000494  min_lr: 0.000012  loss: 0.9631 (0.8882)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3088  data: 0.0002  max mem: 41808
Epoch: [8]  [  60/1349]  eta: 0:08:13  lr: 0.000494  min_lr: 0.000012  loss: 0.9257 (0.8839)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
Epoch: [8]  [  70/1349]  eta: 0:07:56  lr: 0.000494  min_lr: 0.000012  loss: 0.9006 (0.8893)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3089  data: 0.0002  max mem: 41808
Epoch: [8]  [  80/1349]  eta: 0:07:42  lr: 0.000494  min_lr: 0.000012  loss: 0.9006 (0.8859)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3093  data: 0.0002  max mem: 41808
Epoch: [8]  [  90/1349]  eta: 0:07:31  lr: 0.000494  min_lr: 0.000012  loss: 0.7573 (0.8733)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3092  data: 0.0001  max mem: 41808
Epoch: [8]  [ 100/1349]  eta: 0:07:21  lr: 0.000494  min_lr: 0.000012  loss: 0.8479 (0.8795)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3089  data: 0.0001  max mem: 41808
Epoch: [8]  [ 110/1349]  eta: 0:07:12  lr: 0.000494  min_lr: 0.000012  loss: 0.9052 (0.8733)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3066  data: 0.0001  max mem: 41808
[2025-05-23 16:37:52,356] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 16:37:52,356] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-05-23 16:37:52,356] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 16:37:52,356] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-05-23 16:37:53,894] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 10912
[2025-05-23 16:37:53,894] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 10912
[2025-05-23 16:37:53,894] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 16:37:53,894] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 16:37:53,894] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [8]  [ 120/1349]  eta: 0:07:05  lr: 0.000494  min_lr: 0.000012  loss: 0.9107 (0.8785)  loss_scale: 65536.0000 (68244.0992)  weight_decay: 0.0500 (0.0500)  time: 0.3060  data: 0.0001  max mem: 41808
Epoch: [8]  [ 130/1349]  eta: 0:06:57  lr: 0.000494  min_lr: 0.000012  loss: 0.9371 (0.8754)  loss_scale: 65536.0000 (68037.3740)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
Epoch: [8]  [ 140/1349]  eta: 0:06:51  lr: 0.000494  min_lr: 0.000012  loss: 0.8083 (0.8757)  loss_scale: 65536.0000 (67859.9716)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [8]  [ 150/1349]  eta: 0:06:45  lr: 0.000494  min_lr: 0.000012  loss: 0.8363 (0.8728)  loss_scale: 65536.0000 (67706.0662)  weight_decay: 0.0500 (0.0500)  time: 0.3081  data: 0.0001  max mem: 41808
Epoch: [8]  [ 160/1349]  eta: 0:06:40  lr: 0.000494  min_lr: 0.000012  loss: 0.8752 (0.8748)  loss_scale: 65536.0000 (67571.2795)  weight_decay: 0.0500 (0.0500)  time: 0.3089  data: 0.0002  max mem: 41808
Epoch: [8]  [ 170/1349]  eta: 0:06:34  lr: 0.000494  min_lr: 0.000012  loss: 0.8003 (0.8674)  loss_scale: 65536.0000 (67452.2573)  weight_decay: 0.0500 (0.0500)  time: 0.3080  data: 0.0001  max mem: 41808
Epoch: [8]  [ 180/1349]  eta: 0:06:29  lr: 0.000494  min_lr: 0.000012  loss: 0.7827 (0.8678)  loss_scale: 65536.0000 (67346.3867)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0002  max mem: 41808
Epoch: [8]  [ 190/1349]  eta: 0:06:24  lr: 0.000494  min_lr: 0.000012  loss: 0.9342 (0.8698)  loss_scale: 65536.0000 (67251.6021)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0002  max mem: 41808
Epoch: [8]  [ 200/1349]  eta: 0:06:19  lr: 0.000494  min_lr: 0.000012  loss: 0.9304 (0.8724)  loss_scale: 65536.0000 (67166.2488)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
[2025-05-23 16:38:20,718] [INFO] [logging.py:96:log_dist] [Rank 0] step=11000, skipped=64, lr=[1.1735568793810647e-05, 1.1735568793810647e-05, 1.5647425058414198e-05, 1.5647425058414198e-05, 2.0863233411218928e-05, 2.0863233411218928e-05, 2.7817644548291904e-05, 2.7817644548291904e-05, 3.709019273105587e-05, 3.709019273105587e-05, 4.94535903080745e-05, 4.94535903080745e-05, 6.5938120410766e-05, 6.5938120410766e-05, 8.791749388102133e-05, 8.791749388102133e-05, 0.0001172233251746951, 0.0001172233251746951, 0.00015629776689959348, 0.00015629776689959348, 0.00020839702253279128, 0.00020839702253279128, 0.0002778626967103884, 0.0002778626967103884, 0.0003704835956138512, 0.0003704835956138512, 0.0004939781274851349, 0.0004939781274851349], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-05-23 16:38:20,719] [INFO] [timer.py:260:stop] epoch=0/micro_step=11000/global_step=11000, RunningAvgSamplesPerSec=212.78780418833713, CurrSamplesPerSec=212.5866140391586, MemAllocated=1.37GB, MaxMemAllocated=40.83GB
Epoch: [8]  [ 210/1349]  eta: 0:06:15  lr: 0.000494  min_lr: 0.000012  loss: 0.9304 (0.8750)  loss_scale: 65536.0000 (67088.9858)  weight_decay: 0.0500 (0.0500)  time: 0.3097  data: 0.0002  max mem: 41808
Epoch: [8]  [ 220/1349]  eta: 0:06:11  lr: 0.000494  min_lr: 0.000012  loss: 0.9534 (0.8779)  loss_scale: 65536.0000 (67018.7149)  weight_decay: 0.0500 (0.0500)  time: 0.3100  data: 0.0002  max mem: 41808
Epoch: [8]  [ 230/1349]  eta: 0:06:06  lr: 0.000494  min_lr: 0.000012  loss: 0.9534 (0.8794)  loss_scale: 65536.0000 (66954.5281)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [8]  [ 240/1349]  eta: 0:06:02  lr: 0.000494  min_lr: 0.000012  loss: 0.8867 (0.8799)  loss_scale: 65536.0000 (66895.6680)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
[2025-05-23 16:38:33,629] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 16:38:33,629] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 16:38:33,629] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-05-23 16:38:33,629] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [8]  [ 250/1349]  eta: 0:05:58  lr: 0.000494  min_lr: 0.000012  loss: 0.8812 (0.8781)  loss_scale: 65536.0000 (67363.6972)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
Epoch: [8]  [ 260/1349]  eta: 0:05:54  lr: 0.000494  min_lr: 0.000012  loss: 0.8344 (0.8745)  loss_scale: 131072.0000 (69804.6284)  weight_decay: 0.0500 (0.0500)  time: 0.3078  data: 0.0002  max mem: 41808
[2025-05-23 16:38:37,936] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 11055
[2025-05-23 16:38:37,936] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 11055
[2025-05-23 16:38:37,937] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 16:38:37,937] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 16:38:37,937] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [8]  [ 270/1349]  eta: 0:05:50  lr: 0.000494  min_lr: 0.000012  loss: 0.8297 (0.8728)  loss_scale: 131072.0000 (70130.7749)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0002  max mem: 41808
Epoch: [8]  [ 280/1349]  eta: 0:05:46  lr: 0.000494  min_lr: 0.000012  loss: 0.8758 (0.8758)  loss_scale: 65536.0000 (69967.2598)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0002  max mem: 41808
Epoch: [8]  [ 290/1349]  eta: 0:05:42  lr: 0.000494  min_lr: 0.000012  loss: 0.9282 (0.8752)  loss_scale: 65536.0000 (69814.9828)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0002  max mem: 41808
Epoch: [8]  [ 300/1349]  eta: 0:05:38  lr: 0.000494  min_lr: 0.000012  loss: 0.9421 (0.8753)  loss_scale: 65536.0000 (69672.8239)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0002  max mem: 41808
Epoch: [8]  [ 310/1349]  eta: 0:05:35  lr: 0.000494  min_lr: 0.000012  loss: 0.8455 (0.8737)  loss_scale: 65536.0000 (69539.8071)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
Epoch: [8]  [ 320/1349]  eta: 0:05:31  lr: 0.000494  min_lr: 0.000012  loss: 0.9617 (0.8779)  loss_scale: 65536.0000 (69415.0779)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [8]  [ 330/1349]  eta: 0:05:27  lr: 0.000494  min_lr: 0.000012  loss: 0.9777 (0.8781)  loss_scale: 65536.0000 (69297.8852)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [8]  [ 340/1349]  eta: 0:05:24  lr: 0.000494  min_lr: 0.000012  loss: 0.9083 (0.8776)  loss_scale: 65536.0000 (69187.5660)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [8]  [ 350/1349]  eta: 0:05:20  lr: 0.000494  min_lr: 0.000012  loss: 0.8962 (0.8769)  loss_scale: 65536.0000 (69083.5328)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [8]  [ 360/1349]  eta: 0:05:16  lr: 0.000494  min_lr: 0.000012  loss: 0.9455 (0.8784)  loss_scale: 65536.0000 (68985.2632)  weight_decay: 0.0500 (0.0500)  time: 0.3083  data: 0.0001  max mem: 41808
Epoch: [8]  [ 370/1349]  eta: 0:05:13  lr: 0.000494  min_lr: 0.000012  loss: 0.9621 (0.8786)  loss_scale: 65536.0000 (68892.2911)  weight_decay: 0.0500 (0.0500)  time: 0.3092  data: 0.0001  max mem: 41808
Epoch: [8]  [ 380/1349]  eta: 0:05:09  lr: 0.000493  min_lr: 0.000012  loss: 0.9614 (0.8799)  loss_scale: 65536.0000 (68804.1995)  weight_decay: 0.0500 (0.0500)  time: 0.3080  data: 0.0001  max mem: 41808
Epoch: [8]  [ 390/1349]  eta: 0:05:06  lr: 0.000493  min_lr: 0.000012  loss: 0.8082 (0.8757)  loss_scale: 65536.0000 (68720.6138)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
[2025-05-23 16:39:17,625] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 16:39:17,625] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-05-23 16:39:17,625] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 16:39:17,625] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [8]  [ 400/1349]  eta: 0:05:02  lr: 0.000493  min_lr: 0.000012  loss: 0.7991 (0.8768)  loss_scale: 65536.0000 (70112.0798)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
Epoch: [8]  [ 410/1349]  eta: 0:04:59  lr: 0.000493  min_lr: 0.000012  loss: 0.8874 (0.8752)  loss_scale: 131072.0000 (71595.2895)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [8]  [ 420/1349]  eta: 0:04:56  lr: 0.000493  min_lr: 0.000012  loss: 0.8348 (0.8754)  loss_scale: 131072.0000 (73008.0380)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [8]  [ 430/1349]  eta: 0:04:52  lr: 0.000493  min_lr: 0.000012  loss: 0.9571 (0.8777)  loss_scale: 131072.0000 (74355.2297)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
Epoch: [8]  [ 440/1349]  eta: 0:04:49  lr: 0.000493  min_lr: 0.000012  loss: 0.9151 (0.8763)  loss_scale: 131072.0000 (75641.3243)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
[2025-05-23 16:39:32,970] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 11234
[2025-05-23 16:39:32,970] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 11234
[2025-05-23 16:39:32,970] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 16:39:32,970] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 16:39:32,970] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [8]  [ 450/1349]  eta: 0:04:45  lr: 0.000493  min_lr: 0.000012  loss: 0.8357 (0.8770)  loss_scale: 131072.0000 (75562.5721)  weight_decay: 0.0500 (0.0500)  time: 0.3062  data: 0.0001  max mem: 41808
Epoch: [8]  [ 460/1349]  eta: 0:04:42  lr: 0.000493  min_lr: 0.000012  loss: 0.9090 (0.8778)  loss_scale: 65536.0000 (75345.0759)  weight_decay: 0.0500 (0.0500)  time: 0.3079  data: 0.0002  max mem: 41808
Epoch: [8]  [ 470/1349]  eta: 0:04:39  lr: 0.000493  min_lr: 0.000012  loss: 0.9086 (0.8766)  loss_scale: 65536.0000 (75136.8153)  weight_decay: 0.0500 (0.0500)  time: 0.3083  data: 0.0001  max mem: 41808
Epoch: [8]  [ 480/1349]  eta: 0:04:35  lr: 0.000493  min_lr: 0.000012  loss: 0.8510 (0.8749)  loss_scale: 65536.0000 (74937.2141)  weight_decay: 0.0500 (0.0500)  time: 0.3088  data: 0.0001  max mem: 41808
Epoch: [8]  [ 490/1349]  eta: 0:04:32  lr: 0.000493  min_lr: 0.000012  loss: 0.8661 (0.8747)  loss_scale: 65536.0000 (74745.7434)  weight_decay: 0.0500 (0.0500)  time: 0.3092  data: 0.0001  max mem: 41808
Epoch: [8]  [ 500/1349]  eta: 0:04:29  lr: 0.000493  min_lr: 0.000012  loss: 0.8551 (0.8739)  loss_scale: 65536.0000 (74561.9162)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
Epoch: [8]  [ 510/1349]  eta: 0:04:25  lr: 0.000493  min_lr: 0.000012  loss: 0.8366 (0.8738)  loss_scale: 65536.0000 (74385.2838)  weight_decay: 0.0500 (0.0500)  time: 0.3079  data: 0.0001  max mem: 41808
Epoch: [8]  [ 520/1349]  eta: 0:04:22  lr: 0.000493  min_lr: 0.000012  loss: 0.8725 (0.8742)  loss_scale: 65536.0000 (74215.4319)  weight_decay: 0.0500 (0.0500)  time: 0.3080  data: 0.0001  max mem: 41808
Epoch: [8]  [ 530/1349]  eta: 0:04:19  lr: 0.000493  min_lr: 0.000012  loss: 0.9597 (0.8755)  loss_scale: 65536.0000 (74051.9774)  weight_decay: 0.0500 (0.0500)  time: 0.3079  data: 0.0001  max mem: 41808
Epoch: [8]  [ 540/1349]  eta: 0:04:15  lr: 0.000493  min_lr: 0.000012  loss: 0.9800 (0.8768)  loss_scale: 65536.0000 (73894.5656)  weight_decay: 0.0500 (0.0500)  time: 0.3098  data: 0.0002  max mem: 41808
Epoch: [8]  [ 550/1349]  eta: 0:04:12  lr: 0.000493  min_lr: 0.000012  loss: 0.9522 (0.8769)  loss_scale: 65536.0000 (73742.8675)  weight_decay: 0.0500 (0.0500)  time: 0.3108  data: 0.0001  max mem: 41808
Epoch: [8]  [ 560/1349]  eta: 0:04:09  lr: 0.000493  min_lr: 0.000012  loss: 0.8742 (0.8773)  loss_scale: 65536.0000 (73596.5775)  weight_decay: 0.0500 (0.0500)  time: 0.3090  data: 0.0001  max mem: 41808
Epoch: [8]  [ 570/1349]  eta: 0:04:06  lr: 0.000493  min_lr: 0.000012  loss: 0.8742 (0.8776)  loss_scale: 65536.0000 (73455.4116)  weight_decay: 0.0500 (0.0500)  time: 0.3117  data: 0.0001  max mem: 41808
[2025-05-23 16:40:12,861] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 16:40:12,861] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-05-23 16:40:12,861] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 16:40:12,861] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-05-23 16:40:13,780] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 11366
[2025-05-23 16:40:13,780] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 16:40:13,780] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 11366
[2025-05-23 16:40:13,780] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 16:40:13,780] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [8]  [ 580/1349]  eta: 0:04:02  lr: 0.000493  min_lr: 0.000012  loss: 0.9567 (0.8778)  loss_scale: 65536.0000 (73657.5009)  weight_decay: 0.0500 (0.0500)  time: 0.3118  data: 0.0001  max mem: 41808
Epoch: [8]  [ 590/1349]  eta: 0:03:59  lr: 0.000493  min_lr: 0.000012  loss: 0.8867 (0.8779)  loss_scale: 65536.0000 (73520.0812)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0001  max mem: 41808
Epoch: [8]  [ 600/1349]  eta: 0:03:56  lr: 0.000493  min_lr: 0.000012  loss: 0.8867 (0.8781)  loss_scale: 65536.0000 (73387.2346)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [8]  [ 610/1349]  eta: 0:03:53  lr: 0.000493  min_lr: 0.000012  loss: 0.8559 (0.8771)  loss_scale: 65536.0000 (73258.7365)  weight_decay: 0.0500 (0.0500)  time: 0.3078  data: 0.0001  max mem: 41808
Epoch: [8]  [ 620/1349]  eta: 0:03:49  lr: 0.000493  min_lr: 0.000012  loss: 0.8559 (0.8762)  loss_scale: 65536.0000 (73134.3768)  weight_decay: 0.0500 (0.0500)  time: 0.3085  data: 0.0001  max mem: 41808
Epoch: [8]  [ 630/1349]  eta: 0:03:46  lr: 0.000493  min_lr: 0.000012  loss: 0.7527 (0.8746)  loss_scale: 65536.0000 (73013.9588)  weight_decay: 0.0500 (0.0500)  time: 0.3081  data: 0.0001  max mem: 41808
Epoch: [8]  [ 640/1349]  eta: 0:03:43  lr: 0.000493  min_lr: 0.000012  loss: 0.8129 (0.8736)  loss_scale: 65536.0000 (72897.2980)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
Epoch: [8]  [ 650/1349]  eta: 0:03:40  lr: 0.000493  min_lr: 0.000012  loss: 0.8617 (0.8730)  loss_scale: 65536.0000 (72784.2212)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [8]  [ 660/1349]  eta: 0:03:36  lr: 0.000493  min_lr: 0.000012  loss: 0.8846 (0.8736)  loss_scale: 65536.0000 (72674.5658)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [8]  [ 670/1349]  eta: 0:03:33  lr: 0.000493  min_lr: 0.000012  loss: 0.9668 (0.8744)  loss_scale: 65536.0000 (72568.1788)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0002  max mem: 41808
Epoch: [8]  [ 680/1349]  eta: 0:03:30  lr: 0.000493  min_lr: 0.000012  loss: 0.9015 (0.8741)  loss_scale: 65536.0000 (72464.9163)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0002  max mem: 41808
Epoch: [8]  [ 690/1349]  eta: 0:03:27  lr: 0.000493  min_lr: 0.000012  loss: 0.8759 (0.8751)  loss_scale: 65536.0000 (72364.6425)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0002  max mem: 41808
Epoch: [8]  [ 700/1349]  eta: 0:03:24  lr: 0.000493  min_lr: 0.000012  loss: 0.9954 (0.8769)  loss_scale: 65536.0000 (72267.2297)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0002  max mem: 41808
[2025-05-23 16:40:53,458] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 16:40:53,458] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-05-23 16:40:53,458] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 16:40:53,458] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [8]  [ 710/1349]  eta: 0:03:20  lr: 0.000492  min_lr: 0.000012  loss: 0.9875 (0.8775)  loss_scale: 65536.0000 (72909.9522)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
Epoch: [8]  [ 720/1349]  eta: 0:03:17  lr: 0.000492  min_lr: 0.000012  loss: 0.9588 (0.8773)  loss_scale: 131072.0000 (73716.6380)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0001  max mem: 41808
Epoch: [8]  [ 730/1349]  eta: 0:03:14  lr: 0.000492  min_lr: 0.000012  loss: 0.8482 (0.8767)  loss_scale: 131072.0000 (74501.2531)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0002  max mem: 41808
Epoch: [8]  [ 740/1349]  eta: 0:03:11  lr: 0.000492  min_lr: 0.000012  loss: 0.9195 (0.8779)  loss_scale: 131072.0000 (75264.6910)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
[2025-05-23 16:41:05,757] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 11535
[2025-05-23 16:41:05,757] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 11535
[2025-05-23 16:41:05,757] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 16:41:05,757] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 16:41:05,757] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [8]  [ 750/1349]  eta: 0:03:08  lr: 0.000492  min_lr: 0.000012  loss: 0.9554 (0.8781)  loss_scale: 131072.0000 (75309.6778)  weight_decay: 0.0500 (0.0500)  time: 0.3134  data: 0.0001  max mem: 41808
Epoch: [8]  [ 760/1349]  eta: 0:03:04  lr: 0.000492  min_lr: 0.000012  loss: 0.9527 (0.8792)  loss_scale: 65536.0000 (75181.2457)  weight_decay: 0.0500 (0.0500)  time: 0.3144  data: 0.0002  max mem: 41808
Epoch: [8]  [ 770/1349]  eta: 0:03:01  lr: 0.000492  min_lr: 0.000012  loss: 0.8882 (0.8787)  loss_scale: 65536.0000 (75056.1453)  weight_decay: 0.0500 (0.0500)  time: 0.3089  data: 0.0002  max mem: 41808
Epoch: [8]  [ 780/1349]  eta: 0:02:58  lr: 0.000492  min_lr: 0.000012  loss: 0.8344 (0.8770)  loss_scale: 65536.0000 (74934.2484)  weight_decay: 0.0500 (0.0500)  time: 0.3127  data: 0.0001  max mem: 41808
Epoch: [8]  [ 790/1349]  eta: 0:02:55  lr: 0.000492  min_lr: 0.000012  loss: 0.8620 (0.8771)  loss_scale: 65536.0000 (74815.4336)  weight_decay: 0.0500 (0.0500)  time: 0.3310  data: 0.0008  max mem: 41808
Epoch: [8]  [ 800/1349]  eta: 0:02:53  lr: 0.000492  min_lr: 0.000012  loss: 0.8993 (0.8769)  loss_scale: 65536.0000 (74699.5855)  weight_decay: 0.0500 (0.0500)  time: 0.3742  data: 0.0010  max mem: 41808
Epoch: [8]  [ 810/1349]  eta: 0:02:51  lr: 0.000492  min_lr: 0.000012  loss: 0.7790 (0.8754)  loss_scale: 65536.0000 (74586.5943)  weight_decay: 0.0500 (0.0500)  time: 0.4392  data: 0.0004  max mem: 41808
Epoch: [8]  [ 820/1349]  eta: 0:02:49  lr: 0.000492  min_lr: 0.000012  loss: 0.8108 (0.8750)  loss_scale: 65536.0000 (74476.3557)  weight_decay: 0.0500 (0.0500)  time: 0.5174  data: 0.0001  max mem: 41808
Epoch: [8]  [ 830/1349]  eta: 0:02:47  lr: 0.000492  min_lr: 0.000012  loss: 0.8483 (0.8742)  loss_scale: 65536.0000 (74368.7702)  weight_decay: 0.0500 (0.0500)  time: 0.5571  data: 0.0002  max mem: 41808
Epoch: [8]  [ 840/1349]  eta: 0:02:46  lr: 0.000492  min_lr: 0.000012  loss: 0.8410 (0.8734)  loss_scale: 65536.0000 (74263.7432)  weight_decay: 0.0500 (0.0500)  time: 0.5897  data: 0.0002  max mem: 41808
Epoch: [8]  [ 850/1349]  eta: 0:02:45  lr: 0.000492  min_lr: 0.000012  loss: 0.8515 (0.8738)  loss_scale: 65536.0000 (74161.1845)  weight_decay: 0.0500 (0.0500)  time: 0.6423  data: 0.0001  max mem: 41808
Epoch: [8]  [ 860/1349]  eta: 0:02:43  lr: 0.000492  min_lr: 0.000012  loss: 0.9536 (0.8744)  loss_scale: 65536.0000 (74061.0081)  weight_decay: 0.0500 (0.0500)  time: 0.6609  data: 0.0001  max mem: 41808
Epoch: [8]  [ 870/1349]  eta: 0:02:42  lr: 0.000492  min_lr: 0.000012  loss: 0.8977 (0.8747)  loss_scale: 65536.0000 (73963.1320)  weight_decay: 0.0500 (0.0500)  time: 0.6617  data: 0.0001  max mem: 41808
[2025-05-23 16:42:08,182] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 16:42:08,182] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-05-23 16:42:08,194] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 16:42:08,194] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [8]  [ 880/1349]  eta: 0:02:40  lr: 0.000492  min_lr: 0.000012  loss: 0.8881 (0.8753)  loss_scale: 65536.0000 (74536.9716)  weight_decay: 0.0500 (0.0500)  time: 0.6626  data: 0.0001  max mem: 41808
Epoch: [8]  [ 890/1349]  eta: 0:02:38  lr: 0.000492  min_lr: 0.000012  loss: 0.8926 (0.8763)  loss_scale: 131072.0000 (75171.4837)  weight_decay: 0.0500 (0.0500)  time: 0.6622  data: 0.0001  max mem: 41808
Epoch: [8]  [ 900/1349]  eta: 0:02:36  lr: 0.000492  min_lr: 0.000012  loss: 0.9338 (0.8761)  loss_scale: 131072.0000 (75791.9112)  weight_decay: 0.0500 (0.0500)  time: 0.6613  data: 0.0001  max mem: 41808
Epoch: [8]  [ 910/1349]  eta: 0:02:34  lr: 0.000492  min_lr: 0.000012  loss: 0.9402 (0.8768)  loss_scale: 131072.0000 (76398.7179)  weight_decay: 0.0500 (0.0500)  time: 0.6612  data: 0.0001  max mem: 41808
Epoch: [8]  [ 920/1349]  eta: 0:02:32  lr: 0.000492  min_lr: 0.000012  loss: 0.9402 (0.8763)  loss_scale: 131072.0000 (76992.3474)  weight_decay: 0.0500 (0.0500)  time: 0.6608  data: 0.0001  max mem: 41808
Epoch: [8]  [ 930/1349]  eta: 0:02:30  lr: 0.000492  min_lr: 0.000012  loss: 0.8363 (0.8769)  loss_scale: 131072.0000 (77573.2245)  weight_decay: 0.0500 (0.0500)  time: 0.6608  data: 0.0001  max mem: 41808
Epoch: [8]  [ 940/1349]  eta: 0:02:28  lr: 0.000492  min_lr: 0.000012  loss: 0.8948 (0.8765)  loss_scale: 131072.0000 (78141.7556)  weight_decay: 0.0500 (0.0500)  time: 0.6609  data: 0.0002  max mem: 41808
[2025-05-23 16:42:54,413] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 11734
[2025-05-23 16:42:54,413] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 16:42:54,413] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
[2025-05-23 16:42:54,416] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 11734
[2025-05-23 16:42:54,416] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
Epoch: [8]  [ 950/1349]  eta: 0:02:25  lr: 0.000492  min_lr: 0.000012  loss: 0.8016 (0.8763)  loss_scale: 131072.0000 (78078.1157)  weight_decay: 0.0500 (0.0500)  time: 0.6589  data: 0.0002  max mem: 41808
Epoch: [8]  [ 960/1349]  eta: 0:02:23  lr: 0.000492  min_lr: 0.000012  loss: 0.9096 (0.8767)  loss_scale: 65536.0000 (77947.6046)  weight_decay: 0.0500 (0.0500)  time: 0.6587  data: 0.0002  max mem: 41808
Epoch: [8]  [ 970/1349]  eta: 0:02:20  lr: 0.000492  min_lr: 0.000012  loss: 0.9096 (0.8759)  loss_scale: 65536.0000 (77819.7817)  weight_decay: 0.0500 (0.0500)  time: 0.6630  data: 0.0002  max mem: 41808
Epoch: [8]  [ 980/1349]  eta: 0:02:18  lr: 0.000492  min_lr: 0.000012  loss: 0.8609 (0.8760)  loss_scale: 65536.0000 (77694.5647)  weight_decay: 0.0500 (0.0500)  time: 0.6628  data: 0.0001  max mem: 41808
Epoch: [8]  [ 990/1349]  eta: 0:02:15  lr: 0.000492  min_lr: 0.000012  loss: 0.9419 (0.8768)  loss_scale: 65536.0000 (77571.8749)  weight_decay: 0.0500 (0.0500)  time: 0.6610  data: 0.0002  max mem: 41808
Epoch: [8]  [1000/1349]  eta: 0:02:12  lr: 0.000492  min_lr: 0.000012  loss: 0.9477 (0.8770)  loss_scale: 65536.0000 (77451.6364)  weight_decay: 0.0500 (0.0500)  time: 0.6627  data: 0.0002  max mem: 41808
Epoch: [8]  [1010/1349]  eta: 0:02:09  lr: 0.000492  min_lr: 0.000012  loss: 0.9187 (0.8771)  loss_scale: 65536.0000 (77333.7765)  weight_decay: 0.0500 (0.0500)  time: 0.6629  data: 0.0002  max mem: 41808
Epoch: [8]  [1020/1349]  eta: 0:02:06  lr: 0.000491  min_lr: 0.000012  loss: 0.8729 (0.8770)  loss_scale: 65536.0000 (77218.2253)  weight_decay: 0.0500 (0.0500)  time: 0.6610  data: 0.0001  max mem: 41808
Epoch: [8]  [1030/1349]  eta: 0:02:03  lr: 0.000491  min_lr: 0.000012  loss: 0.8729 (0.8772)  loss_scale: 65536.0000 (77104.9156)  weight_decay: 0.0500 (0.0500)  time: 0.6610  data: 0.0001  max mem: 41808
Epoch: [8]  [1040/1349]  eta: 0:02:00  lr: 0.000491  min_lr: 0.000012  loss: 0.8709 (0.8772)  loss_scale: 65536.0000 (76993.7829)  weight_decay: 0.0500 (0.0500)  time: 0.6618  data: 0.0002  max mem: 41808
Epoch: [8]  [1050/1349]  eta: 0:01:57  lr: 0.000491  min_lr: 0.000012  loss: 0.8709 (0.8770)  loss_scale: 65536.0000 (76884.7650)  weight_decay: 0.0500 (0.0500)  time: 0.6609  data: 0.0001  max mem: 41808
Epoch: [8]  [1060/1349]  eta: 0:01:54  lr: 0.000491  min_lr: 0.000012  loss: 0.8825 (0.8775)  loss_scale: 65536.0000 (76777.8021)  weight_decay: 0.0500 (0.0500)  time: 0.6633  data: 0.0002  max mem: 41808
Epoch: [8]  [1070/1349]  eta: 0:01:51  lr: 0.000491  min_lr: 0.000012  loss: 0.9280 (0.8781)  loss_scale: 65536.0000 (76672.8366)  weight_decay: 0.0500 (0.0500)  time: 0.6635  data: 0.0002  max mem: 41808
[2025-05-23 16:44:19,842] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 16:44:19,842] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-05-23 16:44:19,845] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 16:44:19,845] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [8]  [1080/1349]  eta: 0:01:47  lr: 0.000491  min_lr: 0.000012  loss: 0.8909 (0.8783)  loss_scale: 65536.0000 (77176.0666)  weight_decay: 0.0500 (0.0500)  time: 0.6609  data: 0.0003  max mem: 41808
Epoch: [8]  [1090/1349]  eta: 0:01:44  lr: 0.000491  min_lr: 0.000012  loss: 0.8909 (0.8790)  loss_scale: 131072.0000 (77670.0715)  weight_decay: 0.0500 (0.0500)  time: 0.6606  data: 0.0003  max mem: 41808
Epoch: [8]  [1100/1349]  eta: 0:01:41  lr: 0.000491  min_lr: 0.000012  loss: 0.8089 (0.8777)  loss_scale: 131072.0000 (78155.1026)  weight_decay: 0.0500 (0.0500)  time: 0.6607  data: 0.0002  max mem: 41808
Epoch: [8]  [1110/1349]  eta: 0:01:37  lr: 0.000491  min_lr: 0.000012  loss: 0.7481 (0.8782)  loss_scale: 131072.0000 (78631.4023)  weight_decay: 0.0500 (0.0500)  time: 0.6618  data: 0.0002  max mem: 41808
Epoch: [8]  [1120/1349]  eta: 0:01:33  lr: 0.000491  min_lr: 0.000012  loss: 0.8811 (0.8774)  loss_scale: 131072.0000 (79099.2043)  weight_decay: 0.0500 (0.0500)  time: 0.6628  data: 0.0001  max mem: 41808
Epoch: [8]  [1130/1349]  eta: 0:01:30  lr: 0.000491  min_lr: 0.000012  loss: 0.7742 (0.8771)  loss_scale: 131072.0000 (79558.7339)  weight_decay: 0.0500 (0.0500)  time: 0.6642  data: 0.0002  max mem: 41808
Epoch: [8]  [1140/1349]  eta: 0:01:26  lr: 0.000491  min_lr: 0.000012  loss: 0.8588 (0.8762)  loss_scale: 131072.0000 (80010.2086)  weight_decay: 0.0500 (0.0500)  time: 0.6640  data: 0.0001  max mem: 41808
Epoch: [8]  [1150/1349]  eta: 0:01:22  lr: 0.000491  min_lr: 0.000012  loss: 0.8588 (0.8761)  loss_scale: 131072.0000 (80453.8384)  weight_decay: 0.0500 (0.0500)  time: 0.6619  data: 0.0002  max mem: 41808
Epoch: [8]  [1160/1349]  eta: 0:01:19  lr: 0.000491  min_lr: 0.000012  loss: 0.9141 (0.8762)  loss_scale: 131072.0000 (80889.8260)  weight_decay: 0.0500 (0.0500)  time: 0.6604  data: 0.0002  max mem: 41808
Epoch: [8]  [1170/1349]  eta: 0:01:15  lr: 0.000491  min_lr: 0.000012  loss: 0.7785 (0.8751)  loss_scale: 131072.0000 (81318.3672)  weight_decay: 0.0500 (0.0500)  time: 0.6607  data: 0.0001  max mem: 41808
Epoch: [8]  [1180/1349]  eta: 0:01:11  lr: 0.000491  min_lr: 0.000012  loss: 0.8633 (0.8757)  loss_scale: 131072.0000 (81739.6511)  weight_decay: 0.0500 (0.0500)  time: 0.6631  data: 0.0001  max mem: 41808
Epoch: [8]  [1190/1349]  eta: 0:01:07  lr: 0.000491  min_lr: 0.000012  loss: 0.8963 (0.8757)  loss_scale: 131072.0000 (82153.8606)  weight_decay: 0.0500 (0.0500)  time: 0.6657  data: 0.0002  max mem: 41808
[2025-05-23 16:45:44,638] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 16:45:44,638] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 16:45:44,640] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 16:45:44,640] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [8]  [1200/1349]  eta: 0:01:03  lr: 0.000491  min_lr: 0.000012  loss: 0.9358 (0.8761)  loss_scale: 131072.0000 (82779.4438)  weight_decay: 0.0500 (0.0500)  time: 0.6637  data: 0.0001  max mem: 41808
[2025-05-23 16:45:47,950] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 11996
[2025-05-23 16:45:47,950] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 16:45:47,950] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
[2025-05-23 16:45:47,957] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 11996
[2025-05-23 16:45:47,957] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 16:45:49,225] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 11998
[2025-05-23 16:45:49,225] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 16:45:49,225] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
[2025-05-23 16:45:49,227] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 11998
[2025-05-23 16:45:49,227] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 16:45:49,813] [INFO] [logging.py:96:log_dist] [Rank 0] step=12000, skipped=71, lr=[1.1660865743087727e-05, 1.1660865743087727e-05, 1.5547820990783636e-05, 1.5547820990783636e-05, 2.0730427987711514e-05, 2.0730427987711514e-05, 2.764057065028202e-05, 2.764057065028202e-05, 3.6854094200376025e-05, 3.6854094200376025e-05, 4.913879226716804e-05, 4.913879226716804e-05, 6.551838968955738e-05, 6.551838968955738e-05, 8.735785291940985e-05, 8.735785291940985e-05, 0.00011647713722587979, 0.00011647713722587979, 0.00015530284963450637, 0.00015530284963450637, 0.00020707046617934185, 0.00020707046617934185, 0.00027609395490578914, 0.00027609395490578914, 0.00036812527320771884, 0.00036812527320771884, 0.0004908336976102918, 0.0004908336976102918], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-05-23 16:45:49,818] [INFO] [timer.py:260:stop] epoch=0/micro_step=12000/global_step=12000, RunningAvgSamplesPerSec=204.80421855038747, CurrSamplesPerSec=109.81645646520354, MemAllocated=1.37GB, MaxMemAllocated=40.83GB
Epoch: [8]  [1210/1349]  eta: 0:00:59  lr: 0.000491  min_lr: 0.000012  loss: 0.9781 (0.8769)  loss_scale: 131072.0000 (83232.3435)  weight_decay: 0.0500 (0.0500)  time: 0.6609  data: 0.0002  max mem: 41808
Epoch: [8]  [1220/1349]  eta: 0:00:55  lr: 0.000491  min_lr: 0.000012  loss: 0.9795 (0.8768)  loss_scale: 65536.0000 (83087.4103)  weight_decay: 0.0500 (0.0500)  time: 0.6620  data: 0.0002  max mem: 41808
Epoch: [8]  [1230/1349]  eta: 0:00:51  lr: 0.000491  min_lr: 0.000012  loss: 0.8290 (0.8765)  loss_scale: 65536.0000 (82944.8318)  weight_decay: 0.0500 (0.0500)  time: 0.6663  data: 0.0002  max mem: 41808
Epoch: [8]  [1240/1349]  eta: 0:00:47  lr: 0.000491  min_lr: 0.000012  loss: 0.9409 (0.8768)  loss_scale: 65536.0000 (82804.5512)  weight_decay: 0.0500 (0.0500)  time: 0.6657  data: 0.0002  max mem: 41808
Epoch: [8]  [1250/1349]  eta: 0:00:43  lr: 0.000491  min_lr: 0.000012  loss: 0.7540 (0.8751)  loss_scale: 65536.0000 (82666.5132)  weight_decay: 0.0500 (0.0500)  time: 0.6624  data: 0.0002  max mem: 41808
Epoch: [8]  [1260/1349]  eta: 0:00:39  lr: 0.000491  min_lr: 0.000012  loss: 0.6723 (0.8747)  loss_scale: 65536.0000 (82530.6646)  weight_decay: 0.0500 (0.0500)  time: 0.6636  data: 0.0003  max mem: 41808
Epoch: [8]  [1270/1349]  eta: 0:00:34  lr: 0.000491  min_lr: 0.000012  loss: 0.9596 (0.8756)  loss_scale: 65536.0000 (82396.9536)  weight_decay: 0.0500 (0.0500)  time: 0.6627  data: 0.0003  max mem: 41808
Epoch: [8]  [1280/1349]  eta: 0:00:30  lr: 0.000491  min_lr: 0.000012  loss: 0.9446 (0.8756)  loss_scale: 65536.0000 (82265.3302)  weight_decay: 0.0500 (0.0500)  time: 0.6621  data: 0.0002  max mem: 41808
Epoch: [8]  [1290/1349]  eta: 0:00:26  lr: 0.000491  min_lr: 0.000012  loss: 0.8595 (0.8753)  loss_scale: 65536.0000 (82135.7459)  weight_decay: 0.0500 (0.0500)  time: 0.6607  data: 0.0001  max mem: 41808
Epoch: [8]  [1300/1349]  eta: 0:00:21  lr: 0.000491  min_lr: 0.000012  loss: 0.8595 (0.8751)  loss_scale: 65536.0000 (82008.1537)  weight_decay: 0.0500 (0.0500)  time: 0.6609  data: 0.0002  max mem: 41808
Epoch: [8]  [1310/1349]  eta: 0:00:17  lr: 0.000490  min_lr: 0.000012  loss: 0.9053 (0.8757)  loss_scale: 65536.0000 (81882.5080)  weight_decay: 0.0500 (0.0500)  time: 0.6645  data: 0.0002  max mem: 41808
Epoch: [8]  [1320/1349]  eta: 0:00:13  lr: 0.000490  min_lr: 0.000012  loss: 0.9680 (0.8755)  loss_scale: 65536.0000 (81758.7646)  weight_decay: 0.0500 (0.0500)  time: 0.6638  data: 0.0002  max mem: 41808
Epoch: [8]  [1330/1349]  eta: 0:00:08  lr: 0.000490  min_lr: 0.000012  loss: 0.8612 (0.8755)  loss_scale: 65536.0000 (81636.8805)  weight_decay: 0.0500 (0.0500)  time: 0.6594  data: 0.0002  max mem: 41808
[2025-05-23 16:47:14,749] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 16:47:14,750] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-05-23 16:47:14,749] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 16:47:14,750] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [8]  [1340/1349]  eta: 0:00:04  lr: 0.000490  min_lr: 0.000012  loss: 0.8770 (0.8754)  loss_scale: 65536.0000 (81810.0403)  weight_decay: 0.0500 (0.0500)  time: 0.6579  data: 0.0001  max mem: 41808
Epoch: [8]  [1348/1349]  eta: 0:00:00  lr: 0.000490  min_lr: 0.000012  loss: 0.8875 (0.8755)  loss_scale: 131072.0000 (82102.1794)  weight_decay: 0.0500 (0.0500)  time: 0.6583  data: 0.0001  max mem: 41808
Epoch: [8] Total time: 0:10:11 (0.4534 s / it)
Averaged stats: lr: 0.000490  min_lr: 0.000012  loss: 0.8875 (0.8810)  loss_scale: 131072.0000 (82102.1794)  weight_decay: 0.0500 (0.0500)  total_time: 611.6794 (611.6759)
Val:  [  0/346]  eta: 1:26:17  loss: 1.8045 (1.8045)  acc1: 17.1875 (17.1875)  acc5: 99.2188 (99.2188)  time: 14.9637  data: 13.3797  max mem: 41808
Val:  [ 10/346]  eta: 0:16:54  loss: 0.1282 (0.4793)  acc1: 100.0000 (85.3693)  acc5: 100.0000 (99.5739)  time: 3.0208  data: 1.7011  max mem: 41808
Val:  [ 20/346]  eta: 0:12:43  loss: 0.1257 (0.3842)  acc1: 100.0000 (89.5089)  acc5: 100.0000 (99.7768)  time: 1.7112  data: 0.4243  max mem: 41808
Val:  [ 30/346]  eta: 0:10:39  loss: 0.1156 (0.3403)  acc1: 100.0000 (91.4062)  acc5: 100.0000 (99.6724)  time: 1.4761  data: 0.1578  max mem: 41808
Val:  [ 40/346]  eta: 0:09:25  loss: 0.1276 (0.3909)  acc1: 99.2188 (90.1486)  acc5: 100.0000 (99.3331)  time: 1.3269  data: 0.0003  max mem: 41808
Val:  [ 50/346]  eta: 0:08:33  loss: 0.1276 (0.3481)  acc1: 99.2188 (91.6360)  acc5: 100.0000 (99.4638)  time: 1.2829  data: 0.0003  max mem: 41808
Val:  [ 60/346]  eta: 0:07:55  loss: 0.1577 (0.3516)  acc1: 98.4375 (91.2526)  acc5: 100.0000 (99.5389)  time: 1.2880  data: 0.0003  max mem: 41808
Val:  [ 70/346]  eta: 0:07:08  loss: 0.2300 (0.3567)  acc1: 95.3125 (90.7901)  acc5: 100.0000 (99.5929)  time: 1.0964  data: 0.0608  max mem: 41808
Val:  [ 80/346]  eta: 0:06:32  loss: 0.2608 (0.3593)  acc1: 92.1875 (90.9240)  acc5: 100.0000 (99.4888)  time: 0.9036  data: 0.1332  max mem: 41808
Val:  [ 90/346]  eta: 0:06:00  loss: 0.2832 (0.3673)  acc1: 92.9688 (90.7795)  acc5: 100.0000 (99.4248)  time: 0.8988  data: 0.1498  max mem: 41808
Val:  [100/346]  eta: 0:05:34  loss: 0.2081 (0.3489)  acc1: 96.0938 (91.3289)  acc5: 100.0000 (99.4817)  time: 0.8828  data: 0.1561  max mem: 41808
Val:  [110/346]  eta: 0:05:11  loss: 0.2081 (0.3825)  acc1: 96.8750 (90.3435)  acc5: 100.0000 (99.4229)  time: 0.9041  data: 0.1712  max mem: 41808
Val:  [120/346]  eta: 0:04:50  loss: 0.2123 (0.3835)  acc1: 95.3125 (90.3086)  acc5: 100.0000 (99.4383)  time: 0.9090  data: 0.1722  max mem: 41808
Val:  [130/346]  eta: 0:04:31  loss: 0.1414 (0.3903)  acc1: 96.8750 (90.0525)  acc5: 100.0000 (99.3977)  time: 0.9001  data: 0.1677  max mem: 41808
Val:  [140/346]  eta: 0:04:13  loss: 0.2344 (0.3934)  acc1: 96.0938 (89.9490)  acc5: 100.0000 (99.4404)  time: 0.9046  data: 0.1712  max mem: 41808
Val:  [150/346]  eta: 0:03:56  loss: 0.2637 (0.3876)  acc1: 94.5312 (90.1749)  acc5: 100.0000 (99.4774)  time: 0.9087  data: 0.1637  max mem: 41808
Val:  [160/346]  eta: 0:03:41  loss: 0.2648 (0.3829)  acc1: 93.7500 (90.2077)  acc5: 100.0000 (99.5050)  time: 0.9059  data: 0.1586  max mem: 41808
Val:  [170/346]  eta: 0:03:26  loss: 0.1494 (0.3761)  acc1: 98.4375 (90.3920)  acc5: 100.0000 (99.5340)  time: 0.9225  data: 0.1614  max mem: 41808
Val:  [180/346]  eta: 0:03:12  loss: 0.1383 (0.3887)  acc1: 99.2188 (89.9517)  acc5: 100.0000 (99.4518)  time: 0.9293  data: 0.1661  max mem: 41808
Val:  [190/346]  eta: 0:02:59  loss: 0.2477 (0.3831)  acc1: 92.9688 (90.1628)  acc5: 100.0000 (99.4805)  time: 0.9238  data: 0.1747  max mem: 41808
Val:  [200/346]  eta: 0:02:45  loss: 0.2547 (0.3865)  acc1: 96.0938 (89.9953)  acc5: 100.0000 (99.5064)  time: 0.9146  data: 0.1728  max mem: 41808
Val:  [210/346]  eta: 0:02:33  loss: 0.2015 (0.3839)  acc1: 94.5312 (90.0437)  acc5: 100.0000 (99.5298)  time: 0.9024  data: 0.1564  max mem: 41808
Val:  [220/346]  eta: 0:02:20  loss: 0.2015 (0.3792)  acc1: 96.8750 (90.2149)  acc5: 100.0000 (99.5334)  time: 0.8974  data: 0.1517  max mem: 41808
Val:  [230/346]  eta: 0:02:08  loss: 0.1419 (0.3699)  acc1: 96.8750 (90.5506)  acc5: 100.0000 (99.5536)  time: 0.9086  data: 0.1613  max mem: 41808
Val:  [240/346]  eta: 0:01:56  loss: 0.1880 (0.3792)  acc1: 96.8750 (90.2490)  acc5: 100.0000 (99.5494)  time: 0.9323  data: 0.1666  max mem: 41808
Val:  [250/346]  eta: 0:01:44  loss: 0.2100 (0.3756)  acc1: 94.5312 (90.3511)  acc5: 100.0000 (99.5674)  time: 0.9235  data: 0.1615  max mem: 41808
Val:  [260/346]  eta: 0:01:33  loss: 0.1597 (0.3740)  acc1: 96.8750 (90.3915)  acc5: 100.0000 (99.5809)  time: 0.8804  data: 0.1595  max mem: 41808
Val:  [270/346]  eta: 0:01:21  loss: 0.1375 (0.3700)  acc1: 99.2188 (90.5155)  acc5: 100.0000 (99.5906)  time: 0.8770  data: 0.1638  max mem: 41808
Val:  [280/346]  eta: 0:01:10  loss: 0.1098 (0.3666)  acc1: 100.0000 (90.6222)  acc5: 100.0000 (99.5941)  time: 0.9156  data: 0.1667  max mem: 41808
Val:  [290/346]  eta: 0:00:59  loss: 0.1061 (0.3577)  acc1: 100.0000 (90.9418)  acc5: 100.0000 (99.6080)  time: 0.9221  data: 0.1701  max mem: 41808
Val:  [300/346]  eta: 0:00:48  loss: 0.1069 (0.3579)  acc1: 100.0000 (91.0039)  acc5: 100.0000 (99.5951)  time: 0.8986  data: 0.1658  max mem: 41808
Val:  [310/346]  eta: 0:00:37  loss: 0.1319 (0.3555)  acc1: 99.2188 (91.0747)  acc5: 100.0000 (99.5930)  time: 0.8662  data: 0.1479  max mem: 41808
Val:  [320/346]  eta: 0:00:27  loss: 0.1385 (0.3608)  acc1: 99.2188 (90.9146)  acc5: 100.0000 (99.6057)  time: 0.8772  data: 0.1489  max mem: 41808
Val:  [330/346]  eta: 0:00:16  loss: 0.3207 (0.3692)  acc1: 88.2812 (90.6132)  acc5: 100.0000 (99.5728)  time: 0.9099  data: 0.1577  max mem: 41808
Val:  [340/346]  eta: 0:00:06  loss: 0.3730 (0.3768)  acc1: 88.2812 (90.4096)  acc5: 100.0000 (99.5739)  time: 0.9164  data: 0.1659  max mem: 41808
Val:  [345/346]  eta: 0:00:01  loss: 0.2099 (0.3736)  acc1: 95.3125 (90.5191)  acc5: 100.0000 (99.5796)  time: 0.9092  data: 0.1793  max mem: 41808
Val: Total time: 0:05:59 (1.0398 s / it)
* Acc@1 90.475 Acc@5 99.609 loss 0.375
Accuracy of the network on the 88494 val videos: 90.5%
Max accuracy: 91.25%   Max Epoch: 3
Epoch: [9]  [   0/1349]  eta: 1:49:44  lr: 0.000490  min_lr: 0.000012  loss: 0.7452 (0.7452)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 4.8812  data: 4.5385  max mem: 41808
Epoch: [9]  [  10/1349]  eta: 0:16:16  lr: 0.000490  min_lr: 0.000012  loss: 0.8380 (0.8368)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7296  data: 0.4127  max mem: 41808
Epoch: [9]  [  20/1349]  eta: 0:11:45  lr: 0.000490  min_lr: 0.000012  loss: 0.9319 (0.9082)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3132  data: 0.0001  max mem: 41808
Epoch: [9]  [  30/1349]  eta: 0:10:05  lr: 0.000490  min_lr: 0.000012  loss: 0.9840 (0.9323)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3099  data: 0.0001  max mem: 41808
Epoch: [9]  [  40/1349]  eta: 0:09:12  lr: 0.000490  min_lr: 0.000012  loss: 0.9384 (0.9193)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [9]  [  50/1349]  eta: 0:08:38  lr: 0.000490  min_lr: 0.000012  loss: 0.8964 (0.9026)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
Epoch: [9]  [  60/1349]  eta: 0:08:15  lr: 0.000490  min_lr: 0.000012  loss: 0.8964 (0.8982)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
Epoch: [9]  [  70/1349]  eta: 0:07:57  lr: 0.000490  min_lr: 0.000012  loss: 0.9227 (0.8942)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3062  data: 0.0001  max mem: 41808
Epoch: [9]  [  80/1349]  eta: 0:07:42  lr: 0.000490  min_lr: 0.000012  loss: 0.9227 (0.8884)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3060  data: 0.0001  max mem: 41808
Epoch: [9]  [  90/1349]  eta: 0:07:31  lr: 0.000490  min_lr: 0.000012  loss: 0.9684 (0.9007)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3058  data: 0.0001  max mem: 41808
Epoch: [9]  [ 100/1349]  eta: 0:07:21  lr: 0.000490  min_lr: 0.000012  loss: 0.8691 (0.8877)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3061  data: 0.0001  max mem: 41808
Epoch: [9]  [ 110/1349]  eta: 0:07:12  lr: 0.000490  min_lr: 0.000012  loss: 0.8691 (0.8950)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3064  data: 0.0002  max mem: 41808
[2025-05-23 16:54:03,491] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 16:54:03,492] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 16:54:03,492] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 16:54:03,492] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 16:54:04,103] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 12257
[2025-05-23 16:54:04,103] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 12257
[2025-05-23 16:54:04,104] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 16:54:04,104] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 16:54:04,104] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [9]  [ 120/1349]  eta: 0:07:04  lr: 0.000490  min_lr: 0.000012  loss: 0.9872 (0.8935)  loss_scale: 131072.0000 (133238.4793)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0002  max mem: 41808
Epoch: [9]  [ 130/1349]  eta: 0:06:57  lr: 0.000490  min_lr: 0.000012  loss: 0.9405 (0.8931)  loss_scale: 131072.0000 (133073.0992)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
Epoch: [9]  [ 140/1349]  eta: 0:06:51  lr: 0.000490  min_lr: 0.000012  loss: 0.9405 (0.8940)  loss_scale: 131072.0000 (132931.1773)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
Epoch: [9]  [ 150/1349]  eta: 0:06:45  lr: 0.000490  min_lr: 0.000012  loss: 0.9443 (0.8904)  loss_scale: 131072.0000 (132808.0530)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
Epoch: [9]  [ 160/1349]  eta: 0:06:39  lr: 0.000490  min_lr: 0.000012  loss: 0.8049 (0.8858)  loss_scale: 131072.0000 (132700.2236)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [9]  [ 170/1349]  eta: 0:06:34  lr: 0.000490  min_lr: 0.000012  loss: 0.8148 (0.8847)  loss_scale: 131072.0000 (132605.0058)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [9]  [ 180/1349]  eta: 0:06:29  lr: 0.000490  min_lr: 0.000012  loss: 0.8480 (0.8818)  loss_scale: 131072.0000 (132520.3094)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [9]  [ 190/1349]  eta: 0:06:24  lr: 0.000490  min_lr: 0.000012  loss: 0.8772 (0.8845)  loss_scale: 131072.0000 (132444.4817)  weight_decay: 0.0500 (0.0500)  time: 0.3094  data: 0.0002  max mem: 41808
Epoch: [9]  [ 200/1349]  eta: 0:06:19  lr: 0.000490  min_lr: 0.000012  loss: 0.8818 (0.8831)  loss_scale: 131072.0000 (132376.1990)  weight_decay: 0.0500 (0.0500)  time: 0.3096  data: 0.0001  max mem: 41808
Epoch: [9]  [ 210/1349]  eta: 0:06:15  lr: 0.000490  min_lr: 0.000012  loss: 0.8818 (0.8835)  loss_scale: 131072.0000 (132314.3886)  weight_decay: 0.0500 (0.0500)  time: 0.3081  data: 0.0001  max mem: 41808
Epoch: [9]  [ 220/1349]  eta: 0:06:10  lr: 0.000490  min_lr: 0.000012  loss: 0.9053 (0.8872)  loss_scale: 131072.0000 (132258.1719)  weight_decay: 0.0500 (0.0500)  time: 0.3082  data: 0.0001  max mem: 41808
[2025-05-23 16:54:36,435] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 12362
[2025-05-23 16:54:36,435] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 12362
[2025-05-23 16:54:36,435] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 16:54:36,435] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 16:54:36,435] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [9]  [ 230/1349]  eta: 0:06:06  lr: 0.000489  min_lr: 0.000012  loss: 0.9453 (0.8870)  loss_scale: 65536.0000 (129369.7662)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [9]  [ 240/1349]  eta: 0:06:02  lr: 0.000489  min_lr: 0.000012  loss: 0.8456 (0.8873)  loss_scale: 65536.0000 (126721.0622)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [9]  [ 250/1349]  eta: 0:05:58  lr: 0.000489  min_lr: 0.000012  loss: 0.9528 (0.8907)  loss_scale: 65536.0000 (124283.4104)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
Epoch: [9]  [ 260/1349]  eta: 0:05:54  lr: 0.000489  min_lr: 0.000012  loss: 0.9226 (0.8878)  loss_scale: 65536.0000 (122032.5517)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [9]  [ 270/1349]  eta: 0:05:50  lr: 0.000489  min_lr: 0.000012  loss: 0.9601 (0.8902)  loss_scale: 65536.0000 (119947.8081)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [9]  [ 280/1349]  eta: 0:05:46  lr: 0.000489  min_lr: 0.000012  loss: 0.9393 (0.8876)  loss_scale: 65536.0000 (118011.4448)  weight_decay: 0.0500 (0.0500)  time: 0.3066  data: 0.0001  max mem: 41808
Epoch: [9]  [ 290/1349]  eta: 0:05:42  lr: 0.000489  min_lr: 0.000012  loss: 0.8599 (0.8870)  loss_scale: 65536.0000 (116208.1649)  weight_decay: 0.0500 (0.0500)  time: 0.3066  data: 0.0001  max mem: 41808
Epoch: [9]  [ 300/1349]  eta: 0:05:38  lr: 0.000489  min_lr: 0.000012  loss: 0.9340 (0.8889)  loss_scale: 65536.0000 (114524.7043)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0001  max mem: 41808
Epoch: [9]  [ 310/1349]  eta: 0:05:34  lr: 0.000489  min_lr: 0.000012  loss: 0.9375 (0.8909)  loss_scale: 65536.0000 (112949.5048)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
Epoch: [9]  [ 320/1349]  eta: 0:05:31  lr: 0.000489  min_lr: 0.000012  loss: 0.9590 (0.8917)  loss_scale: 65536.0000 (111472.4486)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [9]  [ 330/1349]  eta: 0:05:27  lr: 0.000489  min_lr: 0.000012  loss: 0.9497 (0.8920)  loss_scale: 65536.0000 (110084.6405)  weight_decay: 0.0500 (0.0500)  time: 0.3080  data: 0.0001  max mem: 41808
Epoch: [9]  [ 340/1349]  eta: 0:05:23  lr: 0.000489  min_lr: 0.000012  loss: 0.8510 (0.8912)  loss_scale: 65536.0000 (108778.2287)  weight_decay: 0.0500 (0.0500)  time: 0.3080  data: 0.0001  max mem: 41808
[2025-05-23 16:55:16,070] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 16:55:16,070] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-05-23 16:55:16,070] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 16:55:16,070] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [9]  [ 350/1349]  eta: 0:05:20  lr: 0.000489  min_lr: 0.000012  loss: 0.8510 (0.8909)  loss_scale: 65536.0000 (107732.9687)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [9]  [ 360/1349]  eta: 0:05:16  lr: 0.000489  min_lr: 0.000012  loss: 0.8717 (0.8904)  loss_scale: 131072.0000 (108379.4792)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0001  max mem: 41808
Epoch: [9]  [ 370/1349]  eta: 0:05:13  lr: 0.000489  min_lr: 0.000012  loss: 0.9246 (0.8906)  loss_scale: 131072.0000 (108991.1375)  weight_decay: 0.0500 (0.0500)  time: 0.3078  data: 0.0001  max mem: 41808
Epoch: [9]  [ 380/1349]  eta: 0:05:09  lr: 0.000489  min_lr: 0.000012  loss: 0.9332 (0.8911)  loss_scale: 131072.0000 (109570.6877)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
Epoch: [9]  [ 390/1349]  eta: 0:05:06  lr: 0.000489  min_lr: 0.000012  loss: 0.9627 (0.8908)  loss_scale: 131072.0000 (110120.5934)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [9]  [ 400/1349]  eta: 0:05:02  lr: 0.000489  min_lr: 0.000012  loss: 0.9527 (0.8915)  loss_scale: 131072.0000 (110643.0723)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0002  max mem: 41808
Epoch: [9]  [ 410/1349]  eta: 0:04:59  lr: 0.000489  min_lr: 0.000012  loss: 0.8896 (0.8911)  loss_scale: 131072.0000 (111140.1265)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0002  max mem: 41808
Epoch: [9]  [ 420/1349]  eta: 0:04:55  lr: 0.000489  min_lr: 0.000012  loss: 0.8246 (0.8890)  loss_scale: 131072.0000 (111613.5677)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [9]  [ 430/1349]  eta: 0:04:52  lr: 0.000489  min_lr: 0.000012  loss: 0.8233 (0.8891)  loss_scale: 131072.0000 (112065.0394)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0002  max mem: 41808
Epoch: [9]  [ 440/1349]  eta: 0:04:48  lr: 0.000489  min_lr: 0.000012  loss: 0.8233 (0.8872)  loss_scale: 131072.0000 (112496.0363)  weight_decay: 0.0500 (0.0500)  time: 0.3083  data: 0.0001  max mem: 41808
Epoch: [9]  [ 450/1349]  eta: 0:04:45  lr: 0.000489  min_lr: 0.000012  loss: 0.8763 (0.8858)  loss_scale: 131072.0000 (112907.9202)  weight_decay: 0.0500 (0.0500)  time: 0.3091  data: 0.0001  max mem: 41808
Epoch: [9]  [ 460/1349]  eta: 0:04:42  lr: 0.000489  min_lr: 0.000012  loss: 0.8072 (0.8841)  loss_scale: 131072.0000 (113301.9349)  weight_decay: 0.0500 (0.0500)  time: 0.3084  data: 0.0001  max mem: 41808
Epoch: [9]  [ 470/1349]  eta: 0:04:38  lr: 0.000489  min_lr: 0.000012  loss: 0.8331 (0.8833)  loss_scale: 131072.0000 (113679.2187)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
[2025-05-23 16:55:55,478] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 16:55:55,478] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 16:55:55,478] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 16:55:55,478] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [9]  [ 480/1349]  eta: 0:04:35  lr: 0.000489  min_lr: 0.000012  loss: 0.8616 (0.8836)  loss_scale: 131072.0000 (114858.3119)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
[2025-05-23 16:55:56,407] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 12622
[2025-05-23 16:55:56,407] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 16:55:56,407] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
[2025-05-23 16:55:56,407] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 12622
[2025-05-23 16:55:56,407] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
Epoch: [9]  [ 490/1349]  eta: 0:04:32  lr: 0.000489  min_lr: 0.000012  loss: 0.8874 (0.8834)  loss_scale: 131072.0000 (115188.5295)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [9]  [ 500/1349]  eta: 0:04:28  lr: 0.000488  min_lr: 0.000012  loss: 0.7747 (0.8823)  loss_scale: 131072.0000 (115505.5649)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
Epoch: [9]  [ 510/1349]  eta: 0:04:25  lr: 0.000488  min_lr: 0.000012  loss: 0.8109 (0.8811)  loss_scale: 131072.0000 (115810.1918)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0001  max mem: 41808
Epoch: [9]  [ 520/1349]  eta: 0:04:22  lr: 0.000488  min_lr: 0.000012  loss: 0.8971 (0.8827)  loss_scale: 131072.0000 (116103.1248)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
Epoch: [9]  [ 530/1349]  eta: 0:04:18  lr: 0.000488  min_lr: 0.000012  loss: 0.8995 (0.8819)  loss_scale: 131072.0000 (116385.0245)  weight_decay: 0.0500 (0.0500)  time: 0.3091  data: 0.0002  max mem: 41808
Epoch: [9]  [ 540/1349]  eta: 0:04:15  lr: 0.000488  min_lr: 0.000012  loss: 0.8342 (0.8817)  loss_scale: 131072.0000 (116656.5028)  weight_decay: 0.0500 (0.0500)  time: 0.3095  data: 0.0002  max mem: 41808
Epoch: [9]  [ 550/1349]  eta: 0:04:12  lr: 0.000488  min_lr: 0.000012  loss: 0.8412 (0.8813)  loss_scale: 131072.0000 (116918.1270)  weight_decay: 0.0500 (0.0500)  time: 0.3079  data: 0.0001  max mem: 41808
Epoch: [9]  [ 560/1349]  eta: 0:04:09  lr: 0.000488  min_lr: 0.000012  loss: 0.9048 (0.8822)  loss_scale: 131072.0000 (117170.4242)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0001  max mem: 41808
Epoch: [9]  [ 570/1349]  eta: 0:04:05  lr: 0.000488  min_lr: 0.000012  loss: 0.9646 (0.8827)  loss_scale: 131072.0000 (117413.8844)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
Epoch: [9]  [ 580/1349]  eta: 0:04:02  lr: 0.000488  min_lr: 0.000012  loss: 0.9278 (0.8828)  loss_scale: 131072.0000 (117648.9639)  weight_decay: 0.0500 (0.0500)  time: 0.3066  data: 0.0001  max mem: 41808
Epoch: [9]  [ 590/1349]  eta: 0:03:59  lr: 0.000488  min_lr: 0.000012  loss: 0.8886 (0.8823)  loss_scale: 131072.0000 (117876.0880)  weight_decay: 0.0500 (0.0500)  time: 0.3066  data: 0.0001  max mem: 41808
Epoch: [9]  [ 600/1349]  eta: 0:03:56  lr: 0.000488  min_lr: 0.000012  loss: 0.8594 (0.8820)  loss_scale: 131072.0000 (118095.6539)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
[2025-05-23 16:56:36,059] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 16:56:36,059] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 16:56:36,059] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 16:56:36,059] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [9]  [ 610/1349]  eta: 0:03:52  lr: 0.000488  min_lr: 0.000012  loss: 0.8595 (0.8808)  loss_scale: 131072.0000 (118522.5532)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
[2025-05-23 16:56:36,672] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 12753
[2025-05-23 16:56:36,672] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 16:56:36,672] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
[2025-05-23 16:56:36,672] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 12753
[2025-05-23 16:56:36,672] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
Epoch: [9]  [ 620/1349]  eta: 0:03:49  lr: 0.000488  min_lr: 0.000012  loss: 0.8717 (0.8809)  loss_scale: 131072.0000 (118935.7037)  weight_decay: 0.0500 (0.0500)  time: 0.3063  data: 0.0001  max mem: 41808
Epoch: [9]  [ 630/1349]  eta: 0:03:46  lr: 0.000488  min_lr: 0.000012  loss: 0.8965 (0.8806)  loss_scale: 131072.0000 (119128.0380)  weight_decay: 0.0500 (0.0500)  time: 0.3062  data: 0.0001  max mem: 41808
Epoch: [9]  [ 640/1349]  eta: 0:03:43  lr: 0.000488  min_lr: 0.000012  loss: 0.8965 (0.8807)  loss_scale: 131072.0000 (119314.3713)  weight_decay: 0.0500 (0.0500)  time: 0.3066  data: 0.0001  max mem: 41808
Epoch: [9]  [ 650/1349]  eta: 0:03:39  lr: 0.000488  min_lr: 0.000012  loss: 0.8563 (0.8801)  loss_scale: 131072.0000 (119494.9800)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0001  max mem: 41808
Epoch: [9]  [ 660/1349]  eta: 0:03:36  lr: 0.000488  min_lr: 0.000012  loss: 0.8545 (0.8801)  loss_scale: 131072.0000 (119670.1241)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0001  max mem: 41808
Epoch: [9]  [ 670/1349]  eta: 0:03:33  lr: 0.000488  min_lr: 0.000012  loss: 0.9721 (0.8811)  loss_scale: 131072.0000 (119840.0477)  weight_decay: 0.0500 (0.0500)  time: 0.3082  data: 0.0001  max mem: 41808
Epoch: [9]  [ 680/1349]  eta: 0:03:30  lr: 0.000488  min_lr: 0.000012  loss: 0.9318 (0.8804)  loss_scale: 131072.0000 (120004.9809)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [9]  [ 690/1349]  eta: 0:03:26  lr: 0.000488  min_lr: 0.000012  loss: 0.9132 (0.8806)  loss_scale: 131072.0000 (120165.1404)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
Epoch: [9]  [ 700/1349]  eta: 0:03:23  lr: 0.000488  min_lr: 0.000012  loss: 0.9571 (0.8811)  loss_scale: 131072.0000 (120320.7304)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [9]  [ 710/1349]  eta: 0:03:20  lr: 0.000488  min_lr: 0.000012  loss: 0.8978 (0.8800)  loss_scale: 131072.0000 (120471.9437)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0001  max mem: 41808
Epoch: [9]  [ 720/1349]  eta: 0:03:17  lr: 0.000488  min_lr: 0.000012  loss: 0.8939 (0.8809)  loss_scale: 131072.0000 (120618.9626)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0002  max mem: 41808
Epoch: [9]  [ 730/1349]  eta: 0:03:14  lr: 0.000488  min_lr: 0.000012  loss: 0.8939 (0.8802)  loss_scale: 131072.0000 (120761.9590)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [9]  [ 740/1349]  eta: 0:03:10  lr: 0.000488  min_lr: 0.000012  loss: 0.8861 (0.8810)  loss_scale: 131072.0000 (120901.0958)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
[2025-05-23 16:57:16,307] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 16:57:16,307] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 16:57:16,307] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 16:57:16,307] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 16:57:18,144] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 12888
[2025-05-23 16:57:18,144] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 12888
[2025-05-23 16:57:18,144] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 16:57:18,144] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 16:57:18,144] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [9]  [ 750/1349]  eta: 0:03:07  lr: 0.000487  min_lr: 0.000012  loss: 1.0052 (0.8829)  loss_scale: 131072.0000 (122083.7071)  weight_decay: 0.0500 (0.0500)  time: 0.3064  data: 0.0001  max mem: 41808
Epoch: [9]  [ 760/1349]  eta: 0:03:04  lr: 0.000487  min_lr: 0.000012  loss: 0.9515 (0.8821)  loss_scale: 131072.0000 (122201.8187)  weight_decay: 0.0500 (0.0500)  time: 0.3066  data: 0.0001  max mem: 41808
Epoch: [9]  [ 770/1349]  eta: 0:03:01  lr: 0.000487  min_lr: 0.000012  loss: 0.8856 (0.8829)  loss_scale: 131072.0000 (122316.8664)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
Epoch: [9]  [ 780/1349]  eta: 0:02:58  lr: 0.000487  min_lr: 0.000012  loss: 0.8801 (0.8825)  loss_scale: 131072.0000 (122428.9680)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
Epoch: [9]  [ 790/1349]  eta: 0:02:55  lr: 0.000487  min_lr: 0.000012  loss: 0.8801 (0.8831)  loss_scale: 131072.0000 (122538.2351)  weight_decay: 0.0500 (0.0500)  time: 0.3133  data: 0.0001  max mem: 41808
Epoch: [9]  [ 800/1349]  eta: 0:02:51  lr: 0.000487  min_lr: 0.000012  loss: 0.8447 (0.8823)  loss_scale: 131072.0000 (122644.7740)  weight_decay: 0.0500 (0.0500)  time: 0.3135  data: 0.0002  max mem: 41808
Epoch: [9]  [ 810/1349]  eta: 0:02:48  lr: 0.000487  min_lr: 0.000012  loss: 0.8465 (0.8829)  loss_scale: 131072.0000 (122748.6856)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0002  max mem: 41808
Epoch: [9]  [ 820/1349]  eta: 0:02:45  lr: 0.000487  min_lr: 0.000012  loss: 0.8465 (0.8812)  loss_scale: 131072.0000 (122850.0658)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
Epoch: [9]  [ 830/1349]  eta: 0:02:42  lr: 0.000487  min_lr: 0.000012  loss: 0.7961 (0.8806)  loss_scale: 131072.0000 (122949.0060)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
Epoch: [9]  [ 840/1349]  eta: 0:02:39  lr: 0.000487  min_lr: 0.000012  loss: 0.8478 (0.8805)  loss_scale: 131072.0000 (123045.5933)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
Epoch: [9]  [ 850/1349]  eta: 0:02:36  lr: 0.000487  min_lr: 0.000012  loss: 0.9343 (0.8817)  loss_scale: 131072.0000 (123139.9107)  weight_decay: 0.0500 (0.0500)  time: 0.3064  data: 0.0001  max mem: 41808
[2025-05-23 16:57:52,398] [INFO] [logging.py:96:log_dist] [Rank 0] step=13000, skipped=76, lr=[1.157087418877164e-05, 1.157087418877164e-05, 1.542783225169552e-05, 1.542783225169552e-05, 2.0570443002260693e-05, 2.0570443002260693e-05, 2.742725733634759e-05, 2.742725733634759e-05, 3.6569676448463455e-05, 3.6569676448463455e-05, 4.875956859795127e-05, 4.875956859795127e-05, 6.50127581306017e-05, 6.50127581306017e-05, 8.668367750746893e-05, 8.668367750746893e-05, 0.00011557823667662525, 0.00011557823667662525, 0.00015410431556883367, 0.00015410431556883367, 0.00020547242075844488, 0.00020547242075844488, 0.0002739632276779265, 0.0002739632276779265, 0.00036528430357056865, 0.00036528430357056865, 0.00048704573809409157, 0.00048704573809409157], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-05-23 16:57:52,399] [INFO] [timer.py:260:stop] epoch=0/micro_step=13000/global_step=13000, RunningAvgSamplesPerSec=202.91432369816752, CurrSamplesPerSec=214.27900806074373, MemAllocated=1.37GB, MaxMemAllocated=40.83GB
Epoch: [9]  [ 860/1349]  eta: 0:02:32  lr: 0.000487  min_lr: 0.000012  loss: 0.9343 (0.8803)  loss_scale: 131072.0000 (123232.0372)  weight_decay: 0.0500 (0.0500)  time: 0.3082  data: 0.0001  max mem: 41808
Epoch: [9]  [ 870/1349]  eta: 0:02:29  lr: 0.000487  min_lr: 0.000012  loss: 0.7721 (0.8801)  loss_scale: 131072.0000 (123322.0482)  weight_decay: 0.0500 (0.0500)  time: 0.3083  data: 0.0001  max mem: 41808
[2025-05-23 16:57:57,909] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 16:57:57,909] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 16:57:57,910] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 16:57:57,910] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 16:57:58,521] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 13019
[2025-05-23 16:57:58,521] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 13019
[2025-05-23 16:57:58,521] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 16:57:58,521] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 16:57:58,521] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [9]  [ 880/1349]  eta: 0:02:26  lr: 0.000487  min_lr: 0.000012  loss: 0.9082 (0.8800)  loss_scale: 131072.0000 (123707.5687)  weight_decay: 0.0500 (0.0500)  time: 0.3060  data: 0.0001  max mem: 41808
Epoch: [9]  [ 890/1349]  eta: 0:02:23  lr: 0.000487  min_lr: 0.000012  loss: 0.8690 (0.8798)  loss_scale: 131072.0000 (123790.2222)  weight_decay: 0.0500 (0.0500)  time: 0.3057  data: 0.0001  max mem: 41808
Epoch: [9]  [ 900/1349]  eta: 0:02:20  lr: 0.000487  min_lr: 0.000012  loss: 0.8609 (0.8799)  loss_scale: 131072.0000 (123871.0411)  weight_decay: 0.0500 (0.0500)  time: 0.3062  data: 0.0001  max mem: 41808
Epoch: [9]  [ 910/1349]  eta: 0:02:17  lr: 0.000487  min_lr: 0.000012  loss: 0.8889 (0.8804)  loss_scale: 131072.0000 (123950.0856)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0001  max mem: 41808
[2025-05-23 16:58:08,930] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 13053
[2025-05-23 16:58:08,930] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 16:58:08,930] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 13053
[2025-05-23 16:58:08,930] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 16:58:08,930] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [9]  [ 920/1349]  eta: 0:02:14  lr: 0.000487  min_lr: 0.000012  loss: 0.8841 (0.8802)  loss_scale: 131072.0000 (123386.9967)  weight_decay: 0.0500 (0.0500)  time: 0.3062  data: 0.0001  max mem: 41808
Epoch: [9]  [ 930/1349]  eta: 0:02:10  lr: 0.000487  min_lr: 0.000012  loss: 0.9571 (0.8814)  loss_scale: 65536.0000 (122765.6112)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0001  max mem: 41808
Epoch: [9]  [ 940/1349]  eta: 0:02:07  lr: 0.000487  min_lr: 0.000012  loss: 0.8993 (0.8805)  loss_scale: 65536.0000 (122157.4325)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [9]  [ 950/1349]  eta: 0:02:04  lr: 0.000487  min_lr: 0.000012  loss: 0.8271 (0.8804)  loss_scale: 65536.0000 (121562.0442)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
Epoch: [9]  [ 960/1349]  eta: 0:02:01  lr: 0.000487  min_lr: 0.000012  loss: 0.8533 (0.8803)  loss_scale: 65536.0000 (120979.0468)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
Epoch: [9]  [ 970/1349]  eta: 0:01:58  lr: 0.000487  min_lr: 0.000012  loss: 0.8528 (0.8797)  loss_scale: 65536.0000 (120408.0577)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
Epoch: [9]  [ 980/1349]  eta: 0:01:55  lr: 0.000487  min_lr: 0.000012  loss: 0.8544 (0.8794)  loss_scale: 65536.0000 (119848.7095)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
Epoch: [9]  [ 990/1349]  eta: 0:01:52  lr: 0.000486  min_lr: 0.000012  loss: 0.8797 (0.8792)  loss_scale: 65536.0000 (119300.6498)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [9]  [1000/1349]  eta: 0:01:48  lr: 0.000486  min_lr: 0.000012  loss: 0.8121 (0.8780)  loss_scale: 65536.0000 (118763.5405)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [9]  [1010/1349]  eta: 0:01:45  lr: 0.000486  min_lr: 0.000012  loss: 0.8071 (0.8780)  loss_scale: 65536.0000 (118237.0564)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [9]  [1020/1349]  eta: 0:01:42  lr: 0.000486  min_lr: 0.000012  loss: 0.8911 (0.8784)  loss_scale: 65536.0000 (117720.8854)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [9]  [1030/1349]  eta: 0:01:39  lr: 0.000486  min_lr: 0.000012  loss: 0.8911 (0.8785)  loss_scale: 65536.0000 (117214.7274)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [9]  [1040/1349]  eta: 0:01:36  lr: 0.000486  min_lr: 0.000012  loss: 0.8569 (0.8782)  loss_scale: 65536.0000 (116718.2939)  weight_decay: 0.0500 (0.0500)  time: 0.3064  data: 0.0001  max mem: 41808
[2025-05-23 16:58:48,530] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 16:58:48,530] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-05-23 16:58:48,530] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 16:58:48,530] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [9]  [1050/1349]  eta: 0:01:33  lr: 0.000486  min_lr: 0.000012  loss: 0.8436 (0.8780)  loss_scale: 65536.0000 (116854.8658)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0001  max mem: 41808
Epoch: [9]  [1060/1349]  eta: 0:01:30  lr: 0.000486  min_lr: 0.000012  loss: 0.8748 (0.8786)  loss_scale: 131072.0000 (116988.8633)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
Epoch: [9]  [1070/1349]  eta: 0:01:26  lr: 0.000486  min_lr: 0.000012  loss: 0.9758 (0.8793)  loss_scale: 131072.0000 (117120.3585)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
Epoch: [9]  [1080/1349]  eta: 0:01:23  lr: 0.000486  min_lr: 0.000012  loss: 0.8846 (0.8788)  loss_scale: 131072.0000 (117249.4209)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
Epoch: [9]  [1090/1349]  eta: 0:01:20  lr: 0.000486  min_lr: 0.000012  loss: 0.7972 (0.8776)  loss_scale: 131072.0000 (117376.1173)  weight_decay: 0.0500 (0.0500)  time: 0.3078  data: 0.0001  max mem: 41808
Epoch: [9]  [1100/1349]  eta: 0:01:17  lr: 0.000486  min_lr: 0.000012  loss: 0.8173 (0.8777)  loss_scale: 131072.0000 (117500.5123)  weight_decay: 0.0500 (0.0500)  time: 0.3082  data: 0.0002  max mem: 41808
Epoch: [9]  [1110/1349]  eta: 0:01:14  lr: 0.000486  min_lr: 0.000012  loss: 0.8805 (0.8779)  loss_scale: 131072.0000 (117622.6679)  weight_decay: 0.0500 (0.0500)  time: 0.3084  data: 0.0001  max mem: 41808
Epoch: [9]  [1120/1349]  eta: 0:01:11  lr: 0.000486  min_lr: 0.000012  loss: 0.8371 (0.8775)  loss_scale: 131072.0000 (117742.6441)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0001  max mem: 41808
Epoch: [9]  [1130/1349]  eta: 0:01:08  lr: 0.000486  min_lr: 0.000012  loss: 0.8371 (0.8771)  loss_scale: 131072.0000 (117860.4987)  weight_decay: 0.0500 (0.0500)  time: 0.3066  data: 0.0001  max mem: 41808
Epoch: [9]  [1140/1349]  eta: 0:01:05  lr: 0.000486  min_lr: 0.000012  loss: 0.9005 (0.8771)  loss_scale: 131072.0000 (117976.2875)  weight_decay: 0.0500 (0.0500)  time: 0.3080  data: 0.0001  max mem: 41808
Epoch: [9]  [1150/1349]  eta: 0:01:01  lr: 0.000486  min_lr: 0.000012  loss: 0.8173 (0.8765)  loss_scale: 131072.0000 (118090.0643)  weight_decay: 0.0500 (0.0500)  time: 0.3084  data: 0.0001  max mem: 41808
Epoch: [9]  [1160/1349]  eta: 0:00:58  lr: 0.000486  min_lr: 0.000012  loss: 0.8173 (0.8767)  loss_scale: 131072.0000 (118201.8811)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
[2025-05-23 16:59:27,296] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 13308
[2025-05-23 16:59:27,296] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 13308
[2025-05-23 16:59:27,296] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 16:59:27,296] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 16:59:27,296] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [9]  [1170/1349]  eta: 0:00:55  lr: 0.000486  min_lr: 0.000012  loss: 0.9571 (0.8773)  loss_scale: 131072.0000 (118087.9249)  weight_decay: 0.0500 (0.0500)  time: 0.3063  data: 0.0001  max mem: 41808
Epoch: [9]  [1180/1349]  eta: 0:00:52  lr: 0.000486  min_lr: 0.000012  loss: 0.9571 (0.8774)  loss_scale: 65536.0000 (117642.9467)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
Epoch: [9]  [1190/1349]  eta: 0:00:49  lr: 0.000486  min_lr: 0.000012  loss: 0.9204 (0.8778)  loss_scale: 65536.0000 (117205.4408)  weight_decay: 0.0500 (0.0500)  time: 0.3089  data: 0.0001  max mem: 41808
Epoch: [9]  [1200/1349]  eta: 0:00:46  lr: 0.000486  min_lr: 0.000012  loss: 0.8732 (0.8774)  loss_scale: 65536.0000 (116775.2206)  weight_decay: 0.0500 (0.0500)  time: 0.3090  data: 0.0001  max mem: 41808
Epoch: [9]  [1210/1349]  eta: 0:00:43  lr: 0.000486  min_lr: 0.000012  loss: 0.8207 (0.8772)  loss_scale: 65536.0000 (116352.1057)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
Epoch: [9]  [1220/1349]  eta: 0:00:40  lr: 0.000486  min_lr: 0.000012  loss: 0.8184 (0.8767)  loss_scale: 65536.0000 (115935.9214)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [9]  [1230/1349]  eta: 0:00:37  lr: 0.000485  min_lr: 0.000012  loss: 0.8374 (0.8773)  loss_scale: 65536.0000 (115526.4988)  weight_decay: 0.0500 (0.0500)  time: 0.3081  data: 0.0002  max mem: 41808
Epoch: [9]  [1240/1349]  eta: 0:00:33  lr: 0.000485  min_lr: 0.000012  loss: 0.8723 (0.8767)  loss_scale: 65536.0000 (115123.6745)  weight_decay: 0.0500 (0.0500)  time: 0.3085  data: 0.0001  max mem: 41808
Epoch: [9]  [1250/1349]  eta: 0:00:30  lr: 0.000485  min_lr: 0.000012  loss: 0.8277 (0.8765)  loss_scale: 65536.0000 (114727.2902)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [9]  [1260/1349]  eta: 0:00:27  lr: 0.000485  min_lr: 0.000012  loss: 0.8407 (0.8764)  loss_scale: 65536.0000 (114337.1927)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0002  max mem: 41808
Epoch: [9]  [1270/1349]  eta: 0:00:24  lr: 0.000485  min_lr: 0.000012  loss: 0.8893 (0.8766)  loss_scale: 65536.0000 (113953.2337)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
Epoch: [9]  [1280/1349]  eta: 0:00:21  lr: 0.000485  min_lr: 0.000012  loss: 0.8893 (0.8765)  loss_scale: 65536.0000 (113575.2693)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0001  max mem: 41808
Epoch: [9]  [1290/1349]  eta: 0:00:18  lr: 0.000485  min_lr: 0.000012  loss: 0.8243 (0.8762)  loss_scale: 65536.0000 (113203.1603)  weight_decay: 0.0500 (0.0500)  time: 0.3062  data: 0.0001  max mem: 41808
[2025-05-23 17:00:06,971] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 17:00:06,971] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-05-23 17:00:06,971] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 17:00:06,971] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [9]  [1300/1349]  eta: 0:00:15  lr: 0.000485  min_lr: 0.000012  loss: 0.8243 (0.8769)  loss_scale: 65536.0000 (113088.6395)  weight_decay: 0.0500 (0.0500)  time: 0.3060  data: 0.0001  max mem: 41808
Epoch: [9]  [1310/1349]  eta: 0:00:12  lr: 0.000485  min_lr: 0.000012  loss: 0.8777 (0.8770)  loss_scale: 131072.0000 (113225.8124)  weight_decay: 0.0500 (0.0500)  time: 0.3088  data: 0.0001  max mem: 41808
Epoch: [9]  [1320/1349]  eta: 0:00:09  lr: 0.000485  min_lr: 0.000012  loss: 0.9182 (0.8770)  loss_scale: 131072.0000 (113360.9084)  weight_decay: 0.0500 (0.0500)  time: 0.3092  data: 0.0002  max mem: 41808
Epoch: [9]  [1330/1349]  eta: 0:00:05  lr: 0.000485  min_lr: 0.000012  loss: 0.8330 (0.8767)  loss_scale: 131072.0000 (113493.9745)  weight_decay: 0.0500 (0.0500)  time: 0.3046  data: 0.0001  max mem: 41808
Epoch: [9]  [1340/1349]  eta: 0:00:02  lr: 0.000485  min_lr: 0.000012  loss: 0.8330 (0.8764)  loss_scale: 131072.0000 (113625.0559)  weight_decay: 0.0500 (0.0500)  time: 0.3021  data: 0.0001  max mem: 41808
Epoch: [9]  [1348/1349]  eta: 0:00:00  lr: 0.000485  min_lr: 0.000012  loss: 0.7925 (0.8762)  loss_scale: 131072.0000 (113728.5219)  weight_decay: 0.0500 (0.0500)  time: 0.3019  data: 0.0001  max mem: 41808
Epoch: [9] Total time: 0:06:59 (0.3111 s / it)
Averaged stats: lr: 0.000485  min_lr: 0.000012  loss: 0.7925 (0.8777)  loss_scale: 131072.0000 (113728.5219)  weight_decay: 0.0500 (0.0500)  total_time: 419.6699 (419.6606)
[2025-05-23 17:00:23,197] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-9 is about to be saved!
[2025-05-23 17:00:23,201] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-9 is ready now!
[2025-05-23 17:00:23,201] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /project/mmendoscope/Natural_Comparison/Cholec75/unified_base_st_settingE_videomae-st_Cholec80_0.0005_0.75_online_key_frame_frame16_Fixed_Stride_4/checkpoint-9/mp_rank_00_model_states.pt
[2025-05-23 17:00:23,201] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /project/mmendoscope/Natural_Comparison/Cholec75/unified_base_st_settingE_videomae-st_Cholec80_0.0005_0.75_online_key_frame_frame16_Fixed_Stride_4/checkpoint-9/mp_rank_00_model_states.pt...
[2025-05-23 17:00:26,223] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /project/mmendoscope/Natural_Comparison/Cholec75/unified_base_st_settingE_videomae-st_Cholec80_0.0005_0.75_online_key_frame_frame16_Fixed_Stride_4/checkpoint-9/mp_rank_00_model_states.pt.
[2025-05-23 17:00:26,223] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-9 is ready now!
Val:  [  0/346]  eta: 0:46:21  loss: 2.1182 (2.1182)  acc1: 18.7500 (18.7500)  acc5: 84.3750 (84.3750)  time: 8.0385  data: 6.9965  max mem: 41808
Val:  [ 10/346]  eta: 0:11:20  loss: 0.1267 (0.5324)  acc1: 100.0000 (84.3040)  acc5: 100.0000 (97.0881)  time: 2.0252  data: 1.2333  max mem: 41808
Val:  [ 20/346]  eta: 0:07:54  loss: 0.1257 (0.4188)  acc1: 100.0000 (88.8765)  acc5: 100.0000 (97.9167)  time: 1.1275  data: 0.3286  max mem: 41808
Val:  [ 30/346]  eta: 0:06:27  loss: 0.0930 (0.3498)  acc1: 100.0000 (91.3306)  acc5: 100.0000 (98.2359)  time: 0.7891  data: 0.0003  max mem: 41808
Val:  [ 40/346]  eta: 0:05:41  loss: 0.0976 (0.3997)  acc1: 100.0000 (89.2721)  acc5: 100.0000 (98.5328)  time: 0.7598  data: 0.0003  max mem: 41808
Val:  [ 50/346]  eta: 0:05:10  loss: 0.0991 (0.3531)  acc1: 99.2188 (90.8395)  acc5: 100.0000 (98.8051)  time: 0.7761  data: 0.0003  max mem: 41808
Val:  [ 60/346]  eta: 0:04:48  loss: 0.1244 (0.3540)  acc1: 99.2188 (90.7659)  acc5: 100.0000 (98.9882)  time: 0.7868  data: 0.0172  max mem: 41808
Val:  [ 70/346]  eta: 0:04:32  loss: 0.1918 (0.3672)  acc1: 96.0938 (90.2179)  acc5: 100.0000 (99.1307)  time: 0.8321  data: 0.0798  max mem: 41808
Val:  [ 80/346]  eta: 0:04:19  loss: 0.2213 (0.3569)  acc1: 94.5312 (90.6250)  acc5: 100.0000 (99.2380)  time: 0.8814  data: 0.1368  max mem: 41808
Val:  [ 90/346]  eta: 0:04:07  loss: 0.2213 (0.3579)  acc1: 95.3125 (90.7109)  acc5: 100.0000 (99.2531)  time: 0.8935  data: 0.1444  max mem: 41808
Val:  [100/346]  eta: 0:03:56  loss: 0.2001 (0.3407)  acc1: 96.0938 (91.4140)  acc5: 100.0000 (99.3270)  time: 0.9053  data: 0.1446  max mem: 41808
Val:  [110/346]  eta: 0:03:45  loss: 0.1968 (0.3782)  acc1: 97.6562 (90.3787)  acc5: 100.0000 (98.9865)  time: 0.9051  data: 0.1484  max mem: 41808
Val:  [120/346]  eta: 0:03:35  loss: 0.1546 (0.3751)  acc1: 97.6562 (90.4184)  acc5: 100.0000 (99.0702)  time: 0.8947  data: 0.1445  max mem: 41808
Val:  [130/346]  eta: 0:03:24  loss: 0.1249 (0.3790)  acc1: 99.2188 (90.2791)  acc5: 100.0000 (99.0995)  time: 0.9016  data: 0.1444  max mem: 41808
Val:  [140/346]  eta: 0:03:13  loss: 0.1601 (0.3679)  acc1: 96.0938 (90.6527)  acc5: 100.0000 (99.1633)  time: 0.8760  data: 0.1460  max mem: 41808
Val:  [150/346]  eta: 0:03:03  loss: 0.2129 (0.3655)  acc1: 96.0938 (90.7906)  acc5: 100.0000 (99.2136)  time: 0.8640  data: 0.1403  max mem: 41808
Val:  [160/346]  eta: 0:02:53  loss: 0.2782 (0.3588)  acc1: 96.0938 (90.9695)  acc5: 100.0000 (99.2624)  time: 0.8628  data: 0.1362  max mem: 41808
Val:  [170/346]  eta: 0:02:43  loss: 0.1201 (0.3575)  acc1: 92.9688 (90.8991)  acc5: 100.0000 (99.3056)  time: 0.8701  data: 0.1382  max mem: 41808
Val:  [180/346]  eta: 0:02:33  loss: 0.3104 (0.3715)  acc1: 89.0625 (90.2883)  acc5: 100.0000 (99.2705)  time: 0.8984  data: 0.1518  max mem: 41808
Val:  [190/346]  eta: 0:02:24  loss: 0.3104 (0.3676)  acc1: 89.8438 (90.3673)  acc5: 100.0000 (99.2883)  time: 0.9095  data: 0.1564  max mem: 41808
Val:  [200/346]  eta: 0:02:15  loss: 0.3637 (0.3789)  acc1: 89.8438 (89.8321)  acc5: 100.0000 (99.3081)  time: 0.8999  data: 0.1480  max mem: 41808
Val:  [210/346]  eta: 0:02:05  loss: 0.1353 (0.3704)  acc1: 96.8750 (90.1733)  acc5: 100.0000 (99.3409)  time: 0.8793  data: 0.1502  max mem: 41808
Val:  [220/346]  eta: 0:01:56  loss: 0.1711 (0.3673)  acc1: 97.6562 (90.2432)  acc5: 100.0000 (99.3495)  time: 0.8853  data: 0.1551  max mem: 41808
Val:  [230/346]  eta: 0:01:46  loss: 0.1383 (0.3584)  acc1: 99.2188 (90.6013)  acc5: 100.0000 (99.3676)  time: 0.9040  data: 0.1635  max mem: 41808
Val:  [240/346]  eta: 0:01:37  loss: 0.1427 (0.3706)  acc1: 99.2188 (90.2360)  acc5: 100.0000 (99.3549)  time: 0.9152  data: 0.1621  max mem: 41808
Val:  [250/346]  eta: 0:01:28  loss: 0.1975 (0.3675)  acc1: 94.5312 (90.3418)  acc5: 100.0000 (99.3744)  time: 0.8953  data: 0.1567  max mem: 41808
Val:  [260/346]  eta: 0:01:18  loss: 0.1645 (0.3670)  acc1: 99.2188 (90.3406)  acc5: 100.0000 (99.3744)  time: 0.8852  data: 0.1520  max mem: 41808
Val:  [270/346]  eta: 0:01:09  loss: 0.1229 (0.3616)  acc1: 99.2188 (90.5327)  acc5: 100.0000 (99.3946)  time: 0.8947  data: 0.1508  max mem: 41808
Val:  [280/346]  eta: 0:01:00  loss: 0.1114 (0.3572)  acc1: 100.0000 (90.6639)  acc5: 100.0000 (99.4022)  time: 0.8967  data: 0.1556  max mem: 41808
Val:  [290/346]  eta: 0:00:51  loss: 0.0939 (0.3513)  acc1: 100.0000 (90.8532)  acc5: 100.0000 (99.4228)  time: 0.8997  data: 0.1583  max mem: 41808
Val:  [300/346]  eta: 0:00:42  loss: 0.1001 (0.3534)  acc1: 100.0000 (90.8534)  acc5: 100.0000 (99.2733)  time: 0.9014  data: 0.1624  max mem: 41808
Val:  [310/346]  eta: 0:00:32  loss: 0.1169 (0.3525)  acc1: 96.8750 (90.8762)  acc5: 100.0000 (99.2916)  time: 0.8997  data: 0.1502  max mem: 41808
Val:  [320/346]  eta: 0:00:23  loss: 0.1169 (0.3504)  acc1: 100.0000 (90.9122)  acc5: 100.0000 (99.3137)  time: 0.8942  data: 0.1367  max mem: 41808
Val:  [330/346]  eta: 0:00:14  loss: 0.3419 (0.3561)  acc1: 88.2812 (90.7194)  acc5: 100.0000 (99.3014)  time: 0.9013  data: 0.1418  max mem: 41808
Val:  [340/346]  eta: 0:00:05  loss: 0.4040 (0.3685)  acc1: 88.2812 (90.3272)  acc5: 100.0000 (99.2875)  time: 0.9162  data: 0.1596  max mem: 41808
Val:  [345/346]  eta: 0:00:00  loss: 0.1688 (0.3651)  acc1: 96.8750 (90.4445)  acc5: 100.0000 (99.2971)  time: 0.9063  data: 0.1663  max mem: 41808
Val: Total time: 0:05:16 (0.9150 s / it)
* Acc@1 90.525 Acc@5 99.320 loss 0.364
Accuracy of the network on the 88494 val videos: 90.5%
Max accuracy: 91.25%   Max Epoch: 3
Epoch: [10]  [   0/1349]  eta: 1:47:14  lr: 0.000485  min_lr: 0.000012  loss: 1.0518 (1.0518)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 4.7700  data: 3.0675  max mem: 41808
Epoch: [10]  [  10/1349]  eta: 0:16:10  lr: 0.000485  min_lr: 0.000012  loss: 0.9034 (0.8694)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7250  data: 0.2790  max mem: 41808
Epoch: [10]  [  20/1349]  eta: 0:11:41  lr: 0.000485  min_lr: 0.000012  loss: 0.8574 (0.8609)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3156  data: 0.0001  max mem: 41808
Epoch: [10]  [  30/1349]  eta: 0:10:01  lr: 0.000485  min_lr: 0.000012  loss: 0.8596 (0.8462)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3086  data: 0.0001  max mem: 41808
Epoch: [10]  [  40/1349]  eta: 0:09:09  lr: 0.000485  min_lr: 0.000012  loss: 0.8352 (0.8466)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3062  data: 0.0001  max mem: 41808
Epoch: [10]  [  50/1349]  eta: 0:08:36  lr: 0.000485  min_lr: 0.000012  loss: 0.8600 (0.8627)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3059  data: 0.0001  max mem: 41808
Epoch: [10]  [  60/1349]  eta: 0:08:12  lr: 0.000485  min_lr: 0.000012  loss: 0.8836 (0.8525)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3061  data: 0.0001  max mem: 41808
Epoch: [10]  [  70/1349]  eta: 0:07:55  lr: 0.000485  min_lr: 0.000012  loss: 0.8836 (0.8560)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0001  max mem: 41808
[2025-05-23 17:06:10,768] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 17:06:10,768] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 17:06:10,768] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 17:06:10,768] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 17:06:11,071] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 13566
[2025-05-23 17:06:11,071] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 13566
[2025-05-23 17:06:11,071] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 17:06:11,071] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 17:06:11,071] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [10]  [  80/1349]  eta: 0:07:41  lr: 0.000485  min_lr: 0.000012  loss: 0.8837 (0.8590)  loss_scale: 131072.0000 (132690.1728)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0001  max mem: 41808
Epoch: [10]  [  90/1349]  eta: 0:07:30  lr: 0.000485  min_lr: 0.000012  loss: 0.8833 (0.8571)  loss_scale: 131072.0000 (132512.3516)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
Epoch: [10]  [ 100/1349]  eta: 0:07:20  lr: 0.000485  min_lr: 0.000012  loss: 0.8445 (0.8589)  loss_scale: 131072.0000 (132369.7426)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0002  max mem: 41808
Epoch: [10]  [ 110/1349]  eta: 0:07:11  lr: 0.000484  min_lr: 0.000012  loss: 0.8529 (0.8654)  loss_scale: 131072.0000 (132252.8288)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
Epoch: [10]  [ 120/1349]  eta: 0:07:04  lr: 0.000484  min_lr: 0.000012  loss: 0.9351 (0.8755)  loss_scale: 131072.0000 (132155.2397)  weight_decay: 0.0500 (0.0500)  time: 0.3080  data: 0.0001  max mem: 41808
Epoch: [10]  [ 130/1349]  eta: 0:06:57  lr: 0.000484  min_lr: 0.000012  loss: 0.9459 (0.8783)  loss_scale: 131072.0000 (132072.5496)  weight_decay: 0.0500 (0.0500)  time: 0.3083  data: 0.0001  max mem: 41808
Epoch: [10]  [ 140/1349]  eta: 0:06:50  lr: 0.000484  min_lr: 0.000012  loss: 0.9061 (0.8788)  loss_scale: 131072.0000 (132001.5887)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
Epoch: [10]  [ 150/1349]  eta: 0:06:44  lr: 0.000484  min_lr: 0.000012  loss: 0.8224 (0.8670)  loss_scale: 131072.0000 (131940.0265)  weight_decay: 0.0500 (0.0500)  time: 0.3078  data: 0.0001  max mem: 41808
Epoch: [10]  [ 160/1349]  eta: 0:06:39  lr: 0.000484  min_lr: 0.000012  loss: 0.8819 (0.8720)  loss_scale: 131072.0000 (131886.1118)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [10]  [ 170/1349]  eta: 0:06:33  lr: 0.000484  min_lr: 0.000012  loss: 0.9328 (0.8717)  loss_scale: 131072.0000 (131838.5029)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0002  max mem: 41808
Epoch: [10]  [ 180/1349]  eta: 0:06:28  lr: 0.000484  min_lr: 0.000012  loss: 0.8417 (0.8695)  loss_scale: 131072.0000 (131796.1547)  weight_decay: 0.0500 (0.0500)  time: 0.3086  data: 0.0002  max mem: 41808
Epoch: [10]  [ 190/1349]  eta: 0:06:24  lr: 0.000484  min_lr: 0.000012  loss: 0.8974 (0.8690)  loss_scale: 131072.0000 (131758.2408)  weight_decay: 0.0500 (0.0500)  time: 0.3081  data: 0.0001  max mem: 41808
Epoch: [10]  [ 200/1349]  eta: 0:06:19  lr: 0.000484  min_lr: 0.000011  loss: 0.9087 (0.8683)  loss_scale: 131072.0000 (131724.0995)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0001  max mem: 41808
[2025-05-23 17:06:50,748] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 17:06:50,748] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 17:06:50,748] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 17:06:50,748] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 17:06:51,663] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 13698
[2025-05-23 17:06:51,663] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 13698
[2025-05-23 17:06:51,663] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 17:06:51,663] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 17:06:51,663] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [10]  [ 210/1349]  eta: 0:06:14  lr: 0.000484  min_lr: 0.000011  loss: 0.8598 (0.8675)  loss_scale: 131072.0000 (133556.7773)  weight_decay: 0.0500 (0.0500)  time: 0.3062  data: 0.0001  max mem: 41808
Epoch: [10]  [ 220/1349]  eta: 0:06:10  lr: 0.000484  min_lr: 0.000011  loss: 0.8598 (0.8665)  loss_scale: 131072.0000 (133444.3439)  weight_decay: 0.0500 (0.0500)  time: 0.3057  data: 0.0001  max mem: 41808
Epoch: [10]  [ 230/1349]  eta: 0:06:05  lr: 0.000484  min_lr: 0.000011  loss: 0.9039 (0.8675)  loss_scale: 131072.0000 (133341.6450)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [10]  [ 240/1349]  eta: 0:06:01  lr: 0.000484  min_lr: 0.000011  loss: 0.8239 (0.8653)  loss_scale: 131072.0000 (133247.4689)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0003  max mem: 41808
Epoch: [10]  [ 250/1349]  eta: 0:05:57  lr: 0.000484  min_lr: 0.000011  loss: 0.8906 (0.8712)  loss_scale: 131072.0000 (133160.7968)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0003  max mem: 41808
Epoch: [10]  [ 260/1349]  eta: 0:05:53  lr: 0.000484  min_lr: 0.000011  loss: 1.0049 (0.8743)  loss_scale: 131072.0000 (133080.7663)  weight_decay: 0.0500 (0.0500)  time: 0.3084  data: 0.0002  max mem: 41808
Epoch: [10]  [ 270/1349]  eta: 0:05:49  lr: 0.000484  min_lr: 0.000011  loss: 0.9059 (0.8755)  loss_scale: 131072.0000 (133006.6421)  weight_decay: 0.0500 (0.0500)  time: 0.3083  data: 0.0001  max mem: 41808
Epoch: [10]  [ 280/1349]  eta: 0:05:45  lr: 0.000484  min_lr: 0.000011  loss: 0.8833 (0.8752)  loss_scale: 131072.0000 (132937.7936)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
Epoch: [10]  [ 290/1349]  eta: 0:05:42  lr: 0.000484  min_lr: 0.000011  loss: 0.9307 (0.8753)  loss_scale: 131072.0000 (132873.6770)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [10]  [ 300/1349]  eta: 0:05:38  lr: 0.000484  min_lr: 0.000011  loss: 0.9400 (0.8762)  loss_scale: 131072.0000 (132813.8206)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
Epoch: [10]  [ 310/1349]  eta: 0:05:34  lr: 0.000484  min_lr: 0.000011  loss: 0.9340 (0.8771)  loss_scale: 131072.0000 (132757.8135)  weight_decay: 0.0500 (0.0500)  time: 0.3059  data: 0.0001  max mem: 41808
Epoch: [10]  [ 320/1349]  eta: 0:05:30  lr: 0.000484  min_lr: 0.000011  loss: 0.8719 (0.8765)  loss_scale: 131072.0000 (132705.2960)  weight_decay: 0.0500 (0.0500)  time: 0.3059  data: 0.0001  max mem: 41808
Epoch: [10]  [ 330/1349]  eta: 0:05:27  lr: 0.000483  min_lr: 0.000011  loss: 0.8715 (0.8753)  loss_scale: 131072.0000 (132655.9517)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
[2025-05-23 17:07:31,275] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 17:07:31,276] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 17:07:31,276] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 17:07:31,276] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [10]  [ 340/1349]  eta: 0:05:23  lr: 0.000483  min_lr: 0.000011  loss: 0.8118 (0.8742)  loss_scale: 131072.0000 (134147.0029)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
[2025-05-23 17:07:34,050] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 13836
[2025-05-23 17:07:34,050] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 13836
[2025-05-23 17:07:34,050] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 17:07:34,050] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 17:07:34,050] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [10]  [ 350/1349]  eta: 0:05:19  lr: 0.000483  min_lr: 0.000011  loss: 0.8433 (0.8747)  loss_scale: 131072.0000 (135926.5185)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
Epoch: [10]  [ 360/1349]  eta: 0:05:16  lr: 0.000483  min_lr: 0.000011  loss: 0.8423 (0.8744)  loss_scale: 131072.0000 (135792.0443)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [10]  [ 370/1349]  eta: 0:05:12  lr: 0.000483  min_lr: 0.000011  loss: 0.8874 (0.8741)  loss_scale: 131072.0000 (135664.8194)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
Epoch: [10]  [ 380/1349]  eta: 0:05:09  lr: 0.000483  min_lr: 0.000011  loss: 0.8737 (0.8727)  loss_scale: 131072.0000 (135544.2730)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [10]  [ 390/1349]  eta: 0:05:05  lr: 0.000483  min_lr: 0.000011  loss: 0.8042 (0.8726)  loss_scale: 131072.0000 (135429.8926)  weight_decay: 0.0500 (0.0500)  time: 0.3085  data: 0.0001  max mem: 41808
[2025-05-23 17:07:47,886] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 13881
[2025-05-23 17:07:47,886] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 13881
[2025-05-23 17:07:47,886] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 17:07:47,886] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 17:07:47,886] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [10]  [ 400/1349]  eta: 0:05:02  lr: 0.000483  min_lr: 0.000011  loss: 0.9391 (0.8756)  loss_scale: 65536.0000 (133686.9027)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
Epoch: [10]  [ 410/1349]  eta: 0:04:58  lr: 0.000483  min_lr: 0.000011  loss: 0.8854 (0.8758)  loss_scale: 65536.0000 (132028.7299)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0002  max mem: 41808
Epoch: [10]  [ 420/1349]  eta: 0:04:55  lr: 0.000483  min_lr: 0.000011  loss: 0.8828 (0.8763)  loss_scale: 65536.0000 (130449.3302)  weight_decay: 0.0500 (0.0500)  time: 0.3066  data: 0.0001  max mem: 41808
Epoch: [10]  [ 430/1349]  eta: 0:04:52  lr: 0.000483  min_lr: 0.000011  loss: 0.9244 (0.8767)  loss_scale: 65536.0000 (128943.2204)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
Epoch: [10]  [ 440/1349]  eta: 0:04:48  lr: 0.000483  min_lr: 0.000011  loss: 0.9317 (0.8785)  loss_scale: 65536.0000 (127505.4150)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0002  max mem: 41808
Epoch: [10]  [ 450/1349]  eta: 0:04:45  lr: 0.000483  min_lr: 0.000011  loss: 0.9084 (0.8778)  loss_scale: 65536.0000 (126131.3703)  weight_decay: 0.0500 (0.0500)  time: 0.3079  data: 0.0001  max mem: 41808
Epoch: [10]  [ 460/1349]  eta: 0:04:41  lr: 0.000483  min_lr: 0.000011  loss: 0.8585 (0.8794)  loss_scale: 65536.0000 (124816.9371)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [10]  [ 470/1349]  eta: 0:04:38  lr: 0.000483  min_lr: 0.000011  loss: 0.8534 (0.8771)  loss_scale: 65536.0000 (123558.3185)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
Epoch: [10]  [ 480/1349]  eta: 0:04:35  lr: 0.000483  min_lr: 0.000011  loss: 0.8753 (0.8798)  loss_scale: 65536.0000 (122352.0333)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
Epoch: [10]  [ 490/1349]  eta: 0:04:31  lr: 0.000483  min_lr: 0.000011  loss: 0.9241 (0.8793)  loss_scale: 65536.0000 (121194.8839)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
Epoch: [10]  [ 500/1349]  eta: 0:04:28  lr: 0.000483  min_lr: 0.000011  loss: 0.9046 (0.8784)  loss_scale: 65536.0000 (120083.9281)  weight_decay: 0.0500 (0.0500)  time: 0.3062  data: 0.0001  max mem: 41808
[2025-05-23 17:08:24,148] [INFO] [logging.py:96:log_dist] [Rank 0] step=14000, skipped=83, lr=[1.1465835096791216e-05, 1.1465835096791216e-05, 1.5287780129054954e-05, 1.5287780129054954e-05, 2.0383706838739938e-05, 2.0383706838739938e-05, 2.7178275784986586e-05, 2.7178275784986586e-05, 3.6237701046648784e-05, 3.6237701046648784e-05, 4.831693472886504e-05, 4.831693472886504e-05, 6.442257963848672e-05, 6.442257963848672e-05, 8.589677285131562e-05, 8.589677285131562e-05, 0.00011452903046842084, 0.00011452903046842084, 0.00015270537395789446, 0.00015270537395789446, 0.0002036071652771926, 0.0002036071652771926, 0.00027147622036959013, 0.00027147622036959013, 0.0003619682938261202, 0.0003619682938261202, 0.00048262439176816025, 0.00048262439176816025], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-05-23 17:08:24,149] [INFO] [timer.py:260:stop] epoch=0/micro_step=14000/global_step=14000, RunningAvgSamplesPerSec=203.56929373852154, CurrSamplesPerSec=213.78990191174924, MemAllocated=1.37GB, MaxMemAllocated=40.83GB
Epoch: [10]  [ 510/1349]  eta: 0:04:25  lr: 0.000483  min_lr: 0.000011  loss: 0.9562 (0.8805)  loss_scale: 65536.0000 (119016.4540)  weight_decay: 0.0500 (0.0500)  time: 0.3078  data: 0.0001  max mem: 41808
[2025-05-23 17:08:27,518] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 17:08:27,518] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-05-23 17:08:27,518] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 17:08:27,518] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [10]  [ 520/1349]  eta: 0:04:21  lr: 0.000483  min_lr: 0.000011  loss: 0.9420 (0.8802)  loss_scale: 65536.0000 (118115.7466)  weight_decay: 0.0500 (0.0500)  time: 0.3082  data: 0.0001  max mem: 41808
Epoch: [10]  [ 530/1349]  eta: 0:04:18  lr: 0.000483  min_lr: 0.000011  loss: 0.9057 (0.8804)  loss_scale: 131072.0000 (118359.7439)  weight_decay: 0.0500 (0.0500)  time: 0.3063  data: 0.0001  max mem: 41808
Epoch: [10]  [ 540/1349]  eta: 0:04:15  lr: 0.000482  min_lr: 0.000011  loss: 0.8908 (0.8807)  loss_scale: 131072.0000 (118594.7209)  weight_decay: 0.0500 (0.0500)  time: 0.3066  data: 0.0001  max mem: 41808
Epoch: [10]  [ 550/1349]  eta: 0:04:12  lr: 0.000482  min_lr: 0.000011  loss: 0.9213 (0.8816)  loss_scale: 131072.0000 (118821.1688)  weight_decay: 0.0500 (0.0500)  time: 0.3081  data: 0.0001  max mem: 41808
Epoch: [10]  [ 560/1349]  eta: 0:04:08  lr: 0.000482  min_lr: 0.000011  loss: 0.9213 (0.8824)  loss_scale: 131072.0000 (119039.5437)  weight_decay: 0.0500 (0.0500)  time: 0.3083  data: 0.0001  max mem: 41808
Epoch: [10]  [ 570/1349]  eta: 0:04:05  lr: 0.000482  min_lr: 0.000011  loss: 0.8682 (0.8811)  loss_scale: 131072.0000 (119250.2697)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
[2025-05-23 17:08:45,357] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 14068
[2025-05-23 17:08:45,357] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 14068
[2025-05-23 17:08:45,357] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 17:08:45,357] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 17:08:45,357] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [10]  [ 580/1349]  eta: 0:04:02  lr: 0.000482  min_lr: 0.000011  loss: 0.8396 (0.8813)  loss_scale: 131072.0000 (119115.3460)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [10]  [ 590/1349]  eta: 0:03:59  lr: 0.000482  min_lr: 0.000011  loss: 0.9181 (0.8822)  loss_scale: 65536.0000 (118208.7580)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [10]  [ 600/1349]  eta: 0:03:55  lr: 0.000482  min_lr: 0.000011  loss: 0.9444 (0.8818)  loss_scale: 65536.0000 (117332.3394)  weight_decay: 0.0500 (0.0500)  time: 0.3079  data: 0.0001  max mem: 41808
Epoch: [10]  [ 610/1349]  eta: 0:03:52  lr: 0.000482  min_lr: 0.000011  loss: 0.9577 (0.8818)  loss_scale: 65536.0000 (116484.6088)  weight_decay: 0.0500 (0.0500)  time: 0.3086  data: 0.0001  max mem: 41808
Epoch: [10]  [ 620/1349]  eta: 0:03:49  lr: 0.000482  min_lr: 0.000011  loss: 0.9577 (0.8822)  loss_scale: 65536.0000 (115664.1804)  weight_decay: 0.0500 (0.0500)  time: 0.3085  data: 0.0002  max mem: 41808
Epoch: [10]  [ 630/1349]  eta: 0:03:46  lr: 0.000482  min_lr: 0.000011  loss: 0.8644 (0.8823)  loss_scale: 65536.0000 (114869.7559)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [10]  [ 640/1349]  eta: 0:03:42  lr: 0.000482  min_lr: 0.000011  loss: 0.9117 (0.8829)  loss_scale: 65536.0000 (114100.1186)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
Epoch: [10]  [ 650/1349]  eta: 0:03:39  lr: 0.000482  min_lr: 0.000011  loss: 0.9250 (0.8832)  loss_scale: 65536.0000 (113354.1260)  weight_decay: 0.0500 (0.0500)  time: 0.3066  data: 0.0001  max mem: 41808
Epoch: [10]  [ 660/1349]  eta: 0:03:36  lr: 0.000482  min_lr: 0.000011  loss: 0.9904 (0.8847)  loss_scale: 65536.0000 (112630.7050)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0001  max mem: 41808
Epoch: [10]  [ 670/1349]  eta: 0:03:33  lr: 0.000482  min_lr: 0.000011  loss: 0.9574 (0.8849)  loss_scale: 65536.0000 (111928.8465)  weight_decay: 0.0500 (0.0500)  time: 0.3085  data: 0.0001  max mem: 41808
Epoch: [10]  [ 680/1349]  eta: 0:03:30  lr: 0.000482  min_lr: 0.000011  loss: 0.7942 (0.8830)  loss_scale: 65536.0000 (111247.6006)  weight_decay: 0.0500 (0.0500)  time: 0.3091  data: 0.0001  max mem: 41808
Epoch: [10]  [ 690/1349]  eta: 0:03:26  lr: 0.000482  min_lr: 0.000011  loss: 0.7942 (0.8831)  loss_scale: 65536.0000 (110586.0724)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [10]  [ 700/1349]  eta: 0:03:23  lr: 0.000482  min_lr: 0.000011  loss: 0.8667 (0.8816)  loss_scale: 65536.0000 (109943.4180)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0002  max mem: 41808
[2025-05-23 17:09:25,048] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 17:09:25,049] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-05-23 17:09:25,048] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 17:09:25,049] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [10]  [ 710/1349]  eta: 0:03:20  lr: 0.000482  min_lr: 0.000011  loss: 0.7995 (0.8793)  loss_scale: 65536.0000 (109687.5387)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
Epoch: [10]  [ 720/1349]  eta: 0:03:17  lr: 0.000482  min_lr: 0.000011  loss: 0.8732 (0.8798)  loss_scale: 131072.0000 (109984.1331)  weight_decay: 0.0500 (0.0500)  time: 0.3080  data: 0.0001  max mem: 41808
Epoch: [10]  [ 730/1349]  eta: 0:03:14  lr: 0.000482  min_lr: 0.000011  loss: 0.9341 (0.8803)  loss_scale: 131072.0000 (110272.6129)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
Epoch: [10]  [ 740/1349]  eta: 0:03:10  lr: 0.000482  min_lr: 0.000011  loss: 0.8417 (0.8787)  loss_scale: 131072.0000 (110553.3063)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
Epoch: [10]  [ 750/1349]  eta: 0:03:07  lr: 0.000481  min_lr: 0.000011  loss: 0.8037 (0.8794)  loss_scale: 131072.0000 (110826.5246)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
Epoch: [10]  [ 760/1349]  eta: 0:03:04  lr: 0.000481  min_lr: 0.000011  loss: 0.9124 (0.8802)  loss_scale: 131072.0000 (111092.5624)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
Epoch: [10]  [ 770/1349]  eta: 0:03:01  lr: 0.000481  min_lr: 0.000011  loss: 0.9078 (0.8801)  loss_scale: 131072.0000 (111351.6991)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
Epoch: [10]  [ 780/1349]  eta: 0:02:58  lr: 0.000481  min_lr: 0.000011  loss: 0.8468 (0.8793)  loss_scale: 131072.0000 (111604.1997)  weight_decay: 0.0500 (0.0500)  time: 0.3089  data: 0.0002  max mem: 41808
Epoch: [10]  [ 790/1349]  eta: 0:02:55  lr: 0.000481  min_lr: 0.000011  loss: 0.8561 (0.8790)  loss_scale: 131072.0000 (111850.3161)  weight_decay: 0.0500 (0.0500)  time: 0.3089  data: 0.0002  max mem: 41808
Epoch: [10]  [ 800/1349]  eta: 0:02:51  lr: 0.000481  min_lr: 0.000011  loss: 0.7935 (0.8775)  loss_scale: 131072.0000 (112090.2871)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
Epoch: [10]  [ 810/1349]  eta: 0:02:48  lr: 0.000481  min_lr: 0.000011  loss: 0.8284 (0.8780)  loss_scale: 131072.0000 (112324.3403)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [10]  [ 820/1349]  eta: 0:02:45  lr: 0.000481  min_lr: 0.000011  loss: 0.8378 (0.8767)  loss_scale: 131072.0000 (112552.6918)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
Epoch: [10]  [ 830/1349]  eta: 0:02:42  lr: 0.000481  min_lr: 0.000011  loss: 0.8378 (0.8764)  loss_scale: 131072.0000 (112775.5475)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
[2025-05-23 17:10:04,439] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 17:10:04,439] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 17:10:04,439] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 17:10:04,439] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 17:10:05,391] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 14328
[2025-05-23 17:10:05,391] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 14328
[2025-05-23 17:10:05,391] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 17:10:05,391] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 17:10:05,391] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [10]  [ 840/1349]  eta: 0:02:39  lr: 0.000481  min_lr: 0.000011  loss: 0.9066 (0.8756)  loss_scale: 131072.0000 (113460.6611)  weight_decay: 0.0500 (0.0500)  time: 0.3085  data: 0.0001  max mem: 41808
Epoch: [10]  [ 850/1349]  eta: 0:02:36  lr: 0.000481  min_lr: 0.000011  loss: 0.8000 (0.8747)  loss_scale: 131072.0000 (113667.6099)  weight_decay: 0.0500 (0.0500)  time: 0.3083  data: 0.0001  max mem: 41808
Epoch: [10]  [ 860/1349]  eta: 0:02:32  lr: 0.000481  min_lr: 0.000011  loss: 0.7910 (0.8740)  loss_scale: 131072.0000 (113869.7515)  weight_decay: 0.0500 (0.0500)  time: 0.3127  data: 0.0001  max mem: 41808
Epoch: [10]  [ 870/1349]  eta: 0:02:29  lr: 0.000481  min_lr: 0.000011  loss: 0.8143 (0.8728)  loss_scale: 131072.0000 (114067.2514)  weight_decay: 0.0500 (0.0500)  time: 0.3128  data: 0.0001  max mem: 41808
Epoch: [10]  [ 880/1349]  eta: 0:02:26  lr: 0.000481  min_lr: 0.000011  loss: 0.8543 (0.8727)  loss_scale: 131072.0000 (114260.2679)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [10]  [ 890/1349]  eta: 0:02:23  lr: 0.000481  min_lr: 0.000011  loss: 0.8678 (0.8725)  loss_scale: 131072.0000 (114448.9517)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
Epoch: [10]  [ 900/1349]  eta: 0:02:20  lr: 0.000481  min_lr: 0.000011  loss: 0.8651 (0.8722)  loss_scale: 131072.0000 (114633.4473)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0001  max mem: 41808
Epoch: [10]  [ 910/1349]  eta: 0:02:17  lr: 0.000481  min_lr: 0.000011  loss: 0.9484 (0.8734)  loss_scale: 131072.0000 (114813.8924)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0001  max mem: 41808
Epoch: [10]  [ 920/1349]  eta: 0:02:14  lr: 0.000481  min_lr: 0.000011  loss: 0.9441 (0.8736)  loss_scale: 131072.0000 (114990.4191)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [10]  [ 930/1349]  eta: 0:02:10  lr: 0.000481  min_lr: 0.000011  loss: 0.9206 (0.8735)  loss_scale: 131072.0000 (115163.1536)  weight_decay: 0.0500 (0.0500)  time: 0.3064  data: 0.0002  max mem: 41808
Epoch: [10]  [ 940/1349]  eta: 0:02:07  lr: 0.000481  min_lr: 0.000011  loss: 0.9206 (0.8740)  loss_scale: 131072.0000 (115332.2168)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0002  max mem: 41808
Epoch: [10]  [ 950/1349]  eta: 0:02:04  lr: 0.000480  min_lr: 0.000011  loss: 0.8770 (0.8740)  loss_scale: 131072.0000 (115497.7245)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [10]  [ 960/1349]  eta: 0:02:01  lr: 0.000480  min_lr: 0.000011  loss: 0.8252 (0.8739)  loss_scale: 131072.0000 (115659.7877)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
[2025-05-23 17:10:45,103] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 17:10:45,103] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 17:10:45,103] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 17:10:45,103] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [10]  [ 970/1349]  eta: 0:01:58  lr: 0.000480  min_lr: 0.000011  loss: 0.8661 (0.8734)  loss_scale: 131072.0000 (116358.4593)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
[2025-05-23 17:10:46,329] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 14461
[2025-05-23 17:10:46,329] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 14461
[2025-05-23 17:10:46,329] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 17:10:46,329] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 17:10:46,329] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [10]  [ 980/1349]  eta: 0:01:55  lr: 0.000480  min_lr: 0.000011  loss: 0.8836 (0.8735)  loss_scale: 131072.0000 (116508.4444)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [10]  [ 990/1349]  eta: 0:01:52  lr: 0.000480  min_lr: 0.000011  loss: 0.8909 (0.8739)  loss_scale: 131072.0000 (116655.4026)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [10]  [1000/1349]  eta: 0:01:48  lr: 0.000480  min_lr: 0.000011  loss: 0.8909 (0.8734)  loss_scale: 131072.0000 (116799.4246)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
[2025-05-23 17:10:57,388] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 14497
[2025-05-23 17:10:57,388] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 14497
[2025-05-23 17:10:57,389] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 17:10:57,389] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 17:10:57,389] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [10]  [1010/1349]  eta: 0:01:45  lr: 0.000480  min_lr: 0.000011  loss: 0.8562 (0.8738)  loss_scale: 131072.0000 (116681.3056)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [10]  [1020/1349]  eta: 0:01:42  lr: 0.000480  min_lr: 0.000011  loss: 0.9092 (0.8747)  loss_scale: 65536.0000 (116180.3722)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
Epoch: [10]  [1030/1349]  eta: 0:01:39  lr: 0.000480  min_lr: 0.000011  loss: 0.9092 (0.8743)  loss_scale: 65536.0000 (115689.1562)  weight_decay: 0.0500 (0.0500)  time: 0.3084  data: 0.0001  max mem: 41808
Epoch: [10]  [1040/1349]  eta: 0:01:36  lr: 0.000480  min_lr: 0.000011  loss: 0.8663 (0.8740)  loss_scale: 65536.0000 (115207.3775)  weight_decay: 0.0500 (0.0500)  time: 0.3079  data: 0.0001  max mem: 41808
Epoch: [10]  [1050/1349]  eta: 0:01:33  lr: 0.000480  min_lr: 0.000011  loss: 0.9458 (0.8739)  loss_scale: 65536.0000 (114734.7669)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
Epoch: [10]  [1060/1349]  eta: 0:01:30  lr: 0.000480  min_lr: 0.000011  loss: 0.8851 (0.8738)  loss_scale: 65536.0000 (114271.0650)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [10]  [1070/1349]  eta: 0:01:26  lr: 0.000480  min_lr: 0.000011  loss: 0.8851 (0.8741)  loss_scale: 65536.0000 (113816.0224)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [10]  [1080/1349]  eta: 0:01:23  lr: 0.000480  min_lr: 0.000011  loss: 0.8693 (0.8737)  loss_scale: 65536.0000 (113369.3987)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [10]  [1090/1349]  eta: 0:01:20  lr: 0.000480  min_lr: 0.000011  loss: 0.8839 (0.8737)  loss_scale: 65536.0000 (112930.9624)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
Epoch: [10]  [1100/1349]  eta: 0:01:17  lr: 0.000480  min_lr: 0.000011  loss: 0.8957 (0.8734)  loss_scale: 65536.0000 (112500.4905)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [10]  [1110/1349]  eta: 0:01:14  lr: 0.000480  min_lr: 0.000011  loss: 0.8654 (0.8735)  loss_scale: 65536.0000 (112077.7678)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
Epoch: [10]  [1120/1349]  eta: 0:01:11  lr: 0.000480  min_lr: 0.000011  loss: 0.8630 (0.8734)  loss_scale: 65536.0000 (111662.5870)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
Epoch: [10]  [1130/1349]  eta: 0:01:08  lr: 0.000480  min_lr: 0.000011  loss: 0.8654 (0.8737)  loss_scale: 65536.0000 (111254.7480)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
[2025-05-23 17:11:37,060] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 17:11:37,060] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-05-23 17:11:37,061] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 17:11:37,061] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [10]  [1140/1349]  eta: 0:01:05  lr: 0.000480  min_lr: 0.000011  loss: 0.8254 (0.8724)  loss_scale: 65536.0000 (111141.2445)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
Epoch: [10]  [1150/1349]  eta: 0:01:01  lr: 0.000479  min_lr: 0.000011  loss: 0.7775 (0.8726)  loss_scale: 131072.0000 (111314.4049)  weight_decay: 0.0500 (0.0500)  time: 0.3079  data: 0.0001  max mem: 41808
Epoch: [10]  [1160/1349]  eta: 0:00:58  lr: 0.000479  min_lr: 0.000011  loss: 0.8340 (0.8717)  loss_scale: 131072.0000 (111484.5823)  weight_decay: 0.0500 (0.0500)  time: 0.3079  data: 0.0001  max mem: 41808
Epoch: [10]  [1170/1349]  eta: 0:00:55  lr: 0.000479  min_lr: 0.000011  loss: 0.8505 (0.8711)  loss_scale: 131072.0000 (111651.8531)  weight_decay: 0.0500 (0.0500)  time: 0.3083  data: 0.0001  max mem: 41808
Epoch: [10]  [1180/1349]  eta: 0:00:52  lr: 0.000479  min_lr: 0.000011  loss: 0.8743 (0.8713)  loss_scale: 131072.0000 (111816.2913)  weight_decay: 0.0500 (0.0500)  time: 0.3105  data: 0.0001  max mem: 41808
[2025-05-23 17:11:52,822] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 14677
[2025-05-23 17:11:52,822] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 14677
[2025-05-23 17:11:52,823] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 17:11:52,823] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 17:11:52,823] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [10]  [1190/1349]  eta: 0:00:49  lr: 0.000479  min_lr: 0.000011  loss: 0.8855 (0.8713)  loss_scale: 131072.0000 (111757.8640)  weight_decay: 0.0500 (0.0500)  time: 0.3097  data: 0.0002  max mem: 41808
Epoch: [10]  [1200/1349]  eta: 0:00:46  lr: 0.000479  min_lr: 0.000011  loss: 0.8555 (0.8714)  loss_scale: 65536.0000 (111373.0025)  weight_decay: 0.0500 (0.0500)  time: 0.3079  data: 0.0002  max mem: 41808
Epoch: [10]  [1210/1349]  eta: 0:00:43  lr: 0.000479  min_lr: 0.000011  loss: 0.9077 (0.8722)  loss_scale: 65536.0000 (110994.4971)  weight_decay: 0.0500 (0.0500)  time: 0.3088  data: 0.0001  max mem: 41808
Epoch: [10]  [1220/1349]  eta: 0:00:40  lr: 0.000479  min_lr: 0.000011  loss: 0.9681 (0.8726)  loss_scale: 65536.0000 (110622.1916)  weight_decay: 0.0500 (0.0500)  time: 0.3079  data: 0.0001  max mem: 41808
Epoch: [10]  [1230/1349]  eta: 0:00:37  lr: 0.000479  min_lr: 0.000011  loss: 0.8985 (0.8726)  loss_scale: 65536.0000 (110255.9350)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
Epoch: [10]  [1240/1349]  eta: 0:00:33  lr: 0.000479  min_lr: 0.000011  loss: 0.8552 (0.8726)  loss_scale: 65536.0000 (109895.5810)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [10]  [1250/1349]  eta: 0:00:30  lr: 0.000479  min_lr: 0.000011  loss: 0.8642 (0.8726)  loss_scale: 65536.0000 (109540.9880)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
Epoch: [10]  [1260/1349]  eta: 0:00:27  lr: 0.000479  min_lr: 0.000011  loss: 0.8510 (0.8722)  loss_scale: 65536.0000 (109192.0190)  weight_decay: 0.0500 (0.0500)  time: 0.3066  data: 0.0001  max mem: 41808
Epoch: [10]  [1270/1349]  eta: 0:00:24  lr: 0.000479  min_lr: 0.000011  loss: 0.8929 (0.8722)  loss_scale: 65536.0000 (108848.5413)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
Epoch: [10]  [1280/1349]  eta: 0:00:21  lr: 0.000479  min_lr: 0.000011  loss: 0.9663 (0.8730)  loss_scale: 65536.0000 (108510.4262)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [10]  [1290/1349]  eta: 0:00:18  lr: 0.000479  min_lr: 0.000011  loss: 0.9005 (0.8725)  loss_scale: 65536.0000 (108177.5492)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [10]  [1300/1349]  eta: 0:00:15  lr: 0.000479  min_lr: 0.000011  loss: 0.7634 (0.8716)  loss_scale: 65536.0000 (107849.7894)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
Epoch: [10]  [1310/1349]  eta: 0:00:12  lr: 0.000479  min_lr: 0.000011  loss: 0.8588 (0.8714)  loss_scale: 65536.0000 (107527.0297)  weight_decay: 0.0500 (0.0500)  time: 0.3060  data: 0.0001  max mem: 41808
[2025-05-23 17:12:32,446] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 17:12:32,446] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-05-23 17:12:32,446] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 17:12:32,447] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [10]  [1320/1349]  eta: 0:00:09  lr: 0.000479  min_lr: 0.000011  loss: 0.8601 (0.8711)  loss_scale: 65536.0000 (107457.2112)  weight_decay: 0.0500 (0.0500)  time: 0.3056  data: 0.0001  max mem: 41808
Epoch: [10]  [1330/1349]  eta: 0:00:05  lr: 0.000479  min_lr: 0.000011  loss: 0.8918 (0.8716)  loss_scale: 131072.0000 (107634.6326)  weight_decay: 0.0500 (0.0500)  time: 0.3041  data: 0.0001  max mem: 41808
Epoch: [10]  [1340/1349]  eta: 0:00:02  lr: 0.000478  min_lr: 0.000011  loss: 0.9173 (0.8715)  loss_scale: 131072.0000 (107809.4079)  weight_decay: 0.0500 (0.0500)  time: 0.3019  data: 0.0001  max mem: 41808
Epoch: [10]  [1348/1349]  eta: 0:00:00  lr: 0.000478  min_lr: 0.000011  loss: 0.8737 (0.8712)  loss_scale: 131072.0000 (107947.3625)  weight_decay: 0.0500 (0.0500)  time: 0.3016  data: 0.0001  max mem: 41808
Epoch: [10] Total time: 0:06:59 (0.3111 s / it)
Averaged stats: lr: 0.000478  min_lr: 0.000011  loss: 0.8737 (0.8718)  loss_scale: 131072.0000 (107947.3625)  weight_decay: 0.0500 (0.0500)  total_time: 419.6113 (419.6063)
Val:  [  0/346]  eta: 0:45:23  loss: 1.9450 (1.9450)  acc1: 21.0938 (21.0938)  acc5: 86.7188 (86.7188)  time: 7.8721  data: 6.9919  max mem: 41808
Val:  [ 10/346]  eta: 0:10:45  loss: 0.1480 (0.4430)  acc1: 100.0000 (87.8551)  acc5: 100.0000 (98.7216)  time: 1.9221  data: 1.0694  max mem: 41808
Val:  [ 20/346]  eta: 0:07:34  loss: 0.1418 (0.3514)  acc1: 100.0000 (91.0342)  acc5: 100.0000 (99.3304)  time: 1.0693  data: 0.2445  max mem: 41808
Val:  [ 30/346]  eta: 0:06:24  loss: 0.1282 (0.3315)  acc1: 100.0000 (92.2379)  acc5: 100.0000 (99.4204)  time: 0.8311  data: 0.0299  max mem: 41808
Val:  [ 40/346]  eta: 0:05:38  loss: 0.1434 (0.3883)  acc1: 98.4375 (90.7393)  acc5: 100.0000 (99.3140)  time: 0.8082  data: 0.0242  max mem: 41808
Val:  [ 50/346]  eta: 0:05:10  loss: 0.1321 (0.3507)  acc1: 100.0000 (92.0343)  acc5: 100.0000 (99.4179)  time: 0.7851  data: 0.0381  max mem: 41808
Val:  [ 60/346]  eta: 0:04:52  loss: 0.1688 (0.3683)  acc1: 96.8750 (91.3038)  acc5: 100.0000 (99.5005)  time: 0.8490  data: 0.1122  max mem: 41808
Val:  [ 70/346]  eta: 0:04:38  loss: 0.2932 (0.3807)  acc1: 91.4062 (90.6030)  acc5: 100.0000 (99.5489)  time: 0.9059  data: 0.1406  max mem: 41808
Val:  [ 80/346]  eta: 0:04:23  loss: 0.2416 (0.3827)  acc1: 92.9688 (90.7504)  acc5: 100.0000 (99.5081)  time: 0.8946  data: 0.1387  max mem: 41808
Val:  [ 90/346]  eta: 0:04:10  loss: 0.2269 (0.3823)  acc1: 94.5312 (90.7194)  acc5: 100.0000 (99.5450)  time: 0.8723  data: 0.1439  max mem: 41808
Val:  [100/346]  eta: 0:03:58  loss: 0.2030 (0.3610)  acc1: 97.6562 (91.5842)  acc5: 100.0000 (99.5900)  time: 0.8719  data: 0.1379  max mem: 41808
Val:  [110/346]  eta: 0:03:46  loss: 0.1987 (0.3755)  acc1: 98.4375 (91.1247)  acc5: 100.0000 (99.5284)  time: 0.8786  data: 0.1383  max mem: 41808
Val:  [120/346]  eta: 0:03:35  loss: 0.2578 (0.3767)  acc1: 93.7500 (91.0511)  acc5: 100.0000 (99.5610)  time: 0.8887  data: 0.1483  max mem: 41808
Val:  [130/346]  eta: 0:03:25  loss: 0.1280 (0.4005)  acc1: 96.8750 (90.3805)  acc5: 100.0000 (99.1174)  time: 0.8931  data: 0.1495  max mem: 41808
Val:  [140/346]  eta: 0:03:14  loss: 0.2898 (0.3958)  acc1: 95.3125 (90.4754)  acc5: 100.0000 (99.1800)  time: 0.8764  data: 0.1436  max mem: 41808
Val:  [150/346]  eta: 0:03:04  loss: 0.3325 (0.3925)  acc1: 93.7500 (90.6198)  acc5: 100.0000 (99.2239)  time: 0.8813  data: 0.1486  max mem: 41808
Val:  [160/346]  eta: 0:02:54  loss: 0.2954 (0.3886)  acc1: 92.1875 (90.6493)  acc5: 100.0000 (99.2430)  time: 0.9012  data: 0.1498  max mem: 41808
Val:  [170/346]  eta: 0:02:45  loss: 0.2841 (0.3901)  acc1: 90.6250 (90.3920)  acc5: 100.0000 (99.2873)  time: 0.9197  data: 0.1576  max mem: 41808
Val:  [180/346]  eta: 0:02:35  loss: 0.3103 (0.4038)  acc1: 91.4062 (89.6409)  acc5: 100.0000 (99.2878)  time: 0.9131  data: 0.1511  max mem: 41808
Val:  [190/346]  eta: 0:02:25  loss: 0.3103 (0.4010)  acc1: 92.9688 (89.7251)  acc5: 100.0000 (99.3128)  time: 0.8898  data: 0.1377  max mem: 41808
Val:  [200/346]  eta: 0:02:16  loss: 0.3073 (0.4067)  acc1: 92.9688 (89.4978)  acc5: 100.0000 (99.3237)  time: 0.9107  data: 0.1493  max mem: 41808
Val:  [210/346]  eta: 0:02:06  loss: 0.2363 (0.4027)  acc1: 94.5312 (89.6771)  acc5: 100.0000 (99.3557)  time: 0.9178  data: 0.1533  max mem: 41808
Val:  [220/346]  eta: 0:01:57  loss: 0.1868 (0.4027)  acc1: 98.4375 (89.7730)  acc5: 100.0000 (99.3389)  time: 0.9002  data: 0.1508  max mem: 41808
Val:  [230/346]  eta: 0:01:47  loss: 0.1654 (0.3935)  acc1: 98.4375 (90.1211)  acc5: 100.0000 (99.3676)  time: 0.9014  data: 0.1536  max mem: 41808
Val:  [240/346]  eta: 0:01:38  loss: 0.2010 (0.4019)  acc1: 96.8750 (89.8989)  acc5: 100.0000 (99.3744)  time: 0.9183  data: 0.1570  max mem: 41808
Val:  [250/346]  eta: 0:01:29  loss: 0.2060 (0.3957)  acc1: 96.8750 (90.1239)  acc5: 100.0000 (99.3868)  time: 0.9135  data: 0.1566  max mem: 41808
Val:  [260/346]  eta: 0:01:19  loss: 0.1667 (0.3932)  acc1: 98.4375 (90.1970)  acc5: 100.0000 (99.4103)  time: 0.9313  data: 0.1619  max mem: 41808
Val:  [270/346]  eta: 0:01:10  loss: 0.1451 (0.3895)  acc1: 99.2188 (90.3396)  acc5: 100.0000 (99.4148)  time: 0.9319  data: 0.1706  max mem: 41808
Val:  [280/346]  eta: 0:01:01  loss: 0.1318 (0.3874)  acc1: 100.0000 (90.4610)  acc5: 100.0000 (99.4245)  time: 0.8969  data: 0.1669  max mem: 41808
Val:  [290/346]  eta: 0:00:51  loss: 0.1182 (0.3791)  acc1: 100.0000 (90.7673)  acc5: 100.0000 (99.4443)  time: 0.9039  data: 0.1669  max mem: 41808
Val:  [300/346]  eta: 0:00:42  loss: 0.1306 (0.3795)  acc1: 100.0000 (90.8197)  acc5: 100.0000 (99.3433)  time: 0.9054  data: 0.1598  max mem: 41808
Val:  [310/346]  eta: 0:00:33  loss: 0.1504 (0.3789)  acc1: 98.4375 (90.8134)  acc5: 100.0000 (99.3644)  time: 0.8808  data: 0.1467  max mem: 41808
Val:  [320/346]  eta: 0:00:23  loss: 0.1423 (0.3824)  acc1: 99.2188 (90.6761)  acc5: 100.0000 (99.3794)  time: 0.8798  data: 0.1562  max mem: 41808
Val:  [330/346]  eta: 0:00:14  loss: 0.3562 (0.3882)  acc1: 88.2812 (90.4645)  acc5: 100.0000 (99.3509)  time: 0.8732  data: 0.1538  max mem: 41808
Val:  [340/346]  eta: 0:00:05  loss: 0.3562 (0.3961)  acc1: 88.2812 (90.2699)  acc5: 100.0000 (99.3287)  time: 0.8835  data: 0.1525  max mem: 41808
Val:  [345/346]  eta: 0:00:00  loss: 0.2901 (0.3935)  acc1: 93.1034 (90.3700)  acc5: 100.0000 (99.3355)  time: 0.9004  data: 0.1655  max mem: 41808
Val: Total time: 0:05:18 (0.9215 s / it)
* Acc@1 90.271 Acc@5 99.360 loss 0.394
Accuracy of the network on the 88494 val videos: 90.3%
Max accuracy: 91.25%   Max Epoch: 3
Epoch: [11]  [   0/1349]  eta: 1:25:54  lr: 0.000478  min_lr: 0.000011  loss: 0.9152 (0.9152)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 3.8212  data: 2.9415  max mem: 41808
Epoch: [11]  [  10/1349]  eta: 0:15:40  lr: 0.000478  min_lr: 0.000011  loss: 0.8173 (0.8088)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7021  data: 0.3144  max mem: 41808
Epoch: [11]  [  20/1349]  eta: 0:11:24  lr: 0.000478  min_lr: 0.000011  loss: 0.7957 (0.7692)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3496  data: 0.0259  max mem: 41808
Epoch: [11]  [  30/1349]  eta: 0:09:50  lr: 0.000478  min_lr: 0.000011  loss: 0.7338 (0.7883)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3079  data: 0.0002  max mem: 41808
Epoch: [11]  [  40/1349]  eta: 0:09:01  lr: 0.000478  min_lr: 0.000011  loss: 0.8528 (0.8202)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0002  max mem: 41808
Epoch: [11]  [  50/1349]  eta: 0:08:30  lr: 0.000478  min_lr: 0.000011  loss: 0.9178 (0.8431)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0002  max mem: 41808
Epoch: [11]  [  60/1349]  eta: 0:08:08  lr: 0.000478  min_lr: 0.000011  loss: 0.8904 (0.8439)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
Epoch: [11]  [  70/1349]  eta: 0:07:51  lr: 0.000478  min_lr: 0.000011  loss: 0.8178 (0.8418)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3079  data: 0.0001  max mem: 41808
[2025-05-23 17:18:30,521] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 14919
[2025-05-23 17:18:30,522] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 17:18:30,521] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 14919
[2025-05-23 17:18:30,522] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 17:18:30,522] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [11]  [  80/1349]  eta: 0:07:38  lr: 0.000478  min_lr: 0.000011  loss: 0.8257 (0.8473)  loss_scale: 131072.0000 (130262.9136)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [11]  [  90/1349]  eta: 0:07:27  lr: 0.000478  min_lr: 0.000011  loss: 0.8456 (0.8425)  loss_scale: 65536.0000 (123150.0659)  weight_decay: 0.0500 (0.0500)  time: 0.3060  data: 0.0001  max mem: 41808
Epoch: [11]  [ 100/1349]  eta: 0:07:17  lr: 0.000478  min_lr: 0.000011  loss: 0.8676 (0.8427)  loss_scale: 65536.0000 (117445.7030)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0001  max mem: 41808
Epoch: [11]  [ 110/1349]  eta: 0:07:09  lr: 0.000478  min_lr: 0.000011  loss: 0.9254 (0.8572)  loss_scale: 65536.0000 (112769.1532)  weight_decay: 0.0500 (0.0500)  time: 0.3064  data: 0.0002  max mem: 41808
Epoch: [11]  [ 120/1349]  eta: 0:07:01  lr: 0.000478  min_lr: 0.000011  loss: 0.9616 (0.8579)  loss_scale: 65536.0000 (108865.5868)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0002  max mem: 41808
Epoch: [11]  [ 130/1349]  eta: 0:06:54  lr: 0.000478  min_lr: 0.000011  loss: 0.8388 (0.8525)  loss_scale: 65536.0000 (105557.9847)  weight_decay: 0.0500 (0.0500)  time: 0.3066  data: 0.0002  max mem: 41808
Epoch: [11]  [ 140/1349]  eta: 0:06:48  lr: 0.000478  min_lr: 0.000011  loss: 0.8179 (0.8540)  loss_scale: 65536.0000 (102719.5461)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0001  max mem: 41808
Epoch: [11]  [ 150/1349]  eta: 0:06:42  lr: 0.000478  min_lr: 0.000011  loss: 0.8632 (0.8600)  loss_scale: 65536.0000 (100257.0596)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
[2025-05-23 17:18:55,118] [INFO] [logging.py:96:log_dist] [Rank 0] step=15000, skipped=89, lr=[1.1346029725121774e-05, 1.1346029725121774e-05, 1.5128039633495699e-05, 1.5128039633495699e-05, 2.01707195113276e-05, 2.01707195113276e-05, 2.689429268177013e-05, 2.689429268177013e-05, 3.585905690902684e-05, 3.585905690902684e-05, 4.781207587870245e-05, 4.781207587870245e-05, 6.37494345049366e-05, 6.37494345049366e-05, 8.499924600658213e-05, 8.499924600658213e-05, 0.00011333232800877619, 0.00011333232800877619, 0.00015110977067836826, 0.00015110977067836826, 0.00020147969423782433, 0.00020147969423782433, 0.0002686395923170991, 0.0002686395923170991, 0.00035818612308946546, 0.00035818612308946546, 0.00047758149745262063, 0.00047758149745262063], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-05-23 17:18:55,118] [INFO] [timer.py:260:stop] epoch=0/micro_step=15000/global_step=15000, RunningAvgSamplesPerSec=204.15326264932816, CurrSamplesPerSec=214.17847870386476, MemAllocated=1.37GB, MaxMemAllocated=40.83GB
Epoch: [11]  [ 160/1349]  eta: 0:06:37  lr: 0.000478  min_lr: 0.000011  loss: 0.8951 (0.8634)  loss_scale: 65536.0000 (98100.4720)  weight_decay: 0.0500 (0.0500)  time: 0.3086  data: 0.0001  max mem: 41808
Epoch: [11]  [ 170/1349]  eta: 0:06:32  lr: 0.000478  min_lr: 0.000011  loss: 0.9342 (0.8663)  loss_scale: 65536.0000 (96196.1170)  weight_decay: 0.0500 (0.0500)  time: 0.3085  data: 0.0001  max mem: 41808
Epoch: [11]  [ 180/1349]  eta: 0:06:27  lr: 0.000477  min_lr: 0.000011  loss: 0.9401 (0.8686)  loss_scale: 65536.0000 (94502.1878)  weight_decay: 0.0500 (0.0500)  time: 0.3097  data: 0.0001  max mem: 41808
Epoch: [11]  [ 190/1349]  eta: 0:06:22  lr: 0.000477  min_lr: 0.000011  loss: 0.8348 (0.8668)  loss_scale: 65536.0000 (92985.6335)  weight_decay: 0.0500 (0.0500)  time: 0.3112  data: 0.0001  max mem: 41808
Epoch: [11]  [ 200/1349]  eta: 0:06:18  lr: 0.000477  min_lr: 0.000011  loss: 0.7928 (0.8645)  loss_scale: 65536.0000 (91619.9801)  weight_decay: 0.0500 (0.0500)  time: 0.3082  data: 0.0002  max mem: 41808
[2025-05-23 17:19:10,238] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 17:19:10,239] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-05-23 17:19:10,239] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 17:19:10,239] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [11]  [ 210/1349]  eta: 0:06:13  lr: 0.000477  min_lr: 0.000011  loss: 0.7797 (0.8611)  loss_scale: 65536.0000 (91004.9668)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [11]  [ 220/1349]  eta: 0:06:09  lr: 0.000477  min_lr: 0.000011  loss: 0.8655 (0.8609)  loss_scale: 131072.0000 (92817.9548)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
Epoch: [11]  [ 230/1349]  eta: 0:06:05  lr: 0.000477  min_lr: 0.000011  loss: 0.8655 (0.8597)  loss_scale: 131072.0000 (94473.9740)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
Epoch: [11]  [ 240/1349]  eta: 0:06:01  lr: 0.000477  min_lr: 0.000011  loss: 0.9171 (0.8640)  loss_scale: 131072.0000 (95992.5643)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [11]  [ 250/1349]  eta: 0:05:57  lr: 0.000477  min_lr: 0.000011  loss: 0.9211 (0.8624)  loss_scale: 131072.0000 (97390.1514)  weight_decay: 0.0500 (0.0500)  time: 0.3066  data: 0.0001  max mem: 41808
Epoch: [11]  [ 260/1349]  eta: 0:05:53  lr: 0.000477  min_lr: 0.000011  loss: 0.8704 (0.8625)  loss_scale: 131072.0000 (98680.6437)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0002  max mem: 41808
Epoch: [11]  [ 270/1349]  eta: 0:05:49  lr: 0.000477  min_lr: 0.000011  loss: 0.8833 (0.8646)  loss_scale: 131072.0000 (99875.8967)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [11]  [ 280/1349]  eta: 0:05:45  lr: 0.000477  min_lr: 0.000011  loss: 0.8572 (0.8597)  loss_scale: 131072.0000 (100986.0783)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0001  max mem: 41808
Epoch: [11]  [ 290/1349]  eta: 0:05:41  lr: 0.000477  min_lr: 0.000011  loss: 0.7890 (0.8607)  loss_scale: 131072.0000 (102019.9588)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [11]  [ 300/1349]  eta: 0:05:37  lr: 0.000477  min_lr: 0.000011  loss: 0.8509 (0.8593)  loss_scale: 131072.0000 (102985.1429)  weight_decay: 0.0500 (0.0500)  time: 0.3066  data: 0.0001  max mem: 41808
Epoch: [11]  [ 310/1349]  eta: 0:05:34  lr: 0.000477  min_lr: 0.000011  loss: 0.7584 (0.8570)  loss_scale: 131072.0000 (103888.2572)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
Epoch: [11]  [ 320/1349]  eta: 0:05:30  lr: 0.000477  min_lr: 0.000011  loss: 0.7909 (0.8569)  loss_scale: 131072.0000 (104735.1028)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
Epoch: [11]  [ 330/1349]  eta: 0:05:26  lr: 0.000477  min_lr: 0.000011  loss: 0.8687 (0.8582)  loss_scale: 131072.0000 (105530.7795)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
[2025-05-23 17:19:49,575] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 17:19:49,575] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 17:19:49,575] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 17:19:49,575] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [11]  [ 340/1349]  eta: 0:05:23  lr: 0.000477  min_lr: 0.000011  loss: 0.9076 (0.8589)  loss_scale: 131072.0000 (107817.2903)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
[2025-05-23 17:19:50,798] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 15180
[2025-05-23 17:19:50,798] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 15180
[2025-05-23 17:19:50,798] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 17:19:50,798] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 17:19:50,798] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [11]  [ 350/1349]  eta: 0:05:19  lr: 0.000477  min_lr: 0.000011  loss: 0.7905 (0.8571)  loss_scale: 131072.0000 (108479.8177)  weight_decay: 0.0500 (0.0500)  time: 0.3063  data: 0.0002  max mem: 41808
Epoch: [11]  [ 360/1349]  eta: 0:05:15  lr: 0.000476  min_lr: 0.000011  loss: 0.8204 (0.8583)  loss_scale: 131072.0000 (109105.6399)  weight_decay: 0.0500 (0.0500)  time: 0.3060  data: 0.0002  max mem: 41808
Epoch: [11]  [ 370/1349]  eta: 0:05:12  lr: 0.000476  min_lr: 0.000011  loss: 0.8200 (0.8571)  loss_scale: 131072.0000 (109697.7251)  weight_decay: 0.0500 (0.0500)  time: 0.3061  data: 0.0001  max mem: 41808
Epoch: [11]  [ 380/1349]  eta: 0:05:08  lr: 0.000476  min_lr: 0.000011  loss: 0.8559 (0.8588)  loss_scale: 131072.0000 (110258.7297)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
Epoch: [11]  [ 390/1349]  eta: 0:05:05  lr: 0.000476  min_lr: 0.000011  loss: 0.8559 (0.8579)  loss_scale: 131072.0000 (110791.0384)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [11]  [ 400/1349]  eta: 0:05:01  lr: 0.000476  min_lr: 0.000011  loss: 0.8785 (0.8587)  loss_scale: 131072.0000 (111296.7980)  weight_decay: 0.0500 (0.0500)  time: 0.3066  data: 0.0001  max mem: 41808
Epoch: [11]  [ 410/1349]  eta: 0:04:58  lr: 0.000476  min_lr: 0.000011  loss: 0.8721 (0.8563)  loss_scale: 131072.0000 (111777.9465)  weight_decay: 0.0500 (0.0500)  time: 0.3103  data: 0.0002  max mem: 41808
Epoch: [11]  [ 420/1349]  eta: 0:04:55  lr: 0.000476  min_lr: 0.000011  loss: 0.7220 (0.8530)  loss_scale: 131072.0000 (112236.2375)  weight_decay: 0.0500 (0.0500)  time: 0.3118  data: 0.0002  max mem: 41808
Epoch: [11]  [ 430/1349]  eta: 0:04:51  lr: 0.000476  min_lr: 0.000011  loss: 0.7500 (0.8526)  loss_scale: 131072.0000 (112673.2622)  weight_decay: 0.0500 (0.0500)  time: 0.3085  data: 0.0002  max mem: 41808
Epoch: [11]  [ 440/1349]  eta: 0:04:48  lr: 0.000476  min_lr: 0.000011  loss: 0.8297 (0.8515)  loss_scale: 131072.0000 (113090.4671)  weight_decay: 0.0500 (0.0500)  time: 0.3085  data: 0.0001  max mem: 41808
Epoch: [11]  [ 450/1349]  eta: 0:04:45  lr: 0.000476  min_lr: 0.000011  loss: 0.8297 (0.8515)  loss_scale: 131072.0000 (113489.1707)  weight_decay: 0.0500 (0.0500)  time: 0.3084  data: 0.0001  max mem: 41808
Epoch: [11]  [ 460/1349]  eta: 0:04:41  lr: 0.000476  min_lr: 0.000011  loss: 0.8702 (0.8537)  loss_scale: 131072.0000 (113870.5770)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
[2025-05-23 17:20:30,513] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 17:20:30,513] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 17:20:30,513] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 17:20:30,513] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [11]  [ 470/1349]  eta: 0:04:38  lr: 0.000476  min_lr: 0.000011  loss: 0.9056 (0.8549)  loss_scale: 131072.0000 (114514.0722)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
[2025-05-23 17:20:33,263] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 15318
[2025-05-23 17:20:33,263] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 15318
[2025-05-23 17:20:33,263] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 17:20:33,263] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 17:20:33,263] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [11]  [ 480/1349]  eta: 0:04:35  lr: 0.000476  min_lr: 0.000011  loss: 0.7920 (0.8537)  loss_scale: 131072.0000 (117038.3035)  weight_decay: 0.0500 (0.0500)  time: 0.3057  data: 0.0001  max mem: 41808
Epoch: [11]  [ 490/1349]  eta: 0:04:31  lr: 0.000476  min_lr: 0.000011  loss: 0.8115 (0.8547)  loss_scale: 131072.0000 (117324.1222)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0002  max mem: 41808
Epoch: [11]  [ 500/1349]  eta: 0:04:28  lr: 0.000476  min_lr: 0.000011  loss: 0.8681 (0.8554)  loss_scale: 131072.0000 (117598.5309)  weight_decay: 0.0500 (0.0500)  time: 0.3080  data: 0.0001  max mem: 41808
Epoch: [11]  [ 510/1349]  eta: 0:04:25  lr: 0.000476  min_lr: 0.000011  loss: 0.8997 (0.8565)  loss_scale: 131072.0000 (117862.1996)  weight_decay: 0.0500 (0.0500)  time: 0.3059  data: 0.0001  max mem: 41808
Epoch: [11]  [ 520/1349]  eta: 0:04:21  lr: 0.000476  min_lr: 0.000011  loss: 0.8933 (0.8575)  loss_scale: 131072.0000 (118115.7466)  weight_decay: 0.0500 (0.0500)  time: 0.3058  data: 0.0001  max mem: 41808
Epoch: [11]  [ 530/1349]  eta: 0:04:18  lr: 0.000476  min_lr: 0.000011  loss: 0.8910 (0.8567)  loss_scale: 131072.0000 (118359.7439)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [11]  [ 540/1349]  eta: 0:04:15  lr: 0.000476  min_lr: 0.000011  loss: 0.8474 (0.8569)  loss_scale: 131072.0000 (118594.7209)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [11]  [ 550/1349]  eta: 0:04:11  lr: 0.000475  min_lr: 0.000011  loss: 0.8474 (0.8555)  loss_scale: 131072.0000 (118821.1688)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
Epoch: [11]  [ 560/1349]  eta: 0:04:08  lr: 0.000475  min_lr: 0.000011  loss: 0.8459 (0.8550)  loss_scale: 131072.0000 (119039.5437)  weight_decay: 0.0500 (0.0500)  time: 0.3079  data: 0.0001  max mem: 41808
Epoch: [11]  [ 570/1349]  eta: 0:04:05  lr: 0.000475  min_lr: 0.000011  loss: 0.8672 (0.8544)  loss_scale: 131072.0000 (119250.2697)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
Epoch: [11]  [ 580/1349]  eta: 0:04:02  lr: 0.000475  min_lr: 0.000011  loss: 0.8259 (0.8534)  loss_scale: 131072.0000 (119453.7418)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0002  max mem: 41808
Epoch: [11]  [ 590/1349]  eta: 0:03:58  lr: 0.000475  min_lr: 0.000011  loss: 0.8395 (0.8530)  loss_scale: 131072.0000 (119650.3283)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [11]  [ 600/1349]  eta: 0:03:55  lr: 0.000475  min_lr: 0.000011  loss: 0.8670 (0.8522)  loss_scale: 131072.0000 (119840.3727)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
[2025-05-23 17:21:12,906] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 17:21:12,906] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 17:21:12,906] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 17:21:12,906] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 17:21:13,217] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 15448
[2025-05-23 17:21:13,217] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 15448
[2025-05-23 17:21:13,217] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 17:21:13,217] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 17:21:13,217] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [11]  [ 610/1349]  eta: 0:03:52  lr: 0.000475  min_lr: 0.000011  loss: 0.8485 (0.8520)  loss_scale: 131072.0000 (120238.7169)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0002  max mem: 41808
Epoch: [11]  [ 620/1349]  eta: 0:03:49  lr: 0.000475  min_lr: 0.000011  loss: 0.9050 (0.8529)  loss_scale: 131072.0000 (120413.1659)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0002  max mem: 41808
Epoch: [11]  [ 630/1349]  eta: 0:03:45  lr: 0.000475  min_lr: 0.000011  loss: 0.9025 (0.8523)  loss_scale: 131072.0000 (120582.0856)  weight_decay: 0.0500 (0.0500)  time: 0.3064  data: 0.0002  max mem: 41808
Epoch: [11]  [ 640/1349]  eta: 0:03:42  lr: 0.000475  min_lr: 0.000011  loss: 0.8802 (0.8531)  loss_scale: 131072.0000 (120745.7348)  weight_decay: 0.0500 (0.0500)  time: 0.3061  data: 0.0002  max mem: 41808
Epoch: [11]  [ 650/1349]  eta: 0:03:39  lr: 0.000475  min_lr: 0.000011  loss: 0.8230 (0.8522)  loss_scale: 131072.0000 (120904.3564)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0002  max mem: 41808
Epoch: [11]  [ 660/1349]  eta: 0:03:36  lr: 0.000475  min_lr: 0.000011  loss: 0.8189 (0.8519)  loss_scale: 131072.0000 (121058.1785)  weight_decay: 0.0500 (0.0500)  time: 0.3081  data: 0.0001  max mem: 41808
Epoch: [11]  [ 670/1349]  eta: 0:03:33  lr: 0.000475  min_lr: 0.000011  loss: 0.8987 (0.8527)  loss_scale: 131072.0000 (121207.4158)  weight_decay: 0.0500 (0.0500)  time: 0.3086  data: 0.0001  max mem: 41808
Epoch: [11]  [ 680/1349]  eta: 0:03:29  lr: 0.000475  min_lr: 0.000011  loss: 0.8858 (0.8524)  loss_scale: 131072.0000 (121352.2702)  weight_decay: 0.0500 (0.0500)  time: 0.3078  data: 0.0001  max mem: 41808
Epoch: [11]  [ 690/1349]  eta: 0:03:26  lr: 0.000475  min_lr: 0.000011  loss: 0.8521 (0.8524)  loss_scale: 131072.0000 (121492.9320)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
Epoch: [11]  [ 700/1349]  eta: 0:03:23  lr: 0.000475  min_lr: 0.000011  loss: 0.8521 (0.8516)  loss_scale: 131072.0000 (121629.5806)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [11]  [ 710/1349]  eta: 0:03:20  lr: 0.000475  min_lr: 0.000011  loss: 0.8726 (0.8510)  loss_scale: 131072.0000 (121762.3854)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [11]  [ 720/1349]  eta: 0:03:17  lr: 0.000474  min_lr: 0.000011  loss: 0.8873 (0.8518)  loss_scale: 131072.0000 (121891.5062)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [11]  [ 730/1349]  eta: 0:03:13  lr: 0.000474  min_lr: 0.000011  loss: 0.8179 (0.8516)  loss_scale: 131072.0000 (122017.0944)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
[2025-05-23 17:21:52,874] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 17:21:52,874] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 17:21:52,874] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 17:21:52,875] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [11]  [ 740/1349]  eta: 0:03:10  lr: 0.000474  min_lr: 0.000011  loss: 0.7908 (0.8518)  loss_scale: 131072.0000 (122669.9487)  weight_decay: 0.0500 (0.0500)  time: 0.3082  data: 0.0001  max mem: 41808
[2025-05-23 17:21:55,950] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 15587
[2025-05-23 17:21:55,950] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 15587
[2025-05-23 17:21:55,950] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 17:21:55,950] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 17:21:55,950] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [11]  [ 750/1349]  eta: 0:03:07  lr: 0.000474  min_lr: 0.000011  loss: 0.8295 (0.8515)  loss_scale: 131072.0000 (124003.5366)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0001  max mem: 41808
Epoch: [11]  [ 760/1349]  eta: 0:03:04  lr: 0.000474  min_lr: 0.000011  loss: 0.8416 (0.8522)  loss_scale: 131072.0000 (124096.4205)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
Epoch: [11]  [ 770/1349]  eta: 0:03:01  lr: 0.000474  min_lr: 0.000011  loss: 0.8153 (0.8504)  loss_scale: 131072.0000 (124186.8949)  weight_decay: 0.0500 (0.0500)  time: 0.3084  data: 0.0002  max mem: 41808
Epoch: [11]  [ 780/1349]  eta: 0:02:58  lr: 0.000474  min_lr: 0.000011  loss: 0.7692 (0.8501)  loss_scale: 131072.0000 (124275.0525)  weight_decay: 0.0500 (0.0500)  time: 0.3082  data: 0.0002  max mem: 41808
[2025-05-23 17:22:07,661] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 15625
[2025-05-23 17:22:07,661] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 15625
[2025-05-23 17:22:07,661] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 17:22:07,661] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 17:22:07,661] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [11]  [ 790/1349]  eta: 0:02:54  lr: 0.000474  min_lr: 0.000011  loss: 0.8545 (0.8504)  loss_scale: 131072.0000 (123946.7206)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
Epoch: [11]  [ 800/1349]  eta: 0:02:51  lr: 0.000474  min_lr: 0.000011  loss: 0.8800 (0.8499)  loss_scale: 65536.0000 (123217.4981)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
Epoch: [11]  [ 810/1349]  eta: 0:02:48  lr: 0.000474  min_lr: 0.000011  loss: 0.8155 (0.8497)  loss_scale: 65536.0000 (122506.2589)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
Epoch: [11]  [ 820/1349]  eta: 0:02:45  lr: 0.000474  min_lr: 0.000011  loss: 0.8768 (0.8508)  loss_scale: 65536.0000 (121812.3459)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
Epoch: [11]  [ 830/1349]  eta: 0:02:42  lr: 0.000474  min_lr: 0.000011  loss: 0.9109 (0.8512)  loss_scale: 65536.0000 (121135.1336)  weight_decay: 0.0500 (0.0500)  time: 0.3091  data: 0.0001  max mem: 41808
Epoch: [11]  [ 840/1349]  eta: 0:02:39  lr: 0.000474  min_lr: 0.000011  loss: 0.9274 (0.8518)  loss_scale: 65536.0000 (120474.0262)  weight_decay: 0.0500 (0.0500)  time: 0.3091  data: 0.0001  max mem: 41808
Epoch: [11]  [ 850/1349]  eta: 0:02:35  lr: 0.000474  min_lr: 0.000011  loss: 0.9176 (0.8518)  loss_scale: 65536.0000 (119828.4559)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [11]  [ 860/1349]  eta: 0:02:32  lr: 0.000474  min_lr: 0.000011  loss: 0.9176 (0.8531)  loss_scale: 65536.0000 (119197.8815)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
Epoch: [11]  [ 870/1349]  eta: 0:02:29  lr: 0.000474  min_lr: 0.000011  loss: 0.9235 (0.8529)  loss_scale: 65536.0000 (118581.7865)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0002  max mem: 41808
Epoch: [11]  [ 880/1349]  eta: 0:02:26  lr: 0.000474  min_lr: 0.000011  loss: 0.8620 (0.8526)  loss_scale: 65536.0000 (117979.6776)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
Epoch: [11]  [ 890/1349]  eta: 0:02:23  lr: 0.000474  min_lr: 0.000011  loss: 0.9084 (0.8538)  loss_scale: 65536.0000 (117391.0842)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
Epoch: [11]  [ 900/1349]  eta: 0:02:20  lr: 0.000473  min_lr: 0.000011  loss: 0.9084 (0.8535)  loss_scale: 65536.0000 (116815.5560)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
Epoch: [11]  [ 910/1349]  eta: 0:02:17  lr: 0.000473  min_lr: 0.000011  loss: 0.8581 (0.8538)  loss_scale: 65536.0000 (116252.6630)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
[2025-05-23 17:22:47,343] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 17:22:47,344] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-05-23 17:22:47,343] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 17:22:47,344] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [11]  [ 920/1349]  eta: 0:02:13  lr: 0.000473  min_lr: 0.000011  loss: 0.9583 (0.8545)  loss_scale: 65536.0000 (116128.9381)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
Epoch: [11]  [ 930/1349]  eta: 0:02:10  lr: 0.000473  min_lr: 0.000011  loss: 0.8839 (0.8541)  loss_scale: 131072.0000 (116289.4436)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
Epoch: [11]  [ 940/1349]  eta: 0:02:07  lr: 0.000473  min_lr: 0.000011  loss: 0.9159 (0.8550)  loss_scale: 131072.0000 (116446.5377)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [11]  [ 950/1349]  eta: 0:02:04  lr: 0.000473  min_lr: 0.000011  loss: 0.9381 (0.8547)  loss_scale: 131072.0000 (116600.3281)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
Epoch: [11]  [ 960/1349]  eta: 0:02:01  lr: 0.000473  min_lr: 0.000011  loss: 0.7671 (0.8535)  loss_scale: 131072.0000 (116750.9178)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [11]  [ 970/1349]  eta: 0:01:58  lr: 0.000473  min_lr: 0.000011  loss: 0.8695 (0.8537)  loss_scale: 131072.0000 (116898.4058)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
Epoch: [11]  [ 980/1349]  eta: 0:01:55  lr: 0.000473  min_lr: 0.000011  loss: 0.8854 (0.8544)  loss_scale: 131072.0000 (117042.8869)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
Epoch: [11]  [ 990/1349]  eta: 0:01:51  lr: 0.000473  min_lr: 0.000011  loss: 0.8512 (0.8535)  loss_scale: 131072.0000 (117184.4521)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [11]  [1000/1349]  eta: 0:01:48  lr: 0.000473  min_lr: 0.000011  loss: 0.8800 (0.8538)  loss_scale: 131072.0000 (117323.1888)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
Epoch: [11]  [1010/1349]  eta: 0:01:45  lr: 0.000473  min_lr: 0.000011  loss: 0.8923 (0.8539)  loss_scale: 131072.0000 (117459.1810)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [11]  [1020/1349]  eta: 0:01:42  lr: 0.000473  min_lr: 0.000011  loss: 0.8712 (0.8542)  loss_scale: 131072.0000 (117592.5093)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
Epoch: [11]  [1030/1349]  eta: 0:01:39  lr: 0.000473  min_lr: 0.000011  loss: 0.8995 (0.8546)  loss_scale: 131072.0000 (117723.2512)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [11]  [1040/1349]  eta: 0:01:36  lr: 0.000473  min_lr: 0.000011  loss: 0.8854 (0.8540)  loss_scale: 131072.0000 (117851.4813)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0001  max mem: 41808
[2025-05-23 17:23:26,676] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 17:23:26,676] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 17:23:26,676] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 17:23:26,676] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 17:23:28,517] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 15888
[2025-05-23 17:23:28,517] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 15888
[2025-05-23 17:23:28,517] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 17:23:28,517] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 17:23:28,517] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [11]  [1050/1349]  eta: 0:01:33  lr: 0.000473  min_lr: 0.000011  loss: 0.7980 (0.8532)  loss_scale: 131072.0000 (118725.5414)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [11]  [1060/1349]  eta: 0:01:30  lr: 0.000473  min_lr: 0.000011  loss: 0.8113 (0.8527)  loss_scale: 131072.0000 (118841.9076)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
Epoch: [11]  [1070/1349]  eta: 0:01:26  lr: 0.000472  min_lr: 0.000011  loss: 0.8523 (0.8521)  loss_scale: 131072.0000 (118956.1008)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [11]  [1080/1349]  eta: 0:01:23  lr: 0.000472  min_lr: 0.000011  loss: 0.8560 (0.8519)  loss_scale: 131072.0000 (119068.1813)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
Epoch: [11]  [1090/1349]  eta: 0:01:20  lr: 0.000472  min_lr: 0.000011  loss: 0.8004 (0.8510)  loss_scale: 131072.0000 (119178.2071)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
Epoch: [11]  [1100/1349]  eta: 0:01:17  lr: 0.000472  min_lr: 0.000011  loss: 0.8292 (0.8514)  loss_scale: 131072.0000 (119286.2343)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
Epoch: [11]  [1110/1349]  eta: 0:01:14  lr: 0.000472  min_lr: 0.000011  loss: 0.8373 (0.8504)  loss_scale: 131072.0000 (119392.3168)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
Epoch: [11]  [1120/1349]  eta: 0:01:11  lr: 0.000472  min_lr: 0.000011  loss: 0.8260 (0.8508)  loss_scale: 131072.0000 (119496.5067)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
Epoch: [11]  [1130/1349]  eta: 0:01:08  lr: 0.000472  min_lr: 0.000011  loss: 0.8789 (0.8511)  loss_scale: 131072.0000 (119598.8541)  weight_decay: 0.0500 (0.0500)  time: 0.3062  data: 0.0001  max mem: 41808
Epoch: [11]  [1140/1349]  eta: 0:01:05  lr: 0.000472  min_lr: 0.000011  loss: 0.8789 (0.8512)  loss_scale: 131072.0000 (119699.4075)  weight_decay: 0.0500 (0.0500)  time: 0.3064  data: 0.0001  max mem: 41808
Epoch: [11]  [1150/1349]  eta: 0:01:01  lr: 0.000472  min_lr: 0.000011  loss: 0.9019 (0.8521)  loss_scale: 131072.0000 (119798.2137)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
[2025-05-23 17:24:02,627] [INFO] [logging.py:96:log_dist] [Rank 0] step=16000, skipped=95, lr=[1.1211778870674563e-05, 1.1211778870674563e-05, 1.4949038494232752e-05, 1.4949038494232752e-05, 1.9932051325643668e-05, 1.9932051325643668e-05, 2.657606843419156e-05, 2.657606843419156e-05, 3.543475791225541e-05, 3.543475791225541e-05, 4.7246343883007215e-05, 4.7246343883007215e-05, 6.299512517734295e-05, 6.299512517734295e-05, 8.399350023645727e-05, 8.399350023645727e-05, 0.0001119913336486097, 0.0001119913336486097, 0.00014932177819814625, 0.00014932177819814625, 0.00019909570426419502, 0.00019909570426419502, 0.00026546093901892666, 0.00026546093901892666, 0.00035394791869190225, 0.00035394791869190225, 0.00047193055825586966, 0.00047193055825586966], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-05-23 17:24:02,627] [INFO] [timer.py:260:stop] epoch=0/micro_step=16000/global_step=16000, RunningAvgSamplesPerSec=204.70187484352385, CurrSamplesPerSec=214.4647566058644, MemAllocated=1.37GB, MaxMemAllocated=40.83GB
Epoch: [11]  [1160/1349]  eta: 0:00:58  lr: 0.000472  min_lr: 0.000011  loss: 0.9159 (0.8515)  loss_scale: 131072.0000 (119895.3178)  weight_decay: 0.0500 (0.0500)  time: 0.3091  data: 0.0001  max mem: 41808
[2025-05-23 17:24:04,768] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 16006
[2025-05-23 17:24:04,768] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 16006
[2025-05-23 17:24:04,768] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 17:24:04,768] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 17:24:04,768] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [11]  [1170/1349]  eta: 0:00:55  lr: 0.000472  min_lr: 0.000011  loss: 0.8577 (0.8516)  loss_scale: 131072.0000 (119766.9001)  weight_decay: 0.0500 (0.0500)  time: 0.3089  data: 0.0001  max mem: 41808
Epoch: [11]  [1180/1349]  eta: 0:00:52  lr: 0.000472  min_lr: 0.000011  loss: 0.8970 (0.8519)  loss_scale: 65536.0000 (119307.7053)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
Epoch: [11]  [1190/1349]  eta: 0:00:49  lr: 0.000472  min_lr: 0.000011  loss: 0.9132 (0.8519)  loss_scale: 65536.0000 (118856.2217)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [11]  [1200/1349]  eta: 0:00:46  lr: 0.000472  min_lr: 0.000011  loss: 0.9132 (0.8527)  loss_scale: 65536.0000 (118412.2565)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [11]  [1210/1349]  eta: 0:00:43  lr: 0.000472  min_lr: 0.000011  loss: 0.9503 (0.8533)  loss_scale: 65536.0000 (117975.6235)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0001  max mem: 41808
Epoch: [11]  [1220/1349]  eta: 0:00:40  lr: 0.000472  min_lr: 0.000011  loss: 0.9440 (0.8532)  loss_scale: 65536.0000 (117546.1425)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
Epoch: [11]  [1230/1349]  eta: 0:00:36  lr: 0.000472  min_lr: 0.000011  loss: 0.9502 (0.8539)  loss_scale: 65536.0000 (117123.6393)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [11]  [1240/1349]  eta: 0:00:33  lr: 0.000471  min_lr: 0.000011  loss: 0.9501 (0.8537)  loss_scale: 65536.0000 (116707.9452)  weight_decay: 0.0500 (0.0500)  time: 0.3066  data: 0.0001  max mem: 41808
Epoch: [11]  [1250/1349]  eta: 0:00:30  lr: 0.000471  min_lr: 0.000011  loss: 0.8552 (0.8536)  loss_scale: 65536.0000 (116298.8969)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [11]  [1260/1349]  eta: 0:00:27  lr: 0.000471  min_lr: 0.000011  loss: 0.8400 (0.8534)  loss_scale: 65536.0000 (115896.3362)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
Epoch: [11]  [1270/1349]  eta: 0:00:24  lr: 0.000471  min_lr: 0.000011  loss: 0.8261 (0.8536)  loss_scale: 65536.0000 (115500.1101)  weight_decay: 0.0500 (0.0500)  time: 0.3066  data: 0.0001  max mem: 41808
Epoch: [11]  [1280/1349]  eta: 0:00:21  lr: 0.000471  min_lr: 0.000011  loss: 0.8679 (0.8538)  loss_scale: 65536.0000 (115110.0703)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
Epoch: [11]  [1290/1349]  eta: 0:00:18  lr: 0.000471  min_lr: 0.000011  loss: 0.8941 (0.8540)  loss_scale: 65536.0000 (114726.0728)  weight_decay: 0.0500 (0.0500)  time: 0.3064  data: 0.0001  max mem: 41808
[2025-05-23 17:24:44,377] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 17:24:44,377] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-05-23 17:24:44,377] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 17:24:44,377] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [11]  [1300/1349]  eta: 0:00:15  lr: 0.000471  min_lr: 0.000011  loss: 0.8039 (0.8531)  loss_scale: 65536.0000 (114599.8463)  weight_decay: 0.0500 (0.0500)  time: 0.3062  data: 0.0001  max mem: 41808
Epoch: [11]  [1310/1349]  eta: 0:00:12  lr: 0.000471  min_lr: 0.000011  loss: 0.7634 (0.8527)  loss_scale: 131072.0000 (114725.4920)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
Epoch: [11]  [1320/1349]  eta: 0:00:09  lr: 0.000471  min_lr: 0.000011  loss: 0.8425 (0.8525)  loss_scale: 131072.0000 (114849.2354)  weight_decay: 0.0500 (0.0500)  time: 0.3066  data: 0.0001  max mem: 41808
Epoch: [11]  [1330/1349]  eta: 0:00:05  lr: 0.000471  min_lr: 0.000011  loss: 0.8791 (0.8527)  loss_scale: 131072.0000 (114971.1195)  weight_decay: 0.0500 (0.0500)  time: 0.3044  data: 0.0001  max mem: 41808
Epoch: [11]  [1340/1349]  eta: 0:00:02  lr: 0.000471  min_lr: 0.000011  loss: 0.9116 (0.8531)  loss_scale: 131072.0000 (115091.1857)  weight_decay: 0.0500 (0.0500)  time: 0.3020  data: 0.0001  max mem: 41808
Epoch: [11]  [1348/1349]  eta: 0:00:00  lr: 0.000471  min_lr: 0.000011  loss: 0.9163 (0.8532)  loss_scale: 131072.0000 (115185.9570)  weight_decay: 0.0500 (0.0500)  time: 0.3014  data: 0.0001  max mem: 41808
Epoch: [11] Total time: 0:06:59 (0.3108 s / it)
Averaged stats: lr: 0.000471  min_lr: 0.000011  loss: 0.9163 (0.8579)  loss_scale: 131072.0000 (115185.9570)  weight_decay: 0.0500 (0.0500)  total_time: 419.2352 (419.2305)
Val:  [  0/346]  eta: 1:03:53  loss: 2.8907 (2.8907)  acc1: 2.3438 (2.3438)  acc5: 89.0625 (89.0625)  time: 11.0786  data: 10.1370  max mem: 41808
Val:  [ 10/346]  eta: 0:11:10  loss: 0.1412 (0.5653)  acc1: 100.0000 (84.3750)  acc5: 100.0000 (98.8636)  time: 1.9966  data: 1.2015  max mem: 41808
Val:  [ 20/346]  eta: 0:07:37  loss: 0.1381 (0.4093)  acc1: 100.0000 (89.1369)  acc5: 100.0000 (99.3304)  time: 0.9189  data: 0.1541  max mem: 41808
Val:  [ 30/346]  eta: 0:06:23  loss: 0.1223 (0.3526)  acc1: 99.2188 (91.2802)  acc5: 100.0000 (99.5464)  time: 0.7805  data: 0.0003  max mem: 41808
Val:  [ 40/346]  eta: 0:05:36  loss: 0.1276 (0.4020)  acc1: 99.2188 (89.8628)  acc5: 100.0000 (99.5046)  time: 0.7837  data: 0.0003  max mem: 41808
Val:  [ 50/346]  eta: 0:05:08  loss: 0.1317 (0.3547)  acc1: 99.2188 (91.5135)  acc5: 100.0000 (99.6017)  time: 0.7767  data: 0.0003  max mem: 41808
Val:  [ 60/346]  eta: 0:04:47  loss: 0.1798 (0.3504)  acc1: 97.6562 (91.5471)  acc5: 100.0000 (99.6414)  time: 0.8071  data: 0.0204  max mem: 41808
Val:  [ 70/346]  eta: 0:04:30  loss: 0.3239 (0.3761)  acc1: 88.2812 (90.5370)  acc5: 100.0000 (99.6149)  time: 0.8279  data: 0.0827  max mem: 41808
Val:  [ 80/346]  eta: 0:04:17  loss: 0.2126 (0.3694)  acc1: 90.6250 (90.9529)  acc5: 100.0000 (99.5081)  time: 0.8605  data: 0.1330  max mem: 41808
Val:  [ 90/346]  eta: 0:04:06  loss: 0.2126 (0.3728)  acc1: 96.0938 (90.7194)  acc5: 100.0000 (99.5622)  time: 0.8983  data: 0.1512  max mem: 41808
Val:  [100/346]  eta: 0:03:54  loss: 0.1662 (0.3506)  acc1: 97.6562 (91.5145)  acc5: 100.0000 (99.6055)  time: 0.8859  data: 0.1475  max mem: 41808
Val:  [110/346]  eta: 0:03:44  loss: 0.1583 (0.3607)  acc1: 97.6562 (91.0684)  acc5: 100.0000 (99.6270)  time: 0.8866  data: 0.1454  max mem: 41808
Val:  [120/346]  eta: 0:03:33  loss: 0.2153 (0.3600)  acc1: 94.5312 (91.0059)  acc5: 100.0000 (99.6578)  time: 0.9119  data: 0.1520  max mem: 41808
Val:  [130/346]  eta: 0:03:23  loss: 0.1453 (0.3758)  acc1: 97.6562 (90.4878)  acc5: 100.0000 (99.5825)  time: 0.9003  data: 0.1455  max mem: 41808
Val:  [140/346]  eta: 0:03:14  loss: 0.2863 (0.3773)  acc1: 93.7500 (90.4311)  acc5: 100.0000 (99.6066)  time: 0.9179  data: 0.1495  max mem: 41808
Val:  [150/346]  eta: 0:03:04  loss: 0.3056 (0.3757)  acc1: 92.9688 (90.5215)  acc5: 100.0000 (99.6120)  time: 0.9242  data: 0.1575  max mem: 41808
Val:  [160/346]  eta: 0:02:54  loss: 0.2374 (0.3736)  acc1: 95.3125 (90.5959)  acc5: 100.0000 (99.6264)  time: 0.9087  data: 0.1549  max mem: 41808
Val:  [170/346]  eta: 0:02:44  loss: 0.2239 (0.3690)  acc1: 94.5312 (90.5656)  acc5: 100.0000 (99.6482)  time: 0.9051  data: 0.1560  max mem: 41808
Val:  [180/346]  eta: 0:02:34  loss: 0.1814 (0.3913)  acc1: 94.5312 (89.5891)  acc5: 100.0000 (99.6676)  time: 0.8841  data: 0.1595  max mem: 41808
Val:  [190/346]  eta: 0:02:25  loss: 0.2325 (0.3882)  acc1: 93.7500 (89.6720)  acc5: 100.0000 (99.6810)  time: 0.8849  data: 0.1601  max mem: 41808
Val:  [200/346]  eta: 0:02:15  loss: 0.2822 (0.3924)  acc1: 90.6250 (89.3890)  acc5: 100.0000 (99.6968)  time: 0.8921  data: 0.1560  max mem: 41808
Val:  [210/346]  eta: 0:02:05  loss: 0.1559 (0.3885)  acc1: 98.4375 (89.6142)  acc5: 100.0000 (99.6668)  time: 0.8666  data: 0.1499  max mem: 41808
Val:  [220/346]  eta: 0:01:56  loss: 0.1683 (0.3894)  acc1: 97.6562 (89.6175)  acc5: 100.0000 (99.6076)  time: 0.8677  data: 0.1535  max mem: 41808
Val:  [230/346]  eta: 0:01:47  loss: 0.1683 (0.3796)  acc1: 97.6562 (89.9892)  acc5: 100.0000 (99.6246)  time: 0.9072  data: 0.1570  max mem: 41808
Val:  [240/346]  eta: 0:01:37  loss: 0.1423 (0.3886)  acc1: 98.4375 (89.7335)  acc5: 100.0000 (99.6045)  time: 0.9082  data: 0.1569  max mem: 41808
Val:  [250/346]  eta: 0:01:28  loss: 0.1984 (0.3865)  acc1: 96.0938 (89.8282)  acc5: 100.0000 (99.6203)  time: 0.8878  data: 0.1550  max mem: 41808
Val:  [260/346]  eta: 0:01:18  loss: 0.1694 (0.3856)  acc1: 96.8750 (89.8408)  acc5: 100.0000 (99.6318)  time: 0.8651  data: 0.1467  max mem: 41808
Val:  [270/346]  eta: 0:01:09  loss: 0.1428 (0.3822)  acc1: 98.4375 (89.9792)  acc5: 100.0000 (99.6339)  time: 0.8647  data: 0.1475  max mem: 41808
Val:  [280/346]  eta: 0:01:00  loss: 0.1283 (0.3796)  acc1: 98.4375 (90.1051)  acc5: 100.0000 (99.6330)  time: 0.8932  data: 0.1521  max mem: 41808
Val:  [290/346]  eta: 0:00:51  loss: 0.1153 (0.3706)  acc1: 100.0000 (90.4236)  acc5: 100.0000 (99.6456)  time: 0.9063  data: 0.1507  max mem: 41808
Val:  [300/346]  eta: 0:00:42  loss: 0.1160 (0.3709)  acc1: 100.0000 (90.5004)  acc5: 100.0000 (99.5951)  time: 0.9072  data: 0.1426  max mem: 41808
Val:  [310/346]  eta: 0:00:32  loss: 0.1441 (0.3702)  acc1: 97.6562 (90.5270)  acc5: 100.0000 (99.6081)  time: 0.8873  data: 0.1363  max mem: 41808
Val:  [320/346]  eta: 0:00:23  loss: 0.1441 (0.3707)  acc1: 96.8750 (90.5033)  acc5: 100.0000 (99.6155)  time: 0.8716  data: 0.1383  max mem: 41808
Val:  [330/346]  eta: 0:00:14  loss: 0.4965 (0.3835)  acc1: 84.3750 (90.1081)  acc5: 100.0000 (99.5917)  time: 0.8675  data: 0.1415  max mem: 41808
Val:  [340/346]  eta: 0:00:05  loss: 0.5255 (0.3927)  acc1: 85.9375 (89.8690)  acc5: 100.0000 (99.5784)  time: 0.8829  data: 0.1600  max mem: 41808
Val:  [345/346]  eta: 0:00:00  loss: 0.2357 (0.3895)  acc1: 94.2529 (89.9812)  acc5: 100.0000 (99.5842)  time: 0.8857  data: 0.1792  max mem: 41808
Val: Total time: 0:05:15 (0.9113 s / it)
* Acc@1 90.104 Acc@5 99.610 loss 0.387
Accuracy of the network on the 88494 val videos: 90.1%
Max accuracy: 91.25%   Max Epoch: 3
Epoch: [12]  [   0/1349]  eta: 1:41:17  lr: 0.000471  min_lr: 0.000011  loss: 0.8109 (0.8109)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 4.5051  data: 4.1662  max mem: 41808
Epoch: [12]  [  10/1349]  eta: 0:15:57  lr: 0.000471  min_lr: 0.000011  loss: 0.8885 (0.8644)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7152  data: 0.3981  max mem: 41808
Epoch: [12]  [  20/1349]  eta: 0:11:35  lr: 0.000471  min_lr: 0.000011  loss: 0.9126 (0.8778)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3242  data: 0.0107  max mem: 41808
Epoch: [12]  [  30/1349]  eta: 0:09:58  lr: 0.000471  min_lr: 0.000011  loss: 0.8882 (0.8646)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3097  data: 0.0001  max mem: 41808
Epoch: [12]  [  40/1349]  eta: 0:09:06  lr: 0.000471  min_lr: 0.000011  loss: 0.8418 (0.8607)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [12]  [  50/1349]  eta: 0:08:34  lr: 0.000470  min_lr: 0.000011  loss: 0.7857 (0.8610)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
Epoch: [12]  [  60/1349]  eta: 0:08:11  lr: 0.000470  min_lr: 0.000011  loss: 0.8675 (0.8589)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [12]  [  70/1349]  eta: 0:07:54  lr: 0.000470  min_lr: 0.000011  loss: 0.8695 (0.8636)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
[2025-05-23 17:30:43,724] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 17:30:43,724] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 17:30:43,724] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 17:30:43,724] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [12]  [  80/1349]  eta: 0:07:41  lr: 0.000470  min_lr: 0.000011  loss: 0.8533 (0.8527)  loss_scale: 131072.0000 (140781.0370)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
[2025-05-23 17:30:45,570] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 16269
[2025-05-23 17:30:45,571] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 16269
[2025-05-23 17:30:45,571] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 17:30:45,571] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 17:30:45,571] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [12]  [  90/1349]  eta: 0:07:29  lr: 0.000470  min_lr: 0.000011  loss: 0.8533 (0.8622)  loss_scale: 131072.0000 (139714.1099)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [12]  [ 100/1349]  eta: 0:07:19  lr: 0.000470  min_lr: 0.000011  loss: 0.8140 (0.8551)  loss_scale: 131072.0000 (138858.4554)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
[2025-05-23 17:30:53,549] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 16295
[2025-05-23 17:30:53,549] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 16295
[2025-05-23 17:30:53,549] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 17:30:53,549] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 17:30:53,549] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [12]  [ 110/1349]  eta: 0:07:11  lr: 0.000470  min_lr: 0.000011  loss: 0.8418 (0.8606)  loss_scale: 131072.0000 (135795.3153)  weight_decay: 0.0500 (0.0500)  time: 0.3066  data: 0.0001  max mem: 41808
Epoch: [12]  [ 120/1349]  eta: 0:07:03  lr: 0.000470  min_lr: 0.000011  loss: 0.9063 (0.8573)  loss_scale: 65536.0000 (129988.7603)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0002  max mem: 41808
Epoch: [12]  [ 130/1349]  eta: 0:06:56  lr: 0.000470  min_lr: 0.000011  loss: 0.8379 (0.8542)  loss_scale: 65536.0000 (125068.7023)  weight_decay: 0.0500 (0.0500)  time: 0.3080  data: 0.0001  max mem: 41808
Epoch: [12]  [ 140/1349]  eta: 0:06:50  lr: 0.000470  min_lr: 0.000011  loss: 0.8379 (0.8521)  loss_scale: 65536.0000 (120846.5248)  weight_decay: 0.0500 (0.0500)  time: 0.3100  data: 0.0001  max mem: 41808
Epoch: [12]  [ 150/1349]  eta: 0:06:44  lr: 0.000470  min_lr: 0.000011  loss: 0.7475 (0.8464)  loss_scale: 65536.0000 (117183.5762)  weight_decay: 0.0500 (0.0500)  time: 0.3088  data: 0.0001  max mem: 41808
Epoch: [12]  [ 160/1349]  eta: 0:06:39  lr: 0.000470  min_lr: 0.000011  loss: 0.7475 (0.8459)  loss_scale: 65536.0000 (113975.6522)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
Epoch: [12]  [ 170/1349]  eta: 0:06:33  lr: 0.000470  min_lr: 0.000011  loss: 0.7710 (0.8454)  loss_scale: 65536.0000 (111142.9240)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [12]  [ 180/1349]  eta: 0:06:28  lr: 0.000470  min_lr: 0.000011  loss: 0.8558 (0.8445)  loss_scale: 65536.0000 (108623.2044)  weight_decay: 0.0500 (0.0500)  time: 0.3066  data: 0.0001  max mem: 41808
Epoch: [12]  [ 190/1349]  eta: 0:06:23  lr: 0.000470  min_lr: 0.000011  loss: 0.8558 (0.8412)  loss_scale: 65536.0000 (106367.3298)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
Epoch: [12]  [ 200/1349]  eta: 0:06:18  lr: 0.000470  min_lr: 0.000011  loss: 0.8097 (0.8417)  loss_scale: 65536.0000 (104335.9204)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
Epoch: [12]  [ 210/1349]  eta: 0:06:14  lr: 0.000470  min_lr: 0.000011  loss: 0.8752 (0.8439)  loss_scale: 65536.0000 (102497.0616)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0002  max mem: 41808
Epoch: [12]  [ 220/1349]  eta: 0:06:10  lr: 0.000469  min_lr: 0.000011  loss: 0.8687 (0.8423)  loss_scale: 65536.0000 (100824.6154)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0002  max mem: 41808
Epoch: [12]  [ 230/1349]  eta: 0:06:05  lr: 0.000469  min_lr: 0.000011  loss: 0.7857 (0.8392)  loss_scale: 65536.0000 (99296.9697)  weight_decay: 0.0500 (0.0500)  time: 0.3064  data: 0.0002  max mem: 41808
[2025-05-23 17:31:33,210] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 17:31:33,210] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-05-23 17:31:33,210] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 17:31:33,210] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-05-23 17:31:34,129] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 16427
[2025-05-23 17:31:34,129] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 16427
[2025-05-23 17:31:34,129] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 17:31:34,129] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 17:31:34,129] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [12]  [ 240/1349]  eta: 0:06:01  lr: 0.000469  min_lr: 0.000011  loss: 0.7857 (0.8383)  loss_scale: 65536.0000 (98711.9004)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0002  max mem: 41808
Epoch: [12]  [ 250/1349]  eta: 0:05:57  lr: 0.000469  min_lr: 0.000011  loss: 0.9103 (0.8428)  loss_scale: 65536.0000 (97390.1514)  weight_decay: 0.0500 (0.0500)  time: 0.3064  data: 0.0002  max mem: 41808
Epoch: [12]  [ 260/1349]  eta: 0:05:53  lr: 0.000469  min_lr: 0.000011  loss: 0.9172 (0.8445)  loss_scale: 65536.0000 (96169.6858)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0002  max mem: 41808
Epoch: [12]  [ 270/1349]  eta: 0:05:49  lr: 0.000469  min_lr: 0.000011  loss: 0.9268 (0.8474)  loss_scale: 65536.0000 (95039.2915)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0003  max mem: 41808
Epoch: [12]  [ 280/1349]  eta: 0:05:45  lr: 0.000469  min_lr: 0.000011  loss: 0.8432 (0.8467)  loss_scale: 65536.0000 (93989.3523)  weight_decay: 0.0500 (0.0500)  time: 0.3064  data: 0.0002  max mem: 41808
Epoch: [12]  [ 290/1349]  eta: 0:05:41  lr: 0.000469  min_lr: 0.000011  loss: 0.8432 (0.8485)  loss_scale: 65536.0000 (93011.5739)  weight_decay: 0.0500 (0.0500)  time: 0.3060  data: 0.0002  max mem: 41808
Epoch: [12]  [ 300/1349]  eta: 0:05:37  lr: 0.000469  min_lr: 0.000011  loss: 0.9045 (0.8493)  loss_scale: 65536.0000 (92098.7641)  weight_decay: 0.0500 (0.0500)  time: 0.3059  data: 0.0002  max mem: 41808
Epoch: [12]  [ 310/1349]  eta: 0:05:34  lr: 0.000469  min_lr: 0.000011  loss: 0.8917 (0.8485)  loss_scale: 65536.0000 (91244.6559)  weight_decay: 0.0500 (0.0500)  time: 0.3060  data: 0.0002  max mem: 41808
Epoch: [12]  [ 320/1349]  eta: 0:05:30  lr: 0.000469  min_lr: 0.000011  loss: 0.8091 (0.8465)  loss_scale: 65536.0000 (90443.7632)  weight_decay: 0.0500 (0.0500)  time: 0.3061  data: 0.0002  max mem: 41808
Epoch: [12]  [ 330/1349]  eta: 0:05:26  lr: 0.000469  min_lr: 0.000011  loss: 0.8181 (0.8456)  loss_scale: 65536.0000 (89691.2628)  weight_decay: 0.0500 (0.0500)  time: 0.3059  data: 0.0003  max mem: 41808
Epoch: [12]  [ 340/1349]  eta: 0:05:23  lr: 0.000469  min_lr: 0.000011  loss: 0.8213 (0.8454)  loss_scale: 65536.0000 (88982.8974)  weight_decay: 0.0500 (0.0500)  time: 0.3062  data: 0.0002  max mem: 41808
Epoch: [12]  [ 350/1349]  eta: 0:05:19  lr: 0.000469  min_lr: 0.000011  loss: 0.9287 (0.8479)  loss_scale: 65536.0000 (88314.8946)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0002  max mem: 41808
Epoch: [12]  [ 360/1349]  eta: 0:05:15  lr: 0.000469  min_lr: 0.000011  loss: 0.9152 (0.8466)  loss_scale: 65536.0000 (87683.9003)  weight_decay: 0.0500 (0.0500)  time: 0.3061  data: 0.0002  max mem: 41808
[2025-05-23 17:32:13,637] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 17:32:13,637] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-05-23 17:32:13,637] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 17:32:13,637] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [12]  [ 370/1349]  eta: 0:05:12  lr: 0.000469  min_lr: 0.000011  loss: 0.8405 (0.8477)  loss_scale: 65536.0000 (87616.8625)  weight_decay: 0.0500 (0.0500)  time: 0.3061  data: 0.0002  max mem: 41808
Epoch: [12]  [ 380/1349]  eta: 0:05:08  lr: 0.000468  min_lr: 0.000011  loss: 0.8915 (0.8483)  loss_scale: 131072.0000 (88757.4173)  weight_decay: 0.0500 (0.0500)  time: 0.3062  data: 0.0002  max mem: 41808
Epoch: [12]  [ 390/1349]  eta: 0:05:05  lr: 0.000468  min_lr: 0.000011  loss: 0.9261 (0.8501)  loss_scale: 131072.0000 (89839.6317)  weight_decay: 0.0500 (0.0500)  time: 0.3063  data: 0.0003  max mem: 41808
Epoch: [12]  [ 400/1349]  eta: 0:05:01  lr: 0.000468  min_lr: 0.000011  loss: 0.9261 (0.8506)  loss_scale: 131072.0000 (90867.8703)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0002  max mem: 41808
Epoch: [12]  [ 410/1349]  eta: 0:04:58  lr: 0.000468  min_lr: 0.000011  loss: 0.9161 (0.8526)  loss_scale: 131072.0000 (91846.0730)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0002  max mem: 41808
Epoch: [12]  [ 420/1349]  eta: 0:04:55  lr: 0.000468  min_lr: 0.000011  loss: 0.9150 (0.8529)  loss_scale: 131072.0000 (92777.8052)  weight_decay: 0.0500 (0.0500)  time: 0.3064  data: 0.0002  max mem: 41808
Epoch: [12]  [ 430/1349]  eta: 0:04:51  lr: 0.000468  min_lr: 0.000011  loss: 0.8409 (0.8518)  loss_scale: 131072.0000 (93666.3016)  weight_decay: 0.0500 (0.0500)  time: 0.3060  data: 0.0002  max mem: 41808
Epoch: [12]  [ 440/1349]  eta: 0:04:48  lr: 0.000468  min_lr: 0.000011  loss: 0.8490 (0.8519)  loss_scale: 131072.0000 (94514.5034)  weight_decay: 0.0500 (0.0500)  time: 0.3061  data: 0.0002  max mem: 41808
Epoch: [12]  [ 450/1349]  eta: 0:04:44  lr: 0.000468  min_lr: 0.000011  loss: 0.8818 (0.8520)  loss_scale: 131072.0000 (95325.0909)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0002  max mem: 41808
Epoch: [12]  [ 460/1349]  eta: 0:04:41  lr: 0.000468  min_lr: 0.000011  loss: 0.8765 (0.8524)  loss_scale: 131072.0000 (96100.5119)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0002  max mem: 41808
Epoch: [12]  [ 470/1349]  eta: 0:04:38  lr: 0.000468  min_lr: 0.000011  loss: 0.8385 (0.8517)  loss_scale: 131072.0000 (96843.0064)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0002  max mem: 41808
Epoch: [12]  [ 480/1349]  eta: 0:04:34  lr: 0.000468  min_lr: 0.000011  loss: 0.8917 (0.8536)  loss_scale: 131072.0000 (97554.6279)  weight_decay: 0.0500 (0.0500)  time: 0.3090  data: 0.0002  max mem: 41808
[2025-05-23 17:32:49,258] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 16672
[2025-05-23 17:32:49,258] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 16672
[2025-05-23 17:32:49,258] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 17:32:49,258] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 17:32:49,258] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [12]  [ 490/1349]  eta: 0:04:31  lr: 0.000468  min_lr: 0.000011  loss: 0.8989 (0.8536)  loss_scale: 131072.0000 (97302.9409)  weight_decay: 0.0500 (0.0500)  time: 0.3090  data: 0.0003  max mem: 41808
Epoch: [12]  [ 500/1349]  eta: 0:04:28  lr: 0.000468  min_lr: 0.000011  loss: 0.8367 (0.8516)  loss_scale: 65536.0000 (96668.8703)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0003  max mem: 41808
Epoch: [12]  [ 510/1349]  eta: 0:04:24  lr: 0.000468  min_lr: 0.000011  loss: 0.8509 (0.8522)  loss_scale: 65536.0000 (96059.6164)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0002  max mem: 41808
Epoch: [12]  [ 520/1349]  eta: 0:04:21  lr: 0.000468  min_lr: 0.000011  loss: 0.8509 (0.8515)  loss_scale: 65536.0000 (95473.7505)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0002  max mem: 41808
Epoch: [12]  [ 530/1349]  eta: 0:04:18  lr: 0.000468  min_lr: 0.000011  loss: 0.8422 (0.8507)  loss_scale: 65536.0000 (94909.9510)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0003  max mem: 41808
Epoch: [12]  [ 540/1349]  eta: 0:04:15  lr: 0.000467  min_lr: 0.000011  loss: 0.8131 (0.8511)  loss_scale: 65536.0000 (94366.9945)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0003  max mem: 41808
Epoch: [12]  [ 550/1349]  eta: 0:04:11  lr: 0.000467  min_lr: 0.000011  loss: 0.8842 (0.8518)  loss_scale: 65536.0000 (93843.7459)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0002  max mem: 41808
Epoch: [12]  [ 560/1349]  eta: 0:04:08  lr: 0.000467  min_lr: 0.000011  loss: 0.8842 (0.8524)  loss_scale: 65536.0000 (93339.1515)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0002  max mem: 41808
Epoch: [12]  [ 570/1349]  eta: 0:04:05  lr: 0.000467  min_lr: 0.000011  loss: 0.8710 (0.8517)  loss_scale: 65536.0000 (92852.2312)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0002  max mem: 41808
Epoch: [12]  [ 580/1349]  eta: 0:04:02  lr: 0.000467  min_lr: 0.000011  loss: 0.8615 (0.8526)  loss_scale: 65536.0000 (92382.0723)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0002  max mem: 41808
Epoch: [12]  [ 590/1349]  eta: 0:03:58  lr: 0.000467  min_lr: 0.000011  loss: 0.9294 (0.8529)  loss_scale: 65536.0000 (91927.8240)  weight_decay: 0.0500 (0.0500)  time: 0.3079  data: 0.0002  max mem: 41808
Epoch: [12]  [ 600/1349]  eta: 0:03:55  lr: 0.000467  min_lr: 0.000011  loss: 0.8973 (0.8538)  loss_scale: 65536.0000 (91488.6922)  weight_decay: 0.0500 (0.0500)  time: 0.3078  data: 0.0002  max mem: 41808
Epoch: [12]  [ 610/1349]  eta: 0:03:52  lr: 0.000467  min_lr: 0.000011  loss: 0.8169 (0.8531)  loss_scale: 65536.0000 (91063.9345)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0002  max mem: 41808
[2025-05-23 17:33:28,917] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 17:33:28,917] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-05-23 17:33:28,917] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 17:33:28,918] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [12]  [ 620/1349]  eta: 0:03:49  lr: 0.000467  min_lr: 0.000011  loss: 0.8030 (0.8533)  loss_scale: 65536.0000 (91497.1208)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0002  max mem: 41808
Epoch: [12]  [ 630/1349]  eta: 0:03:45  lr: 0.000467  min_lr: 0.000011  loss: 0.8469 (0.8521)  loss_scale: 131072.0000 (92124.2979)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0002  max mem: 41808
Epoch: [12]  [ 640/1349]  eta: 0:03:42  lr: 0.000467  min_lr: 0.000011  loss: 0.8571 (0.8530)  loss_scale: 131072.0000 (92731.9064)  weight_decay: 0.0500 (0.0500)  time: 0.3086  data: 0.0002  max mem: 41808
Epoch: [12]  [ 650/1349]  eta: 0:03:39  lr: 0.000467  min_lr: 0.000011  loss: 0.8732 (0.8520)  loss_scale: 131072.0000 (93320.8479)  weight_decay: 0.0500 (0.0500)  time: 0.3078  data: 0.0002  max mem: 41808
Epoch: [12]  [ 660/1349]  eta: 0:03:36  lr: 0.000467  min_lr: 0.000011  loss: 0.8244 (0.8519)  loss_scale: 131072.0000 (93891.9697)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0002  max mem: 41808
Epoch: [12]  [ 670/1349]  eta: 0:03:33  lr: 0.000467  min_lr: 0.000011  loss: 0.8176 (0.8513)  loss_scale: 131072.0000 (94446.0686)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0002  max mem: 41808
Epoch: [12]  [ 680/1349]  eta: 0:03:29  lr: 0.000467  min_lr: 0.000011  loss: 0.8620 (0.8523)  loss_scale: 131072.0000 (94983.8943)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0002  max mem: 41808
Epoch: [12]  [ 690/1349]  eta: 0:03:26  lr: 0.000466  min_lr: 0.000011  loss: 0.9446 (0.8534)  loss_scale: 131072.0000 (95506.1534)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0002  max mem: 41808
Epoch: [12]  [ 700/1349]  eta: 0:03:23  lr: 0.000466  min_lr: 0.000011  loss: 0.8409 (0.8525)  loss_scale: 131072.0000 (96013.5121)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0002  max mem: 41808
Epoch: [12]  [ 710/1349]  eta: 0:03:20  lr: 0.000466  min_lr: 0.000011  loss: 0.7570 (0.8521)  loss_scale: 131072.0000 (96506.5992)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0002  max mem: 41808
Epoch: [12]  [ 720/1349]  eta: 0:03:17  lr: 0.000466  min_lr: 0.000011  loss: 0.8787 (0.8525)  loss_scale: 131072.0000 (96986.0083)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0002  max mem: 41808
Epoch: [12]  [ 730/1349]  eta: 0:03:13  lr: 0.000466  min_lr: 0.000011  loss: 0.8970 (0.8527)  loss_scale: 131072.0000 (97452.3010)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0002  max mem: 41808
[2025-05-23 17:34:07,064] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 16925
[2025-05-23 17:34:07,064] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 17:34:07,064] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 16925
[2025-05-23 17:34:07,064] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
[2025-05-23 17:34:07,064] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
Epoch: [12]  [ 740/1349]  eta: 0:03:10  lr: 0.000466  min_lr: 0.000011  loss: 0.8091 (0.8524)  loss_scale: 131072.0000 (97552.2375)  weight_decay: 0.0500 (0.0500)  time: 0.3079  data: 0.0002  max mem: 41808
Epoch: [12]  [ 750/1349]  eta: 0:03:07  lr: 0.000466  min_lr: 0.000011  loss: 0.8441 (0.8527)  loss_scale: 65536.0000 (97125.9228)  weight_decay: 0.0500 (0.0500)  time: 0.3080  data: 0.0002  max mem: 41808
Epoch: [12]  [ 760/1349]  eta: 0:03:04  lr: 0.000466  min_lr: 0.000011  loss: 0.8954 (0.8538)  loss_scale: 65536.0000 (96710.8121)  weight_decay: 0.0500 (0.0500)  time: 0.3087  data: 0.0001  max mem: 41808
Epoch: [12]  [ 770/1349]  eta: 0:03:01  lr: 0.000466  min_lr: 0.000011  loss: 0.8283 (0.8529)  loss_scale: 65536.0000 (96306.4695)  weight_decay: 0.0500 (0.0500)  time: 0.3089  data: 0.0001  max mem: 41808
Epoch: [12]  [ 780/1349]  eta: 0:02:58  lr: 0.000466  min_lr: 0.000011  loss: 0.8117 (0.8536)  loss_scale: 65536.0000 (95912.4814)  weight_decay: 0.0500 (0.0500)  time: 0.3090  data: 0.0001  max mem: 41808
Epoch: [12]  [ 790/1349]  eta: 0:02:54  lr: 0.000466  min_lr: 0.000011  loss: 0.9469 (0.8547)  loss_scale: 65536.0000 (95528.4551)  weight_decay: 0.0500 (0.0500)  time: 0.3093  data: 0.0002  max mem: 41808
Epoch: [12]  [ 800/1349]  eta: 0:02:51  lr: 0.000466  min_lr: 0.000011  loss: 0.9336 (0.8553)  loss_scale: 65536.0000 (95154.0175)  weight_decay: 0.0500 (0.0500)  time: 0.3085  data: 0.0001  max mem: 41808
Epoch: [12]  [ 810/1349]  eta: 0:02:48  lr: 0.000466  min_lr: 0.000011  loss: 0.8149 (0.8547)  loss_scale: 65536.0000 (94788.8138)  weight_decay: 0.0500 (0.0500)  time: 0.3090  data: 0.0002  max mem: 41808
[2025-05-23 17:34:29,937] [INFO] [logging.py:96:log_dist] [Rank 0] step=17000, skipped=101, lr=[1.1063442010314765e-05, 1.1063442010314765e-05, 1.475125601375302e-05, 1.475125601375302e-05, 1.9668341351670693e-05, 1.9668341351670693e-05, 2.6224455135560924e-05, 2.6224455135560924e-05, 3.49659401807479e-05, 3.49659401807479e-05, 4.662125357433053e-05, 4.662125357433053e-05, 6.216167143244071e-05, 6.216167143244071e-05, 8.288222857658761e-05, 8.288222857658761e-05, 0.00011050963810211682, 0.00011050963810211682, 0.00014734618413615575, 0.00014734618413615575, 0.00019646157884820766, 0.00019646157884820766, 0.0002619487717976102, 0.0002619487717976102, 0.0003492650290634803, 0.0003492650290634803, 0.00046568670541797375, 0.00046568670541797375], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-05-23 17:34:29,938] [INFO] [timer.py:260:stop] epoch=0/micro_step=17000/global_step=17000, RunningAvgSamplesPerSec=205.1989528937128, CurrSamplesPerSec=213.56946137321984, MemAllocated=1.37GB, MaxMemAllocated=40.83GB
Epoch: [12]  [ 820/1349]  eta: 0:02:45  lr: 0.000466  min_lr: 0.000011  loss: 0.7458 (0.8544)  loss_scale: 65536.0000 (94432.5067)  weight_decay: 0.0500 (0.0500)  time: 0.3096  data: 0.0002  max mem: 41808
Epoch: [12]  [ 830/1349]  eta: 0:02:42  lr: 0.000466  min_lr: 0.000011  loss: 0.8137 (0.8548)  loss_scale: 65536.0000 (94084.7750)  weight_decay: 0.0500 (0.0500)  time: 0.3082  data: 0.0002  max mem: 41808
Epoch: [12]  [ 840/1349]  eta: 0:02:39  lr: 0.000465  min_lr: 0.000011  loss: 0.8501 (0.8548)  loss_scale: 65536.0000 (93745.3127)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0002  max mem: 41808
Epoch: [12]  [ 850/1349]  eta: 0:02:35  lr: 0.000465  min_lr: 0.000011  loss: 0.9026 (0.8563)  loss_scale: 65536.0000 (93413.8284)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0002  max mem: 41808
Epoch: [12]  [ 860/1349]  eta: 0:02:32  lr: 0.000465  min_lr: 0.000011  loss: 0.9144 (0.8565)  loss_scale: 65536.0000 (93090.0441)  weight_decay: 0.0500 (0.0500)  time: 0.3089  data: 0.0001  max mem: 41808
[2025-05-23 17:34:46,891] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 17:34:46,891] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-05-23 17:34:46,891] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 17:34:46,891] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [12]  [ 870/1349]  eta: 0:02:29  lr: 0.000465  min_lr: 0.000011  loss: 0.8528 (0.8563)  loss_scale: 65536.0000 (93149.9059)  weight_decay: 0.0500 (0.0500)  time: 0.3086  data: 0.0001  max mem: 41808
Epoch: [12]  [ 880/1349]  eta: 0:02:26  lr: 0.000465  min_lr: 0.000011  loss: 0.8423 (0.8561)  loss_scale: 131072.0000 (93580.3496)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
Epoch: [12]  [ 890/1349]  eta: 0:02:23  lr: 0.000465  min_lr: 0.000011  loss: 0.8352 (0.8560)  loss_scale: 131072.0000 (94001.1313)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
Epoch: [12]  [ 900/1349]  eta: 0:02:20  lr: 0.000465  min_lr: 0.000011  loss: 0.8431 (0.8560)  loss_scale: 131072.0000 (94412.5727)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0002  max mem: 41808
Epoch: [12]  [ 910/1349]  eta: 0:02:17  lr: 0.000465  min_lr: 0.000011  loss: 0.8474 (0.8563)  loss_scale: 131072.0000 (94814.9813)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [12]  [ 920/1349]  eta: 0:02:13  lr: 0.000465  min_lr: 0.000011  loss: 0.8613 (0.8560)  loss_scale: 131072.0000 (95208.6515)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [12]  [ 930/1349]  eta: 0:02:10  lr: 0.000465  min_lr: 0.000011  loss: 0.8613 (0.8562)  loss_scale: 131072.0000 (95593.8647)  weight_decay: 0.0500 (0.0500)  time: 0.3080  data: 0.0001  max mem: 41808
Epoch: [12]  [ 940/1349]  eta: 0:02:07  lr: 0.000465  min_lr: 0.000011  loss: 0.8901 (0.8560)  loss_scale: 131072.0000 (95970.8905)  weight_decay: 0.0500 (0.0500)  time: 0.3087  data: 0.0001  max mem: 41808
Epoch: [12]  [ 950/1349]  eta: 0:02:04  lr: 0.000465  min_lr: 0.000011  loss: 0.8702 (0.8562)  loss_scale: 131072.0000 (96339.9874)  weight_decay: 0.0500 (0.0500)  time: 0.3080  data: 0.0001  max mem: 41808
Epoch: [12]  [ 960/1349]  eta: 0:02:01  lr: 0.000465  min_lr: 0.000011  loss: 0.8509 (0.8560)  loss_scale: 131072.0000 (96701.4027)  weight_decay: 0.0500 (0.0500)  time: 0.3081  data: 0.0002  max mem: 41808
Epoch: [12]  [ 970/1349]  eta: 0:01:58  lr: 0.000465  min_lr: 0.000011  loss: 0.8864 (0.8564)  loss_scale: 131072.0000 (97055.3738)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0001  max mem: 41808
Epoch: [12]  [ 980/1349]  eta: 0:01:55  lr: 0.000465  min_lr: 0.000011  loss: 0.9620 (0.8570)  loss_scale: 131072.0000 (97402.1284)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
Epoch: [12]  [ 990/1349]  eta: 0:01:51  lr: 0.000465  min_lr: 0.000011  loss: 0.8723 (0.8564)  loss_scale: 131072.0000 (97741.8850)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
[2025-05-23 17:35:26,265] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 17:35:26,265] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 17:35:26,265] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 17:35:26,265] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 17:35:26,570] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 17183
[2025-05-23 17:35:26,570] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 17:35:26,570] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 17183
[2025-05-23 17:35:26,570] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 17:35:26,570] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [12]  [1000/1349]  eta: 0:01:48  lr: 0.000464  min_lr: 0.000011  loss: 0.8723 (0.8568)  loss_scale: 131072.0000 (98205.7942)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0002  max mem: 41808
Epoch: [12]  [1010/1349]  eta: 0:01:45  lr: 0.000464  min_lr: 0.000011  loss: 0.9422 (0.8572)  loss_scale: 131072.0000 (98530.8803)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
[2025-05-23 17:35:31,793] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 17200
[2025-05-23 17:35:31,793] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 17200
[2025-05-23 17:35:31,794] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 17:35:31,794] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 17:35:31,794] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [12]  [1020/1349]  eta: 0:01:42  lr: 0.000464  min_lr: 0.000011  loss: 0.8905 (0.8568)  loss_scale: 131072.0000 (98271.9060)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
Epoch: [12]  [1030/1349]  eta: 0:01:39  lr: 0.000464  min_lr: 0.000011  loss: 0.9192 (0.8574)  loss_scale: 65536.0000 (97954.3899)  weight_decay: 0.0500 (0.0500)  time: 0.3081  data: 0.0001  max mem: 41808
Epoch: [12]  [1040/1349]  eta: 0:01:36  lr: 0.000464  min_lr: 0.000011  loss: 0.9163 (0.8576)  loss_scale: 65536.0000 (97642.9741)  weight_decay: 0.0500 (0.0500)  time: 0.3081  data: 0.0001  max mem: 41808
Epoch: [12]  [1050/1349]  eta: 0:01:33  lr: 0.000464  min_lr: 0.000011  loss: 0.8620 (0.8570)  loss_scale: 65536.0000 (97337.4843)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [12]  [1060/1349]  eta: 0:01:30  lr: 0.000464  min_lr: 0.000011  loss: 0.7841 (0.8562)  loss_scale: 65536.0000 (97037.7531)  weight_decay: 0.0500 (0.0500)  time: 0.3060  data: 0.0001  max mem: 41808
Epoch: [12]  [1070/1349]  eta: 0:01:26  lr: 0.000464  min_lr: 0.000011  loss: 0.8108 (0.8556)  loss_scale: 65536.0000 (96743.6190)  weight_decay: 0.0500 (0.0500)  time: 0.3058  data: 0.0001  max mem: 41808
Epoch: [12]  [1080/1349]  eta: 0:01:23  lr: 0.000464  min_lr: 0.000011  loss: 0.8108 (0.8554)  loss_scale: 65536.0000 (96454.9269)  weight_decay: 0.0500 (0.0500)  time: 0.3059  data: 0.0001  max mem: 41808
Epoch: [12]  [1090/1349]  eta: 0:01:20  lr: 0.000464  min_lr: 0.000011  loss: 0.7714 (0.8551)  loss_scale: 65536.0000 (96171.5270)  weight_decay: 0.0500 (0.0500)  time: 0.3066  data: 0.0001  max mem: 41808
Epoch: [12]  [1100/1349]  eta: 0:01:17  lr: 0.000464  min_lr: 0.000011  loss: 0.8405 (0.8555)  loss_scale: 65536.0000 (95893.2752)  weight_decay: 0.0500 (0.0500)  time: 0.3079  data: 0.0001  max mem: 41808
Epoch: [12]  [1110/1349]  eta: 0:01:14  lr: 0.000464  min_lr: 0.000011  loss: 0.8665 (0.8546)  loss_scale: 65536.0000 (95620.0324)  weight_decay: 0.0500 (0.0500)  time: 0.3079  data: 0.0001  max mem: 41808
Epoch: [12]  [1120/1349]  eta: 0:01:11  lr: 0.000464  min_lr: 0.000011  loss: 0.8495 (0.8553)  loss_scale: 65536.0000 (95351.6646)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [12]  [1130/1349]  eta: 0:01:08  lr: 0.000464  min_lr: 0.000011  loss: 0.8940 (0.8558)  loss_scale: 65536.0000 (95088.0424)  weight_decay: 0.0500 (0.0500)  time: 0.3066  data: 0.0001  max mem: 41808
Epoch: [12]  [1140/1349]  eta: 0:01:05  lr: 0.000464  min_lr: 0.000011  loss: 0.8713 (0.8556)  loss_scale: 65536.0000 (94829.0412)  weight_decay: 0.0500 (0.0500)  time: 0.3081  data: 0.0001  max mem: 41808
[2025-05-23 17:36:11,448] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 17:36:11,448] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-05-23 17:36:11,448] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 17:36:11,448] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [12]  [1150/1349]  eta: 0:01:01  lr: 0.000463  min_lr: 0.000011  loss: 0.8520 (0.8550)  loss_scale: 65536.0000 (95143.9235)  weight_decay: 0.0500 (0.0500)  time: 0.3085  data: 0.0001  max mem: 41808
Epoch: [12]  [1160/1349]  eta: 0:00:58  lr: 0.000463  min_lr: 0.000011  loss: 0.8795 (0.8554)  loss_scale: 131072.0000 (95453.3816)  weight_decay: 0.0500 (0.0500)  time: 0.3066  data: 0.0002  max mem: 41808
Epoch: [12]  [1170/1349]  eta: 0:00:55  lr: 0.000463  min_lr: 0.000011  loss: 0.9171 (0.8560)  loss_scale: 131072.0000 (95757.5542)  weight_decay: 0.0500 (0.0500)  time: 0.3063  data: 0.0002  max mem: 41808
Epoch: [12]  [1180/1349]  eta: 0:00:52  lr: 0.000463  min_lr: 0.000011  loss: 0.9324 (0.8570)  loss_scale: 131072.0000 (96056.5758)  weight_decay: 0.0500 (0.0500)  time: 0.3063  data: 0.0002  max mem: 41808
Epoch: [12]  [1190/1349]  eta: 0:00:49  lr: 0.000463  min_lr: 0.000011  loss: 0.8851 (0.8573)  loss_scale: 131072.0000 (96350.5760)  weight_decay: 0.0500 (0.0500)  time: 0.3063  data: 0.0002  max mem: 41808
Epoch: [12]  [1200/1349]  eta: 0:00:46  lr: 0.000463  min_lr: 0.000011  loss: 0.8593 (0.8573)  loss_scale: 131072.0000 (96639.6803)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0002  max mem: 41808
Epoch: [12]  [1210/1349]  eta: 0:00:43  lr: 0.000463  min_lr: 0.000011  loss: 0.8448 (0.8574)  loss_scale: 131072.0000 (96924.0099)  weight_decay: 0.0500 (0.0500)  time: 0.3064  data: 0.0002  max mem: 41808
Epoch: [12]  [1220/1349]  eta: 0:00:40  lr: 0.000463  min_lr: 0.000011  loss: 0.8795 (0.8578)  loss_scale: 131072.0000 (97203.6822)  weight_decay: 0.0500 (0.0500)  time: 0.3062  data: 0.0003  max mem: 41808
Epoch: [12]  [1230/1349]  eta: 0:00:36  lr: 0.000463  min_lr: 0.000011  loss: 0.8743 (0.8575)  loss_scale: 131072.0000 (97478.8107)  weight_decay: 0.0500 (0.0500)  time: 0.3059  data: 0.0002  max mem: 41808
Epoch: [12]  [1240/1349]  eta: 0:00:33  lr: 0.000463  min_lr: 0.000011  loss: 0.8867 (0.8578)  loss_scale: 131072.0000 (97749.5052)  weight_decay: 0.0500 (0.0500)  time: 0.3060  data: 0.0002  max mem: 41808
Epoch: [12]  [1250/1349]  eta: 0:00:30  lr: 0.000463  min_lr: 0.000011  loss: 0.8867 (0.8575)  loss_scale: 131072.0000 (98015.8721)  weight_decay: 0.0500 (0.0500)  time: 0.3064  data: 0.0002  max mem: 41808
Epoch: [12]  [1260/1349]  eta: 0:00:27  lr: 0.000463  min_lr: 0.000011  loss: 0.8298 (0.8574)  loss_scale: 131072.0000 (98278.0143)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0002  max mem: 41808
[2025-05-23 17:36:50,672] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 17:36:50,672] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 17:36:50,672] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 17:36:50,672] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [12]  [1270/1349]  eta: 0:00:24  lr: 0.000463  min_lr: 0.000011  loss: 0.8450 (0.8574)  loss_scale: 131072.0000 (98742.2817)  weight_decay: 0.0500 (0.0500)  time: 0.3064  data: 0.0001  max mem: 41808
[2025-05-23 17:36:51,281] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 17459
[2025-05-23 17:36:51,281] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 17459
[2025-05-23 17:36:51,281] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 17:36:51,281] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 17:36:51,281] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [12]  [1280/1349]  eta: 0:00:21  lr: 0.000463  min_lr: 0.000011  loss: 0.8548 (0.8576)  loss_scale: 131072.0000 (98994.6604)  weight_decay: 0.0500 (0.0500)  time: 0.3055  data: 0.0002  max mem: 41808
Epoch: [12]  [1290/1349]  eta: 0:00:18  lr: 0.000462  min_lr: 0.000011  loss: 0.9103 (0.8577)  loss_scale: 131072.0000 (99243.1294)  weight_decay: 0.0500 (0.0500)  time: 0.3060  data: 0.0002  max mem: 41808
Epoch: [12]  [1300/1349]  eta: 0:00:15  lr: 0.000462  min_lr: 0.000011  loss: 0.8258 (0.8570)  loss_scale: 131072.0000 (99487.7786)  weight_decay: 0.0500 (0.0500)  time: 0.3064  data: 0.0002  max mem: 41808
Epoch: [12]  [1310/1349]  eta: 0:00:12  lr: 0.000462  min_lr: 0.000011  loss: 0.7806 (0.8571)  loss_scale: 131072.0000 (99728.6957)  weight_decay: 0.0500 (0.0500)  time: 0.3061  data: 0.0002  max mem: 41808
Epoch: [12]  [1320/1349]  eta: 0:00:09  lr: 0.000462  min_lr: 0.000011  loss: 0.9058 (0.8576)  loss_scale: 131072.0000 (99965.9652)  weight_decay: 0.0500 (0.0500)  time: 0.3059  data: 0.0002  max mem: 41808
Epoch: [12]  [1330/1349]  eta: 0:00:05  lr: 0.000462  min_lr: 0.000011  loss: 0.9058 (0.8576)  loss_scale: 131072.0000 (100199.6694)  weight_decay: 0.0500 (0.0500)  time: 0.3039  data: 0.0002  max mem: 41808
Epoch: [12]  [1340/1349]  eta: 0:00:02  lr: 0.000462  min_lr: 0.000011  loss: 0.8524 (0.8573)  loss_scale: 131072.0000 (100429.8881)  weight_decay: 0.0500 (0.0500)  time: 0.3017  data: 0.0001  max mem: 41808
Epoch: [12]  [1348/1349]  eta: 0:00:00  lr: 0.000462  min_lr: 0.000011  loss: 0.8622 (0.8576)  loss_scale: 131072.0000 (100611.6056)  weight_decay: 0.0500 (0.0500)  time: 0.3009  data: 0.0002  max mem: 41808
Epoch: [12] Total time: 0:06:59 (0.3107 s / it)
Averaged stats: lr: 0.000462  min_lr: 0.000011  loss: 0.8622 (0.8568)  loss_scale: 131072.0000 (100611.6056)  weight_decay: 0.0500 (0.0500)  total_time: 419.1912 (419.1870)
Val:  [  0/346]  eta: 1:12:16  loss: 2.9366 (2.9366)  acc1: 0.0000 (0.0000)  acc5: 98.4375 (98.4375)  time: 12.5324  data: 11.5364  max mem: 41808
Val:  [ 10/346]  eta: 0:11:00  loss: 0.1324 (0.5217)  acc1: 100.0000 (85.8665)  acc5: 100.0000 (99.2188)  time: 1.9659  data: 1.1326  max mem: 41808
Val:  [ 20/346]  eta: 0:08:19  loss: 0.1251 (0.4261)  acc1: 100.0000 (89.3973)  acc5: 100.0000 (99.5908)  time: 0.9827  data: 0.1586  max mem: 41808
Val:  [ 30/346]  eta: 0:06:46  loss: 0.1321 (0.3650)  acc1: 98.4375 (91.2298)  acc5: 100.0000 (99.7228)  time: 0.9125  data: 0.1126  max mem: 41808
Val:  [ 40/346]  eta: 0:05:54  loss: 0.1321 (0.4317)  acc1: 98.4375 (89.3674)  acc5: 100.0000 (99.4855)  time: 0.7646  data: 0.0003  max mem: 41808
Val:  [ 50/346]  eta: 0:05:17  loss: 0.1012 (0.3753)  acc1: 99.2188 (91.1458)  acc5: 100.0000 (99.5711)  time: 0.7442  data: 0.0003  max mem: 41808
Val:  [ 60/346]  eta: 0:04:53  loss: 0.1323 (0.3530)  acc1: 99.2188 (91.8801)  acc5: 100.0000 (99.6286)  time: 0.7601  data: 0.0003  max mem: 41808
Val:  [ 70/346]  eta: 0:04:33  loss: 0.2762 (0.3877)  acc1: 92.1875 (90.5700)  acc5: 100.0000 (99.6259)  time: 0.7838  data: 0.0234  max mem: 41808
Val:  [ 80/346]  eta: 0:04:19  loss: 0.2346 (0.3805)  acc1: 88.2812 (90.9433)  acc5: 100.0000 (99.4213)  time: 0.8124  data: 0.0872  max mem: 41808
Val:  [ 90/346]  eta: 0:04:07  loss: 0.2017 (0.3751)  acc1: 93.7500 (91.0800)  acc5: 100.0000 (99.4763)  time: 0.8694  data: 0.1308  max mem: 41808
Val:  [100/346]  eta: 0:03:55  loss: 0.1899 (0.3537)  acc1: 96.8750 (91.8394)  acc5: 100.0000 (99.5282)  time: 0.8819  data: 0.1342  max mem: 41808
Val:  [110/346]  eta: 0:03:43  loss: 0.1802 (0.3645)  acc1: 97.6562 (91.3288)  acc5: 100.0000 (99.4651)  time: 0.8699  data: 0.1311  max mem: 41808
Val:  [120/346]  eta: 0:03:32  loss: 0.1848 (0.3660)  acc1: 96.8750 (91.2255)  acc5: 100.0000 (99.5093)  time: 0.8639  data: 0.1310  max mem: 41808
Val:  [130/346]  eta: 0:03:22  loss: 0.1192 (0.3696)  acc1: 97.6562 (90.9470)  acc5: 100.0000 (99.4573)  time: 0.8781  data: 0.1425  max mem: 41808
Val:  [140/346]  eta: 0:03:12  loss: 0.1982 (0.3635)  acc1: 95.3125 (91.1126)  acc5: 100.0000 (99.4958)  time: 0.8978  data: 0.1488  max mem: 41808
Val:  [150/346]  eta: 0:03:02  loss: 0.3078 (0.3669)  acc1: 92.9688 (91.0234)  acc5: 100.0000 (99.5188)  time: 0.9060  data: 0.1457  max mem: 41808
Val:  [160/346]  eta: 0:02:53  loss: 0.2901 (0.3673)  acc1: 91.4062 (90.8725)  acc5: 100.0000 (99.5439)  time: 0.9033  data: 0.1550  max mem: 41808
Val:  [170/346]  eta: 0:02:43  loss: 0.4148 (0.3779)  acc1: 84.3750 (90.4240)  acc5: 100.0000 (99.5705)  time: 0.9079  data: 0.1704  max mem: 41808
Val:  [180/346]  eta: 0:02:33  loss: 0.4506 (0.3978)  acc1: 82.8125 (89.7272)  acc5: 100.0000 (99.5943)  time: 0.8961  data: 0.1583  max mem: 41808
Val:  [190/346]  eta: 0:02:24  loss: 0.4140 (0.4035)  acc1: 87.5000 (89.4593)  acc5: 100.0000 (99.5910)  time: 0.8937  data: 0.1478  max mem: 41808
Val:  [200/346]  eta: 0:02:15  loss: 0.4140 (0.4166)  acc1: 88.2812 (89.0042)  acc5: 100.0000 (99.5375)  time: 0.9083  data: 0.1526  max mem: 41808
Val:  [210/346]  eta: 0:02:05  loss: 0.2171 (0.4086)  acc1: 94.5312 (89.2773)  acc5: 100.0000 (99.5594)  time: 0.8865  data: 0.1523  max mem: 41808
Val:  [220/346]  eta: 0:01:56  loss: 0.1762 (0.4062)  acc1: 96.8750 (89.3913)  acc5: 100.0000 (99.4980)  time: 0.9045  data: 0.1570  max mem: 41808
Val:  [230/346]  eta: 0:01:47  loss: 0.1762 (0.3968)  acc1: 96.0938 (89.7288)  acc5: 100.0000 (99.5198)  time: 0.9219  data: 0.1635  max mem: 41808
Val:  [240/346]  eta: 0:01:37  loss: 0.2086 (0.4037)  acc1: 96.0938 (89.5779)  acc5: 100.0000 (99.4943)  time: 0.9057  data: 0.1615  max mem: 41808
Val:  [250/346]  eta: 0:01:28  loss: 0.2171 (0.3975)  acc1: 94.5312 (89.7815)  acc5: 100.0000 (99.5051)  time: 0.9076  data: 0.1621  max mem: 41808
Val:  [260/346]  eta: 0:01:19  loss: 0.1503 (0.3952)  acc1: 96.8750 (89.8587)  acc5: 100.0000 (99.5181)  time: 0.9249  data: 0.1562  max mem: 41808
Val:  [270/346]  eta: 0:01:10  loss: 0.1545 (0.3939)  acc1: 97.6562 (89.9158)  acc5: 100.0000 (99.5301)  time: 0.9205  data: 0.1518  max mem: 41808
Val:  [280/346]  eta: 0:01:00  loss: 0.1639 (0.3903)  acc1: 96.8750 (90.0439)  acc5: 100.0000 (99.5440)  time: 0.8841  data: 0.1508  max mem: 41808
Val:  [290/346]  eta: 0:00:51  loss: 0.0997 (0.3818)  acc1: 100.0000 (90.3189)  acc5: 100.0000 (99.5597)  time: 0.9061  data: 0.1616  max mem: 41808
Val:  [300/346]  eta: 0:00:42  loss: 0.1075 (0.3815)  acc1: 100.0000 (90.3862)  acc5: 100.0000 (99.4446)  time: 0.9074  data: 0.1657  max mem: 41808
Val:  [310/346]  eta: 0:00:33  loss: 0.1233 (0.3832)  acc1: 98.4375 (90.3210)  acc5: 100.0000 (99.4549)  time: 0.8817  data: 0.1494  max mem: 41808
Val:  [320/346]  eta: 0:00:23  loss: 0.1705 (0.3880)  acc1: 94.5312 (90.1796)  acc5: 100.0000 (99.4597)  time: 0.9002  data: 0.1475  max mem: 41808
Val:  [330/346]  eta: 0:00:14  loss: 0.3695 (0.3958)  acc1: 89.0625 (89.9334)  acc5: 100.0000 (99.3792)  time: 0.8822  data: 0.1466  max mem: 41808
Val:  [340/346]  eta: 0:00:05  loss: 0.3990 (0.4041)  acc1: 88.2812 (89.7086)  acc5: 100.0000 (99.3631)  time: 0.8838  data: 0.1606  max mem: 41808
Val:  [345/346]  eta: 0:00:00  loss: 0.2174 (0.4005)  acc1: 95.3125 (89.8253)  acc5: 100.0000 (99.3717)  time: 0.8853  data: 0.1787  max mem: 41808
Val: Total time: 0:05:16 (0.9161 s / it)
* Acc@1 89.916 Acc@5 99.386 loss 0.399
Accuracy of the network on the 88494 val videos: 89.9%
Max accuracy: 91.25%   Max Epoch: 3
Epoch: [13]  [   0/1349]  eta: 1:48:17  lr: 0.000462  min_lr: 0.000011  loss: 0.7174 (0.7174)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 4.8168  data: 4.4649  max mem: 41808
Epoch: [13]  [  10/1349]  eta: 0:16:22  lr: 0.000462  min_lr: 0.000011  loss: 0.8277 (0.8385)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7338  data: 0.4140  max mem: 41808
Epoch: [13]  [  20/1349]  eta: 0:11:47  lr: 0.000462  min_lr: 0.000011  loss: 0.8679 (0.8734)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3184  data: 0.0045  max mem: 41808
[2025-05-23 17:42:45,624] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 17565
[2025-05-23 17:42:45,624] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 17565
[2025-05-23 17:42:45,624] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 17:42:45,624] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 17:42:45,624] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [13]  [  30/1349]  eta: 0:10:06  lr: 0.000462  min_lr: 0.000011  loss: 0.9050 (0.8720)  loss_scale: 131072.0000 (124729.8065)  weight_decay: 0.0500 (0.0500)  time: 0.3086  data: 0.0001  max mem: 41808
Epoch: [13]  [  40/1349]  eta: 0:09:12  lr: 0.000462  min_lr: 0.000011  loss: 0.9050 (0.8872)  loss_scale: 65536.0000 (110292.2927)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0001  max mem: 41808
Epoch: [13]  [  50/1349]  eta: 0:08:38  lr: 0.000462  min_lr: 0.000011  loss: 0.8679 (0.8672)  loss_scale: 65536.0000 (101516.5490)  weight_decay: 0.0500 (0.0500)  time: 0.3064  data: 0.0001  max mem: 41808
Epoch: [13]  [  60/1349]  eta: 0:08:15  lr: 0.000462  min_lr: 0.000011  loss: 0.7602 (0.8541)  loss_scale: 65536.0000 (95618.0984)  weight_decay: 0.0500 (0.0500)  time: 0.3062  data: 0.0001  max mem: 41808
Epoch: [13]  [  70/1349]  eta: 0:07:57  lr: 0.000462  min_lr: 0.000011  loss: 0.7602 (0.8486)  loss_scale: 65536.0000 (91381.1831)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [13]  [  80/1349]  eta: 0:07:43  lr: 0.000462  min_lr: 0.000011  loss: 0.8510 (0.8458)  loss_scale: 65536.0000 (88190.4198)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
Epoch: [13]  [  90/1349]  eta: 0:07:31  lr: 0.000461  min_lr: 0.000011  loss: 0.8544 (0.8451)  loss_scale: 65536.0000 (85700.9231)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
Epoch: [13]  [ 100/1349]  eta: 0:07:21  lr: 0.000461  min_lr: 0.000011  loss: 0.8136 (0.8438)  loss_scale: 65536.0000 (83704.3960)  weight_decay: 0.0500 (0.0500)  time: 0.3064  data: 0.0001  max mem: 41808
Epoch: [13]  [ 110/1349]  eta: 0:07:12  lr: 0.000461  min_lr: 0.000011  loss: 0.8534 (0.8452)  loss_scale: 65536.0000 (82067.6036)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
Epoch: [13]  [ 120/1349]  eta: 0:07:05  lr: 0.000461  min_lr: 0.000011  loss: 0.8817 (0.8441)  loss_scale: 65536.0000 (80701.3554)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0002  max mem: 41808
Epoch: [13]  [ 130/1349]  eta: 0:06:58  lr: 0.000461  min_lr: 0.000011  loss: 0.8553 (0.8458)  loss_scale: 65536.0000 (79543.6947)  weight_decay: 0.0500 (0.0500)  time: 0.3089  data: 0.0002  max mem: 41808
Epoch: [13]  [ 140/1349]  eta: 0:06:51  lr: 0.000461  min_lr: 0.000011  loss: 0.8553 (0.8465)  loss_scale: 65536.0000 (78550.2411)  weight_decay: 0.0500 (0.0500)  time: 0.3086  data: 0.0002  max mem: 41808
Epoch: [13]  [ 150/1349]  eta: 0:06:45  lr: 0.000461  min_lr: 0.000011  loss: 0.9091 (0.8484)  loss_scale: 65536.0000 (77688.3709)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0002  max mem: 41808
[2025-05-23 17:43:25,264] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 17:43:25,264] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-05-23 17:43:25,264] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 17:43:25,264] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [13]  [ 160/1349]  eta: 0:06:40  lr: 0.000461  min_lr: 0.000011  loss: 0.8096 (0.8439)  loss_scale: 65536.0000 (78561.7888)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [13]  [ 170/1349]  eta: 0:06:34  lr: 0.000461  min_lr: 0.000011  loss: 0.8049 (0.8449)  loss_scale: 131072.0000 (81632.5614)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
Epoch: [13]  [ 180/1349]  eta: 0:06:29  lr: 0.000461  min_lr: 0.000011  loss: 0.8968 (0.8489)  loss_scale: 131072.0000 (84364.0221)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0002  max mem: 41808
Epoch: [13]  [ 190/1349]  eta: 0:06:24  lr: 0.000461  min_lr: 0.000011  loss: 0.9062 (0.8495)  loss_scale: 131072.0000 (86809.4660)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0001  max mem: 41808
Epoch: [13]  [ 200/1349]  eta: 0:06:20  lr: 0.000461  min_lr: 0.000011  loss: 0.8976 (0.8506)  loss_scale: 131072.0000 (89011.5821)  weight_decay: 0.0500 (0.0500)  time: 0.3078  data: 0.0001  max mem: 41808
[2025-05-23 17:43:41,688] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 17747
[2025-05-23 17:43:41,688] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 17747
[2025-05-23 17:43:41,688] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 17:43:41,688] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 17:43:41,688] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [13]  [ 210/1349]  eta: 0:06:16  lr: 0.000461  min_lr: 0.000011  loss: 0.8976 (0.8502)  loss_scale: 131072.0000 (90694.3697)  weight_decay: 0.0500 (0.0500)  time: 0.3130  data: 0.0001  max mem: 41808
Epoch: [13]  [ 220/1349]  eta: 0:06:11  lr: 0.000461  min_lr: 0.000011  loss: 0.8501 (0.8513)  loss_scale: 65536.0000 (89555.9819)  weight_decay: 0.0500 (0.0500)  time: 0.3132  data: 0.0001  max mem: 41808
Epoch: [13]  [ 230/1349]  eta: 0:06:07  lr: 0.000460  min_lr: 0.000011  loss: 0.8701 (0.8545)  loss_scale: 65536.0000 (88516.1558)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
Epoch: [13]  [ 240/1349]  eta: 0:06:03  lr: 0.000460  min_lr: 0.000011  loss: 0.8565 (0.8517)  loss_scale: 65536.0000 (87562.6224)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
Epoch: [13]  [ 250/1349]  eta: 0:05:58  lr: 0.000460  min_lr: 0.000011  loss: 0.7361 (0.8493)  loss_scale: 65536.0000 (86685.0677)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
Epoch: [13]  [ 260/1349]  eta: 0:05:54  lr: 0.000460  min_lr: 0.000011  loss: 0.7483 (0.8464)  loss_scale: 65536.0000 (85874.7586)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
Epoch: [13]  [ 270/1349]  eta: 0:05:50  lr: 0.000460  min_lr: 0.000011  loss: 0.7699 (0.8463)  loss_scale: 65536.0000 (85124.2509)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
Epoch: [13]  [ 280/1349]  eta: 0:05:46  lr: 0.000460  min_lr: 0.000011  loss: 0.8173 (0.8438)  loss_scale: 65536.0000 (84427.1601)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
Epoch: [13]  [ 290/1349]  eta: 0:05:43  lr: 0.000460  min_lr: 0.000011  loss: 0.8026 (0.8420)  loss_scale: 65536.0000 (83777.9794)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0002  max mem: 41808
Epoch: [13]  [ 300/1349]  eta: 0:05:39  lr: 0.000460  min_lr: 0.000011  loss: 0.8300 (0.8440)  loss_scale: 65536.0000 (83171.9336)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
Epoch: [13]  [ 310/1349]  eta: 0:05:35  lr: 0.000460  min_lr: 0.000011  loss: 0.8843 (0.8460)  loss_scale: 65536.0000 (82604.8617)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [13]  [ 320/1349]  eta: 0:05:31  lr: 0.000460  min_lr: 0.000011  loss: 0.8345 (0.8445)  loss_scale: 65536.0000 (82073.1215)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
Epoch: [13]  [ 330/1349]  eta: 0:05:28  lr: 0.000460  min_lr: 0.000011  loss: 0.7505 (0.8433)  loss_scale: 65536.0000 (81573.5106)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
[2025-05-23 17:44:21,357] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 17:44:21,357] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-05-23 17:44:21,358] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 17:44:21,358] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [13]  [ 340/1349]  eta: 0:05:24  lr: 0.000460  min_lr: 0.000011  loss: 0.7505 (0.8413)  loss_scale: 65536.0000 (81487.5777)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [13]  [ 350/1349]  eta: 0:05:20  lr: 0.000460  min_lr: 0.000011  loss: 0.7358 (0.8392)  loss_scale: 131072.0000 (82900.2393)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [13]  [ 360/1349]  eta: 0:05:17  lr: 0.000460  min_lr: 0.000011  loss: 0.8607 (0.8407)  loss_scale: 131072.0000 (84234.6371)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [13]  [ 370/1349]  eta: 0:05:13  lr: 0.000460  min_lr: 0.000011  loss: 0.9250 (0.8416)  loss_scale: 131072.0000 (85497.0997)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0001  max mem: 41808
Epoch: [13]  [ 380/1349]  eta: 0:05:10  lr: 0.000459  min_lr: 0.000011  loss: 0.9183 (0.8421)  loss_scale: 131072.0000 (86693.2913)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
[2025-05-23 17:44:34,867] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 17920
[2025-05-23 17:44:34,867] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 17920
[2025-05-23 17:44:34,867] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 17:44:34,867] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 17:44:34,867] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [13]  [ 390/1349]  eta: 0:05:06  lr: 0.000459  min_lr: 0.000011  loss: 0.8107 (0.8413)  loss_scale: 131072.0000 (86487.4066)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0002  max mem: 41808
Epoch: [13]  [ 400/1349]  eta: 0:05:03  lr: 0.000459  min_lr: 0.000011  loss: 0.8107 (0.8412)  loss_scale: 65536.0000 (85964.9277)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0002  max mem: 41808
Epoch: [13]  [ 410/1349]  eta: 0:04:59  lr: 0.000459  min_lr: 0.000011  loss: 0.8945 (0.8430)  loss_scale: 65536.0000 (85467.8735)  weight_decay: 0.0500 (0.0500)  time: 0.3087  data: 0.0002  max mem: 41808
Epoch: [13]  [ 420/1349]  eta: 0:04:56  lr: 0.000459  min_lr: 0.000011  loss: 0.8415 (0.8402)  loss_scale: 65536.0000 (84994.4323)  weight_decay: 0.0500 (0.0500)  time: 0.3089  data: 0.0002  max mem: 41808
Epoch: [13]  [ 430/1349]  eta: 0:04:52  lr: 0.000459  min_lr: 0.000011  loss: 0.8027 (0.8406)  loss_scale: 65536.0000 (84542.9606)  weight_decay: 0.0500 (0.0500)  time: 0.3078  data: 0.0001  max mem: 41808
Epoch: [13]  [ 440/1349]  eta: 0:04:49  lr: 0.000459  min_lr: 0.000011  loss: 0.9035 (0.8413)  loss_scale: 65536.0000 (84111.9637)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0001  max mem: 41808
Epoch: [13]  [ 450/1349]  eta: 0:04:45  lr: 0.000459  min_lr: 0.000011  loss: 0.9035 (0.8426)  loss_scale: 65536.0000 (83700.0798)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
Epoch: [13]  [ 460/1349]  eta: 0:04:42  lr: 0.000459  min_lr: 0.000011  loss: 0.8225 (0.8402)  loss_scale: 65536.0000 (83306.0651)  weight_decay: 0.0500 (0.0500)  time: 0.3093  data: 0.0001  max mem: 41808
[2025-05-23 17:44:59,237] [INFO] [logging.py:96:log_dist] [Rank 0] step=18000, skipped=107, lr=[1.0901416338308092e-05, 1.0901416338308092e-05, 1.4535221784410789e-05, 1.4535221784410789e-05, 1.9380295712547718e-05, 1.9380295712547718e-05, 2.5840394283396957e-05, 2.5840394283396957e-05, 3.445385904452928e-05, 3.445385904452928e-05, 4.5938478726039035e-05, 4.5938478726039035e-05, 6.125130496805205e-05, 6.125130496805205e-05, 8.16684066240694e-05, 8.16684066240694e-05, 0.00010889120883209253, 0.00010889120883209253, 0.00014518827844279004, 0.00014518827844279004, 0.0001935843712570534, 0.0001935843712570534, 0.0002581124950094045, 0.0002581124950094045, 0.0003441499933458727, 0.0003441499933458727, 0.0004588666577944969, 0.0004588666577944969], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-05-23 17:44:59,238] [INFO] [timer.py:260:stop] epoch=0/micro_step=18000/global_step=18000, RunningAvgSamplesPerSec=205.63571930687186, CurrSamplesPerSec=213.9079631942026, MemAllocated=1.37GB, MaxMemAllocated=40.83GB
Epoch: [13]  [ 470/1349]  eta: 0:04:39  lr: 0.000459  min_lr: 0.000011  loss: 0.8777 (0.8412)  loss_scale: 65536.0000 (82928.7813)  weight_decay: 0.0500 (0.0500)  time: 0.3097  data: 0.0001  max mem: 41808
Epoch: [13]  [ 480/1349]  eta: 0:04:35  lr: 0.000459  min_lr: 0.000011  loss: 0.8692 (0.8415)  loss_scale: 65536.0000 (82567.1850)  weight_decay: 0.0500 (0.0500)  time: 0.3080  data: 0.0001  max mem: 41808
Epoch: [13]  [ 490/1349]  eta: 0:04:32  lr: 0.000459  min_lr: 0.000011  loss: 0.8757 (0.8445)  loss_scale: 65536.0000 (82220.3177)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [13]  [ 500/1349]  eta: 0:04:29  lr: 0.000459  min_lr: 0.000011  loss: 0.9255 (0.8457)  loss_scale: 65536.0000 (81887.2974)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [13]  [ 510/1349]  eta: 0:04:25  lr: 0.000459  min_lr: 0.000011  loss: 0.8419 (0.8443)  loss_scale: 65536.0000 (81567.3112)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
[2025-05-23 17:45:14,609] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 17:45:14,609] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-05-23 17:45:14,609] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 17:45:14,609] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [13]  [ 520/1349]  eta: 0:04:22  lr: 0.000458  min_lr: 0.000011  loss: 0.8572 (0.8443)  loss_scale: 65536.0000 (82391.7083)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
Epoch: [13]  [ 530/1349]  eta: 0:04:19  lr: 0.000458  min_lr: 0.000011  loss: 0.8695 (0.8444)  loss_scale: 131072.0000 (83308.4746)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [13]  [ 540/1349]  eta: 0:04:15  lr: 0.000458  min_lr: 0.000011  loss: 0.8474 (0.8426)  loss_scale: 131072.0000 (84191.3494)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
Epoch: [13]  [ 550/1349]  eta: 0:04:12  lr: 0.000458  min_lr: 0.000011  loss: 0.8794 (0.8451)  loss_scale: 131072.0000 (85042.1779)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0002  max mem: 41808
Epoch: [13]  [ 560/1349]  eta: 0:04:09  lr: 0.000458  min_lr: 0.000011  loss: 0.9589 (0.8449)  loss_scale: 131072.0000 (85862.6738)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
Epoch: [13]  [ 570/1349]  eta: 0:04:06  lr: 0.000458  min_lr: 0.000011  loss: 0.8005 (0.8444)  loss_scale: 131072.0000 (86654.4308)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
Epoch: [13]  [ 580/1349]  eta: 0:04:02  lr: 0.000458  min_lr: 0.000011  loss: 0.8407 (0.8452)  loss_scale: 131072.0000 (87418.9329)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
Epoch: [13]  [ 590/1349]  eta: 0:03:59  lr: 0.000458  min_lr: 0.000011  loss: 0.8778 (0.8458)  loss_scale: 131072.0000 (88157.5635)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
Epoch: [13]  [ 600/1349]  eta: 0:03:56  lr: 0.000458  min_lr: 0.000011  loss: 0.8716 (0.8454)  loss_scale: 131072.0000 (88871.6140)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [13]  [ 610/1349]  eta: 0:03:53  lr: 0.000458  min_lr: 0.000011  loss: 0.7923 (0.8446)  loss_scale: 131072.0000 (89562.2913)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0002  max mem: 41808
Epoch: [13]  [ 620/1349]  eta: 0:03:49  lr: 0.000458  min_lr: 0.000011  loss: 0.7602 (0.8442)  loss_scale: 131072.0000 (90230.7246)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0002  max mem: 41808
Epoch: [13]  [ 630/1349]  eta: 0:03:46  lr: 0.000458  min_lr: 0.000011  loss: 0.7774 (0.8439)  loss_scale: 131072.0000 (90877.9715)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0002  max mem: 41808
[2025-05-23 17:45:53,932] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 17:45:53,932] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 17:45:53,932] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 17:45:53,932] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [13]  [ 640/1349]  eta: 0:03:43  lr: 0.000458  min_lr: 0.000011  loss: 0.8471 (0.8458)  loss_scale: 131072.0000 (91709.5039)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0002  max mem: 41808
[2025-05-23 17:45:55,160] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 18181
[2025-05-23 17:45:55,160] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 18181
[2025-05-23 17:45:55,160] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 17:45:55,160] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 17:45:55,160] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [13]  [ 650/1349]  eta: 0:03:40  lr: 0.000458  min_lr: 0.000011  loss: 0.8659 (0.8452)  loss_scale: 131072.0000 (92918.1690)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0002  max mem: 41808
[2025-05-23 17:45:58,224] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 18191
[2025-05-23 17:45:58,224] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 18191
[2025-05-23 17:45:58,224] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 17:45:58,224] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 17:45:58,224] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [13]  [ 660/1349]  eta: 0:03:36  lr: 0.000457  min_lr: 0.000011  loss: 0.8804 (0.8457)  loss_scale: 131072.0000 (92801.3555)  weight_decay: 0.0500 (0.0500)  time: 0.3066  data: 0.0002  max mem: 41808
Epoch: [13]  [ 670/1349]  eta: 0:03:33  lr: 0.000457  min_lr: 0.000011  loss: 0.9046 (0.8455)  loss_scale: 65536.0000 (92395.0164)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [13]  [ 680/1349]  eta: 0:03:30  lr: 0.000457  min_lr: 0.000011  loss: 0.8163 (0.8455)  loss_scale: 65536.0000 (92000.6109)  weight_decay: 0.0500 (0.0500)  time: 0.3082  data: 0.0001  max mem: 41808
Epoch: [13]  [ 690/1349]  eta: 0:03:27  lr: 0.000457  min_lr: 0.000011  loss: 0.8423 (0.8456)  loss_scale: 65536.0000 (91617.6208)  weight_decay: 0.0500 (0.0500)  time: 0.3084  data: 0.0001  max mem: 41808
Epoch: [13]  [ 700/1349]  eta: 0:03:23  lr: 0.000457  min_lr: 0.000011  loss: 0.8925 (0.8466)  loss_scale: 65536.0000 (91245.5578)  weight_decay: 0.0500 (0.0500)  time: 0.3080  data: 0.0001  max mem: 41808
Epoch: [13]  [ 710/1349]  eta: 0:03:20  lr: 0.000457  min_lr: 0.000011  loss: 0.9068 (0.8463)  loss_scale: 65536.0000 (90883.9606)  weight_decay: 0.0500 (0.0500)  time: 0.3085  data: 0.0001  max mem: 41808
Epoch: [13]  [ 720/1349]  eta: 0:03:17  lr: 0.000457  min_lr: 0.000011  loss: 0.8862 (0.8470)  loss_scale: 65536.0000 (90532.3939)  weight_decay: 0.0500 (0.0500)  time: 0.3086  data: 0.0001  max mem: 41808
Epoch: [13]  [ 730/1349]  eta: 0:03:14  lr: 0.000457  min_lr: 0.000011  loss: 0.8805 (0.8471)  loss_scale: 65536.0000 (90190.4460)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [13]  [ 740/1349]  eta: 0:03:11  lr: 0.000457  min_lr: 0.000011  loss: 0.8805 (0.8479)  loss_scale: 65536.0000 (89857.7274)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
Epoch: [13]  [ 750/1349]  eta: 0:03:08  lr: 0.000457  min_lr: 0.000011  loss: 0.8246 (0.8478)  loss_scale: 65536.0000 (89533.8695)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0002  max mem: 41808
Epoch: [13]  [ 760/1349]  eta: 0:03:04  lr: 0.000457  min_lr: 0.000011  loss: 0.8611 (0.8485)  loss_scale: 65536.0000 (89218.5230)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0002  max mem: 41808
Epoch: [13]  [ 770/1349]  eta: 0:03:01  lr: 0.000457  min_lr: 0.000011  loss: 0.8525 (0.8474)  loss_scale: 65536.0000 (88911.3567)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
Epoch: [13]  [ 780/1349]  eta: 0:02:58  lr: 0.000457  min_lr: 0.000011  loss: 0.7910 (0.8474)  loss_scale: 65536.0000 (88612.0563)  weight_decay: 0.0500 (0.0500)  time: 0.3080  data: 0.0001  max mem: 41808
[2025-05-23 17:46:37,949] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 17:46:37,949] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 17:46:37,949] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-05-23 17:46:37,949] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-05-23 17:46:38,561] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 18322
[2025-05-23 17:46:38,561] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 18322
[2025-05-23 17:46:38,561] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 17:46:38,561] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 17:46:38,561] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [13]  [ 790/1349]  eta: 0:02:55  lr: 0.000457  min_lr: 0.000011  loss: 0.8907 (0.8489)  loss_scale: 65536.0000 (88486.0278)  weight_decay: 0.0500 (0.0500)  time: 0.3097  data: 0.0001  max mem: 41808
Epoch: [13]  [ 800/1349]  eta: 0:02:52  lr: 0.000456  min_lr: 0.000011  loss: 0.8746 (0.8488)  loss_scale: 65536.0000 (88199.5106)  weight_decay: 0.0500 (0.0500)  time: 0.3080  data: 0.0001  max mem: 41808
Epoch: [13]  [ 810/1349]  eta: 0:02:48  lr: 0.000456  min_lr: 0.000011  loss: 0.8265 (0.8494)  loss_scale: 65536.0000 (87920.0592)  weight_decay: 0.0500 (0.0500)  time: 0.3063  data: 0.0002  max mem: 41808
Epoch: [13]  [ 820/1349]  eta: 0:02:45  lr: 0.000456  min_lr: 0.000011  loss: 0.8771 (0.8485)  loss_scale: 65536.0000 (87647.4153)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0002  max mem: 41808
Epoch: [13]  [ 830/1349]  eta: 0:02:42  lr: 0.000456  min_lr: 0.000011  loss: 0.8174 (0.8485)  loss_scale: 65536.0000 (87381.3333)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0002  max mem: 41808
Epoch: [13]  [ 840/1349]  eta: 0:02:39  lr: 0.000456  min_lr: 0.000011  loss: 0.8797 (0.8495)  loss_scale: 65536.0000 (87121.5791)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0002  max mem: 41808
Epoch: [13]  [ 850/1349]  eta: 0:02:36  lr: 0.000456  min_lr: 0.000011  loss: 0.8770 (0.8496)  loss_scale: 65536.0000 (86867.9295)  weight_decay: 0.0500 (0.0500)  time: 0.3064  data: 0.0003  max mem: 41808
Epoch: [13]  [ 860/1349]  eta: 0:02:33  lr: 0.000456  min_lr: 0.000011  loss: 0.8241 (0.8486)  loss_scale: 65536.0000 (86620.1719)  weight_decay: 0.0500 (0.0500)  time: 0.3066  data: 0.0003  max mem: 41808
Epoch: [13]  [ 870/1349]  eta: 0:02:29  lr: 0.000456  min_lr: 0.000011  loss: 0.8221 (0.8482)  loss_scale: 65536.0000 (86378.1033)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0002  max mem: 41808
Epoch: [13]  [ 880/1349]  eta: 0:02:26  lr: 0.000456  min_lr: 0.000011  loss: 0.7649 (0.8468)  loss_scale: 65536.0000 (86141.5301)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0002  max mem: 41808
Epoch: [13]  [ 890/1349]  eta: 0:02:23  lr: 0.000456  min_lr: 0.000011  loss: 0.7256 (0.8462)  loss_scale: 65536.0000 (85910.2671)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0002  max mem: 41808
Epoch: [13]  [ 900/1349]  eta: 0:02:20  lr: 0.000456  min_lr: 0.000011  loss: 0.7944 (0.8459)  loss_scale: 65536.0000 (85684.1376)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0002  max mem: 41808
Epoch: [13]  [ 910/1349]  eta: 0:02:17  lr: 0.000456  min_lr: 0.000011  loss: 0.8935 (0.8460)  loss_scale: 65536.0000 (85462.9726)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0002  max mem: 41808
[2025-05-23 17:47:18,181] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 17:47:18,181] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-05-23 17:47:18,181] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 17:47:18,181] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [13]  [ 920/1349]  eta: 0:02:14  lr: 0.000456  min_lr: 0.000011  loss: 0.8273 (0.8450)  loss_scale: 65536.0000 (85744.7123)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0002  max mem: 41808
Epoch: [13]  [ 930/1349]  eta: 0:02:10  lr: 0.000455  min_lr: 0.000011  loss: 0.8732 (0.8459)  loss_scale: 131072.0000 (86231.5789)  weight_decay: 0.0500 (0.0500)  time: 0.3078  data: 0.0001  max mem: 41808
Epoch: [13]  [ 940/1349]  eta: 0:02:07  lr: 0.000455  min_lr: 0.000011  loss: 0.9380 (0.8451)  loss_scale: 131072.0000 (86708.0978)  weight_decay: 0.0500 (0.0500)  time: 0.3092  data: 0.0001  max mem: 41808
Epoch: [13]  [ 950/1349]  eta: 0:02:04  lr: 0.000455  min_lr: 0.000011  loss: 0.7720 (0.8442)  loss_scale: 131072.0000 (87174.5952)  weight_decay: 0.0500 (0.0500)  time: 0.3095  data: 0.0002  max mem: 41808
Epoch: [13]  [ 960/1349]  eta: 0:02:01  lr: 0.000455  min_lr: 0.000011  loss: 0.8597 (0.8453)  loss_scale: 131072.0000 (87631.3840)  weight_decay: 0.0500 (0.0500)  time: 0.3088  data: 0.0002  max mem: 41808
Epoch: [13]  [ 970/1349]  eta: 0:01:58  lr: 0.000455  min_lr: 0.000011  loss: 0.9474 (0.8454)  loss_scale: 131072.0000 (88078.7642)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0002  max mem: 41808
Epoch: [13]  [ 980/1349]  eta: 0:01:55  lr: 0.000455  min_lr: 0.000011  loss: 0.9120 (0.8460)  loss_scale: 131072.0000 (88517.0234)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
Epoch: [13]  [ 990/1349]  eta: 0:01:52  lr: 0.000455  min_lr: 0.000011  loss: 0.8944 (0.8458)  loss_scale: 131072.0000 (88946.4379)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
Epoch: [13]  [1000/1349]  eta: 0:01:48  lr: 0.000455  min_lr: 0.000011  loss: 0.8728 (0.8461)  loss_scale: 131072.0000 (89367.2727)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
Epoch: [13]  [1010/1349]  eta: 0:01:45  lr: 0.000455  min_lr: 0.000011  loss: 0.9309 (0.8464)  loss_scale: 131072.0000 (89779.7824)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0002  max mem: 41808
Epoch: [13]  [1020/1349]  eta: 0:01:42  lr: 0.000455  min_lr: 0.000011  loss: 0.9173 (0.8459)  loss_scale: 131072.0000 (90184.2116)  weight_decay: 0.0500 (0.0500)  time: 0.3088  data: 0.0001  max mem: 41808
Epoch: [13]  [1030/1349]  eta: 0:01:39  lr: 0.000455  min_lr: 0.000011  loss: 0.8102 (0.8454)  loss_scale: 131072.0000 (90580.7953)  weight_decay: 0.0500 (0.0500)  time: 0.3090  data: 0.0001  max mem: 41808
Epoch: [13]  [1040/1349]  eta: 0:01:36  lr: 0.000455  min_lr: 0.000011  loss: 0.8184 (0.8450)  loss_scale: 131072.0000 (90969.7598)  weight_decay: 0.0500 (0.0500)  time: 0.3079  data: 0.0001  max mem: 41808
[2025-05-23 17:47:57,655] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 17:47:57,655] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 17:47:57,656] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 17:47:57,656] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 17:48:00,118] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 18587
[2025-05-23 17:48:00,118] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 18587
[2025-05-23 17:48:00,118] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 17:48:00,118] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 17:48:00,118] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [13]  [1050/1349]  eta: 0:01:33  lr: 0.000455  min_lr: 0.000011  loss: 0.8780 (0.8452)  loss_scale: 131072.0000 (92349.0162)  weight_decay: 0.0500 (0.0500)  time: 0.3085  data: 0.0001  max mem: 41808
Epoch: [13]  [1060/1349]  eta: 0:01:30  lr: 0.000455  min_lr: 0.000011  loss: 0.8684 (0.8452)  loss_scale: 131072.0000 (92713.9830)  weight_decay: 0.0500 (0.0500)  time: 0.3078  data: 0.0001  max mem: 41808
Epoch: [13]  [1070/1349]  eta: 0:01:27  lr: 0.000454  min_lr: 0.000011  loss: 0.8393 (0.8453)  loss_scale: 131072.0000 (93072.1345)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0001  max mem: 41808
Epoch: [13]  [1080/1349]  eta: 0:01:23  lr: 0.000454  min_lr: 0.000011  loss: 0.8586 (0.8457)  loss_scale: 131072.0000 (93423.6596)  weight_decay: 0.0500 (0.0500)  time: 0.3080  data: 0.0001  max mem: 41808
Epoch: [13]  [1090/1349]  eta: 0:01:20  lr: 0.000454  min_lr: 0.000011  loss: 0.8650 (0.8458)  loss_scale: 131072.0000 (93768.7406)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [13]  [1100/1349]  eta: 0:01:17  lr: 0.000454  min_lr: 0.000011  loss: 0.8597 (0.8457)  loss_scale: 131072.0000 (94107.5531)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [13]  [1110/1349]  eta: 0:01:14  lr: 0.000454  min_lr: 0.000011  loss: 0.8939 (0.8459)  loss_scale: 131072.0000 (94440.2664)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
Epoch: [13]  [1120/1349]  eta: 0:01:11  lr: 0.000454  min_lr: 0.000011  loss: 0.8728 (0.8457)  loss_scale: 131072.0000 (94767.0437)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [13]  [1130/1349]  eta: 0:01:08  lr: 0.000454  min_lr: 0.000011  loss: 0.8462 (0.8458)  loss_scale: 131072.0000 (95088.0424)  weight_decay: 0.0500 (0.0500)  time: 0.3087  data: 0.0001  max mem: 41808
Epoch: [13]  [1140/1349]  eta: 0:01:05  lr: 0.000454  min_lr: 0.000011  loss: 0.8433 (0.8454)  loss_scale: 131072.0000 (95403.4145)  weight_decay: 0.0500 (0.0500)  time: 0.3086  data: 0.0001  max mem: 41808
Epoch: [13]  [1150/1349]  eta: 0:01:02  lr: 0.000454  min_lr: 0.000011  loss: 0.7583 (0.8446)  loss_scale: 131072.0000 (95713.3067)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [13]  [1160/1349]  eta: 0:00:58  lr: 0.000454  min_lr: 0.000011  loss: 0.8703 (0.8452)  loss_scale: 131072.0000 (96017.8605)  weight_decay: 0.0500 (0.0500)  time: 0.3081  data: 0.0002  max mem: 41808
Epoch: [13]  [1170/1349]  eta: 0:00:55  lr: 0.000454  min_lr: 0.000011  loss: 0.8939 (0.8452)  loss_scale: 131072.0000 (96317.2126)  weight_decay: 0.0500 (0.0500)  time: 0.3085  data: 0.0001  max mem: 41808
[2025-05-23 17:48:39,833] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 17:48:39,833] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 17:48:39,833] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 17:48:39,833] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [13]  [1180/1349]  eta: 0:00:52  lr: 0.000454  min_lr: 0.000011  loss: 0.8669 (0.8453)  loss_scale: 131072.0000 (96833.4632)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0001  max mem: 41808
[2025-05-23 17:48:42,587] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 18725
[2025-05-23 17:48:42,587] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 18725
[2025-05-23 17:48:42,587] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 17:48:42,587] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 17:48:42,587] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [13]  [1190/1349]  eta: 0:00:49  lr: 0.000454  min_lr: 0.000011  loss: 0.8610 (0.8454)  loss_scale: 131072.0000 (97891.3048)  weight_decay: 0.0500 (0.0500)  time: 0.3063  data: 0.0001  max mem: 41808
Epoch: [13]  [1200/1349]  eta: 0:00:46  lr: 0.000453  min_lr: 0.000011  loss: 0.8963 (0.8457)  loss_scale: 131072.0000 (98167.5803)  weight_decay: 0.0500 (0.0500)  time: 0.3064  data: 0.0001  max mem: 41808
Epoch: [13]  [1210/1349]  eta: 0:00:43  lr: 0.000453  min_lr: 0.000011  loss: 0.8963 (0.8455)  loss_scale: 131072.0000 (98439.2931)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [13]  [1220/1349]  eta: 0:00:40  lr: 0.000453  min_lr: 0.000011  loss: 0.8702 (0.8456)  loss_scale: 131072.0000 (98706.5553)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
[2025-05-23 17:48:52,709] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 18758
[2025-05-23 17:48:52,709] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 17:48:52,709] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 18758
[2025-05-23 17:48:52,709] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 17:48:52,709] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [13]  [1230/1349]  eta: 0:00:37  lr: 0.000453  min_lr: 0.000011  loss: 0.8066 (0.8453)  loss_scale: 65536.0000 (98437.0950)  weight_decay: 0.0500 (0.0500)  time: 0.3059  data: 0.0002  max mem: 41808
Epoch: [13]  [1240/1349]  eta: 0:00:33  lr: 0.000453  min_lr: 0.000011  loss: 0.8724 (0.8458)  loss_scale: 65536.0000 (98171.9774)  weight_decay: 0.0500 (0.0500)  time: 0.3058  data: 0.0001  max mem: 41808
Epoch: [13]  [1250/1349]  eta: 0:00:30  lr: 0.000453  min_lr: 0.000011  loss: 0.8936 (0.8458)  loss_scale: 65536.0000 (97911.0983)  weight_decay: 0.0500 (0.0500)  time: 0.3062  data: 0.0001  max mem: 41808
Epoch: [13]  [1260/1349]  eta: 0:00:27  lr: 0.000453  min_lr: 0.000011  loss: 0.8476 (0.8455)  loss_scale: 65536.0000 (97654.3569)  weight_decay: 0.0500 (0.0500)  time: 0.3060  data: 0.0001  max mem: 41808
Epoch: [13]  [1270/1349]  eta: 0:00:24  lr: 0.000453  min_lr: 0.000011  loss: 0.8169 (0.8451)  loss_scale: 65536.0000 (97401.6554)  weight_decay: 0.0500 (0.0500)  time: 0.3058  data: 0.0001  max mem: 41808
Epoch: [13]  [1280/1349]  eta: 0:00:21  lr: 0.000453  min_lr: 0.000011  loss: 0.8322 (0.8452)  loss_scale: 65536.0000 (97152.8993)  weight_decay: 0.0500 (0.0500)  time: 0.3059  data: 0.0001  max mem: 41808
Epoch: [13]  [1290/1349]  eta: 0:00:18  lr: 0.000453  min_lr: 0.000011  loss: 0.8322 (0.8447)  loss_scale: 65536.0000 (96907.9969)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [13]  [1300/1349]  eta: 0:00:15  lr: 0.000453  min_lr: 0.000011  loss: 0.8607 (0.8452)  loss_scale: 65536.0000 (96666.8593)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [13]  [1310/1349]  eta: 0:00:12  lr: 0.000453  min_lr: 0.000011  loss: 0.8975 (0.8453)  loss_scale: 65536.0000 (96429.4005)  weight_decay: 0.0500 (0.0500)  time: 0.3061  data: 0.0001  max mem: 41808
Epoch: [13]  [1320/1349]  eta: 0:00:09  lr: 0.000453  min_lr: 0.000011  loss: 0.8975 (0.8454)  loss_scale: 65536.0000 (96195.5367)  weight_decay: 0.0500 (0.0500)  time: 0.3055  data: 0.0001  max mem: 41808
Epoch: [13]  [1330/1349]  eta: 0:00:05  lr: 0.000452  min_lr: 0.000011  loss: 0.8797 (0.8455)  loss_scale: 65536.0000 (95965.1871)  weight_decay: 0.0500 (0.0500)  time: 0.3036  data: 0.0001  max mem: 41808
Epoch: [13]  [1340/1349]  eta: 0:00:02  lr: 0.000452  min_lr: 0.000011  loss: 0.7938 (0.8448)  loss_scale: 65536.0000 (95738.2729)  weight_decay: 0.0500 (0.0500)  time: 0.3010  data: 0.0001  max mem: 41808
Epoch: [13]  [1348/1349]  eta: 0:00:00  lr: 0.000452  min_lr: 0.000011  loss: 0.7961 (0.8447)  loss_scale: 65536.0000 (95559.1638)  weight_decay: 0.0500 (0.0500)  time: 0.3005  data: 0.0001  max mem: 41808
Epoch: [13] Total time: 0:06:59 (0.3112 s / it)
Averaged stats: lr: 0.000452  min_lr: 0.000011  loss: 0.7961 (0.8467)  loss_scale: 65536.0000 (95559.1638)  weight_decay: 0.0500 (0.0500)  total_time: 419.7793 (419.7744)
Val:  [  0/346]  eta: 0:47:17  loss: 1.7256 (1.7256)  acc1: 27.3438 (27.3438)  acc5: 90.6250 (90.6250)  time: 8.2011  data: 7.1453  max mem: 41808
Val:  [ 10/346]  eta: 0:11:07  loss: 0.1263 (0.4291)  acc1: 100.0000 (87.2869)  acc5: 100.0000 (98.1534)  time: 1.9864  data: 1.1584  max mem: 41808
Val:  [ 20/346]  eta: 0:08:08  loss: 0.1249 (0.3454)  acc1: 100.0000 (90.5134)  acc5: 100.0000 (98.8839)  time: 1.1641  data: 0.3633  max mem: 41808
Val:  [ 30/346]  eta: 0:06:37  loss: 0.1121 (0.3043)  acc1: 99.2188 (92.2883)  acc5: 100.0000 (99.2440)  time: 0.8564  data: 0.0836  max mem: 41808
Val:  [ 40/346]  eta: 0:05:46  loss: 0.1319 (0.3647)  acc1: 99.2188 (90.7584)  acc5: 100.0000 (98.9520)  time: 0.7456  data: 0.0014  max mem: 41808
Val:  [ 50/346]  eta: 0:05:12  loss: 0.1252 (0.3235)  acc1: 99.2188 (92.2641)  acc5: 100.0000 (99.1422)  time: 0.7418  data: 0.0014  max mem: 41808
Val:  [ 60/346]  eta: 0:04:53  loss: 0.1252 (0.3139)  acc1: 98.4375 (92.6101)  acc5: 100.0000 (99.2828)  time: 0.8090  data: 0.0593  max mem: 41808
Val:  [ 70/346]  eta: 0:04:36  loss: 0.2612 (0.3300)  acc1: 95.3125 (91.8354)  acc5: 100.0000 (99.3838)  time: 0.8663  data: 0.1309  max mem: 41808
Val:  [ 80/346]  eta: 0:04:24  loss: 0.1950 (0.3290)  acc1: 96.8750 (92.0814)  acc5: 100.0000 (99.4599)  time: 0.8911  data: 0.1386  max mem: 41808
Val:  [ 90/346]  eta: 0:04:12  loss: 0.2043 (0.3272)  acc1: 96.8750 (91.9643)  acc5: 100.0000 (99.5192)  time: 0.9231  data: 0.1400  max mem: 41808
Val:  [100/346]  eta: 0:03:59  loss: 0.1483 (0.3094)  acc1: 98.4375 (92.6903)  acc5: 100.0000 (99.5668)  time: 0.8932  data: 0.1475  max mem: 41808
Val:  [110/346]  eta: 0:03:47  loss: 0.1483 (0.3251)  acc1: 98.4375 (92.1453)  acc5: 100.0000 (99.4792)  time: 0.8801  data: 0.1445  max mem: 41808
Val:  [120/346]  eta: 0:03:36  loss: 0.1536 (0.3312)  acc1: 96.0938 (91.9873)  acc5: 100.0000 (99.4835)  time: 0.8865  data: 0.1387  max mem: 41808
Val:  [130/346]  eta: 0:03:25  loss: 0.1181 (0.3383)  acc1: 98.4375 (91.6508)  acc5: 100.0000 (99.4752)  time: 0.8842  data: 0.1426  max mem: 41808
Val:  [140/346]  eta: 0:03:15  loss: 0.2104 (0.3344)  acc1: 95.3125 (91.8218)  acc5: 100.0000 (99.4958)  time: 0.8895  data: 0.1543  max mem: 41808
Val:  [150/346]  eta: 0:03:05  loss: 0.2608 (0.3336)  acc1: 92.9688 (91.8564)  acc5: 100.0000 (99.5240)  time: 0.9082  data: 0.1527  max mem: 41808
Val:  [160/346]  eta: 0:02:55  loss: 0.2576 (0.3331)  acc1: 92.9688 (91.7993)  acc5: 100.0000 (99.5439)  time: 0.9022  data: 0.1491  max mem: 41808
Val:  [170/346]  eta: 0:02:45  loss: 0.2947 (0.3444)  acc1: 90.6250 (91.4017)  acc5: 100.0000 (99.5340)  time: 0.8690  data: 0.1500  max mem: 41808
Val:  [180/346]  eta: 0:02:35  loss: 0.3707 (0.3602)  acc1: 88.2812 (90.7027)  acc5: 100.0000 (99.5252)  time: 0.8773  data: 0.1446  max mem: 41808
Val:  [190/346]  eta: 0:02:25  loss: 0.3123 (0.3584)  acc1: 91.4062 (90.7313)  acc5: 100.0000 (99.5501)  time: 0.9024  data: 0.1485  max mem: 41808
Val:  [200/346]  eta: 0:02:16  loss: 0.3123 (0.3705)  acc1: 89.8438 (90.3490)  acc5: 100.0000 (99.5219)  time: 0.9198  data: 0.1578  max mem: 41808
Val:  [210/346]  eta: 0:02:06  loss: 0.2425 (0.3652)  acc1: 93.7500 (90.5658)  acc5: 100.0000 (99.5446)  time: 0.8984  data: 0.1518  max mem: 41808
Val:  [220/346]  eta: 0:01:57  loss: 0.1556 (0.3627)  acc1: 97.6562 (90.6745)  acc5: 100.0000 (99.5404)  time: 0.8841  data: 0.1605  max mem: 41808
Val:  [230/346]  eta: 0:01:47  loss: 0.1643 (0.3547)  acc1: 97.6562 (90.9666)  acc5: 100.0000 (99.5603)  time: 0.8897  data: 0.1614  max mem: 41808
Val:  [240/346]  eta: 0:01:38  loss: 0.1897 (0.3652)  acc1: 96.8750 (90.6607)  acc5: 100.0000 (99.5300)  time: 0.9068  data: 0.1510  max mem: 41808
Val:  [250/346]  eta: 0:01:28  loss: 0.2368 (0.3628)  acc1: 94.5312 (90.7495)  acc5: 100.0000 (99.5269)  time: 0.9215  data: 0.1504  max mem: 41808
Val:  [260/346]  eta: 0:01:19  loss: 0.1611 (0.3610)  acc1: 97.6562 (90.7956)  acc5: 100.0000 (99.5390)  time: 0.8849  data: 0.1455  max mem: 41808
Val:  [270/346]  eta: 0:01:10  loss: 0.1356 (0.3576)  acc1: 99.2188 (90.9219)  acc5: 100.0000 (99.5560)  time: 0.8837  data: 0.1482  max mem: 41808
Val:  [280/346]  eta: 0:01:00  loss: 0.1353 (0.3557)  acc1: 99.2188 (91.0031)  acc5: 100.0000 (99.5663)  time: 0.9183  data: 0.1465  max mem: 41808
Val:  [290/346]  eta: 0:00:51  loss: 0.1088 (0.3482)  acc1: 100.0000 (91.2613)  acc5: 100.0000 (99.5812)  time: 0.9296  data: 0.1509  max mem: 41808
Val:  [300/346]  eta: 0:00:42  loss: 0.1088 (0.3482)  acc1: 100.0000 (91.3362)  acc5: 100.0000 (99.5769)  time: 0.9182  data: 0.1576  max mem: 41808
Val:  [310/346]  eta: 0:00:33  loss: 0.1286 (0.3535)  acc1: 100.0000 (91.1651)  acc5: 100.0000 (99.5880)  time: 0.8953  data: 0.1537  max mem: 41808
Val:  [320/346]  eta: 0:00:23  loss: 0.1370 (0.3555)  acc1: 97.6562 (91.0728)  acc5: 100.0000 (99.6009)  time: 0.8799  data: 0.1547  max mem: 41808
Val:  [330/346]  eta: 0:00:14  loss: 0.2627 (0.3600)  acc1: 92.1875 (90.9484)  acc5: 100.0000 (99.5256)  time: 0.8838  data: 0.1550  max mem: 41808
Val:  [340/346]  eta: 0:00:05  loss: 0.3491 (0.3682)  acc1: 90.6250 (90.6662)  acc5: 100.0000 (99.5349)  time: 0.8812  data: 0.1584  max mem: 41808
Val:  [345/346]  eta: 0:00:00  loss: 0.1845 (0.3651)  acc1: 95.4023 (90.7723)  acc5: 100.0000 (99.5412)  time: 0.8833  data: 0.1769  max mem: 41808
Val: Total time: 0:05:18 (0.9198 s / it)
* Acc@1 90.830 Acc@5 99.565 loss 0.364
Accuracy of the network on the 88494 val videos: 90.8%
Max accuracy: 91.25%   Max Epoch: 3
Epoch: [14]  [   0/1349]  eta: 1:54:22  lr: 0.000452  min_lr: 0.000011  loss: 0.8019 (0.8019)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 5.0873  data: 4.1323  max mem: 41808
[2025-05-23 17:54:55,455] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 17:54:55,456] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-05-23 17:54:55,457] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 17:54:55,458] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [14]  [  10/1349]  eta: 0:16:44  lr: 0.000452  min_lr: 0.000011  loss: 0.7815 (0.7796)  loss_scale: 131072.0000 (125114.1818)  weight_decay: 0.0500 (0.0500)  time: 0.7500  data: 0.3758  max mem: 41808
Epoch: [14]  [  20/1349]  eta: 0:11:59  lr: 0.000452  min_lr: 0.000011  loss: 0.7815 (0.7985)  loss_scale: 131072.0000 (127951.2381)  weight_decay: 0.0500 (0.0500)  time: 0.3137  data: 0.0001  max mem: 41808
Epoch: [14]  [  30/1349]  eta: 0:10:15  lr: 0.000452  min_lr: 0.000011  loss: 0.7898 (0.7935)  loss_scale: 131072.0000 (128957.9355)  weight_decay: 0.0500 (0.0500)  time: 0.3104  data: 0.0001  max mem: 41808
Epoch: [14]  [  40/1349]  eta: 0:09:20  lr: 0.000452  min_lr: 0.000011  loss: 0.7504 (0.7849)  loss_scale: 131072.0000 (129473.5610)  weight_decay: 0.0500 (0.0500)  time: 0.3092  data: 0.0001  max mem: 41808
Epoch: [14]  [  50/1349]  eta: 0:08:45  lr: 0.000452  min_lr: 0.000011  loss: 0.7310 (0.7867)  loss_scale: 131072.0000 (129786.9804)  weight_decay: 0.0500 (0.0500)  time: 0.3079  data: 0.0001  max mem: 41808
Epoch: [14]  [  60/1349]  eta: 0:08:20  lr: 0.000452  min_lr: 0.000011  loss: 0.8040 (0.7916)  loss_scale: 131072.0000 (129997.6393)  weight_decay: 0.0500 (0.0500)  time: 0.3081  data: 0.0001  max mem: 41808
[2025-05-23 17:55:15,902] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 18953
[2025-05-23 17:55:15,902] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 18953
[2025-05-23 17:55:15,902] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 17:55:15,902] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 17:55:15,902] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [14]  [  70/1349]  eta: 0:08:02  lr: 0.000452  min_lr: 0.000011  loss: 0.7924 (0.7959)  loss_scale: 131072.0000 (126456.7887)  weight_decay: 0.0500 (0.0500)  time: 0.3082  data: 0.0001  max mem: 41808
Epoch: [14]  [  80/1349]  eta: 0:07:47  lr: 0.000452  min_lr: 0.000011  loss: 0.8107 (0.8023)  loss_scale: 65536.0000 (118935.7037)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [14]  [  90/1349]  eta: 0:07:35  lr: 0.000452  min_lr: 0.000011  loss: 0.8220 (0.8045)  loss_scale: 65536.0000 (113067.6044)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
Epoch: [14]  [ 100/1349]  eta: 0:07:25  lr: 0.000452  min_lr: 0.000011  loss: 0.8624 (0.8147)  loss_scale: 65536.0000 (108361.5050)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
Epoch: [14]  [ 110/1349]  eta: 0:07:16  lr: 0.000452  min_lr: 0.000011  loss: 0.8640 (0.8164)  loss_scale: 65536.0000 (104503.3514)  weight_decay: 0.0500 (0.0500)  time: 0.3102  data: 0.0001  max mem: 41808
[2025-05-23 17:55:30,106] [INFO] [logging.py:96:log_dist] [Rank 0] step=19000, skipped=114, lr=[1.072613570277336e-05, 1.072613570277336e-05, 1.430151427036448e-05, 1.430151427036448e-05, 1.9068685693819305e-05, 1.9068685693819305e-05, 2.5424914258425743e-05, 2.5424914258425743e-05, 3.389988567790099e-05, 3.389988567790099e-05, 4.519984757053465e-05, 4.519984757053465e-05, 6.0266463427379535e-05, 6.0266463427379535e-05, 8.035528456983938e-05, 8.035528456983938e-05, 0.0001071403794264525, 0.0001071403794264525, 0.00014285383923527, 0.00014285383923527, 0.00019047178564702668, 0.00019047178564702668, 0.00025396238086270226, 0.00025396238086270226, 0.0003386165078169363, 0.0003386165078169363, 0.00045148867708924844, 0.00045148867708924844], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-05-23 17:55:30,106] [INFO] [timer.py:260:stop] epoch=0/micro_step=19000/global_step=19000, RunningAvgSamplesPerSec=206.00418553026114, CurrSamplesPerSec=213.33794497816038, MemAllocated=1.37GB, MaxMemAllocated=40.83GB
Epoch: [14]  [ 120/1349]  eta: 0:07:08  lr: 0.000451  min_lr: 0.000011  loss: 0.9025 (0.8247)  loss_scale: 65536.0000 (101282.9091)  weight_decay: 0.0500 (0.0500)  time: 0.3114  data: 0.0001  max mem: 41808
Epoch: [14]  [ 130/1349]  eta: 0:07:01  lr: 0.000451  min_lr: 0.000011  loss: 0.7979 (0.8196)  loss_scale: 65536.0000 (98554.1374)  weight_decay: 0.0500 (0.0500)  time: 0.3081  data: 0.0001  max mem: 41808
Epoch: [14]  [ 140/1349]  eta: 0:06:54  lr: 0.000451  min_lr: 0.000011  loss: 0.7997 (0.8271)  loss_scale: 65536.0000 (96212.4255)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [14]  [ 150/1349]  eta: 0:06:48  lr: 0.000451  min_lr: 0.000011  loss: 0.9076 (0.8330)  loss_scale: 65536.0000 (94180.8742)  weight_decay: 0.0500 (0.0500)  time: 0.3085  data: 0.0001  max mem: 41808
Epoch: [14]  [ 160/1349]  eta: 0:06:42  lr: 0.000451  min_lr: 0.000011  loss: 0.8382 (0.8300)  loss_scale: 65536.0000 (92401.6894)  weight_decay: 0.0500 (0.0500)  time: 0.3087  data: 0.0001  max mem: 41808
Epoch: [14]  [ 170/1349]  eta: 0:06:37  lr: 0.000451  min_lr: 0.000011  loss: 0.7819 (0.8313)  loss_scale: 65536.0000 (90830.5965)  weight_decay: 0.0500 (0.0500)  time: 0.3081  data: 0.0001  max mem: 41808
Epoch: [14]  [ 180/1349]  eta: 0:06:31  lr: 0.000451  min_lr: 0.000011  loss: 0.7422 (0.8271)  loss_scale: 65536.0000 (89433.1050)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [14]  [ 190/1349]  eta: 0:06:26  lr: 0.000451  min_lr: 0.000011  loss: 0.7417 (0.8250)  loss_scale: 65536.0000 (88181.9476)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0001  max mem: 41808
[2025-05-23 17:55:55,665] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 17:55:55,665] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 17:55:55,665] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-05-23 17:55:55,665] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [14]  [ 200/1349]  eta: 0:06:22  lr: 0.000451  min_lr: 0.000011  loss: 0.8724 (0.8297)  loss_scale: 65536.0000 (88685.5323)  weight_decay: 0.0500 (0.0500)  time: 0.3078  data: 0.0001  max mem: 41808
Epoch: [14]  [ 210/1349]  eta: 0:06:17  lr: 0.000451  min_lr: 0.000011  loss: 0.9004 (0.8286)  loss_scale: 131072.0000 (90694.3697)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0001  max mem: 41808
Epoch: [14]  [ 220/1349]  eta: 0:06:12  lr: 0.000451  min_lr: 0.000011  loss: 0.8444 (0.8292)  loss_scale: 131072.0000 (92521.4118)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0001  max mem: 41808
Epoch: [14]  [ 230/1349]  eta: 0:06:08  lr: 0.000451  min_lr: 0.000011  loss: 0.8701 (0.8286)  loss_scale: 131072.0000 (94190.2684)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
Epoch: [14]  [ 240/1349]  eta: 0:06:04  lr: 0.000451  min_lr: 0.000011  loss: 0.9073 (0.8337)  loss_scale: 131072.0000 (95720.6307)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [14]  [ 250/1349]  eta: 0:05:59  lr: 0.000450  min_lr: 0.000011  loss: 0.8579 (0.8330)  loss_scale: 131072.0000 (97129.0518)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
Epoch: [14]  [ 260/1349]  eta: 0:05:56  lr: 0.000450  min_lr: 0.000011  loss: 0.8537 (0.8349)  loss_scale: 131072.0000 (98429.5479)  weight_decay: 0.0500 (0.0500)  time: 0.3094  data: 0.0001  max mem: 41808
Epoch: [14]  [ 270/1349]  eta: 0:05:51  lr: 0.000450  min_lr: 0.000011  loss: 0.8806 (0.8343)  loss_scale: 131072.0000 (99634.0664)  weight_decay: 0.0500 (0.0500)  time: 0.3091  data: 0.0001  max mem: 41808
Epoch: [14]  [ 280/1349]  eta: 0:05:47  lr: 0.000450  min_lr: 0.000011  loss: 0.8509 (0.8334)  loss_scale: 131072.0000 (100752.8541)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
Epoch: [14]  [ 290/1349]  eta: 0:05:44  lr: 0.000450  min_lr: 0.000011  loss: 0.8556 (0.8328)  loss_scale: 131072.0000 (101794.7491)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
Epoch: [14]  [ 300/1349]  eta: 0:05:40  lr: 0.000450  min_lr: 0.000011  loss: 0.8556 (0.8316)  loss_scale: 131072.0000 (102767.4153)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0002  max mem: 41808
Epoch: [14]  [ 310/1349]  eta: 0:05:36  lr: 0.000450  min_lr: 0.000011  loss: 0.8989 (0.8332)  loss_scale: 131072.0000 (103677.5305)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0002  max mem: 41808
Epoch: [14]  [ 320/1349]  eta: 0:05:32  lr: 0.000450  min_lr: 0.000011  loss: 0.8980 (0.8333)  loss_scale: 131072.0000 (104530.9408)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
[2025-05-23 17:56:35,054] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 17:56:35,054] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 17:56:35,054] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 17:56:35,054] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [14]  [ 330/1349]  eta: 0:05:28  lr: 0.000450  min_lr: 0.000011  loss: 0.8778 (0.8330)  loss_scale: 131072.0000 (108104.7009)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
Epoch: [14]  [ 340/1349]  eta: 0:05:25  lr: 0.000450  min_lr: 0.000011  loss: 0.8506 (0.8323)  loss_scale: 262144.0000 (112621.9824)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
[2025-05-23 17:56:42,115] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 19233
[2025-05-23 17:56:42,115] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 19233
[2025-05-23 17:56:42,115] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 17:56:42,115] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 17:56:42,115] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [14]  [ 350/1349]  eta: 0:05:21  lr: 0.000450  min_lr: 0.000011  loss: 0.8700 (0.8339)  loss_scale: 262144.0000 (115388.1709)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
[2025-05-23 17:56:43,647] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 19238
[2025-05-23 17:56:43,647] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 19238
[2025-05-23 17:56:43,647] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 17:56:43,647] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 17:56:43,647] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [14]  [ 360/1349]  eta: 0:05:17  lr: 0.000450  min_lr: 0.000011  loss: 0.9166 (0.8351)  loss_scale: 131072.0000 (114188.7645)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0001  max mem: 41808
Epoch: [14]  [ 370/1349]  eta: 0:05:14  lr: 0.000450  min_lr: 0.000011  loss: 0.9215 (0.8341)  loss_scale: 65536.0000 (112877.3693)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
Epoch: [14]  [ 380/1349]  eta: 0:05:10  lr: 0.000449  min_lr: 0.000011  loss: 0.9215 (0.8369)  loss_scale: 65536.0000 (111634.8136)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [14]  [ 390/1349]  eta: 0:05:07  lr: 0.000449  min_lr: 0.000011  loss: 0.8854 (0.8365)  loss_scale: 65536.0000 (110455.8159)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
Epoch: [14]  [ 400/1349]  eta: 0:05:03  lr: 0.000449  min_lr: 0.000011  loss: 0.8546 (0.8368)  loss_scale: 65536.0000 (109335.6209)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
Epoch: [14]  [ 410/1349]  eta: 0:05:00  lr: 0.000449  min_lr: 0.000011  loss: 0.8088 (0.8352)  loss_scale: 65536.0000 (108269.9367)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [14]  [ 420/1349]  eta: 0:04:56  lr: 0.000449  min_lr: 0.000011  loss: 0.8265 (0.8360)  loss_scale: 65536.0000 (107254.8789)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [14]  [ 430/1349]  eta: 0:04:53  lr: 0.000449  min_lr: 0.000011  loss: 0.8812 (0.8367)  loss_scale: 65536.0000 (106286.9234)  weight_decay: 0.0500 (0.0500)  time: 0.3080  data: 0.0001  max mem: 41808
Epoch: [14]  [ 440/1349]  eta: 0:04:49  lr: 0.000449  min_lr: 0.000011  loss: 0.9176 (0.8376)  loss_scale: 65536.0000 (105362.8662)  weight_decay: 0.0500 (0.0500)  time: 0.3095  data: 0.0001  max mem: 41808
Epoch: [14]  [ 450/1349]  eta: 0:04:46  lr: 0.000449  min_lr: 0.000011  loss: 0.9104 (0.8382)  loss_scale: 65536.0000 (104479.7871)  weight_decay: 0.0500 (0.0500)  time: 0.3095  data: 0.0001  max mem: 41808
Epoch: [14]  [ 460/1349]  eta: 0:04:43  lr: 0.000449  min_lr: 0.000011  loss: 0.8224 (0.8365)  loss_scale: 65536.0000 (103635.0195)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0002  max mem: 41808
Epoch: [14]  [ 470/1349]  eta: 0:04:39  lr: 0.000449  min_lr: 0.000011  loss: 0.8025 (0.8369)  loss_scale: 65536.0000 (102826.1231)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0002  max mem: 41808
Epoch: [14]  [ 480/1349]  eta: 0:04:36  lr: 0.000449  min_lr: 0.000011  loss: 0.8609 (0.8368)  loss_scale: 65536.0000 (102050.8607)  weight_decay: 0.0500 (0.0500)  time: 0.3062  data: 0.0001  max mem: 41808
[2025-05-23 17:57:23,321] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 17:57:23,321] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 17:57:23,321] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-05-23 17:57:23,322] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [14]  [ 490/1349]  eta: 0:04:32  lr: 0.000449  min_lr: 0.000011  loss: 0.8842 (0.8382)  loss_scale: 65536.0000 (102641.9226)  weight_decay: 0.0500 (0.0500)  time: 0.3062  data: 0.0001  max mem: 41808
Epoch: [14]  [ 500/1349]  eta: 0:04:29  lr: 0.000448  min_lr: 0.000011  loss: 0.8670 (0.8390)  loss_scale: 131072.0000 (103209.3892)  weight_decay: 0.0500 (0.0500)  time: 0.3064  data: 0.0001  max mem: 41808
Epoch: [14]  [ 510/1349]  eta: 0:04:26  lr: 0.000448  min_lr: 0.000011  loss: 0.9112 (0.8417)  loss_scale: 131072.0000 (103754.6458)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
Epoch: [14]  [ 520/1349]  eta: 0:04:22  lr: 0.000448  min_lr: 0.000011  loss: 0.9112 (0.8409)  loss_scale: 131072.0000 (104278.9712)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
Epoch: [14]  [ 530/1349]  eta: 0:04:19  lr: 0.000448  min_lr: 0.000011  loss: 0.7689 (0.8387)  loss_scale: 131072.0000 (104783.5480)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
Epoch: [14]  [ 540/1349]  eta: 0:04:16  lr: 0.000448  min_lr: 0.000011  loss: 0.8046 (0.8388)  loss_scale: 131072.0000 (105269.4713)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [14]  [ 550/1349]  eta: 0:04:12  lr: 0.000448  min_lr: 0.000011  loss: 0.8444 (0.8376)  loss_scale: 131072.0000 (105737.7568)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0002  max mem: 41808
Epoch: [14]  [ 560/1349]  eta: 0:04:09  lr: 0.000448  min_lr: 0.000011  loss: 0.8444 (0.8375)  loss_scale: 131072.0000 (106189.3476)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0002  max mem: 41808
Epoch: [14]  [ 570/1349]  eta: 0:04:06  lr: 0.000448  min_lr: 0.000011  loss: 0.8367 (0.8369)  loss_scale: 131072.0000 (106625.1208)  weight_decay: 0.0500 (0.0500)  time: 0.3078  data: 0.0001  max mem: 41808
Epoch: [14]  [ 580/1349]  eta: 0:04:03  lr: 0.000448  min_lr: 0.000011  loss: 0.8367 (0.8364)  loss_scale: 131072.0000 (107045.8933)  weight_decay: 0.0500 (0.0500)  time: 0.3079  data: 0.0001  max mem: 41808
Epoch: [14]  [ 590/1349]  eta: 0:03:59  lr: 0.000448  min_lr: 0.000011  loss: 0.9000 (0.8358)  loss_scale: 131072.0000 (107452.4264)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
Epoch: [14]  [ 600/1349]  eta: 0:03:56  lr: 0.000448  min_lr: 0.000011  loss: 0.8904 (0.8367)  loss_scale: 131072.0000 (107845.4309)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
[2025-05-23 17:58:02,652] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 17:58:02,652] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 17:58:02,652] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 17:58:02,652] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [14]  [ 610/1349]  eta: 0:03:53  lr: 0.000448  min_lr: 0.000011  loss: 0.8368 (0.8352)  loss_scale: 131072.0000 (108654.6121)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
[2025-05-23 17:58:06,036] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 19506
[2025-05-23 17:58:06,036] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 17:58:06,036] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 19506
[2025-05-23 17:58:06,036] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 17:58:06,036] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [14]  [ 620/1349]  eta: 0:03:49  lr: 0.000448  min_lr: 0.000011  loss: 0.7933 (0.8354)  loss_scale: 262144.0000 (110915.1948)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [14]  [ 630/1349]  eta: 0:03:46  lr: 0.000447  min_lr: 0.000011  loss: 0.9134 (0.8374)  loss_scale: 131072.0000 (111234.6371)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [14]  [ 640/1349]  eta: 0:03:43  lr: 0.000447  min_lr: 0.000011  loss: 0.9134 (0.8382)  loss_scale: 131072.0000 (111544.1123)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [14]  [ 650/1349]  eta: 0:03:40  lr: 0.000447  min_lr: 0.000011  loss: 0.8540 (0.8374)  loss_scale: 131072.0000 (111844.0799)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
Epoch: [14]  [ 660/1349]  eta: 0:03:37  lr: 0.000447  min_lr: 0.000011  loss: 0.8167 (0.8379)  loss_scale: 131072.0000 (112134.9713)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [14]  [ 670/1349]  eta: 0:03:33  lr: 0.000447  min_lr: 0.000011  loss: 0.8791 (0.8379)  loss_scale: 131072.0000 (112417.1923)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
Epoch: [14]  [ 680/1349]  eta: 0:03:30  lr: 0.000447  min_lr: 0.000011  loss: 0.8677 (0.8372)  loss_scale: 131072.0000 (112691.1248)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
Epoch: [14]  [ 690/1349]  eta: 0:03:27  lr: 0.000447  min_lr: 0.000011  loss: 0.8445 (0.8377)  loss_scale: 131072.0000 (112957.1288)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
Epoch: [14]  [ 700/1349]  eta: 0:03:24  lr: 0.000447  min_lr: 0.000011  loss: 0.8445 (0.8370)  loss_scale: 131072.0000 (113215.5435)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
Epoch: [14]  [ 710/1349]  eta: 0:03:20  lr: 0.000447  min_lr: 0.000011  loss: 0.8634 (0.8375)  loss_scale: 131072.0000 (113466.6892)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
Epoch: [14]  [ 720/1349]  eta: 0:03:17  lr: 0.000447  min_lr: 0.000011  loss: 0.8860 (0.8384)  loss_scale: 131072.0000 (113710.8682)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [14]  [ 730/1349]  eta: 0:03:14  lr: 0.000447  min_lr: 0.000011  loss: 0.8860 (0.8384)  loss_scale: 131072.0000 (113948.3666)  weight_decay: 0.0500 (0.0500)  time: 0.3078  data: 0.0001  max mem: 41808
Epoch: [14]  [ 740/1349]  eta: 0:03:11  lr: 0.000447  min_lr: 0.000011  loss: 0.8822 (0.8393)  loss_scale: 131072.0000 (114179.4548)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0001  max mem: 41808
[2025-05-23 17:58:45,710] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 17:58:45,711] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 17:58:45,711] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 17:58:45,711] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [14]  [ 750/1349]  eta: 0:03:08  lr: 0.000447  min_lr: 0.000011  loss: 0.8274 (0.8376)  loss_scale: 131072.0000 (114753.4487)  weight_decay: 0.0500 (0.0500)  time: 0.3082  data: 0.0002  max mem: 41808
Epoch: [14]  [ 760/1349]  eta: 0:03:04  lr: 0.000446  min_lr: 0.000011  loss: 0.7698 (0.8380)  loss_scale: 262144.0000 (116690.2497)  weight_decay: 0.0500 (0.0500)  time: 0.3078  data: 0.0001  max mem: 41808
[2025-05-23 17:58:51,854] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 19655
[2025-05-23 17:58:51,854] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 17:58:51,854] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 19655
[2025-05-23 17:58:51,854] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
[2025-05-23 17:58:51,854] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
Epoch: [14]  [ 770/1349]  eta: 0:03:01  lr: 0.000446  min_lr: 0.000011  loss: 0.8410 (0.8369)  loss_scale: 262144.0000 (118236.8042)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0002  max mem: 41808
Epoch: [14]  [ 780/1349]  eta: 0:02:58  lr: 0.000446  min_lr: 0.000011  loss: 0.7466 (0.8366)  loss_scale: 131072.0000 (118401.1472)  weight_decay: 0.0500 (0.0500)  time: 0.3095  data: 0.0002  max mem: 41808
Epoch: [14]  [ 790/1349]  eta: 0:02:55  lr: 0.000446  min_lr: 0.000011  loss: 0.8337 (0.8367)  loss_scale: 131072.0000 (118561.3350)  weight_decay: 0.0500 (0.0500)  time: 0.3105  data: 0.0001  max mem: 41808
Epoch: [14]  [ 800/1349]  eta: 0:02:52  lr: 0.000446  min_lr: 0.000011  loss: 0.8337 (0.8361)  loss_scale: 131072.0000 (118717.5231)  weight_decay: 0.0500 (0.0500)  time: 0.3089  data: 0.0002  max mem: 41808
Epoch: [14]  [ 810/1349]  eta: 0:02:49  lr: 0.000446  min_lr: 0.000011  loss: 0.8375 (0.8365)  loss_scale: 131072.0000 (118869.8594)  weight_decay: 0.0500 (0.0500)  time: 0.3084  data: 0.0002  max mem: 41808
Epoch: [14]  [ 820/1349]  eta: 0:02:45  lr: 0.000446  min_lr: 0.000011  loss: 0.8853 (0.8369)  loss_scale: 131072.0000 (119018.4848)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0002  max mem: 41808
Epoch: [14]  [ 830/1349]  eta: 0:02:42  lr: 0.000446  min_lr: 0.000011  loss: 0.8278 (0.8364)  loss_scale: 131072.0000 (119163.5331)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0002  max mem: 41808
Epoch: [14]  [ 840/1349]  eta: 0:02:39  lr: 0.000446  min_lr: 0.000011  loss: 0.8246 (0.8363)  loss_scale: 131072.0000 (119305.1320)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [14]  [ 850/1349]  eta: 0:02:36  lr: 0.000446  min_lr: 0.000011  loss: 0.8283 (0.8365)  loss_scale: 131072.0000 (119443.4031)  weight_decay: 0.0500 (0.0500)  time: 0.3062  data: 0.0001  max mem: 41808
Epoch: [14]  [ 860/1349]  eta: 0:02:33  lr: 0.000446  min_lr: 0.000011  loss: 0.8526 (0.8361)  loss_scale: 131072.0000 (119578.4623)  weight_decay: 0.0500 (0.0500)  time: 0.3059  data: 0.0001  max mem: 41808
Epoch: [14]  [ 870/1349]  eta: 0:02:30  lr: 0.000446  min_lr: 0.000011  loss: 0.8588 (0.8360)  loss_scale: 131072.0000 (119710.4202)  weight_decay: 0.0500 (0.0500)  time: 0.3060  data: 0.0001  max mem: 41808
Epoch: [14]  [ 880/1349]  eta: 0:02:26  lr: 0.000445  min_lr: 0.000011  loss: 0.8802 (0.8368)  loss_scale: 131072.0000 (119839.3825)  weight_decay: 0.0500 (0.0500)  time: 0.3060  data: 0.0001  max mem: 41808
Epoch: [14]  [ 890/1349]  eta: 0:02:23  lr: 0.000445  min_lr: 0.000011  loss: 0.8785 (0.8373)  loss_scale: 131072.0000 (119965.4501)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0002  max mem: 41808
[2025-05-23 17:59:31,537] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 17:59:31,537] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 17:59:31,537] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 17:59:31,537] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [14]  [ 900/1349]  eta: 0:02:20  lr: 0.000445  min_lr: 0.000011  loss: 0.8397 (0.8366)  loss_scale: 131072.0000 (120525.1410)  weight_decay: 0.0500 (0.0500)  time: 0.3078  data: 0.0001  max mem: 41808
[2025-05-23 17:59:34,309] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 19793
[2025-05-23 17:59:34,309] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 19793
[2025-05-23 17:59:34,309] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 17:59:34,309] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 17:59:34,309] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [14]  [ 910/1349]  eta: 0:02:17  lr: 0.000445  min_lr: 0.000011  loss: 0.7359 (0.8361)  loss_scale: 131072.0000 (121504.1756)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0001  max mem: 41808
Epoch: [14]  [ 920/1349]  eta: 0:02:14  lr: 0.000445  min_lr: 0.000011  loss: 0.8479 (0.8375)  loss_scale: 131072.0000 (121608.0608)  weight_decay: 0.0500 (0.0500)  time: 0.3080  data: 0.0001  max mem: 41808
Epoch: [14]  [ 930/1349]  eta: 0:02:11  lr: 0.000445  min_lr: 0.000011  loss: 0.9513 (0.8381)  loss_scale: 131072.0000 (121709.7143)  weight_decay: 0.0500 (0.0500)  time: 0.3082  data: 0.0001  max mem: 41808
Epoch: [14]  [ 940/1349]  eta: 0:02:07  lr: 0.000445  min_lr: 0.000011  loss: 0.8345 (0.8368)  loss_scale: 131072.0000 (121809.2072)  weight_decay: 0.0500 (0.0500)  time: 0.3078  data: 0.0001  max mem: 41808
[2025-05-23 17:59:45,702] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 19830
[2025-05-23 17:59:45,702] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 19830
[2025-05-23 17:59:45,703] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 17:59:45,703] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 17:59:45,703] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [14]  [ 950/1349]  eta: 0:02:04  lr: 0.000445  min_lr: 0.000011  loss: 0.8345 (0.8372)  loss_scale: 131072.0000 (121424.2187)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [14]  [ 960/1349]  eta: 0:02:01  lr: 0.000445  min_lr: 0.000011  loss: 0.8521 (0.8370)  loss_scale: 65536.0000 (120842.6556)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [14]  [ 970/1349]  eta: 0:01:58  lr: 0.000445  min_lr: 0.000011  loss: 0.8330 (0.8371)  loss_scale: 65536.0000 (120273.0711)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
Epoch: [14]  [ 980/1349]  eta: 0:01:55  lr: 0.000445  min_lr: 0.000011  loss: 0.8434 (0.8367)  loss_scale: 65536.0000 (119715.0989)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
Epoch: [14]  [ 990/1349]  eta: 0:01:52  lr: 0.000445  min_lr: 0.000011  loss: 0.8242 (0.8361)  loss_scale: 65536.0000 (119168.3875)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
Epoch: [14]  [1000/1349]  eta: 0:01:49  lr: 0.000444  min_lr: 0.000011  loss: 0.8822 (0.8370)  loss_scale: 65536.0000 (118632.5994)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [14]  [1010/1349]  eta: 0:01:45  lr: 0.000444  min_lr: 0.000011  loss: 0.8979 (0.8372)  loss_scale: 65536.0000 (118107.4105)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0001  max mem: 41808
Epoch: [14]  [1020/1349]  eta: 0:01:42  lr: 0.000444  min_lr: 0.000011  loss: 0.8431 (0.8369)  loss_scale: 65536.0000 (117592.5093)  weight_decay: 0.0500 (0.0500)  time: 0.3079  data: 0.0001  max mem: 41808
Epoch: [14]  [1030/1349]  eta: 0:01:39  lr: 0.000444  min_lr: 0.000011  loss: 0.7838 (0.8366)  loss_scale: 65536.0000 (117087.5965)  weight_decay: 0.0500 (0.0500)  time: 0.3097  data: 0.0002  max mem: 41808
Epoch: [14]  [1040/1349]  eta: 0:01:36  lr: 0.000444  min_lr: 0.000011  loss: 0.8924 (0.8373)  loss_scale: 65536.0000 (116592.3842)  weight_decay: 0.0500 (0.0500)  time: 0.3096  data: 0.0001  max mem: 41808
Epoch: [14]  [1050/1349]  eta: 0:01:33  lr: 0.000444  min_lr: 0.000011  loss: 0.8801 (0.8371)  loss_scale: 65536.0000 (116106.5956)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [14]  [1060/1349]  eta: 0:01:30  lr: 0.000444  min_lr: 0.000011  loss: 0.7712 (0.8360)  loss_scale: 65536.0000 (115629.9642)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [14]  [1070/1349]  eta: 0:01:27  lr: 0.000444  min_lr: 0.000011  loss: 0.8246 (0.8363)  loss_scale: 65536.0000 (115162.2334)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
[2025-05-23 18:00:25,420] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 18:00:25,420] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-05-23 18:00:25,420] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 18:00:25,420] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [14]  [1080/1349]  eta: 0:01:23  lr: 0.000444  min_lr: 0.000011  loss: 0.8836 (0.8361)  loss_scale: 65536.0000 (115188.1591)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [14]  [1090/1349]  eta: 0:01:20  lr: 0.000444  min_lr: 0.000011  loss: 0.7751 (0.8357)  loss_scale: 131072.0000 (115333.7489)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [14]  [1100/1349]  eta: 0:01:17  lr: 0.000444  min_lr: 0.000011  loss: 0.7915 (0.8358)  loss_scale: 131072.0000 (115476.6939)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [14]  [1110/1349]  eta: 0:01:14  lr: 0.000444  min_lr: 0.000011  loss: 0.8783 (0.8359)  loss_scale: 131072.0000 (115617.0657)  weight_decay: 0.0500 (0.0500)  time: 0.3095  data: 0.0001  max mem: 41808
[2025-05-23 18:00:37,760] [INFO] [logging.py:96:log_dist] [Rank 0] step=20000, skipped=120, lr=[1.0538069443988876e-05, 1.0538069443988876e-05, 1.4050759258651833e-05, 1.4050759258651833e-05, 1.8734345678202446e-05, 1.8734345678202446e-05, 2.4979127570936595e-05, 2.4979127570936595e-05, 3.330550342791546e-05, 3.330550342791546e-05, 4.4407337903887275e-05, 4.4407337903887275e-05, 5.92097838718497e-05, 5.92097838718497e-05, 7.89463784957996e-05, 7.89463784957996e-05, 0.00010526183799439947, 0.00010526183799439947, 0.00014034911732586596, 0.00014034911732586596, 0.00018713215643448796, 0.00018713215643448796, 0.0002495095419126506, 0.0002495095419126506, 0.00033267938921686745, 0.00033267938921686745, 0.0004435725189558233, 0.0004435725189558233], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-05-23 18:00:37,761] [INFO] [timer.py:260:stop] epoch=0/micro_step=20000/global_step=20000, RunningAvgSamplesPerSec=206.3534478939086, CurrSamplesPerSec=213.92637410015755, MemAllocated=1.37GB, MaxMemAllocated=40.83GB
Epoch: [14]  [1120/1349]  eta: 0:01:11  lr: 0.000444  min_lr: 0.000011  loss: 0.8217 (0.8355)  loss_scale: 131072.0000 (115754.9331)  weight_decay: 0.0500 (0.0500)  time: 0.3091  data: 0.0002  max mem: 41808
Epoch: [14]  [1130/1349]  eta: 0:01:08  lr: 0.000443  min_lr: 0.000011  loss: 0.7995 (0.8346)  loss_scale: 131072.0000 (115890.3625)  weight_decay: 0.0500 (0.0500)  time: 0.3080  data: 0.0002  max mem: 41808
Epoch: [14]  [1140/1349]  eta: 0:01:05  lr: 0.000443  min_lr: 0.000011  loss: 0.8613 (0.8350)  loss_scale: 131072.0000 (116023.4181)  weight_decay: 0.0500 (0.0500)  time: 0.3091  data: 0.0001  max mem: 41808
Epoch: [14]  [1150/1349]  eta: 0:01:02  lr: 0.000443  min_lr: 0.000011  loss: 0.8294 (0.8348)  loss_scale: 131072.0000 (116154.1616)  weight_decay: 0.0500 (0.0500)  time: 0.3084  data: 0.0001  max mem: 41808
Epoch: [14]  [1160/1349]  eta: 0:00:58  lr: 0.000443  min_lr: 0.000011  loss: 0.8294 (0.8350)  loss_scale: 131072.0000 (116282.6529)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
Epoch: [14]  [1170/1349]  eta: 0:00:55  lr: 0.000443  min_lr: 0.000011  loss: 0.8084 (0.8346)  loss_scale: 131072.0000 (116408.9496)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0002  max mem: 41808
Epoch: [14]  [1180/1349]  eta: 0:00:52  lr: 0.000443  min_lr: 0.000011  loss: 0.8084 (0.8348)  loss_scale: 131072.0000 (116533.1075)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [14]  [1190/1349]  eta: 0:00:49  lr: 0.000443  min_lr: 0.000011  loss: 0.8216 (0.8347)  loss_scale: 131072.0000 (116655.1805)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
Epoch: [14]  [1200/1349]  eta: 0:00:46  lr: 0.000443  min_lr: 0.000011  loss: 0.8273 (0.8352)  loss_scale: 131072.0000 (116775.2206)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0002  max mem: 41808
[2025-05-23 18:01:04,851] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 18:01:04,851] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 18:01:04,851] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 18:01:04,851] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 18:01:05,155] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 20088
[2025-05-23 18:01:05,155] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 20088
[2025-05-23 18:01:05,155] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 18:01:05,155] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 18:01:05,155] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [14]  [1210/1349]  eta: 0:00:43  lr: 0.000443  min_lr: 0.000011  loss: 0.8677 (0.8350)  loss_scale: 131072.0000 (117001.5128)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0002  max mem: 41808
Epoch: [14]  [1220/1349]  eta: 0:00:40  lr: 0.000443  min_lr: 0.000011  loss: 0.8439 (0.8352)  loss_scale: 131072.0000 (117116.7502)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
Epoch: [14]  [1230/1349]  eta: 0:00:37  lr: 0.000443  min_lr: 0.000011  loss: 0.8439 (0.8354)  loss_scale: 131072.0000 (117230.1154)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
Epoch: [14]  [1240/1349]  eta: 0:00:33  lr: 0.000443  min_lr: 0.000011  loss: 0.7909 (0.8352)  loss_scale: 131072.0000 (117341.6535)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0001  max mem: 41808
Epoch: [14]  [1250/1349]  eta: 0:00:30  lr: 0.000442  min_lr: 0.000011  loss: 0.8759 (0.8353)  loss_scale: 131072.0000 (117451.4085)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0001  max mem: 41808
Epoch: [14]  [1260/1349]  eta: 0:00:27  lr: 0.000442  min_lr: 0.000011  loss: 0.8964 (0.8355)  loss_scale: 131072.0000 (117559.4227)  weight_decay: 0.0500 (0.0500)  time: 0.3063  data: 0.0001  max mem: 41808
Epoch: [14]  [1270/1349]  eta: 0:00:24  lr: 0.000442  min_lr: 0.000011  loss: 0.7837 (0.8353)  loss_scale: 131072.0000 (117665.7372)  weight_decay: 0.0500 (0.0500)  time: 0.3062  data: 0.0001  max mem: 41808
Epoch: [14]  [1280/1349]  eta: 0:00:21  lr: 0.000442  min_lr: 0.000011  loss: 0.7767 (0.8351)  loss_scale: 131072.0000 (117770.3919)  weight_decay: 0.0500 (0.0500)  time: 0.3062  data: 0.0001  max mem: 41808
Epoch: [14]  [1290/1349]  eta: 0:00:18  lr: 0.000442  min_lr: 0.000011  loss: 0.8443 (0.8352)  loss_scale: 131072.0000 (117873.4253)  weight_decay: 0.0500 (0.0500)  time: 0.3061  data: 0.0001  max mem: 41808
Epoch: [14]  [1300/1349]  eta: 0:00:15  lr: 0.000442  min_lr: 0.000011  loss: 0.8225 (0.8350)  loss_scale: 131072.0000 (117974.8747)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0001  max mem: 41808
Epoch: [14]  [1310/1349]  eta: 0:00:12  lr: 0.000442  min_lr: 0.000010  loss: 0.8189 (0.8350)  loss_scale: 131072.0000 (118074.7765)  weight_decay: 0.0500 (0.0500)  time: 0.3066  data: 0.0001  max mem: 41808
Epoch: [14]  [1320/1349]  eta: 0:00:09  lr: 0.000442  min_lr: 0.000010  loss: 0.8201 (0.8350)  loss_scale: 131072.0000 (118173.1658)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0001  max mem: 41808
Epoch: [14]  [1330/1349]  eta: 0:00:05  lr: 0.000442  min_lr: 0.000010  loss: 0.8651 (0.8353)  loss_scale: 131072.0000 (118270.0766)  weight_decay: 0.0500 (0.0500)  time: 0.3046  data: 0.0001  max mem: 41808
[2025-05-23 18:01:44,680] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 18:01:44,680] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 18:01:44,680] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 18:01:44,680] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [14]  [1340/1349]  eta: 0:00:02  lr: 0.000442  min_lr: 0.000010  loss: 0.9069 (0.8359)  loss_scale: 131072.0000 (119342.9620)  weight_decay: 0.0500 (0.0500)  time: 0.3022  data: 0.0001  max mem: 41808
[2025-05-23 18:01:48,295] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 20229
[2025-05-23 18:01:48,295] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 20229
[2025-05-23 18:01:48,295] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 18:01:48,295] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 18:01:48,296] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [14]  [1348/1349]  eta: 0:00:00  lr: 0.000442  min_lr: 0.000010  loss: 0.9304 (0.8364)  loss_scale: 262144.0000 (119606.8436)  weight_decay: 0.0500 (0.0500)  time: 0.3011  data: 0.0001  max mem: 41808
Epoch: [14] Total time: 0:07:00 (0.3114 s / it)
Averaged stats: lr: 0.000442  min_lr: 0.000010  loss: 0.9304 (0.8381)  loss_scale: 262144.0000 (119606.8436)  weight_decay: 0.0500 (0.0500)  total_time: 420.0594 (420.0554)
Val:  [  0/346]  eta: 1:06:08  loss: 2.4845 (2.4845)  acc1: 5.4688 (5.4688)  acc5: 78.1250 (78.1250)  time: 11.4698  data: 10.5495  max mem: 41808
Val:  [ 10/346]  eta: 0:11:32  loss: 0.1274 (0.5485)  acc1: 100.0000 (84.0199)  acc5: 100.0000 (97.5852)  time: 2.0620  data: 1.2924  max mem: 41808
Val:  [ 20/346]  eta: 0:08:10  loss: 0.1270 (0.4329)  acc1: 100.0000 (88.3185)  acc5: 100.0000 (98.5863)  time: 1.0068  data: 0.2420  max mem: 41808
Val:  [ 30/346]  eta: 0:06:43  loss: 0.1076 (0.3637)  acc1: 100.0000 (90.9022)  acc5: 100.0000 (99.0423)  time: 0.8457  data: 0.0588  max mem: 41808
Val:  [ 40/346]  eta: 0:05:50  loss: 0.1194 (0.3941)  acc1: 99.2188 (90.3773)  acc5: 100.0000 (98.6852)  time: 0.7677  data: 0.0003  max mem: 41808
Val:  [ 50/346]  eta: 0:05:18  loss: 0.1194 (0.3509)  acc1: 99.2188 (91.7739)  acc5: 100.0000 (98.9124)  time: 0.7653  data: 0.0003  max mem: 41808
Val:  [ 60/346]  eta: 0:04:53  loss: 0.1447 (0.3387)  acc1: 96.8750 (92.1747)  acc5: 100.0000 (99.0779)  time: 0.7856  data: 0.0003  max mem: 41808
Val:  [ 70/346]  eta: 0:04:34  loss: 0.2955 (0.3581)  acc1: 90.6250 (91.2742)  acc5: 100.0000 (99.1967)  time: 0.7812  data: 0.0003  max mem: 41808
Val:  [ 80/346]  eta: 0:04:18  loss: 0.2856 (0.3572)  acc1: 91.4062 (91.4834)  acc5: 100.0000 (99.1995)  time: 0.8049  data: 0.0497  max mem: 41808
Val:  [ 90/346]  eta: 0:04:07  loss: 0.2372 (0.3647)  acc1: 95.3125 (91.2345)  acc5: 100.0000 (99.2273)  time: 0.8646  data: 0.1166  max mem: 41808
Val:  [100/346]  eta: 0:03:55  loss: 0.1641 (0.3468)  acc1: 98.4375 (91.9400)  acc5: 100.0000 (99.3038)  time: 0.8887  data: 0.1326  max mem: 41808
Val:  [110/346]  eta: 0:03:44  loss: 0.1641 (0.3638)  acc1: 99.2188 (91.4907)  acc5: 100.0000 (99.2610)  time: 0.8907  data: 0.1368  max mem: 41808
Val:  [120/346]  eta: 0:03:34  loss: 0.1731 (0.3672)  acc1: 97.6562 (91.3804)  acc5: 100.0000 (99.2962)  time: 0.9126  data: 0.1499  max mem: 41808
Val:  [130/346]  eta: 0:03:23  loss: 0.1223 (0.3804)  acc1: 98.4375 (90.8457)  acc5: 100.0000 (99.1949)  time: 0.8920  data: 0.1525  max mem: 41808
Val:  [140/346]  eta: 0:03:13  loss: 0.1574 (0.3714)  acc1: 96.8750 (91.1735)  acc5: 100.0000 (99.2520)  time: 0.8836  data: 0.1455  max mem: 41808
Val:  [150/346]  eta: 0:03:03  loss: 0.2532 (0.3707)  acc1: 95.3125 (91.2355)  acc5: 100.0000 (99.2912)  time: 0.8939  data: 0.1447  max mem: 41808
Val:  [160/346]  eta: 0:02:53  loss: 0.2486 (0.3679)  acc1: 96.8750 (91.3141)  acc5: 100.0000 (99.3012)  time: 0.8919  data: 0.1498  max mem: 41808
Val:  [170/346]  eta: 0:02:43  loss: 0.2486 (0.3742)  acc1: 91.4062 (90.9539)  acc5: 100.0000 (99.3421)  time: 0.8965  data: 0.1529  max mem: 41808
Val:  [180/346]  eta: 0:02:34  loss: 0.2763 (0.3888)  acc1: 89.0625 (90.1286)  acc5: 100.0000 (99.3785)  time: 0.8971  data: 0.1461  max mem: 41808
Val:  [190/346]  eta: 0:02:24  loss: 0.1936 (0.3888)  acc1: 95.3125 (90.1342)  acc5: 100.0000 (99.3946)  time: 0.9032  data: 0.1584  max mem: 41808
Val:  [200/346]  eta: 0:02:15  loss: 0.3935 (0.4020)  acc1: 90.6250 (89.6105)  acc5: 100.0000 (99.3975)  time: 0.9090  data: 0.1757  max mem: 41808
Val:  [210/346]  eta: 0:02:05  loss: 0.1896 (0.3943)  acc1: 91.4062 (89.8586)  acc5: 100.0000 (99.4261)  time: 0.9017  data: 0.1598  max mem: 41808
Val:  [220/346]  eta: 0:01:56  loss: 0.1711 (0.3923)  acc1: 95.3125 (89.9498)  acc5: 100.0000 (99.4344)  time: 0.9049  data: 0.1499  max mem: 41808
Val:  [230/346]  eta: 0:01:47  loss: 0.1364 (0.3827)  acc1: 99.2188 (90.3037)  acc5: 100.0000 (99.4521)  time: 0.9233  data: 0.1562  max mem: 41808
Val:  [240/346]  eta: 0:01:38  loss: 0.1352 (0.3827)  acc1: 99.2188 (90.3106)  acc5: 100.0000 (99.4586)  time: 0.9357  data: 0.1570  max mem: 41808
Val:  [250/346]  eta: 0:01:28  loss: 0.1676 (0.3797)  acc1: 97.6562 (90.4258)  acc5: 100.0000 (99.4709)  time: 0.9225  data: 0.1560  max mem: 41808
Val:  [260/346]  eta: 0:01:19  loss: 0.1362 (0.3793)  acc1: 98.4375 (90.4304)  acc5: 100.0000 (99.4403)  time: 0.9090  data: 0.1476  max mem: 41808
Val:  [270/346]  eta: 0:01:10  loss: 0.1472 (0.3743)  acc1: 97.6562 (90.6019)  acc5: 100.0000 (99.4609)  time: 0.9211  data: 0.1466  max mem: 41808
Val:  [280/346]  eta: 0:01:01  loss: 0.1222 (0.3727)  acc1: 96.8750 (90.6862)  acc5: 100.0000 (99.4523)  time: 0.9438  data: 0.1606  max mem: 41808
Val:  [290/346]  eta: 0:00:51  loss: 0.1069 (0.3640)  acc1: 100.0000 (90.9874)  acc5: 100.0000 (99.4711)  time: 0.9334  data: 0.1642  max mem: 41808
Val:  [300/346]  eta: 0:00:42  loss: 0.1078 (0.3643)  acc1: 100.0000 (91.0455)  acc5: 100.0000 (99.3148)  time: 0.9064  data: 0.1576  max mem: 41808
Val:  [310/346]  eta: 0:00:33  loss: 0.1193 (0.3668)  acc1: 99.2188 (90.9817)  acc5: 100.0000 (99.3117)  time: 0.9027  data: 0.1547  max mem: 41808
Val:  [320/346]  eta: 0:00:24  loss: 0.1460 (0.3654)  acc1: 96.0938 (91.0071)  acc5: 100.0000 (99.3331)  time: 0.8986  data: 0.1584  max mem: 41808
Val:  [330/346]  eta: 0:00:14  loss: 0.3028 (0.3726)  acc1: 91.4062 (90.8162)  acc5: 100.0000 (99.3061)  time: 0.8958  data: 0.1581  max mem: 41808
Val:  [340/346]  eta: 0:00:05  loss: 0.3490 (0.3808)  acc1: 90.6250 (90.5929)  acc5: 100.0000 (99.3218)  time: 0.8860  data: 0.1623  max mem: 41808
Val:  [345/346]  eta: 0:00:00  loss: 0.1698 (0.3777)  acc1: 96.8750 (90.6999)  acc5: 100.0000 (99.3288)  time: 0.8711  data: 0.1762  max mem: 41808
Val: Total time: 0:05:18 (0.9215 s / it)
* Acc@1 90.716 Acc@5 99.351 loss 0.375
Accuracy of the network on the 88494 val videos: 90.7%
Max accuracy: 91.25%   Max Epoch: 3
Epoch: [15]  [   0/1349]  eta: 1:36:21  lr: 0.000442  min_lr: 0.000010  loss: 0.9133 (0.9133)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 4.2861  data: 3.6798  max mem: 41808
Epoch: [15]  [  10/1349]  eta: 0:17:13  lr: 0.000442  min_lr: 0.000010  loss: 0.8829 (0.8419)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7722  data: 0.3365  max mem: 41808
Epoch: [15]  [  20/1349]  eta: 0:12:12  lr: 0.000441  min_lr: 0.000010  loss: 0.8489 (0.8002)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3646  data: 0.0011  max mem: 41808
[2025-05-23 18:07:21,756] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 20259
[2025-05-23 18:07:21,756] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 20259
[2025-05-23 18:07:21,756] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 18:07:21,756] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 18:07:21,756] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [15]  [  30/1349]  eta: 0:10:23  lr: 0.000441  min_lr: 0.000010  loss: 0.8489 (0.8155)  loss_scale: 131072.0000 (116273.5484)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0001  max mem: 41808
Epoch: [15]  [  40/1349]  eta: 0:09:25  lr: 0.000441  min_lr: 0.000010  loss: 0.8873 (0.8296)  loss_scale: 65536.0000 (103898.5366)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [15]  [  50/1349]  eta: 0:08:49  lr: 0.000441  min_lr: 0.000010  loss: 0.8698 (0.8262)  loss_scale: 65536.0000 (96376.4706)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0001  max mem: 41808
Epoch: [15]  [  60/1349]  eta: 0:08:24  lr: 0.000441  min_lr: 0.000010  loss: 0.8466 (0.8214)  loss_scale: 65536.0000 (91320.6557)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
Epoch: [15]  [  70/1349]  eta: 0:08:05  lr: 0.000441  min_lr: 0.000010  loss: 0.8606 (0.8283)  loss_scale: 65536.0000 (87689.0141)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
Epoch: [15]  [  80/1349]  eta: 0:07:50  lr: 0.000441  min_lr: 0.000010  loss: 0.8447 (0.8337)  loss_scale: 65536.0000 (84954.0741)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
Epoch: [15]  [  90/1349]  eta: 0:07:38  lr: 0.000441  min_lr: 0.000010  loss: 0.8889 (0.8349)  loss_scale: 65536.0000 (82820.2198)  weight_decay: 0.0500 (0.0500)  time: 0.3088  data: 0.0001  max mem: 41808
Epoch: [15]  [ 100/1349]  eta: 0:07:27  lr: 0.000441  min_lr: 0.000010  loss: 0.9423 (0.8454)  loss_scale: 65536.0000 (81108.9109)  weight_decay: 0.0500 (0.0500)  time: 0.3087  data: 0.0001  max mem: 41808
Epoch: [15]  [ 110/1349]  eta: 0:07:18  lr: 0.000441  min_lr: 0.000010  loss: 0.8447 (0.8395)  loss_scale: 65536.0000 (79705.9459)  weight_decay: 0.0500 (0.0500)  time: 0.3090  data: 0.0001  max mem: 41808
Epoch: [15]  [ 120/1349]  eta: 0:07:10  lr: 0.000441  min_lr: 0.000010  loss: 0.8128 (0.8419)  loss_scale: 65536.0000 (78534.8760)  weight_decay: 0.0500 (0.0500)  time: 0.3091  data: 0.0002  max mem: 41808
Epoch: [15]  [ 130/1349]  eta: 0:07:02  lr: 0.000441  min_lr: 0.000010  loss: 0.8314 (0.8430)  loss_scale: 65536.0000 (77542.5954)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
Epoch: [15]  [ 140/1349]  eta: 0:06:56  lr: 0.000440  min_lr: 0.000010  loss: 0.8670 (0.8433)  loss_scale: 65536.0000 (76691.0638)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [15]  [ 150/1349]  eta: 0:06:49  lr: 0.000440  min_lr: 0.000010  loss: 0.8638 (0.8404)  loss_scale: 65536.0000 (75952.3179)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0001  max mem: 41808
[2025-05-23 18:08:01,496] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 18:08:01,496] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 18:08:01,496] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-05-23 18:08:01,496] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [15]  [ 160/1349]  eta: 0:06:43  lr: 0.000440  min_lr: 0.000010  loss: 0.8991 (0.8457)  loss_scale: 65536.0000 (78561.7888)  weight_decay: 0.0500 (0.0500)  time: 0.3090  data: 0.0001  max mem: 41808
Epoch: [15]  [ 170/1349]  eta: 0:06:38  lr: 0.000440  min_lr: 0.000010  loss: 0.9564 (0.8461)  loss_scale: 131072.0000 (81632.5614)  weight_decay: 0.0500 (0.0500)  time: 0.3086  data: 0.0001  max mem: 41808
Epoch: [15]  [ 180/1349]  eta: 0:06:32  lr: 0.000440  min_lr: 0.000010  loss: 0.8448 (0.8480)  loss_scale: 131072.0000 (84364.0221)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
Epoch: [15]  [ 190/1349]  eta: 0:06:27  lr: 0.000440  min_lr: 0.000010  loss: 0.8484 (0.8474)  loss_scale: 131072.0000 (86809.4660)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0002  max mem: 41808
Epoch: [15]  [ 200/1349]  eta: 0:06:23  lr: 0.000440  min_lr: 0.000010  loss: 0.7864 (0.8425)  loss_scale: 131072.0000 (89011.5821)  weight_decay: 0.0500 (0.0500)  time: 0.3083  data: 0.0001  max mem: 41808
Epoch: [15]  [ 210/1349]  eta: 0:06:18  lr: 0.000440  min_lr: 0.000010  loss: 0.7832 (0.8417)  loss_scale: 131072.0000 (91004.9668)  weight_decay: 0.0500 (0.0500)  time: 0.3083  data: 0.0001  max mem: 41808
Epoch: [15]  [ 220/1349]  eta: 0:06:13  lr: 0.000440  min_lr: 0.000010  loss: 0.8763 (0.8439)  loss_scale: 131072.0000 (92817.9548)  weight_decay: 0.0500 (0.0500)  time: 0.3084  data: 0.0001  max mem: 41808
Epoch: [15]  [ 230/1349]  eta: 0:06:09  lr: 0.000440  min_lr: 0.000010  loss: 0.9018 (0.8432)  loss_scale: 131072.0000 (94473.9740)  weight_decay: 0.0500 (0.0500)  time: 0.3084  data: 0.0001  max mem: 41808
Epoch: [15]  [ 240/1349]  eta: 0:06:05  lr: 0.000440  min_lr: 0.000010  loss: 0.8318 (0.8382)  loss_scale: 131072.0000 (95992.5643)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
Epoch: [15]  [ 250/1349]  eta: 0:06:00  lr: 0.000440  min_lr: 0.000010  loss: 0.7905 (0.8382)  loss_scale: 131072.0000 (97390.1514)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
Epoch: [15]  [ 260/1349]  eta: 0:05:56  lr: 0.000439  min_lr: 0.000010  loss: 0.8741 (0.8407)  loss_scale: 131072.0000 (98680.6437)  weight_decay: 0.0500 (0.0500)  time: 0.3078  data: 0.0001  max mem: 41808
Epoch: [15]  [ 270/1349]  eta: 0:05:52  lr: 0.000439  min_lr: 0.000010  loss: 0.8515 (0.8398)  loss_scale: 131072.0000 (99875.8967)  weight_decay: 0.0500 (0.0500)  time: 0.3090  data: 0.0001  max mem: 41808
Epoch: [15]  [ 280/1349]  eta: 0:05:48  lr: 0.000439  min_lr: 0.000010  loss: 0.7539 (0.8375)  loss_scale: 131072.0000 (100986.0783)  weight_decay: 0.0500 (0.0500)  time: 0.3098  data: 0.0001  max mem: 41808
[2025-05-23 18:08:40,965] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 18:08:40,965] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 18:08:40,965] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 18:08:40,965] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [15]  [ 290/1349]  eta: 0:05:44  lr: 0.000439  min_lr: 0.000010  loss: 0.8429 (0.8364)  loss_scale: 131072.0000 (106524.1512)  weight_decay: 0.0500 (0.0500)  time: 0.3098  data: 0.0001  max mem: 41808
[2025-05-23 18:08:45,612] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 20531
[2025-05-23 18:08:45,612] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 20531
[2025-05-23 18:08:45,612] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 18:08:45,612] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 18:08:45,612] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [15]  [ 300/1349]  eta: 0:05:41  lr: 0.000439  min_lr: 0.000010  loss: 0.8493 (0.8380)  loss_scale: 262144.0000 (109516.9701)  weight_decay: 0.0500 (0.0500)  time: 0.3090  data: 0.0002  max mem: 41808
Epoch: [15]  [ 310/1349]  eta: 0:05:37  lr: 0.000439  min_lr: 0.000010  loss: 0.8567 (0.8359)  loss_scale: 131072.0000 (110210.0579)  weight_decay: 0.0500 (0.0500)  time: 0.3081  data: 0.0001  max mem: 41808
Epoch: [15]  [ 320/1349]  eta: 0:05:33  lr: 0.000439  min_lr: 0.000010  loss: 0.8267 (0.8344)  loss_scale: 131072.0000 (110859.9626)  weight_decay: 0.0500 (0.0500)  time: 0.3093  data: 0.0001  max mem: 41808
Epoch: [15]  [ 330/1349]  eta: 0:05:29  lr: 0.000439  min_lr: 0.000010  loss: 0.7564 (0.8337)  loss_scale: 131072.0000 (111470.5982)  weight_decay: 0.0500 (0.0500)  time: 0.3090  data: 0.0001  max mem: 41808
Epoch: [15]  [ 340/1349]  eta: 0:05:26  lr: 0.000439  min_lr: 0.000010  loss: 0.8765 (0.8355)  loss_scale: 131072.0000 (112045.4194)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
Epoch: [15]  [ 350/1349]  eta: 0:05:22  lr: 0.000439  min_lr: 0.000010  loss: 0.9776 (0.8399)  loss_scale: 131072.0000 (112587.4872)  weight_decay: 0.0500 (0.0500)  time: 0.3080  data: 0.0001  max mem: 41808
Epoch: [15]  [ 360/1349]  eta: 0:05:18  lr: 0.000439  min_lr: 0.000010  loss: 0.9660 (0.8413)  loss_scale: 131072.0000 (113099.5235)  weight_decay: 0.0500 (0.0500)  time: 0.3084  data: 0.0001  max mem: 41808
Epoch: [15]  [ 370/1349]  eta: 0:05:15  lr: 0.000439  min_lr: 0.000010  loss: 0.9080 (0.8426)  loss_scale: 131072.0000 (113583.9569)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0002  max mem: 41808
Epoch: [15]  [ 380/1349]  eta: 0:05:11  lr: 0.000438  min_lr: 0.000010  loss: 0.8861 (0.8434)  loss_scale: 131072.0000 (114042.9606)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0002  max mem: 41808
Epoch: [15]  [ 390/1349]  eta: 0:05:07  lr: 0.000438  min_lr: 0.000010  loss: 0.8562 (0.8432)  loss_scale: 131072.0000 (114478.4859)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0002  max mem: 41808
Epoch: [15]  [ 400/1349]  eta: 0:05:04  lr: 0.000438  min_lr: 0.000010  loss: 0.8562 (0.8431)  loss_scale: 131072.0000 (114892.2893)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0002  max mem: 41808
Epoch: [15]  [ 410/1349]  eta: 0:05:00  lr: 0.000438  min_lr: 0.000010  loss: 0.8173 (0.8411)  loss_scale: 131072.0000 (115285.9562)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [15]  [ 420/1349]  eta: 0:04:57  lr: 0.000438  min_lr: 0.000010  loss: 0.8947 (0.8434)  loss_scale: 131072.0000 (115660.9216)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
[2025-05-23 18:09:25,365] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 18:09:25,365] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 18:09:25,365] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 18:09:25,365] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [15]  [ 430/1349]  eta: 0:04:54  lr: 0.000438  min_lr: 0.000010  loss: 0.9237 (0.8433)  loss_scale: 131072.0000 (117843.1555)  weight_decay: 0.0500 (0.0500)  time: 0.3088  data: 0.0001  max mem: 41808
Epoch: [15]  [ 440/1349]  eta: 0:04:50  lr: 0.000438  min_lr: 0.000010  loss: 0.8647 (0.8440)  loss_scale: 262144.0000 (121115.2834)  weight_decay: 0.0500 (0.0500)  time: 0.3089  data: 0.0001  max mem: 41808
[2025-05-23 18:09:33,034] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 20685
[2025-05-23 18:09:33,034] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 20685
[2025-05-23 18:09:33,034] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 18:09:33,034] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 18:09:33,034] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [15]  [ 450/1349]  eta: 0:04:47  lr: 0.000438  min_lr: 0.000010  loss: 0.8494 (0.8429)  loss_scale: 262144.0000 (123951.6807)  weight_decay: 0.0500 (0.0500)  time: 0.3064  data: 0.0001  max mem: 41808
Epoch: [15]  [ 460/1349]  eta: 0:04:43  lr: 0.000438  min_lr: 0.000010  loss: 0.8494 (0.8420)  loss_scale: 131072.0000 (124106.1345)  weight_decay: 0.0500 (0.0500)  time: 0.3058  data: 0.0001  max mem: 41808
Epoch: [15]  [ 470/1349]  eta: 0:04:40  lr: 0.000438  min_lr: 0.000010  loss: 0.8678 (0.8426)  loss_scale: 131072.0000 (124254.0297)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0001  max mem: 41808
Epoch: [15]  [ 480/1349]  eta: 0:04:36  lr: 0.000438  min_lr: 0.000010  loss: 0.8486 (0.8436)  loss_scale: 131072.0000 (124395.7755)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
Epoch: [15]  [ 490/1349]  eta: 0:04:33  lr: 0.000438  min_lr: 0.000010  loss: 0.7592 (0.8416)  loss_scale: 131072.0000 (124531.7475)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [15]  [ 500/1349]  eta: 0:04:30  lr: 0.000437  min_lr: 0.000010  loss: 0.7592 (0.8407)  loss_scale: 131072.0000 (124662.2914)  weight_decay: 0.0500 (0.0500)  time: 0.3080  data: 0.0001  max mem: 41808
Epoch: [15]  [ 510/1349]  eta: 0:04:26  lr: 0.000437  min_lr: 0.000010  loss: 0.7999 (0.8405)  loss_scale: 131072.0000 (124787.7260)  weight_decay: 0.0500 (0.0500)  time: 0.3095  data: 0.0002  max mem: 41808
Epoch: [15]  [ 520/1349]  eta: 0:04:23  lr: 0.000437  min_lr: 0.000010  loss: 0.8405 (0.8408)  loss_scale: 131072.0000 (124908.3455)  weight_decay: 0.0500 (0.0500)  time: 0.3093  data: 0.0001  max mem: 41808
Epoch: [15]  [ 530/1349]  eta: 0:04:20  lr: 0.000437  min_lr: 0.000010  loss: 0.8792 (0.8414)  loss_scale: 131072.0000 (125024.4218)  weight_decay: 0.0500 (0.0500)  time: 0.3093  data: 0.0001  max mem: 41808
Epoch: [15]  [ 540/1349]  eta: 0:04:16  lr: 0.000437  min_lr: 0.000010  loss: 0.8876 (0.8419)  loss_scale: 131072.0000 (125136.2070)  weight_decay: 0.0500 (0.0500)  time: 0.3108  data: 0.0001  max mem: 41808
Epoch: [15]  [ 550/1349]  eta: 0:04:13  lr: 0.000437  min_lr: 0.000010  loss: 0.8448 (0.8422)  loss_scale: 131072.0000 (125243.9347)  weight_decay: 0.0500 (0.0500)  time: 0.3105  data: 0.0001  max mem: 41808
Epoch: [15]  [ 560/1349]  eta: 0:04:10  lr: 0.000437  min_lr: 0.000010  loss: 0.8240 (0.8419)  loss_scale: 131072.0000 (125347.8217)  weight_decay: 0.0500 (0.0500)  time: 0.3082  data: 0.0001  max mem: 41808
Epoch: [15]  [ 570/1349]  eta: 0:04:06  lr: 0.000437  min_lr: 0.000010  loss: 0.7903 (0.8413)  loss_scale: 131072.0000 (125448.0701)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0001  max mem: 41808
[2025-05-23 18:10:12,829] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 18:10:12,829] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 18:10:12,829] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 18:10:12,829] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [15]  [ 580/1349]  eta: 0:04:03  lr: 0.000437  min_lr: 0.000010  loss: 0.7484 (0.8401)  loss_scale: 131072.0000 (125996.0620)  weight_decay: 0.0500 (0.0500)  time: 0.3079  data: 0.0002  max mem: 41808
[2025-05-23 18:10:15,613] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 20823
[2025-05-23 18:10:15,613] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 20823
[2025-05-23 18:10:15,613] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 18:10:15,613] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 18:10:15,613] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [15]  [ 590/1349]  eta: 0:04:00  lr: 0.000437  min_lr: 0.000010  loss: 0.7639 (0.8385)  loss_scale: 131072.0000 (127634.4095)  weight_decay: 0.0500 (0.0500)  time: 0.3081  data: 0.0002  max mem: 41808
Epoch: [15]  [ 600/1349]  eta: 0:03:57  lr: 0.000437  min_lr: 0.000010  loss: 0.8348 (0.8384)  loss_scale: 131072.0000 (127691.6073)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [15]  [ 610/1349]  eta: 0:03:53  lr: 0.000436  min_lr: 0.000010  loss: 0.8348 (0.8382)  loss_scale: 131072.0000 (127746.9329)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
Epoch: [15]  [ 620/1349]  eta: 0:03:50  lr: 0.000436  min_lr: 0.000010  loss: 0.8038 (0.8377)  loss_scale: 131072.0000 (127800.4767)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
[2025-05-23 18:10:28,496] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 20865
[2025-05-23 18:10:28,496] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 20865
[2025-05-23 18:10:28,496] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 18:10:28,496] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 18:10:28,496] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [15]  [ 630/1349]  eta: 0:03:47  lr: 0.000436  min_lr: 0.000010  loss: 0.8380 (0.8382)  loss_scale: 131072.0000 (127748.4628)  weight_decay: 0.0500 (0.0500)  time: 0.3064  data: 0.0001  max mem: 41808
Epoch: [15]  [ 640/1349]  eta: 0:03:44  lr: 0.000436  min_lr: 0.000010  loss: 0.8600 (0.8380)  loss_scale: 65536.0000 (126777.9095)  weight_decay: 0.0500 (0.0500)  time: 0.3061  data: 0.0001  max mem: 41808
Epoch: [15]  [ 650/1349]  eta: 0:03:40  lr: 0.000436  min_lr: 0.000010  loss: 0.8695 (0.8386)  loss_scale: 65536.0000 (125837.1736)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
Epoch: [15]  [ 660/1349]  eta: 0:03:37  lr: 0.000436  min_lr: 0.000010  loss: 0.8781 (0.8386)  loss_scale: 65536.0000 (124924.9017)  weight_decay: 0.0500 (0.0500)  time: 0.3066  data: 0.0001  max mem: 41808
Epoch: [15]  [ 670/1349]  eta: 0:03:34  lr: 0.000436  min_lr: 0.000010  loss: 0.8396 (0.8392)  loss_scale: 65536.0000 (124039.8212)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0002  max mem: 41808
Epoch: [15]  [ 680/1349]  eta: 0:03:31  lr: 0.000436  min_lr: 0.000010  loss: 0.8891 (0.8394)  loss_scale: 65536.0000 (123180.7342)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [15]  [ 690/1349]  eta: 0:03:27  lr: 0.000436  min_lr: 0.000010  loss: 0.8936 (0.8392)  loss_scale: 65536.0000 (122346.5123)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
Epoch: [15]  [ 700/1349]  eta: 0:03:24  lr: 0.000436  min_lr: 0.000010  loss: 0.8558 (0.8388)  loss_scale: 65536.0000 (121536.0913)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
Epoch: [15]  [ 710/1349]  eta: 0:03:21  lr: 0.000436  min_lr: 0.000010  loss: 0.8320 (0.8381)  loss_scale: 65536.0000 (120748.4669)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [15]  [ 720/1349]  eta: 0:03:18  lr: 0.000436  min_lr: 0.000010  loss: 0.7794 (0.8363)  loss_scale: 65536.0000 (119982.6907)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
Epoch: [15]  [ 730/1349]  eta: 0:03:14  lr: 0.000435  min_lr: 0.000010  loss: 0.7793 (0.8357)  loss_scale: 65536.0000 (119237.8659)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [15]  [ 740/1349]  eta: 0:03:11  lr: 0.000435  min_lr: 0.000010  loss: 0.8045 (0.8357)  loss_scale: 65536.0000 (118513.1444)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [15]  [ 750/1349]  eta: 0:03:08  lr: 0.000435  min_lr: 0.000010  loss: 0.8307 (0.8359)  loss_scale: 65536.0000 (117807.7230)  weight_decay: 0.0500 (0.0500)  time: 0.3062  data: 0.0001  max mem: 41808
[2025-05-23 18:11:08,115] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 18:11:08,116] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 18:11:08,116] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-05-23 18:11:08,116] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [15]  [ 760/1349]  eta: 0:03:05  lr: 0.000435  min_lr: 0.000010  loss: 0.8473 (0.8363)  loss_scale: 65536.0000 (117293.0775)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
[2025-05-23 18:11:09,652] [INFO] [logging.py:96:log_dist] [Rank 0] step=21000, skipped=127, lr=[1.0337721137663205e-05, 1.0337721137663205e-05, 1.378362818355094e-05, 1.378362818355094e-05, 1.8378170911401252e-05, 1.8378170911401252e-05, 2.4504227881868335e-05, 2.4504227881868335e-05, 3.267230384249111e-05, 3.267230384249111e-05, 4.3563071789988155e-05, 4.3563071789988155e-05, 5.808409571998421e-05, 5.808409571998421e-05, 7.744546095997894e-05, 7.744546095997894e-05, 0.00010326061461330525, 0.00010326061461330525, 0.000137680819484407, 0.000137680819484407, 0.00018357442597920933, 0.00018357442597920933, 0.0002447659013056124, 0.0002447659013056124, 0.0003263545350741499, 0.0003263545350741499, 0.0004351393800988666, 0.0004351393800988666], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-05-23 18:11:09,652] [INFO] [timer.py:260:stop] epoch=0/micro_step=21000/global_step=21000, RunningAvgSamplesPerSec=206.6261460054133, CurrSamplesPerSec=213.80250252881254, MemAllocated=1.37GB, MaxMemAllocated=40.83GB
Epoch: [15]  [ 770/1349]  eta: 0:03:02  lr: 0.000435  min_lr: 0.000010  loss: 0.8740 (0.8358)  loss_scale: 131072.0000 (117471.7925)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
Epoch: [15]  [ 780/1349]  eta: 0:02:58  lr: 0.000435  min_lr: 0.000010  loss: 0.8935 (0.8363)  loss_scale: 131072.0000 (117645.9309)  weight_decay: 0.0500 (0.0500)  time: 0.3061  data: 0.0001  max mem: 41808
Epoch: [15]  [ 790/1349]  eta: 0:02:55  lr: 0.000435  min_lr: 0.000010  loss: 0.8226 (0.8346)  loss_scale: 131072.0000 (117815.6662)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0001  max mem: 41808
Epoch: [15]  [ 800/1349]  eta: 0:02:52  lr: 0.000435  min_lr: 0.000010  loss: 0.7537 (0.8341)  loss_scale: 131072.0000 (117981.1635)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [15]  [ 810/1349]  eta: 0:02:49  lr: 0.000435  min_lr: 0.000010  loss: 0.7537 (0.8344)  loss_scale: 131072.0000 (118142.5795)  weight_decay: 0.0500 (0.0500)  time: 0.3093  data: 0.0001  max mem: 41808
Epoch: [15]  [ 820/1349]  eta: 0:02:46  lr: 0.000435  min_lr: 0.000010  loss: 0.8628 (0.8350)  loss_scale: 131072.0000 (118300.0633)  weight_decay: 0.0500 (0.0500)  time: 0.3090  data: 0.0001  max mem: 41808
Epoch: [15]  [ 830/1349]  eta: 0:02:42  lr: 0.000435  min_lr: 0.000010  loss: 0.8765 (0.8350)  loss_scale: 131072.0000 (118453.7569)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [15]  [ 840/1349]  eta: 0:02:39  lr: 0.000434  min_lr: 0.000010  loss: 0.8783 (0.8357)  loss_scale: 131072.0000 (118603.7955)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
Epoch: [15]  [ 850/1349]  eta: 0:02:36  lr: 0.000434  min_lr: 0.000010  loss: 0.8863 (0.8362)  loss_scale: 131072.0000 (118750.3079)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [15]  [ 860/1349]  eta: 0:02:33  lr: 0.000434  min_lr: 0.000010  loss: 0.8534 (0.8361)  loss_scale: 131072.0000 (118893.4170)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0001  max mem: 41808
Epoch: [15]  [ 870/1349]  eta: 0:02:30  lr: 0.000434  min_lr: 0.000010  loss: 0.8432 (0.8361)  loss_scale: 131072.0000 (119033.2400)  weight_decay: 0.0500 (0.0500)  time: 0.3079  data: 0.0001  max mem: 41808
[2025-05-23 18:11:44,713] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 21113
[2025-05-23 18:11:44,713] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 21113
[2025-05-23 18:11:44,713] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 18:11:44,713] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 18:11:44,713] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [15]  [ 880/1349]  eta: 0:02:27  lr: 0.000434  min_lr: 0.000010  loss: 0.8468 (0.8360)  loss_scale: 131072.0000 (118946.7242)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0001  max mem: 41808
Epoch: [15]  [ 890/1349]  eta: 0:02:23  lr: 0.000434  min_lr: 0.000010  loss: 0.8183 (0.8356)  loss_scale: 65536.0000 (118347.2772)  weight_decay: 0.0500 (0.0500)  time: 0.3064  data: 0.0001  max mem: 41808
Epoch: [15]  [ 900/1349]  eta: 0:02:20  lr: 0.000434  min_lr: 0.000010  loss: 0.8176 (0.8352)  loss_scale: 65536.0000 (117761.1365)  weight_decay: 0.0500 (0.0500)  time: 0.3066  data: 0.0001  max mem: 41808
Epoch: [15]  [ 910/1349]  eta: 0:02:17  lr: 0.000434  min_lr: 0.000010  loss: 0.9088 (0.8361)  loss_scale: 65536.0000 (117187.8639)  weight_decay: 0.0500 (0.0500)  time: 0.3066  data: 0.0001  max mem: 41808
Epoch: [15]  [ 920/1349]  eta: 0:02:14  lr: 0.000434  min_lr: 0.000010  loss: 0.9089 (0.8368)  loss_scale: 65536.0000 (116627.0402)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
Epoch: [15]  [ 930/1349]  eta: 0:02:11  lr: 0.000434  min_lr: 0.000010  loss: 0.8905 (0.8373)  loss_scale: 65536.0000 (116078.2642)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
Epoch: [15]  [ 940/1349]  eta: 0:02:08  lr: 0.000434  min_lr: 0.000010  loss: 0.8712 (0.8379)  loss_scale: 65536.0000 (115541.1520)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
Epoch: [15]  [ 950/1349]  eta: 0:02:04  lr: 0.000434  min_lr: 0.000010  loss: 0.8761 (0.8385)  loss_scale: 65536.0000 (115015.3354)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0002  max mem: 41808
Epoch: [15]  [ 960/1349]  eta: 0:02:01  lr: 0.000433  min_lr: 0.000010  loss: 0.8674 (0.8383)  loss_scale: 65536.0000 (114500.4620)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
Epoch: [15]  [ 970/1349]  eta: 0:01:58  lr: 0.000433  min_lr: 0.000010  loss: 0.8615 (0.8382)  loss_scale: 65536.0000 (113996.1936)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
Epoch: [15]  [ 980/1349]  eta: 0:01:55  lr: 0.000433  min_lr: 0.000010  loss: 0.8259 (0.8383)  loss_scale: 65536.0000 (113502.2059)  weight_decay: 0.0500 (0.0500)  time: 0.3062  data: 0.0001  max mem: 41808
Epoch: [15]  [ 990/1349]  eta: 0:01:52  lr: 0.000433  min_lr: 0.000010  loss: 0.8001 (0.8383)  loss_scale: 65536.0000 (113018.1877)  weight_decay: 0.0500 (0.0500)  time: 0.3059  data: 0.0001  max mem: 41808
Epoch: [15]  [1000/1349]  eta: 0:01:49  lr: 0.000433  min_lr: 0.000010  loss: 0.8396 (0.8378)  loss_scale: 65536.0000 (112543.8402)  weight_decay: 0.0500 (0.0500)  time: 0.3060  data: 0.0001  max mem: 41808
[2025-05-23 18:12:24,271] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 18:12:24,271] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-05-23 18:12:24,271] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 18:12:24,271] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [15]  [1010/1349]  eta: 0:01:46  lr: 0.000433  min_lr: 0.000010  loss: 0.8556 (0.8371)  loss_scale: 65536.0000 (112338.1682)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [15]  [1020/1349]  eta: 0:01:42  lr: 0.000433  min_lr: 0.000010  loss: 0.8556 (0.8369)  loss_scale: 131072.0000 (112521.6533)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [15]  [1030/1349]  eta: 0:01:39  lr: 0.000433  min_lr: 0.000010  loss: 0.8243 (0.8364)  loss_scale: 131072.0000 (112701.5790)  weight_decay: 0.0500 (0.0500)  time: 0.3063  data: 0.0001  max mem: 41808
Epoch: [15]  [1040/1349]  eta: 0:01:36  lr: 0.000433  min_lr: 0.000010  loss: 0.8243 (0.8365)  loss_scale: 131072.0000 (112878.0480)  weight_decay: 0.0500 (0.0500)  time: 0.3059  data: 0.0001  max mem: 41808
Epoch: [15]  [1050/1349]  eta: 0:01:33  lr: 0.000433  min_lr: 0.000010  loss: 0.8604 (0.8366)  loss_scale: 131072.0000 (113051.1589)  weight_decay: 0.0500 (0.0500)  time: 0.3057  data: 0.0001  max mem: 41808
Epoch: [15]  [1060/1349]  eta: 0:01:30  lr: 0.000433  min_lr: 0.000010  loss: 0.8812 (0.8369)  loss_scale: 131072.0000 (113221.0066)  weight_decay: 0.0500 (0.0500)  time: 0.3061  data: 0.0001  max mem: 41808
Epoch: [15]  [1070/1349]  eta: 0:01:27  lr: 0.000432  min_lr: 0.000010  loss: 0.8812 (0.8368)  loss_scale: 131072.0000 (113387.6825)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0001  max mem: 41808
Epoch: [15]  [1080/1349]  eta: 0:01:24  lr: 0.000432  min_lr: 0.000010  loss: 0.7804 (0.8368)  loss_scale: 131072.0000 (113551.2747)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
[2025-05-23 18:12:49,417] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 21324
[2025-05-23 18:12:49,417] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 18:12:49,417] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 21324
[2025-05-23 18:12:49,417] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 18:12:49,417] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [15]  [1090/1349]  eta: 0:01:20  lr: 0.000432  min_lr: 0.000010  loss: 0.7558 (0.8362)  loss_scale: 131072.0000 (113591.7287)  weight_decay: 0.0500 (0.0500)  time: 0.3081  data: 0.0001  max mem: 41808
Epoch: [15]  [1100/1349]  eta: 0:01:17  lr: 0.000432  min_lr: 0.000010  loss: 0.8890 (0.8372)  loss_scale: 65536.0000 (113155.2552)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [15]  [1110/1349]  eta: 0:01:14  lr: 0.000432  min_lr: 0.000010  loss: 0.9008 (0.8374)  loss_scale: 65536.0000 (112726.6391)  weight_decay: 0.0500 (0.0500)  time: 0.3060  data: 0.0001  max mem: 41808
Epoch: [15]  [1120/1349]  eta: 0:01:11  lr: 0.000432  min_lr: 0.000010  loss: 0.8633 (0.8380)  loss_scale: 65536.0000 (112305.6699)  weight_decay: 0.0500 (0.0500)  time: 0.3061  data: 0.0001  max mem: 41808
Epoch: [15]  [1130/1349]  eta: 0:01:08  lr: 0.000432  min_lr: 0.000010  loss: 0.8895 (0.8384)  loss_scale: 65536.0000 (111892.1450)  weight_decay: 0.0500 (0.0500)  time: 0.3066  data: 0.0001  max mem: 41808
Epoch: [15]  [1140/1349]  eta: 0:01:05  lr: 0.000432  min_lr: 0.000010  loss: 0.8933 (0.8384)  loss_scale: 65536.0000 (111485.8685)  weight_decay: 0.0500 (0.0500)  time: 0.3066  data: 0.0001  max mem: 41808
Epoch: [15]  [1150/1349]  eta: 0:01:02  lr: 0.000432  min_lr: 0.000010  loss: 0.8774 (0.8383)  loss_scale: 65536.0000 (111086.6516)  weight_decay: 0.0500 (0.0500)  time: 0.3062  data: 0.0001  max mem: 41808
Epoch: [15]  [1160/1349]  eta: 0:00:58  lr: 0.000432  min_lr: 0.000010  loss: 0.8105 (0.8382)  loss_scale: 65536.0000 (110694.3118)  weight_decay: 0.0500 (0.0500)  time: 0.3061  data: 0.0001  max mem: 41808
Epoch: [15]  [1170/1349]  eta: 0:00:55  lr: 0.000432  min_lr: 0.000010  loss: 0.8536 (0.8389)  loss_scale: 65536.0000 (110308.6729)  weight_decay: 0.0500 (0.0500)  time: 0.3058  data: 0.0001  max mem: 41808
Epoch: [15]  [1180/1349]  eta: 0:00:52  lr: 0.000431  min_lr: 0.000010  loss: 0.8536 (0.8390)  loss_scale: 65536.0000 (109929.5648)  weight_decay: 0.0500 (0.0500)  time: 0.3061  data: 0.0001  max mem: 41808
Epoch: [15]  [1190/1349]  eta: 0:00:49  lr: 0.000431  min_lr: 0.000010  loss: 0.8483 (0.8392)  loss_scale: 65536.0000 (109556.8228)  weight_decay: 0.0500 (0.0500)  time: 0.3062  data: 0.0001  max mem: 41808
Epoch: [15]  [1200/1349]  eta: 0:00:46  lr: 0.000431  min_lr: 0.000010  loss: 0.8483 (0.8389)  loss_scale: 65536.0000 (109190.2881)  weight_decay: 0.0500 (0.0500)  time: 0.3058  data: 0.0001  max mem: 41808
Epoch: [15]  [1210/1349]  eta: 0:00:43  lr: 0.000431  min_lr: 0.000010  loss: 0.7811 (0.8384)  loss_scale: 65536.0000 (108829.8068)  weight_decay: 0.0500 (0.0500)  time: 0.3056  data: 0.0001  max mem: 41808
[2025-05-23 18:13:28,931] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 18:13:28,931] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 18:13:28,931] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-05-23 18:13:28,931] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [15]  [1220/1349]  eta: 0:00:40  lr: 0.000431  min_lr: 0.000010  loss: 0.8180 (0.8385)  loss_scale: 65536.0000 (108636.2523)  weight_decay: 0.0500 (0.0500)  time: 0.3053  data: 0.0001  max mem: 41808
Epoch: [15]  [1230/1349]  eta: 0:00:37  lr: 0.000431  min_lr: 0.000010  loss: 0.8289 (0.8384)  loss_scale: 131072.0000 (108818.5085)  weight_decay: 0.0500 (0.0500)  time: 0.3053  data: 0.0001  max mem: 41808
Epoch: [15]  [1240/1349]  eta: 0:00:33  lr: 0.000431  min_lr: 0.000010  loss: 0.8645 (0.8389)  loss_scale: 131072.0000 (108997.8276)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0001  max mem: 41808
Epoch: [15]  [1250/1349]  eta: 0:00:30  lr: 0.000431  min_lr: 0.000010  loss: 0.8485 (0.8384)  loss_scale: 131072.0000 (109174.2798)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0001  max mem: 41808
Epoch: [15]  [1260/1349]  eta: 0:00:27  lr: 0.000431  min_lr: 0.000010  loss: 0.7079 (0.8375)  loss_scale: 131072.0000 (109347.9334)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [15]  [1270/1349]  eta: 0:00:24  lr: 0.000431  min_lr: 0.000010  loss: 0.7455 (0.8374)  loss_scale: 131072.0000 (109518.8544)  weight_decay: 0.0500 (0.0500)  time: 0.3059  data: 0.0001  max mem: 41808
Epoch: [15]  [1280/1349]  eta: 0:00:21  lr: 0.000431  min_lr: 0.000010  loss: 0.8761 (0.8372)  loss_scale: 131072.0000 (109687.1069)  weight_decay: 0.0500 (0.0500)  time: 0.3060  data: 0.0001  max mem: 41808
Epoch: [15]  [1290/1349]  eta: 0:00:18  lr: 0.000431  min_lr: 0.000010  loss: 0.9030 (0.8372)  loss_scale: 131072.0000 (109852.7529)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0001  max mem: 41808
Epoch: [15]  [1300/1349]  eta: 0:00:15  lr: 0.000430  min_lr: 0.000010  loss: 0.9030 (0.8372)  loss_scale: 131072.0000 (110015.8524)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [15]  [1310/1349]  eta: 0:00:12  lr: 0.000430  min_lr: 0.000010  loss: 0.7442 (0.8366)  loss_scale: 131072.0000 (110176.4638)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
Epoch: [15]  [1320/1349]  eta: 0:00:09  lr: 0.000430  min_lr: 0.000010  loss: 0.8375 (0.8369)  loss_scale: 131072.0000 (110334.6435)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0001  max mem: 41808
Epoch: [15]  [1330/1349]  eta: 0:00:05  lr: 0.000430  min_lr: 0.000010  loss: 0.8375 (0.8367)  loss_scale: 131072.0000 (110490.4463)  weight_decay: 0.0500 (0.0500)  time: 0.3045  data: 0.0001  max mem: 41808
Epoch: [15]  [1340/1349]  eta: 0:00:02  lr: 0.000430  min_lr: 0.000010  loss: 0.8925 (0.8377)  loss_scale: 131072.0000 (110643.9254)  weight_decay: 0.0500 (0.0500)  time: 0.3023  data: 0.0001  max mem: 41808
[2025-05-23 18:14:08,083] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 18:14:08,083] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 18:14:08,083] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 18:14:08,083] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [15]  [1348/1349]  eta: 0:00:00  lr: 0.000430  min_lr: 0.000010  loss: 0.9423 (0.8379)  loss_scale: 131072.0000 (111056.5574)  weight_decay: 0.0500 (0.0500)  time: 0.3018  data: 0.0001  max mem: 41808
Epoch: [15] Total time: 0:07:00 (0.3114 s / it)
Averaged stats: lr: 0.000430  min_lr: 0.000010  loss: 0.9423 (0.8364)  loss_scale: 131072.0000 (111056.5574)  weight_decay: 0.0500 (0.0500)  total_time: 420.0612 (420.0597)
Val:  [  0/346]  eta: 0:58:49  loss: 2.9214 (2.9214)  acc1: 1.5625 (1.5625)  acc5: 77.3438 (77.3438)  time: 10.2012  data: 9.2955  max mem: 41808
Val:  [ 10/346]  eta: 0:11:03  loss: 0.1719 (0.5364)  acc1: 99.2188 (85.0852)  acc5: 100.0000 (97.8693)  time: 1.9738  data: 1.0842  max mem: 41808
Val:  [ 20/346]  eta: 0:08:00  loss: 0.1448 (0.4201)  acc1: 100.0000 (88.8021)  acc5: 100.0000 (98.5863)  time: 1.0387  data: 0.1722  max mem: 41808
Val:  [ 30/346]  eta: 0:06:42  loss: 0.1123 (0.3515)  acc1: 100.0000 (91.3810)  acc5: 100.0000 (98.9919)  time: 0.8872  data: 0.0408  max mem: 41808
Val:  [ 40/346]  eta: 0:05:57  loss: 0.1403 (0.4116)  acc1: 99.2188 (89.6151)  acc5: 100.0000 (98.8186)  time: 0.8456  data: 0.0003  max mem: 41808
Val:  [ 50/346]  eta: 0:05:26  loss: 0.1403 (0.3616)  acc1: 99.2188 (91.3297)  acc5: 100.0000 (99.0196)  time: 0.8427  data: 0.0003  max mem: 41808
Val:  [ 60/346]  eta: 0:05:04  loss: 0.1384 (0.3427)  acc1: 97.6562 (91.9442)  acc5: 100.0000 (99.1803)  time: 0.8493  data: 0.0003  max mem: 41808
Val:  [ 70/346]  eta: 0:04:45  loss: 0.2111 (0.3714)  acc1: 96.0938 (90.9331)  acc5: 100.0000 (99.2958)  time: 0.8515  data: 0.0003  max mem: 41808
Val:  [ 80/346]  eta: 0:04:28  loss: 0.1637 (0.3626)  acc1: 98.4375 (91.3966)  acc5: 100.0000 (99.3248)  time: 0.8510  data: 0.0003  max mem: 41808
Val:  [ 90/346]  eta: 0:04:14  loss: 0.2049 (0.3622)  acc1: 96.0938 (91.2517)  acc5: 100.0000 (99.3819)  time: 0.8592  data: 0.0003  max mem: 41808
Val:  [100/346]  eta: 0:04:01  loss: 0.2049 (0.3454)  acc1: 96.0938 (91.8626)  acc5: 100.0000 (99.4431)  time: 0.8554  data: 0.0003  max mem: 41808
Val:  [110/346]  eta: 0:03:48  loss: 0.1666 (0.3616)  acc1: 98.4375 (91.2725)  acc5: 100.0000 (99.3595)  time: 0.8481  data: 0.0003  max mem: 41808
Val:  [120/346]  eta: 0:03:36  loss: 0.1865 (0.3576)  acc1: 95.3125 (91.4127)  acc5: 100.0000 (99.3866)  time: 0.8568  data: 0.0003  max mem: 41808
Val:  [130/346]  eta: 0:03:25  loss: 0.1200 (0.3596)  acc1: 100.0000 (91.3168)  acc5: 100.0000 (99.3917)  time: 0.8567  data: 0.0003  max mem: 41808
Val:  [140/346]  eta: 0:03:14  loss: 0.2705 (0.3652)  acc1: 94.5312 (91.0960)  acc5: 100.0000 (99.4238)  time: 0.8553  data: 0.0003  max mem: 41808
Val:  [150/346]  eta: 0:03:04  loss: 0.2750 (0.3680)  acc1: 92.1875 (91.0493)  acc5: 100.0000 (99.4257)  time: 0.8585  data: 0.0003  max mem: 41808
Val:  [160/346]  eta: 0:02:53  loss: 0.2655 (0.3660)  acc1: 92.1875 (91.0763)  acc5: 100.0000 (99.4371)  time: 0.8598  data: 0.0003  max mem: 41808
Val:  [170/346]  eta: 0:02:43  loss: 0.2744 (0.3684)  acc1: 87.5000 (90.8626)  acc5: 100.0000 (99.4700)  time: 0.8586  data: 0.0003  max mem: 41808
Val:  [180/346]  eta: 0:02:33  loss: 0.2744 (0.3887)  acc1: 87.5000 (89.9344)  acc5: 100.0000 (99.4950)  time: 0.8615  data: 0.0003  max mem: 41808
Val:  [190/346]  eta: 0:02:23  loss: 0.2444 (0.3952)  acc1: 90.6250 (89.6801)  acc5: 100.0000 (99.4887)  time: 0.8649  data: 0.0003  max mem: 41808
Val:  [200/346]  eta: 0:02:14  loss: 0.4036 (0.4052)  acc1: 89.0625 (89.2840)  acc5: 100.0000 (99.4753)  time: 0.8697  data: 0.0003  max mem: 41808
Val:  [210/346]  eta: 0:02:04  loss: 0.2100 (0.3983)  acc1: 94.5312 (89.5624)  acc5: 100.0000 (99.5001)  time: 0.8675  data: 0.0003  max mem: 41808
Val:  [220/346]  eta: 0:01:55  loss: 0.1742 (0.3983)  acc1: 97.6562 (89.5963)  acc5: 100.0000 (99.4945)  time: 0.8607  data: 0.0003  max mem: 41808
Val:  [230/346]  eta: 0:01:45  loss: 0.1656 (0.3888)  acc1: 97.6562 (89.9554)  acc5: 100.0000 (99.5164)  time: 0.8691  data: 0.0003  max mem: 41808
Val:  [240/346]  eta: 0:01:36  loss: 0.1751 (0.3961)  acc1: 97.6562 (89.7497)  acc5: 100.0000 (99.5267)  time: 0.8599  data: 0.0003  max mem: 41808
Val:  [250/346]  eta: 0:01:27  loss: 0.2122 (0.3896)  acc1: 96.0938 (89.9932)  acc5: 100.0000 (99.5425)  time: 0.8583  data: 0.0003  max mem: 41808
Val:  [260/346]  eta: 0:01:18  loss: 0.1717 (0.3880)  acc1: 96.8750 (90.0383)  acc5: 100.0000 (99.5510)  time: 0.8659  data: 0.0003  max mem: 41808
Val:  [270/346]  eta: 0:01:08  loss: 0.1443 (0.3852)  acc1: 96.8750 (90.1609)  acc5: 100.0000 (99.5647)  time: 0.8599  data: 0.0003  max mem: 41808
Val:  [280/346]  eta: 0:00:59  loss: 0.1183 (0.3812)  acc1: 96.8750 (90.2830)  acc5: 100.0000 (99.5746)  time: 0.8645  data: 0.0003  max mem: 41808
Val:  [290/346]  eta: 0:00:50  loss: 0.1119 (0.3733)  acc1: 100.0000 (90.5445)  acc5: 100.0000 (99.5892)  time: 0.8651  data: 0.0002  max mem: 41808
Val:  [300/346]  eta: 0:00:41  loss: 0.1175 (0.3748)  acc1: 100.0000 (90.5523)  acc5: 100.0000 (99.5120)  time: 0.8667  data: 0.0003  max mem: 41808
Val:  [310/346]  eta: 0:00:32  loss: 0.1384 (0.3766)  acc1: 98.4375 (90.4994)  acc5: 100.0000 (99.5177)  time: 0.8599  data: 0.0003  max mem: 41808
Val:  [320/346]  eta: 0:00:23  loss: 0.1318 (0.3785)  acc1: 98.4375 (90.4230)  acc5: 100.0000 (99.5327)  time: 0.8425  data: 0.0003  max mem: 41808
Val:  [330/346]  eta: 0:00:14  loss: 0.4802 (0.3911)  acc1: 86.7188 (90.0042)  acc5: 100.0000 (99.4501)  time: 0.8485  data: 0.0002  max mem: 41808
Val:  [340/346]  eta: 0:00:05  loss: 0.4802 (0.3976)  acc1: 87.5000 (89.8346)  acc5: 100.0000 (99.4570)  time: 0.8343  data: 0.0002  max mem: 41808
Val:  [345/346]  eta: 0:00:00  loss: 0.1688 (0.3943)  acc1: 95.3125 (89.9519)  acc5: 100.0000 (99.4644)  time: 0.7941  data: 0.0001  max mem: 41808
Val: Total time: 0:05:08 (0.8928 s / it)
* Acc@1 89.965 Acc@5 99.487 loss 0.392
Accuracy of the network on the 88494 val videos: 90.0%
Max accuracy: 91.25%   Max Epoch: 3
Epoch: [16]  [   0/1349]  eta: 2:01:10  lr: 0.000430  min_lr: 0.000010  loss: 0.8621 (0.8621)  loss_scale: 262144.0000 (262144.0000)  weight_decay: 0.0500 (0.0500)  time: 5.3897  data: 4.4751  max mem: 41808
Epoch: [16]  [  10/1349]  eta: 0:17:14  lr: 0.000430  min_lr: 0.000010  loss: 0.7968 (0.7891)  loss_scale: 262144.0000 (262144.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7727  data: 0.4070  max mem: 41808
[2025-05-23 18:19:27,658] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 21598
[2025-05-23 18:19:27,658] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 21598
[2025-05-23 18:19:27,658] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 18:19:27,658] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 18:19:27,658] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [16]  [  20/1349]  eta: 0:12:13  lr: 0.000430  min_lr: 0.000010  loss: 0.7968 (0.8153)  loss_scale: 262144.0000 (218453.3333)  weight_decay: 0.0500 (0.0500)  time: 0.3099  data: 0.0001  max mem: 41808
Epoch: [16]  [  30/1349]  eta: 0:10:24  lr: 0.000430  min_lr: 0.000010  loss: 0.9025 (0.8441)  loss_scale: 131072.0000 (190265.8065)  weight_decay: 0.0500 (0.0500)  time: 0.3088  data: 0.0001  max mem: 41808
Epoch: [16]  [  40/1349]  eta: 0:09:26  lr: 0.000430  min_lr: 0.000010  loss: 0.9036 (0.8632)  loss_scale: 131072.0000 (175828.2927)  weight_decay: 0.0500 (0.0500)  time: 0.3084  data: 0.0001  max mem: 41808
Epoch: [16]  [  50/1349]  eta: 0:08:50  lr: 0.000430  min_lr: 0.000010  loss: 0.8817 (0.8557)  loss_scale: 131072.0000 (167052.5490)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [16]  [  60/1349]  eta: 0:08:24  lr: 0.000429  min_lr: 0.000010  loss: 0.7653 (0.8382)  loss_scale: 131072.0000 (161154.0984)  weight_decay: 0.0500 (0.0500)  time: 0.3063  data: 0.0001  max mem: 41808
Epoch: [16]  [  70/1349]  eta: 0:08:05  lr: 0.000429  min_lr: 0.000010  loss: 0.8185 (0.8329)  loss_scale: 131072.0000 (156917.1831)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0001  max mem: 41808
Epoch: [16]  [  80/1349]  eta: 0:07:50  lr: 0.000429  min_lr: 0.000010  loss: 0.8180 (0.8228)  loss_scale: 131072.0000 (153726.4198)  weight_decay: 0.0500 (0.0500)  time: 0.3084  data: 0.0002  max mem: 41808
Epoch: [16]  [  90/1349]  eta: 0:07:38  lr: 0.000429  min_lr: 0.000010  loss: 0.7841 (0.8266)  loss_scale: 131072.0000 (151236.9231)  weight_decay: 0.0500 (0.0500)  time: 0.3081  data: 0.0002  max mem: 41808
Epoch: [16]  [ 100/1349]  eta: 0:07:27  lr: 0.000429  min_lr: 0.000010  loss: 0.8356 (0.8298)  loss_scale: 131072.0000 (149240.3960)  weight_decay: 0.0500 (0.0500)  time: 0.3062  data: 0.0001  max mem: 41808
Epoch: [16]  [ 110/1349]  eta: 0:07:17  lr: 0.000429  min_lr: 0.000010  loss: 0.8789 (0.8290)  loss_scale: 131072.0000 (147603.6036)  weight_decay: 0.0500 (0.0500)  time: 0.3064  data: 0.0001  max mem: 41808
Epoch: [16]  [ 120/1349]  eta: 0:07:09  lr: 0.000429  min_lr: 0.000010  loss: 0.8789 (0.8298)  loss_scale: 131072.0000 (146237.3554)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0002  max mem: 41808
Epoch: [16]  [ 130/1349]  eta: 0:07:02  lr: 0.000429  min_lr: 0.000010  loss: 0.7641 (0.8197)  loss_scale: 131072.0000 (145079.6947)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0002  max mem: 41808
Epoch: [16]  [ 140/1349]  eta: 0:06:55  lr: 0.000429  min_lr: 0.000010  loss: 0.7316 (0.8161)  loss_scale: 131072.0000 (144086.2411)  weight_decay: 0.0500 (0.0500)  time: 0.3081  data: 0.0002  max mem: 41808
[2025-05-23 18:20:07,334] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 18:20:07,334] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 18:20:07,334] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 18:20:07,334] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 18:20:07,636] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 21728
[2025-05-23 18:20:07,636] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 21728
[2025-05-23 18:20:07,636] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 18:20:07,636] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 18:20:07,636] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [16]  [ 150/1349]  eta: 0:06:49  lr: 0.000429  min_lr: 0.000010  loss: 0.8405 (0.8190)  loss_scale: 131072.0000 (144092.3974)  weight_decay: 0.0500 (0.0500)  time: 0.3083  data: 0.0002  max mem: 41808
Epoch: [16]  [ 160/1349]  eta: 0:06:43  lr: 0.000429  min_lr: 0.000010  loss: 0.8889 (0.8202)  loss_scale: 131072.0000 (143283.6770)  weight_decay: 0.0500 (0.0500)  time: 0.3087  data: 0.0002  max mem: 41808
Epoch: [16]  [ 170/1349]  eta: 0:06:37  lr: 0.000428  min_lr: 0.000010  loss: 0.8645 (0.8218)  loss_scale: 131072.0000 (142569.5439)  weight_decay: 0.0500 (0.0500)  time: 0.3092  data: 0.0001  max mem: 41808
Epoch: [16]  [ 180/1349]  eta: 0:06:32  lr: 0.000428  min_lr: 0.000010  loss: 0.9020 (0.8254)  loss_scale: 131072.0000 (141934.3204)  weight_decay: 0.0500 (0.0500)  time: 0.3078  data: 0.0001  max mem: 41808
Epoch: [16]  [ 190/1349]  eta: 0:06:27  lr: 0.000428  min_lr: 0.000010  loss: 0.7933 (0.8182)  loss_scale: 131072.0000 (141365.6126)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [16]  [ 200/1349]  eta: 0:06:22  lr: 0.000428  min_lr: 0.000010  loss: 0.7743 (0.8181)  loss_scale: 131072.0000 (140853.4925)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
Epoch: [16]  [ 210/1349]  eta: 0:06:17  lr: 0.000428  min_lr: 0.000010  loss: 0.7859 (0.8192)  loss_scale: 131072.0000 (140389.9147)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
Epoch: [16]  [ 220/1349]  eta: 0:06:13  lr: 0.000428  min_lr: 0.000010  loss: 0.7294 (0.8134)  loss_scale: 131072.0000 (139968.2896)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
Epoch: [16]  [ 230/1349]  eta: 0:06:08  lr: 0.000428  min_lr: 0.000010  loss: 0.7525 (0.8162)  loss_scale: 131072.0000 (139583.1688)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
Epoch: [16]  [ 240/1349]  eta: 0:06:04  lr: 0.000428  min_lr: 0.000010  loss: 0.8607 (0.8188)  loss_scale: 131072.0000 (139230.0083)  weight_decay: 0.0500 (0.0500)  time: 0.3063  data: 0.0001  max mem: 41808
Epoch: [16]  [ 250/1349]  eta: 0:06:00  lr: 0.000428  min_lr: 0.000010  loss: 0.8671 (0.8189)  loss_scale: 131072.0000 (138904.9880)  weight_decay: 0.0500 (0.0500)  time: 0.3061  data: 0.0001  max mem: 41808
Epoch: [16]  [ 260/1349]  eta: 0:05:56  lr: 0.000428  min_lr: 0.000010  loss: 0.8845 (0.8222)  loss_scale: 131072.0000 (138604.8736)  weight_decay: 0.0500 (0.0500)  time: 0.3061  data: 0.0001  max mem: 41808
Epoch: [16]  [ 270/1349]  eta: 0:05:51  lr: 0.000428  min_lr: 0.000010  loss: 0.9102 (0.8248)  loss_scale: 131072.0000 (138326.9077)  weight_decay: 0.0500 (0.0500)  time: 0.3059  data: 0.0001  max mem: 41808
[2025-05-23 18:20:47,252] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 18:20:47,252] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 18:20:47,252] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 18:20:47,252] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 18:20:49,088] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 21863
[2025-05-23 18:20:49,088] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 21863
[2025-05-23 18:20:49,088] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 18:20:49,088] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 18:20:49,088] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [16]  [ 280/1349]  eta: 0:05:47  lr: 0.000427  min_lr: 0.000010  loss: 0.8916 (0.8268)  loss_scale: 131072.0000 (140867.4164)  weight_decay: 0.0500 (0.0500)  time: 0.3055  data: 0.0001  max mem: 41808
Epoch: [16]  [ 290/1349]  eta: 0:05:43  lr: 0.000427  min_lr: 0.000010  loss: 0.8265 (0.8269)  loss_scale: 131072.0000 (140530.8041)  weight_decay: 0.0500 (0.0500)  time: 0.3061  data: 0.0001  max mem: 41808
Epoch: [16]  [ 300/1349]  eta: 0:05:40  lr: 0.000427  min_lr: 0.000010  loss: 0.8494 (0.8278)  loss_scale: 131072.0000 (140216.5581)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [16]  [ 310/1349]  eta: 0:05:36  lr: 0.000427  min_lr: 0.000010  loss: 0.8829 (0.8285)  loss_scale: 131072.0000 (139922.5209)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [16]  [ 320/1349]  eta: 0:05:32  lr: 0.000427  min_lr: 0.000010  loss: 0.8101 (0.8277)  loss_scale: 131072.0000 (139646.8037)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0001  max mem: 41808
Epoch: [16]  [ 330/1349]  eta: 0:05:28  lr: 0.000427  min_lr: 0.000010  loss: 0.8095 (0.8261)  loss_scale: 131072.0000 (139387.7462)  weight_decay: 0.0500 (0.0500)  time: 0.3058  data: 0.0001  max mem: 41808
Epoch: [16]  [ 340/1349]  eta: 0:05:24  lr: 0.000427  min_lr: 0.000010  loss: 0.8311 (0.8267)  loss_scale: 131072.0000 (139143.8827)  weight_decay: 0.0500 (0.0500)  time: 0.3060  data: 0.0001  max mem: 41808
Epoch: [16]  [ 350/1349]  eta: 0:05:21  lr: 0.000427  min_lr: 0.000010  loss: 0.8343 (0.8254)  loss_scale: 131072.0000 (138913.9145)  weight_decay: 0.0500 (0.0500)  time: 0.3066  data: 0.0002  max mem: 41808
Epoch: [16]  [ 360/1349]  eta: 0:05:17  lr: 0.000427  min_lr: 0.000010  loss: 0.8127 (0.8229)  loss_scale: 131072.0000 (138696.6870)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
Epoch: [16]  [ 370/1349]  eta: 0:05:14  lr: 0.000427  min_lr: 0.000010  loss: 0.8361 (0.8234)  loss_scale: 131072.0000 (138491.1698)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
Epoch: [16]  [ 380/1349]  eta: 0:05:10  lr: 0.000427  min_lr: 0.000010  loss: 0.8992 (0.8255)  loss_scale: 131072.0000 (138296.4409)  weight_decay: 0.0500 (0.0500)  time: 0.3079  data: 0.0001  max mem: 41808
Epoch: [16]  [ 390/1349]  eta: 0:05:07  lr: 0.000426  min_lr: 0.000010  loss: 0.8249 (0.8246)  loss_scale: 131072.0000 (138111.6726)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0002  max mem: 41808
Epoch: [16]  [ 400/1349]  eta: 0:05:03  lr: 0.000426  min_lr: 0.000010  loss: 0.7973 (0.8247)  loss_scale: 131072.0000 (137936.1197)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
[2025-05-23 18:21:28,746] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 18:21:28,746] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 18:21:28,746] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 18:21:28,746] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [16]  [ 410/1349]  eta: 0:05:00  lr: 0.000426  min_lr: 0.000010  loss: 0.8673 (0.8260)  loss_scale: 131072.0000 (138725.8394)  weight_decay: 0.0500 (0.0500)  time: 0.3095  data: 0.0001  max mem: 41808
[2025-05-23 18:21:30,897] [INFO] [logging.py:96:log_dist] [Rank 0] step=22000, skipped=132, lr=[1.0125627246535443e-05, 1.0125627246535443e-05, 1.3500836328713923e-05, 1.3500836328713923e-05, 1.80011151049519e-05, 1.80011151049519e-05, 2.4001486806602532e-05, 2.4001486806602532e-05, 3.2001982408803376e-05, 3.2001982408803376e-05, 4.26693098784045e-05, 4.26693098784045e-05, 5.6892413171206e-05, 5.6892413171206e-05, 7.585655089494133e-05, 7.585655089494133e-05, 0.00010114206785992177, 0.00010114206785992177, 0.0001348560904798957, 0.0001348560904798957, 0.00017980812063986095, 0.00017980812063986095, 0.00023974416085314793, 0.00023974416085314793, 0.00031965888113753057, 0.00031965888113753057, 0.0004262118415167074, 0.0004262118415167074], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-05-23 18:21:30,898] [INFO] [timer.py:260:stop] epoch=0/micro_step=22000/global_step=22000, RunningAvgSamplesPerSec=206.91908556628368, CurrSamplesPerSec=214.0519254553385, MemAllocated=1.37GB, MaxMemAllocated=40.83GB
[2025-05-23 18:21:32,428] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 22004
[2025-05-23 18:21:32,428] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 22004
[2025-05-23 18:21:32,428] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 18:21:32,428] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 18:21:32,428] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [16]  [ 420/1349]  eta: 0:04:56  lr: 0.000426  min_lr: 0.000010  loss: 0.8991 (0.8276)  loss_scale: 262144.0000 (141346.0523)  weight_decay: 0.0500 (0.0500)  time: 0.3090  data: 0.0001  max mem: 41808
Epoch: [16]  [ 430/1349]  eta: 0:04:53  lr: 0.000426  min_lr: 0.000010  loss: 0.9096 (0.8290)  loss_scale: 131072.0000 (141107.6752)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
[2025-05-23 18:21:37,653] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 22021
[2025-05-23 18:21:37,653] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 22021
[2025-05-23 18:21:37,653] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 18:21:37,653] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 18:21:37,653] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [16]  [ 440/1349]  eta: 0:04:49  lr: 0.000426  min_lr: 0.000010  loss: 0.9072 (0.8296)  loss_scale: 131072.0000 (140285.6780)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [16]  [ 450/1349]  eta: 0:04:46  lr: 0.000426  min_lr: 0.000010  loss: 0.8288 (0.8292)  loss_scale: 65536.0000 (138628.2572)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0002  max mem: 41808
Epoch: [16]  [ 460/1349]  eta: 0:04:42  lr: 0.000426  min_lr: 0.000010  loss: 0.8247 (0.8295)  loss_scale: 65536.0000 (137042.7419)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
Epoch: [16]  [ 470/1349]  eta: 0:04:39  lr: 0.000426  min_lr: 0.000010  loss: 0.8301 (0.8293)  loss_scale: 65536.0000 (135524.5520)  weight_decay: 0.0500 (0.0500)  time: 0.3079  data: 0.0001  max mem: 41808
Epoch: [16]  [ 480/1349]  eta: 0:04:36  lr: 0.000426  min_lr: 0.000010  loss: 0.8934 (0.8312)  loss_scale: 65536.0000 (134069.4886)  weight_decay: 0.0500 (0.0500)  time: 0.3080  data: 0.0001  max mem: 41808
Epoch: [16]  [ 490/1349]  eta: 0:04:32  lr: 0.000426  min_lr: 0.000010  loss: 0.9434 (0.8310)  loss_scale: 65536.0000 (132673.6945)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0001  max mem: 41808
Epoch: [16]  [ 500/1349]  eta: 0:04:29  lr: 0.000425  min_lr: 0.000010  loss: 0.8139 (0.8310)  loss_scale: 65536.0000 (131333.6208)  weight_decay: 0.0500 (0.0500)  time: 0.3084  data: 0.0001  max mem: 41808
Epoch: [16]  [ 510/1349]  eta: 0:04:26  lr: 0.000425  min_lr: 0.000010  loss: 0.8062 (0.8304)  loss_scale: 65536.0000 (130045.9961)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0002  max mem: 41808
Epoch: [16]  [ 520/1349]  eta: 0:04:22  lr: 0.000425  min_lr: 0.000010  loss: 0.8426 (0.8316)  loss_scale: 65536.0000 (128807.8004)  weight_decay: 0.0500 (0.0500)  time: 0.3066  data: 0.0001  max mem: 41808
Epoch: [16]  [ 530/1349]  eta: 0:04:19  lr: 0.000425  min_lr: 0.000010  loss: 0.8586 (0.8316)  loss_scale: 65536.0000 (127616.2411)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [16]  [ 540/1349]  eta: 0:04:16  lr: 0.000425  min_lr: 0.000010  loss: 0.8429 (0.8309)  loss_scale: 65536.0000 (126468.7320)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [16]  [ 550/1349]  eta: 0:04:12  lr: 0.000425  min_lr: 0.000010  loss: 0.7794 (0.8317)  loss_scale: 65536.0000 (125362.8748)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0002  max mem: 41808
Epoch: [16]  [ 560/1349]  eta: 0:04:09  lr: 0.000425  min_lr: 0.000010  loss: 0.8746 (0.8324)  loss_scale: 65536.0000 (124296.4421)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0002  max mem: 41808
[2025-05-23 18:22:17,332] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 18:22:17,332] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-05-23 18:22:17,332] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 18:22:17,332] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [16]  [ 570/1349]  eta: 0:04:06  lr: 0.000425  min_lr: 0.000010  loss: 0.8880 (0.8328)  loss_scale: 65536.0000 (123841.2329)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
Epoch: [16]  [ 580/1349]  eta: 0:04:03  lr: 0.000425  min_lr: 0.000010  loss: 0.8880 (0.8329)  loss_scale: 131072.0000 (123965.6867)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [16]  [ 590/1349]  eta: 0:03:59  lr: 0.000425  min_lr: 0.000010  loss: 0.9017 (0.8339)  loss_scale: 131072.0000 (124085.9289)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
Epoch: [16]  [ 600/1349]  eta: 0:03:56  lr: 0.000425  min_lr: 0.000010  loss: 0.8088 (0.8326)  loss_scale: 131072.0000 (124202.1697)  weight_decay: 0.0500 (0.0500)  time: 0.3085  data: 0.0002  max mem: 41808
Epoch: [16]  [ 610/1349]  eta: 0:03:53  lr: 0.000424  min_lr: 0.000010  loss: 0.8042 (0.8326)  loss_scale: 131072.0000 (124314.6056)  weight_decay: 0.0500 (0.0500)  time: 0.3079  data: 0.0002  max mem: 41808
Epoch: [16]  [ 620/1349]  eta: 0:03:50  lr: 0.000424  min_lr: 0.000010  loss: 0.8582 (0.8333)  loss_scale: 131072.0000 (124423.4203)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
Epoch: [16]  [ 630/1349]  eta: 0:03:46  lr: 0.000424  min_lr: 0.000010  loss: 0.8687 (0.8339)  loss_scale: 131072.0000 (124528.7861)  weight_decay: 0.0500 (0.0500)  time: 0.3078  data: 0.0001  max mem: 41808
Epoch: [16]  [ 640/1349]  eta: 0:03:43  lr: 0.000424  min_lr: 0.000010  loss: 0.8320 (0.8332)  loss_scale: 131072.0000 (124630.8643)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
Epoch: [16]  [ 650/1349]  eta: 0:03:40  lr: 0.000424  min_lr: 0.000010  loss: 0.8462 (0.8337)  loss_scale: 131072.0000 (124729.8065)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [16]  [ 660/1349]  eta: 0:03:37  lr: 0.000424  min_lr: 0.000010  loss: 0.8725 (0.8348)  loss_scale: 131072.0000 (124825.7549)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0002  max mem: 41808
Epoch: [16]  [ 670/1349]  eta: 0:03:33  lr: 0.000424  min_lr: 0.000010  loss: 0.8271 (0.8336)  loss_scale: 131072.0000 (124918.8435)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
Epoch: [16]  [ 680/1349]  eta: 0:03:30  lr: 0.000424  min_lr: 0.000010  loss: 0.7860 (0.8332)  loss_scale: 131072.0000 (125009.1982)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [16]  [ 690/1349]  eta: 0:03:27  lr: 0.000424  min_lr: 0.000010  loss: 0.8598 (0.8337)  loss_scale: 131072.0000 (125096.9378)  weight_decay: 0.0500 (0.0500)  time: 0.3083  data: 0.0001  max mem: 41808
[2025-05-23 18:22:56,731] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 18:22:56,731] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 18:22:56,731] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 18:22:56,731] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [16]  [ 700/1349]  eta: 0:03:24  lr: 0.000424  min_lr: 0.000010  loss: 0.8686 (0.8338)  loss_scale: 131072.0000 (126491.0243)  weight_decay: 0.0500 (0.0500)  time: 0.3084  data: 0.0001  max mem: 41808
[2025-05-23 18:22:59,190] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 22286
[2025-05-23 18:22:59,190] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 22286
[2025-05-23 18:22:59,190] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 18:22:59,190] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 18:22:59,190] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [16]  [ 710/1349]  eta: 0:03:20  lr: 0.000423  min_lr: 0.000010  loss: 0.8728 (0.8343)  loss_scale: 131072.0000 (126739.8031)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0002  max mem: 41808
Epoch: [16]  [ 720/1349]  eta: 0:03:17  lr: 0.000423  min_lr: 0.000010  loss: 0.8373 (0.8344)  loss_scale: 131072.0000 (126799.8890)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0002  max mem: 41808
Epoch: [16]  [ 730/1349]  eta: 0:03:14  lr: 0.000423  min_lr: 0.000010  loss: 0.9169 (0.8359)  loss_scale: 131072.0000 (126858.3311)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
Epoch: [16]  [ 740/1349]  eta: 0:03:11  lr: 0.000423  min_lr: 0.000010  loss: 0.9169 (0.8363)  loss_scale: 131072.0000 (126915.1957)  weight_decay: 0.0500 (0.0500)  time: 0.3086  data: 0.0004  max mem: 41808
Epoch: [16]  [ 750/1349]  eta: 0:03:08  lr: 0.000423  min_lr: 0.000010  loss: 0.9112 (0.8362)  loss_scale: 131072.0000 (126970.5459)  weight_decay: 0.0500 (0.0500)  time: 0.3089  data: 0.0004  max mem: 41808
Epoch: [16]  [ 760/1349]  eta: 0:03:05  lr: 0.000423  min_lr: 0.000010  loss: 0.8624 (0.8356)  loss_scale: 131072.0000 (127024.4415)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
Epoch: [16]  [ 770/1349]  eta: 0:03:01  lr: 0.000423  min_lr: 0.000010  loss: 0.8458 (0.8360)  loss_scale: 131072.0000 (127076.9390)  weight_decay: 0.0500 (0.0500)  time: 0.3078  data: 0.0002  max mem: 41808
Epoch: [16]  [ 780/1349]  eta: 0:02:58  lr: 0.000423  min_lr: 0.000010  loss: 0.8458 (0.8365)  loss_scale: 131072.0000 (127128.0922)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
Epoch: [16]  [ 790/1349]  eta: 0:02:55  lr: 0.000423  min_lr: 0.000010  loss: 0.8613 (0.8368)  loss_scale: 131072.0000 (127177.9520)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [16]  [ 800/1349]  eta: 0:02:52  lr: 0.000423  min_lr: 0.000010  loss: 0.8410 (0.8375)  loss_scale: 131072.0000 (127226.5668)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [16]  [ 810/1349]  eta: 0:02:49  lr: 0.000423  min_lr: 0.000010  loss: 0.8240 (0.8375)  loss_scale: 131072.0000 (127273.9827)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [16]  [ 820/1349]  eta: 0:02:45  lr: 0.000422  min_lr: 0.000010  loss: 0.8362 (0.8377)  loss_scale: 131072.0000 (127320.2436)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [16]  [ 830/1349]  eta: 0:02:42  lr: 0.000422  min_lr: 0.000010  loss: 0.7982 (0.8376)  loss_scale: 131072.0000 (127365.3911)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
[2025-05-23 18:23:38,867] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 18:23:38,867] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 18:23:38,867] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 18:23:38,867] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 18:23:39,173] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 22416
[2025-05-23 18:23:39,173] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 22416
[2025-05-23 18:23:39,173] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 18:23:39,173] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 18:23:39,173] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [16]  [ 840/1349]  eta: 0:02:39  lr: 0.000422  min_lr: 0.000010  loss: 0.7875 (0.8374)  loss_scale: 131072.0000 (127565.3175)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0002  max mem: 41808
Epoch: [16]  [ 850/1349]  eta: 0:02:36  lr: 0.000422  min_lr: 0.000010  loss: 0.8593 (0.8377)  loss_scale: 131072.0000 (127606.5241)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0002  max mem: 41808
Epoch: [16]  [ 860/1349]  eta: 0:02:33  lr: 0.000422  min_lr: 0.000010  loss: 0.8094 (0.8370)  loss_scale: 131072.0000 (127646.7735)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [16]  [ 870/1349]  eta: 0:02:30  lr: 0.000422  min_lr: 0.000010  loss: 0.7787 (0.8369)  loss_scale: 131072.0000 (127686.0987)  weight_decay: 0.0500 (0.0500)  time: 0.3078  data: 0.0001  max mem: 41808
Epoch: [16]  [ 880/1349]  eta: 0:02:26  lr: 0.000422  min_lr: 0.000010  loss: 0.8484 (0.8369)  loss_scale: 131072.0000 (127724.5312)  weight_decay: 0.0500 (0.0500)  time: 0.3078  data: 0.0001  max mem: 41808
Epoch: [16]  [ 890/1349]  eta: 0:02:23  lr: 0.000422  min_lr: 0.000010  loss: 0.8892 (0.8373)  loss_scale: 131072.0000 (127762.1010)  weight_decay: 0.0500 (0.0500)  time: 0.3084  data: 0.0001  max mem: 41808
Epoch: [16]  [ 900/1349]  eta: 0:02:20  lr: 0.000422  min_lr: 0.000010  loss: 0.9104 (0.8383)  loss_scale: 131072.0000 (127798.8368)  weight_decay: 0.0500 (0.0500)  time: 0.3084  data: 0.0001  max mem: 41808
Epoch: [16]  [ 910/1349]  eta: 0:02:17  lr: 0.000422  min_lr: 0.000010  loss: 0.9102 (0.8390)  loss_scale: 131072.0000 (127834.7662)  weight_decay: 0.0500 (0.0500)  time: 0.3080  data: 0.0001  max mem: 41808
Epoch: [16]  [ 920/1349]  eta: 0:02:14  lr: 0.000422  min_lr: 0.000010  loss: 0.9025 (0.8389)  loss_scale: 131072.0000 (127869.9153)  weight_decay: 0.0500 (0.0500)  time: 0.3091  data: 0.0001  max mem: 41808
Epoch: [16]  [ 930/1349]  eta: 0:02:11  lr: 0.000421  min_lr: 0.000010  loss: 0.8845 (0.8396)  loss_scale: 131072.0000 (127904.3093)  weight_decay: 0.0500 (0.0500)  time: 0.3089  data: 0.0001  max mem: 41808
Epoch: [16]  [ 940/1349]  eta: 0:02:07  lr: 0.000421  min_lr: 0.000010  loss: 0.8772 (0.8399)  loss_scale: 131072.0000 (127937.9724)  weight_decay: 0.0500 (0.0500)  time: 0.3080  data: 0.0001  max mem: 41808
Epoch: [16]  [ 950/1349]  eta: 0:02:04  lr: 0.000421  min_lr: 0.000010  loss: 0.8055 (0.8392)  loss_scale: 131072.0000 (127970.9274)  weight_decay: 0.0500 (0.0500)  time: 0.3081  data: 0.0001  max mem: 41808
Epoch: [16]  [ 960/1349]  eta: 0:02:01  lr: 0.000421  min_lr: 0.000010  loss: 0.8419 (0.8396)  loss_scale: 131072.0000 (128003.1967)  weight_decay: 0.0500 (0.0500)  time: 0.3088  data: 0.0001  max mem: 41808
[2025-05-23 18:24:18,936] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 18:24:18,936] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 18:24:18,936] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 18:24:18,936] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 18:24:19,545] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 22547
[2025-05-23 18:24:19,545] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 22547
[2025-05-23 18:24:19,545] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 18:24:19,545] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 18:24:19,545] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [16]  [ 970/1349]  eta: 0:01:58  lr: 0.000421  min_lr: 0.000010  loss: 0.8585 (0.8394)  loss_scale: 131072.0000 (128304.7745)  weight_decay: 0.0500 (0.0500)  time: 0.3084  data: 0.0002  max mem: 41808
Epoch: [16]  [ 980/1349]  eta: 0:01:55  lr: 0.000421  min_lr: 0.000010  loss: 0.7902 (0.8389)  loss_scale: 131072.0000 (128332.9827)  weight_decay: 0.0500 (0.0500)  time: 0.3083  data: 0.0002  max mem: 41808
Epoch: [16]  [ 990/1349]  eta: 0:01:52  lr: 0.000421  min_lr: 0.000010  loss: 0.8337 (0.8391)  loss_scale: 131072.0000 (128360.6216)  weight_decay: 0.0500 (0.0500)  time: 0.3084  data: 0.0001  max mem: 41808
Epoch: [16]  [1000/1349]  eta: 0:01:49  lr: 0.000421  min_lr: 0.000010  loss: 0.8705 (0.8398)  loss_scale: 131072.0000 (128387.7083)  weight_decay: 0.0500 (0.0500)  time: 0.3078  data: 0.0001  max mem: 41808
Epoch: [16]  [1010/1349]  eta: 0:01:45  lr: 0.000421  min_lr: 0.000010  loss: 0.8705 (0.8395)  loss_scale: 131072.0000 (128414.2591)  weight_decay: 0.0500 (0.0500)  time: 0.3081  data: 0.0001  max mem: 41808
Epoch: [16]  [1020/1349]  eta: 0:01:42  lr: 0.000421  min_lr: 0.000010  loss: 0.8542 (0.8397)  loss_scale: 131072.0000 (128440.2899)  weight_decay: 0.0500 (0.0500)  time: 0.3078  data: 0.0001  max mem: 41808
Epoch: [16]  [1030/1349]  eta: 0:01:39  lr: 0.000420  min_lr: 0.000010  loss: 0.8429 (0.8396)  loss_scale: 131072.0000 (128465.8157)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [16]  [1040/1349]  eta: 0:01:36  lr: 0.000420  min_lr: 0.000010  loss: 0.8518 (0.8395)  loss_scale: 131072.0000 (128490.8511)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [16]  [1050/1349]  eta: 0:01:33  lr: 0.000420  min_lr: 0.000010  loss: 0.8518 (0.8391)  loss_scale: 131072.0000 (128515.4101)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [16]  [1060/1349]  eta: 0:01:30  lr: 0.000420  min_lr: 0.000010  loss: 0.8443 (0.8389)  loss_scale: 131072.0000 (128539.5061)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [16]  [1070/1349]  eta: 0:01:27  lr: 0.000420  min_lr: 0.000010  loss: 0.8876 (0.8391)  loss_scale: 131072.0000 (128563.1522)  weight_decay: 0.0500 (0.0500)  time: 0.3084  data: 0.0001  max mem: 41808
Epoch: [16]  [1080/1349]  eta: 0:01:24  lr: 0.000420  min_lr: 0.000010  loss: 0.9021 (0.8395)  loss_scale: 131072.0000 (128586.3608)  weight_decay: 0.0500 (0.0500)  time: 0.3097  data: 0.0002  max mem: 41808
Epoch: [16]  [1090/1349]  eta: 0:01:20  lr: 0.000420  min_lr: 0.000010  loss: 0.9021 (0.8398)  loss_scale: 131072.0000 (128609.1439)  weight_decay: 0.0500 (0.0500)  time: 0.3100  data: 0.0001  max mem: 41808
[2025-05-23 18:24:59,325] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 18:24:59,326] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 18:24:59,326] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 18:24:59,326] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 18:25:00,863] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 22681
[2025-05-23 18:25:00,863] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 22681
[2025-05-23 18:25:00,864] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 18:25:00,864] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 18:25:00,864] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [16]  [1100/1349]  eta: 0:01:17  lr: 0.000420  min_lr: 0.000010  loss: 0.9362 (0.8403)  loss_scale: 131072.0000 (129226.7539)  weight_decay: 0.0500 (0.0500)  time: 0.3087  data: 0.0001  max mem: 41808
Epoch: [16]  [1110/1349]  eta: 0:01:14  lr: 0.000420  min_lr: 0.000010  loss: 0.9692 (0.8408)  loss_scale: 131072.0000 (129243.3627)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [16]  [1120/1349]  eta: 0:01:11  lr: 0.000420  min_lr: 0.000010  loss: 0.9178 (0.8413)  loss_scale: 131072.0000 (129259.6753)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
Epoch: [16]  [1130/1349]  eta: 0:01:08  lr: 0.000420  min_lr: 0.000010  loss: 0.8188 (0.8409)  loss_scale: 131072.0000 (129275.6994)  weight_decay: 0.0500 (0.0500)  time: 0.3080  data: 0.0001  max mem: 41808
Epoch: [16]  [1140/1349]  eta: 0:01:05  lr: 0.000419  min_lr: 0.000010  loss: 0.8188 (0.8414)  loss_scale: 131072.0000 (129291.4426)  weight_decay: 0.0500 (0.0500)  time: 0.3078  data: 0.0001  max mem: 41808
Epoch: [16]  [1150/1349]  eta: 0:01:02  lr: 0.000419  min_lr: 0.000010  loss: 0.8678 (0.8411)  loss_scale: 131072.0000 (129306.9123)  weight_decay: 0.0500 (0.0500)  time: 0.3078  data: 0.0002  max mem: 41808
Epoch: [16]  [1160/1349]  eta: 0:00:58  lr: 0.000419  min_lr: 0.000010  loss: 0.8989 (0.8419)  loss_scale: 131072.0000 (129322.1154)  weight_decay: 0.0500 (0.0500)  time: 0.3081  data: 0.0002  max mem: 41808
Epoch: [16]  [1170/1349]  eta: 0:00:55  lr: 0.000419  min_lr: 0.000010  loss: 0.8250 (0.8410)  loss_scale: 131072.0000 (129337.0589)  weight_decay: 0.0500 (0.0500)  time: 0.3087  data: 0.0001  max mem: 41808
Epoch: [16]  [1180/1349]  eta: 0:00:52  lr: 0.000419  min_lr: 0.000010  loss: 0.8328 (0.8419)  loss_scale: 131072.0000 (129351.7494)  weight_decay: 0.0500 (0.0500)  time: 0.3089  data: 0.0001  max mem: 41808
Epoch: [16]  [1190/1349]  eta: 0:00:49  lr: 0.000419  min_lr: 0.000010  loss: 0.8987 (0.8411)  loss_scale: 131072.0000 (129366.1931)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0002  max mem: 41808
[2025-05-23 18:25:29,810] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 22775
[2025-05-23 18:25:29,810] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 22775
[2025-05-23 18:25:29,810] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 18:25:29,810] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 18:25:29,810] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [16]  [1200/1349]  eta: 0:00:46  lr: 0.000419  min_lr: 0.000010  loss: 0.7855 (0.8407)  loss_scale: 65536.0000 (128834.7177)  weight_decay: 0.0500 (0.0500)  time: 0.3062  data: 0.0001  max mem: 41808
Epoch: [16]  [1210/1349]  eta: 0:00:43  lr: 0.000419  min_lr: 0.000010  loss: 0.8669 (0.8414)  loss_scale: 65536.0000 (128312.0198)  weight_decay: 0.0500 (0.0500)  time: 0.3063  data: 0.0002  max mem: 41808
Epoch: [16]  [1220/1349]  eta: 0:00:40  lr: 0.000419  min_lr: 0.000010  loss: 0.8845 (0.8408)  loss_scale: 65536.0000 (127797.8837)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
Epoch: [16]  [1230/1349]  eta: 0:00:37  lr: 0.000419  min_lr: 0.000010  loss: 0.7963 (0.8407)  loss_scale: 65536.0000 (127292.1007)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
Epoch: [16]  [1240/1349]  eta: 0:00:33  lr: 0.000418  min_lr: 0.000010  loss: 0.8799 (0.8409)  loss_scale: 65536.0000 (126794.4690)  weight_decay: 0.0500 (0.0500)  time: 0.3080  data: 0.0001  max mem: 41808
Epoch: [16]  [1250/1349]  eta: 0:00:30  lr: 0.000418  min_lr: 0.000010  loss: 0.8661 (0.8408)  loss_scale: 65536.0000 (126304.7930)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
Epoch: [16]  [1260/1349]  eta: 0:00:27  lr: 0.000418  min_lr: 0.000010  loss: 0.8673 (0.8412)  loss_scale: 65536.0000 (125822.8834)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
Epoch: [16]  [1270/1349]  eta: 0:00:24  lr: 0.000418  min_lr: 0.000010  loss: 0.8720 (0.8411)  loss_scale: 65536.0000 (125348.5570)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [16]  [1280/1349]  eta: 0:00:21  lr: 0.000418  min_lr: 0.000010  loss: 0.8828 (0.8417)  loss_scale: 65536.0000 (124881.6362)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [16]  [1290/1349]  eta: 0:00:18  lr: 0.000418  min_lr: 0.000010  loss: 0.8448 (0.8416)  loss_scale: 65536.0000 (124421.9489)  weight_decay: 0.0500 (0.0500)  time: 0.3084  data: 0.0001  max mem: 41808
Epoch: [16]  [1300/1349]  eta: 0:00:15  lr: 0.000418  min_lr: 0.000010  loss: 0.8356 (0.8420)  loss_scale: 65536.0000 (123969.3282)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
Epoch: [16]  [1310/1349]  eta: 0:00:12  lr: 0.000418  min_lr: 0.000010  loss: 0.9364 (0.8422)  loss_scale: 65536.0000 (123523.6125)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
[2025-05-23 18:26:09,468] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 18:26:09,468] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-05-23 18:26:09,468] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 18:26:09,469] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [16]  [1320/1349]  eta: 0:00:09  lr: 0.000418  min_lr: 0.000010  loss: 0.9044 (0.8421)  loss_scale: 65536.0000 (123134.2559)  weight_decay: 0.0500 (0.0500)  time: 0.3079  data: 0.0001  max mem: 41808
Epoch: [16]  [1330/1349]  eta: 0:00:05  lr: 0.000418  min_lr: 0.000010  loss: 0.8966 (0.8423)  loss_scale: 131072.0000 (123193.8933)  weight_decay: 0.0500 (0.0500)  time: 0.3056  data: 0.0001  max mem: 41808
Epoch: [16]  [1340/1349]  eta: 0:00:02  lr: 0.000418  min_lr: 0.000010  loss: 0.8622 (0.8419)  loss_scale: 131072.0000 (123252.6413)  weight_decay: 0.0500 (0.0500)  time: 0.3026  data: 0.0001  max mem: 41808
Epoch: [16]  [1348/1349]  eta: 0:00:00  lr: 0.000417  min_lr: 0.000010  loss: 0.8770 (0.8419)  loss_scale: 131072.0000 (123299.0126)  weight_decay: 0.0500 (0.0500)  time: 0.3020  data: 0.0001  max mem: 41808
Epoch: [16] Total time: 0:07:00 (0.3116 s / it)
Averaged stats: lr: 0.000417  min_lr: 0.000010  loss: 0.8770 (0.8369)  loss_scale: 131072.0000 (123299.0126)  weight_decay: 0.0500 (0.0500)  total_time: 420.3654 (420.3588)
Val:  [  0/346]  eta: 1:32:24  loss: 2.0742 (2.0742)  acc1: 17.9688 (17.9688)  acc5: 89.8438 (89.8438)  time: 16.0240  data: 15.2440  max mem: 41808
Val:  [ 10/346]  eta: 0:12:17  loss: 0.1108 (0.4348)  acc1: 100.0000 (87.1449)  acc5: 100.0000 (99.0057)  time: 2.1944  data: 1.3861  max mem: 41808
Val:  [ 20/346]  eta: 0:08:34  loss: 0.1084 (0.3663)  acc1: 100.0000 (90.2530)  acc5: 100.0000 (99.1815)  time: 0.8547  data: 0.0624  max mem: 41808
Val:  [ 30/346]  eta: 0:06:58  loss: 0.0956 (0.3205)  acc1: 100.0000 (91.9607)  acc5: 100.0000 (99.4456)  time: 0.8437  data: 0.0624  max mem: 41808
Val:  [ 40/346]  eta: 0:06:05  loss: 0.1054 (0.3735)  acc1: 99.2188 (90.4726)  acc5: 100.0000 (99.4284)  time: 0.7939  data: 0.0003  max mem: 41808
Val:  [ 50/346]  eta: 0:05:29  loss: 0.1077 (0.3311)  acc1: 99.2188 (91.9577)  acc5: 100.0000 (99.5251)  time: 0.7916  data: 0.0003  max mem: 41808
Val:  [ 60/346]  eta: 0:05:04  loss: 0.1282 (0.3164)  acc1: 97.6562 (92.3412)  acc5: 100.0000 (99.5902)  time: 0.8003  data: 0.0003  max mem: 41808
Val:  [ 70/346]  eta: 0:04:41  loss: 0.2408 (0.3353)  acc1: 94.5312 (91.4943)  acc5: 100.0000 (99.6479)  time: 0.7759  data: 0.0003  max mem: 41808
Val:  [ 80/346]  eta: 0:04:23  loss: 0.2529 (0.3360)  acc1: 92.9688 (91.6377)  acc5: 100.0000 (99.5081)  time: 0.7591  data: 0.0253  max mem: 41808
Val:  [ 90/346]  eta: 0:04:11  loss: 0.2628 (0.3376)  acc1: 93.7500 (91.5350)  acc5: 100.0000 (99.5622)  time: 0.8464  data: 0.0912  max mem: 41808
Val:  [100/346]  eta: 0:03:58  loss: 0.1812 (0.3190)  acc1: 96.8750 (92.2416)  acc5: 100.0000 (99.6055)  time: 0.8841  data: 0.1373  max mem: 41808
Val:  [110/346]  eta: 0:03:46  loss: 0.1448 (0.3308)  acc1: 98.4375 (91.8778)  acc5: 100.0000 (99.6059)  time: 0.8726  data: 0.1318  max mem: 41808
Val:  [120/346]  eta: 0:03:36  loss: 0.1956 (0.3376)  acc1: 97.6562 (91.7162)  acc5: 100.0000 (99.5739)  time: 0.9133  data: 0.1421  max mem: 41808
Val:  [130/346]  eta: 0:03:26  loss: 0.1073 (0.3552)  acc1: 100.0000 (91.1140)  acc5: 100.0000 (99.4752)  time: 0.9138  data: 0.1554  max mem: 41808
Val:  [140/346]  eta: 0:03:15  loss: 0.2297 (0.3606)  acc1: 95.3125 (90.9574)  acc5: 100.0000 (99.5069)  time: 0.8928  data: 0.1465  max mem: 41808
Val:  [150/346]  eta: 0:03:05  loss: 0.2885 (0.3582)  acc1: 93.7500 (91.0700)  acc5: 100.0000 (99.5085)  time: 0.8920  data: 0.1442  max mem: 41808
Val:  [160/346]  eta: 0:02:55  loss: 0.2131 (0.3517)  acc1: 94.5312 (91.2510)  acc5: 100.0000 (99.5196)  time: 0.9014  data: 0.1481  max mem: 41808
Val:  [170/346]  eta: 0:02:45  loss: 0.2131 (0.3527)  acc1: 93.7500 (91.0773)  acc5: 100.0000 (99.5477)  time: 0.9138  data: 0.1468  max mem: 41808
Val:  [180/346]  eta: 0:02:36  loss: 0.2548 (0.3722)  acc1: 91.4062 (90.0768)  acc5: 100.0000 (99.5727)  time: 0.9122  data: 0.1375  max mem: 41808
Val:  [190/346]  eta: 0:02:26  loss: 0.2157 (0.3701)  acc1: 91.4062 (90.1792)  acc5: 100.0000 (99.5869)  time: 0.8985  data: 0.1430  max mem: 41808
Val:  [200/346]  eta: 0:02:16  loss: 0.3612 (0.3827)  acc1: 88.2812 (89.7505)  acc5: 100.0000 (99.5569)  time: 0.8945  data: 0.1530  max mem: 41808
Val:  [210/346]  eta: 0:02:07  loss: 0.1645 (0.3749)  acc1: 97.6562 (90.0585)  acc5: 100.0000 (99.5779)  time: 0.9238  data: 0.1582  max mem: 41808
Val:  [220/346]  eta: 0:01:57  loss: 0.1492 (0.3709)  acc1: 99.2188 (90.2467)  acc5: 100.0000 (99.5263)  time: 0.9147  data: 0.1668  max mem: 41808
Val:  [230/346]  eta: 0:01:48  loss: 0.1296 (0.3624)  acc1: 99.2188 (90.5438)  acc5: 100.0000 (99.5434)  time: 0.8806  data: 0.1592  max mem: 41808
Val:  [240/346]  eta: 0:01:38  loss: 0.1888 (0.3679)  acc1: 96.8750 (90.4759)  acc5: 100.0000 (99.5559)  time: 0.8965  data: 0.1527  max mem: 41808
Val:  [250/346]  eta: 0:01:29  loss: 0.1888 (0.3647)  acc1: 96.8750 (90.6001)  acc5: 100.0000 (99.5736)  time: 0.9048  data: 0.1506  max mem: 41808
Val:  [260/346]  eta: 0:01:19  loss: 0.1461 (0.3608)  acc1: 97.6562 (90.7238)  acc5: 100.0000 (99.5869)  time: 0.9015  data: 0.1534  max mem: 41808
Val:  [270/346]  eta: 0:01:10  loss: 0.1315 (0.3565)  acc1: 97.6562 (90.8700)  acc5: 100.0000 (99.6022)  time: 0.9155  data: 0.1680  max mem: 41808
Val:  [280/346]  eta: 0:01:01  loss: 0.1116 (0.3548)  acc1: 98.4375 (90.9558)  acc5: 100.0000 (99.6052)  time: 0.9069  data: 0.1625  max mem: 41808
Val:  [290/346]  eta: 0:00:51  loss: 0.0985 (0.3461)  acc1: 100.0000 (91.2586)  acc5: 100.0000 (99.6188)  time: 0.9029  data: 0.1575  max mem: 41808
Val:  [300/346]  eta: 0:00:42  loss: 0.0988 (0.3467)  acc1: 100.0000 (91.2972)  acc5: 100.0000 (99.4887)  time: 0.8965  data: 0.1594  max mem: 41808
Val:  [310/346]  eta: 0:00:33  loss: 0.1211 (0.3472)  acc1: 100.0000 (91.2731)  acc5: 100.0000 (99.4951)  time: 0.8556  data: 0.1435  max mem: 41808
Val:  [320/346]  eta: 0:00:23  loss: 0.1268 (0.3502)  acc1: 97.6562 (91.1385)  acc5: 100.0000 (99.5035)  time: 0.8522  data: 0.1378  max mem: 41808
Val:  [330/346]  eta: 0:00:14  loss: 0.3744 (0.3572)  acc1: 89.0625 (90.9177)  acc5: 100.0000 (99.4265)  time: 0.8904  data: 0.1559  max mem: 41808
Val:  [340/346]  eta: 0:00:05  loss: 0.4015 (0.3649)  acc1: 89.0625 (90.7281)  acc5: 100.0000 (99.4318)  time: 0.8995  data: 0.1662  max mem: 41808
Val:  [345/346]  eta: 0:00:00  loss: 0.1773 (0.3617)  acc1: 95.4023 (90.8355)  acc5: 100.0000 (99.4395)  time: 0.8775  data: 0.1678  max mem: 41808
Val: Total time: 0:05:18 (0.9197 s / it)
* Acc@1 90.883 Acc@5 99.442 loss 0.361
Accuracy of the network on the 88494 val videos: 90.9%
Max accuracy: 91.25%   Max Epoch: 3
Epoch: [17]  [   0/1349]  eta: 1:43:22  lr: 0.000417  min_lr: 0.000010  loss: 0.4893 (0.4893)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 4.5976  data: 3.9540  max mem: 41808
Epoch: [17]  [  10/1349]  eta: 0:16:40  lr: 0.000417  min_lr: 0.000010  loss: 0.8710 (0.8634)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7473  data: 0.4007  max mem: 41808
Epoch: [17]  [  20/1349]  eta: 0:11:55  lr: 0.000417  min_lr: 0.000010  loss: 0.8270 (0.8304)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3356  data: 0.0227  max mem: 41808
Epoch: [17]  [  30/1349]  eta: 0:10:11  lr: 0.000417  min_lr: 0.000010  loss: 0.8120 (0.8344)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0001  max mem: 41808
Epoch: [17]  [  40/1349]  eta: 0:09:16  lr: 0.000417  min_lr: 0.000010  loss: 0.8810 (0.8581)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3064  data: 0.0001  max mem: 41808
Epoch: [17]  [  50/1349]  eta: 0:08:42  lr: 0.000417  min_lr: 0.000010  loss: 0.8713 (0.8547)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3064  data: 0.0001  max mem: 41808
Epoch: [17]  [  60/1349]  eta: 0:08:18  lr: 0.000417  min_lr: 0.000010  loss: 0.7633 (0.8292)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3085  data: 0.0001  max mem: 41808
[2025-05-23 18:32:01,989] [INFO] [logging.py:96:log_dist] [Rank 0] step=23000, skipped=139, lr=[9.902355683915493e-06, 9.902355683915493e-06, 1.3203140911887324e-05, 1.3203140911887324e-05, 1.7604187882516432e-05, 1.7604187882516432e-05, 2.347225051002191e-05, 2.347225051002191e-05, 3.129633401336255e-05, 3.129633401336255e-05, 4.1728445351150066e-05, 4.1728445351150066e-05, 5.563792713486675e-05, 5.563792713486675e-05, 7.418390284648901e-05, 7.418390284648901e-05, 9.891187046198533e-05, 9.891187046198533e-05, 0.0001318824939493138, 0.0001318824939493138, 0.00017584332526575171, 0.00017584332526575171, 0.00023445776702100228, 0.00023445776702100228, 0.00031261035602800304, 0.00031261035602800304, 0.0004168138080373374, 0.0004168138080373374], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-05-23 18:32:01,989] [INFO] [timer.py:260:stop] epoch=0/micro_step=23000/global_step=23000, RunningAvgSamplesPerSec=207.17181883160399, CurrSamplesPerSec=213.4867435403738, MemAllocated=1.37GB, MaxMemAllocated=40.83GB
Epoch: [17]  [  70/1349]  eta: 0:08:00  lr: 0.000417  min_lr: 0.000010  loss: 0.7633 (0.8230)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3090  data: 0.0001  max mem: 41808
Epoch: [17]  [  80/1349]  eta: 0:07:45  lr: 0.000417  min_lr: 0.000010  loss: 0.7992 (0.8287)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3066  data: 0.0001  max mem: 41808
Epoch: [17]  [  90/1349]  eta: 0:07:33  lr: 0.000417  min_lr: 0.000010  loss: 0.8604 (0.8291)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3059  data: 0.0001  max mem: 41808
[2025-05-23 18:32:12,080] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 18:32:12,081] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 18:32:12,080] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 18:32:12,081] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [17]  [ 100/1349]  eta: 0:07:23  lr: 0.000416  min_lr: 0.000010  loss: 0.8575 (0.8244)  loss_scale: 131072.0000 (133667.4851)  weight_decay: 0.0500 (0.0500)  time: 0.3059  data: 0.0001  max mem: 41808
[2025-05-23 18:32:13,302] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 23036
[2025-05-23 18:32:13,302] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 23036
[2025-05-23 18:32:13,302] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 18:32:13,302] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 18:32:13,302] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [17]  [ 110/1349]  eta: 0:07:14  lr: 0.000416  min_lr: 0.000010  loss: 0.8585 (0.8246)  loss_scale: 131072.0000 (135795.3153)  weight_decay: 0.0500 (0.0500)  time: 0.3056  data: 0.0001  max mem: 41808
Epoch: [17]  [ 120/1349]  eta: 0:07:06  lr: 0.000416  min_lr: 0.000010  loss: 0.8612 (0.8278)  loss_scale: 131072.0000 (135404.9587)  weight_decay: 0.0500 (0.0500)  time: 0.3058  data: 0.0001  max mem: 41808
Epoch: [17]  [ 130/1349]  eta: 0:06:59  lr: 0.000416  min_lr: 0.000010  loss: 0.8807 (0.8330)  loss_scale: 131072.0000 (135074.1985)  weight_decay: 0.0500 (0.0500)  time: 0.3061  data: 0.0001  max mem: 41808
Epoch: [17]  [ 140/1349]  eta: 0:06:52  lr: 0.000416  min_lr: 0.000010  loss: 0.8682 (0.8293)  loss_scale: 131072.0000 (134790.3546)  weight_decay: 0.0500 (0.0500)  time: 0.3063  data: 0.0001  max mem: 41808
Epoch: [17]  [ 150/1349]  eta: 0:06:46  lr: 0.000416  min_lr: 0.000010  loss: 0.8660 (0.8313)  loss_scale: 131072.0000 (134544.1060)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0001  max mem: 41808
Epoch: [17]  [ 160/1349]  eta: 0:06:40  lr: 0.000416  min_lr: 0.000010  loss: 0.9271 (0.8382)  loss_scale: 131072.0000 (134328.4472)  weight_decay: 0.0500 (0.0500)  time: 0.3066  data: 0.0001  max mem: 41808
Epoch: [17]  [ 170/1349]  eta: 0:06:35  lr: 0.000416  min_lr: 0.000010  loss: 0.8555 (0.8381)  loss_scale: 131072.0000 (134138.0117)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0001  max mem: 41808
Epoch: [17]  [ 180/1349]  eta: 0:06:29  lr: 0.000416  min_lr: 0.000010  loss: 0.8085 (0.8339)  loss_scale: 131072.0000 (133968.6188)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
Epoch: [17]  [ 190/1349]  eta: 0:06:24  lr: 0.000416  min_lr: 0.000010  loss: 0.7700 (0.8330)  loss_scale: 131072.0000 (133816.9634)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0001  max mem: 41808
Epoch: [17]  [ 200/1349]  eta: 0:06:20  lr: 0.000416  min_lr: 0.000010  loss: 0.8027 (0.8300)  loss_scale: 131072.0000 (133680.3980)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
Epoch: [17]  [ 210/1349]  eta: 0:06:15  lr: 0.000415  min_lr: 0.000010  loss: 0.8122 (0.8296)  loss_scale: 131072.0000 (133556.7773)  weight_decay: 0.0500 (0.0500)  time: 0.3079  data: 0.0001  max mem: 41808
Epoch: [17]  [ 220/1349]  eta: 0:06:11  lr: 0.000415  min_lr: 0.000010  loss: 0.8559 (0.8276)  loss_scale: 131072.0000 (133444.3439)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [17]  [ 230/1349]  eta: 0:06:06  lr: 0.000415  min_lr: 0.000010  loss: 0.8216 (0.8284)  loss_scale: 131072.0000 (133341.6450)  weight_decay: 0.0500 (0.0500)  time: 0.3062  data: 0.0001  max mem: 41808
[2025-05-23 18:32:52,861] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 18:32:52,861] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 18:32:52,861] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 18:32:52,861] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [17]  [ 240/1349]  eta: 0:06:02  lr: 0.000415  min_lr: 0.000010  loss: 0.8055 (0.8273)  loss_scale: 131072.0000 (138142.2739)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
[2025-05-23 18:32:56,243] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 23176
[2025-05-23 18:32:56,243] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 23176
[2025-05-23 18:32:56,243] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 18:32:56,243] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 18:32:56,243] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [17]  [ 250/1349]  eta: 0:05:58  lr: 0.000415  min_lr: 0.000010  loss: 0.8126 (0.8290)  loss_scale: 262144.0000 (138904.9880)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
Epoch: [17]  [ 260/1349]  eta: 0:05:54  lr: 0.000415  min_lr: 0.000010  loss: 0.8685 (0.8297)  loss_scale: 131072.0000 (138604.8736)  weight_decay: 0.0500 (0.0500)  time: 0.3078  data: 0.0001  max mem: 41808
Epoch: [17]  [ 270/1349]  eta: 0:05:50  lr: 0.000415  min_lr: 0.000010  loss: 0.8446 (0.8277)  loss_scale: 131072.0000 (138326.9077)  weight_decay: 0.0500 (0.0500)  time: 0.3083  data: 0.0001  max mem: 41808
Epoch: [17]  [ 280/1349]  eta: 0:05:46  lr: 0.000415  min_lr: 0.000010  loss: 0.7959 (0.8274)  loss_scale: 131072.0000 (138068.7260)  weight_decay: 0.0500 (0.0500)  time: 0.3079  data: 0.0001  max mem: 41808
Epoch: [17]  [ 290/1349]  eta: 0:05:42  lr: 0.000415  min_lr: 0.000010  loss: 0.8409 (0.8287)  loss_scale: 131072.0000 (137828.2887)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0001  max mem: 41808
Epoch: [17]  [ 300/1349]  eta: 0:05:38  lr: 0.000415  min_lr: 0.000010  loss: 0.8904 (0.8308)  loss_scale: 131072.0000 (137603.8272)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [17]  [ 310/1349]  eta: 0:05:35  lr: 0.000414  min_lr: 0.000010  loss: 0.8492 (0.8286)  loss_scale: 131072.0000 (137393.8006)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
Epoch: [17]  [ 320/1349]  eta: 0:05:31  lr: 0.000414  min_lr: 0.000010  loss: 0.7796 (0.8272)  loss_scale: 131072.0000 (137196.8598)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [17]  [ 330/1349]  eta: 0:05:27  lr: 0.000414  min_lr: 0.000010  loss: 0.8093 (0.8280)  loss_scale: 131072.0000 (137011.8187)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [17]  [ 340/1349]  eta: 0:05:24  lr: 0.000414  min_lr: 0.000010  loss: 0.8130 (0.8271)  loss_scale: 131072.0000 (136837.6305)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [17]  [ 350/1349]  eta: 0:05:20  lr: 0.000414  min_lr: 0.000010  loss: 0.8604 (0.8288)  loss_scale: 131072.0000 (136673.3675)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
[2025-05-23 18:33:30,390] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 23287
[2025-05-23 18:33:30,390] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 23287
[2025-05-23 18:33:30,390] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 18:33:30,390] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 18:33:30,390] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [17]  [ 360/1349]  eta: 0:05:16  lr: 0.000414  min_lr: 0.000010  loss: 0.8598 (0.8295)  loss_scale: 131072.0000 (135247.4238)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
Epoch: [17]  [ 370/1349]  eta: 0:05:13  lr: 0.000414  min_lr: 0.000010  loss: 0.8456 (0.8295)  loss_scale: 65536.0000 (133368.4097)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
Epoch: [17]  [ 380/1349]  eta: 0:05:09  lr: 0.000414  min_lr: 0.000010  loss: 0.8578 (0.8287)  loss_scale: 65536.0000 (131588.0315)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
Epoch: [17]  [ 390/1349]  eta: 0:05:06  lr: 0.000414  min_lr: 0.000010  loss: 0.8328 (0.8280)  loss_scale: 65536.0000 (129898.7212)  weight_decay: 0.0500 (0.0500)  time: 0.3079  data: 0.0001  max mem: 41808
Epoch: [17]  [ 400/1349]  eta: 0:05:02  lr: 0.000414  min_lr: 0.000010  loss: 0.8446 (0.8284)  loss_scale: 65536.0000 (128293.6658)  weight_decay: 0.0500 (0.0500)  time: 0.3093  data: 0.0001  max mem: 41808
Epoch: [17]  [ 410/1349]  eta: 0:04:59  lr: 0.000413  min_lr: 0.000010  loss: 0.8940 (0.8307)  loss_scale: 65536.0000 (126766.7153)  weight_decay: 0.0500 (0.0500)  time: 0.3095  data: 0.0001  max mem: 41808
Epoch: [17]  [ 420/1349]  eta: 0:04:56  lr: 0.000413  min_lr: 0.000010  loss: 0.9070 (0.8299)  loss_scale: 65536.0000 (125312.3040)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0001  max mem: 41808
Epoch: [17]  [ 430/1349]  eta: 0:04:52  lr: 0.000413  min_lr: 0.000010  loss: 0.8212 (0.8299)  loss_scale: 65536.0000 (123925.3828)  weight_decay: 0.0500 (0.0500)  time: 0.3080  data: 0.0001  max mem: 41808
Epoch: [17]  [ 440/1349]  eta: 0:04:49  lr: 0.000413  min_lr: 0.000010  loss: 0.8373 (0.8299)  loss_scale: 65536.0000 (122601.3605)  weight_decay: 0.0500 (0.0500)  time: 0.3088  data: 0.0001  max mem: 41808
Epoch: [17]  [ 450/1349]  eta: 0:04:45  lr: 0.000413  min_lr: 0.000010  loss: 0.7954 (0.8285)  loss_scale: 65536.0000 (121336.0532)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
Epoch: [17]  [ 460/1349]  eta: 0:04:42  lr: 0.000413  min_lr: 0.000010  loss: 0.7359 (0.8285)  loss_scale: 65536.0000 (120125.6399)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [17]  [ 470/1349]  eta: 0:04:39  lr: 0.000413  min_lr: 0.000010  loss: 0.8331 (0.8285)  loss_scale: 65536.0000 (118966.6242)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [17]  [ 480/1349]  eta: 0:04:35  lr: 0.000413  min_lr: 0.000010  loss: 0.7853 (0.8277)  loss_scale: 65536.0000 (117855.8004)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
[2025-05-23 18:34:10,113] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 18:34:10,113] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-05-23 18:34:10,113] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 18:34:10,114] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [17]  [ 490/1349]  eta: 0:04:32  lr: 0.000413  min_lr: 0.000010  loss: 0.8312 (0.8277)  loss_scale: 65536.0000 (117858.0204)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [17]  [ 500/1349]  eta: 0:04:29  lr: 0.000413  min_lr: 0.000010  loss: 0.8852 (0.8284)  loss_scale: 131072.0000 (118121.7725)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
Epoch: [17]  [ 510/1349]  eta: 0:04:25  lr: 0.000412  min_lr: 0.000010  loss: 0.9393 (0.8298)  loss_scale: 131072.0000 (118375.2016)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [17]  [ 520/1349]  eta: 0:04:22  lr: 0.000412  min_lr: 0.000010  loss: 0.8672 (0.8291)  loss_scale: 131072.0000 (118618.9021)  weight_decay: 0.0500 (0.0500)  time: 0.3081  data: 0.0001  max mem: 41808
Epoch: [17]  [ 530/1349]  eta: 0:04:19  lr: 0.000412  min_lr: 0.000010  loss: 0.7934 (0.8278)  loss_scale: 131072.0000 (118853.4237)  weight_decay: 0.0500 (0.0500)  time: 0.3082  data: 0.0001  max mem: 41808
Epoch: [17]  [ 540/1349]  eta: 0:04:15  lr: 0.000412  min_lr: 0.000010  loss: 0.7243 (0.8267)  loss_scale: 131072.0000 (119079.2754)  weight_decay: 0.0500 (0.0500)  time: 0.3078  data: 0.0001  max mem: 41808
Epoch: [17]  [ 550/1349]  eta: 0:04:12  lr: 0.000412  min_lr: 0.000010  loss: 0.7386 (0.8266)  loss_scale: 131072.0000 (119296.9292)  weight_decay: 0.0500 (0.0500)  time: 0.3078  data: 0.0001  max mem: 41808
Epoch: [17]  [ 560/1349]  eta: 0:04:09  lr: 0.000412  min_lr: 0.000010  loss: 0.7862 (0.8270)  loss_scale: 131072.0000 (119506.8235)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0001  max mem: 41808
Epoch: [17]  [ 570/1349]  eta: 0:04:05  lr: 0.000412  min_lr: 0.000010  loss: 0.7941 (0.8256)  loss_scale: 131072.0000 (119709.3660)  weight_decay: 0.0500 (0.0500)  time: 0.3078  data: 0.0001  max mem: 41808
Epoch: [17]  [ 580/1349]  eta: 0:04:02  lr: 0.000412  min_lr: 0.000010  loss: 0.7941 (0.8250)  loss_scale: 131072.0000 (119904.9363)  weight_decay: 0.0500 (0.0500)  time: 0.3081  data: 0.0001  max mem: 41808
[2025-05-23 18:34:41,510] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 23518
[2025-05-23 18:34:41,510] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 23518
[2025-05-23 18:34:41,510] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 18:34:41,510] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 18:34:41,510] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [17]  [ 590/1349]  eta: 0:03:59  lr: 0.000412  min_lr: 0.000010  loss: 0.7973 (0.8243)  loss_scale: 131072.0000 (119428.5482)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
Epoch: [17]  [ 600/1349]  eta: 0:03:56  lr: 0.000412  min_lr: 0.000010  loss: 0.8611 (0.8257)  loss_scale: 65536.0000 (118531.8336)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
Epoch: [17]  [ 610/1349]  eta: 0:03:52  lr: 0.000412  min_lr: 0.000010  loss: 0.9255 (0.8271)  loss_scale: 65536.0000 (117664.4714)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [17]  [ 620/1349]  eta: 0:03:49  lr: 0.000411  min_lr: 0.000010  loss: 0.9240 (0.8281)  loss_scale: 65536.0000 (116825.0435)  weight_decay: 0.0500 (0.0500)  time: 0.3080  data: 0.0002  max mem: 41808
Epoch: [17]  [ 630/1349]  eta: 0:03:46  lr: 0.000411  min_lr: 0.000010  loss: 0.8622 (0.8283)  loss_scale: 65536.0000 (116012.2219)  weight_decay: 0.0500 (0.0500)  time: 0.3085  data: 0.0002  max mem: 41808
Epoch: [17]  [ 640/1349]  eta: 0:03:43  lr: 0.000411  min_lr: 0.000010  loss: 0.8543 (0.8280)  loss_scale: 65536.0000 (115224.7613)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [17]  [ 650/1349]  eta: 0:03:40  lr: 0.000411  min_lr: 0.000010  loss: 0.8760 (0.8287)  loss_scale: 65536.0000 (114461.4931)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
Epoch: [17]  [ 660/1349]  eta: 0:03:36  lr: 0.000411  min_lr: 0.000010  loss: 0.8760 (0.8285)  loss_scale: 65536.0000 (113721.3192)  weight_decay: 0.0500 (0.0500)  time: 0.3087  data: 0.0001  max mem: 41808
Epoch: [17]  [ 670/1349]  eta: 0:03:33  lr: 0.000411  min_lr: 0.000010  loss: 0.8241 (0.8282)  loss_scale: 65536.0000 (113003.2072)  weight_decay: 0.0500 (0.0500)  time: 0.3085  data: 0.0001  max mem: 41808
Epoch: [17]  [ 680/1349]  eta: 0:03:30  lr: 0.000411  min_lr: 0.000010  loss: 0.8153 (0.8277)  loss_scale: 65536.0000 (112306.1850)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
Epoch: [17]  [ 690/1349]  eta: 0:03:27  lr: 0.000411  min_lr: 0.000010  loss: 0.8165 (0.8285)  loss_scale: 65536.0000 (111629.3372)  weight_decay: 0.0500 (0.0500)  time: 0.3082  data: 0.0001  max mem: 41808
Epoch: [17]  [ 700/1349]  eta: 0:03:23  lr: 0.000411  min_lr: 0.000010  loss: 0.8254 (0.8281)  loss_scale: 65536.0000 (110971.8003)  weight_decay: 0.0500 (0.0500)  time: 0.3079  data: 0.0001  max mem: 41808
Epoch: [17]  [ 710/1349]  eta: 0:03:20  lr: 0.000411  min_lr: 0.000010  loss: 0.8376 (0.8289)  loss_scale: 65536.0000 (110332.7595)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
[2025-05-23 18:35:21,212] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 18:35:21,212] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-05-23 18:35:21,212] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 18:35:21,212] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [17]  [ 720/1349]  eta: 0:03:17  lr: 0.000410  min_lr: 0.000010  loss: 0.8585 (0.8292)  loss_scale: 65536.0000 (110347.7171)  weight_decay: 0.0500 (0.0500)  time: 0.3063  data: 0.0002  max mem: 41808
Epoch: [17]  [ 730/1349]  eta: 0:03:14  lr: 0.000410  min_lr: 0.000010  loss: 0.8336 (0.8286)  loss_scale: 131072.0000 (110631.2230)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0001  max mem: 41808
Epoch: [17]  [ 740/1349]  eta: 0:03:11  lr: 0.000410  min_lr: 0.000010  loss: 0.8706 (0.8291)  loss_scale: 131072.0000 (110907.0769)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [17]  [ 750/1349]  eta: 0:03:07  lr: 0.000410  min_lr: 0.000010  loss: 0.8089 (0.8274)  loss_scale: 131072.0000 (111175.5846)  weight_decay: 0.0500 (0.0500)  time: 0.3059  data: 0.0001  max mem: 41808
Epoch: [17]  [ 760/1349]  eta: 0:03:04  lr: 0.000410  min_lr: 0.000010  loss: 0.7727 (0.8272)  loss_scale: 131072.0000 (111437.0355)  weight_decay: 0.0500 (0.0500)  time: 0.3061  data: 0.0001  max mem: 41808
Epoch: [17]  [ 770/1349]  eta: 0:03:01  lr: 0.000410  min_lr: 0.000010  loss: 0.8186 (0.8278)  loss_scale: 131072.0000 (111691.7043)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0002  max mem: 41808
Epoch: [17]  [ 780/1349]  eta: 0:02:58  lr: 0.000410  min_lr: 0.000010  loss: 0.7394 (0.8261)  loss_scale: 131072.0000 (111939.8515)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0002  max mem: 41808
Epoch: [17]  [ 790/1349]  eta: 0:02:55  lr: 0.000410  min_lr: 0.000010  loss: 0.7394 (0.8259)  loss_scale: 131072.0000 (112181.7244)  weight_decay: 0.0500 (0.0500)  time: 0.3063  data: 0.0001  max mem: 41808
Epoch: [17]  [ 800/1349]  eta: 0:02:52  lr: 0.000410  min_lr: 0.000010  loss: 0.8115 (0.8253)  loss_scale: 131072.0000 (112417.5581)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [17]  [ 810/1349]  eta: 0:02:48  lr: 0.000410  min_lr: 0.000010  loss: 0.8438 (0.8256)  loss_scale: 131072.0000 (112647.5758)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0002  max mem: 41808
Epoch: [17]  [ 820/1349]  eta: 0:02:45  lr: 0.000409  min_lr: 0.000010  loss: 0.9368 (0.8265)  loss_scale: 131072.0000 (112871.9903)  weight_decay: 0.0500 (0.0500)  time: 0.3081  data: 0.0001  max mem: 41808
Epoch: [17]  [ 830/1349]  eta: 0:02:42  lr: 0.000409  min_lr: 0.000010  loss: 0.8895 (0.8270)  loss_scale: 131072.0000 (113091.0036)  weight_decay: 0.0500 (0.0500)  time: 0.3087  data: 0.0001  max mem: 41808
Epoch: [17]  [ 840/1349]  eta: 0:02:39  lr: 0.000409  min_lr: 0.000010  loss: 0.8863 (0.8268)  loss_scale: 131072.0000 (113304.8086)  weight_decay: 0.0500 (0.0500)  time: 0.3082  data: 0.0001  max mem: 41808
[2025-05-23 18:36:00,565] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 18:36:00,565] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 18:36:00,565] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 18:36:00,565] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 18:36:02,102] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 23780
[2025-05-23 18:36:02,102] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 23780
[2025-05-23 18:36:02,102] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 18:36:02,103] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 18:36:02,103] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [17]  [ 850/1349]  eta: 0:02:36  lr: 0.000409  min_lr: 0.000010  loss: 0.8682 (0.8269)  loss_scale: 131072.0000 (114283.6945)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0001  max mem: 41808
Epoch: [17]  [ 860/1349]  eta: 0:02:33  lr: 0.000409  min_lr: 0.000010  loss: 0.8565 (0.8273)  loss_scale: 131072.0000 (114478.6806)  weight_decay: 0.0500 (0.0500)  time: 0.3084  data: 0.0001  max mem: 41808
Epoch: [17]  [ 870/1349]  eta: 0:02:29  lr: 0.000409  min_lr: 0.000010  loss: 0.8557 (0.8279)  loss_scale: 131072.0000 (114669.1894)  weight_decay: 0.0500 (0.0500)  time: 0.3089  data: 0.0002  max mem: 41808
Epoch: [17]  [ 880/1349]  eta: 0:02:26  lr: 0.000409  min_lr: 0.000010  loss: 0.8932 (0.8281)  loss_scale: 131072.0000 (114855.3734)  weight_decay: 0.0500 (0.0500)  time: 0.3096  data: 0.0001  max mem: 41808
Epoch: [17]  [ 890/1349]  eta: 0:02:23  lr: 0.000409  min_lr: 0.000010  loss: 0.8932 (0.8285)  loss_scale: 131072.0000 (115037.3782)  weight_decay: 0.0500 (0.0500)  time: 0.3094  data: 0.0001  max mem: 41808
Epoch: [17]  [ 900/1349]  eta: 0:02:20  lr: 0.000409  min_lr: 0.000010  loss: 0.9154 (0.8290)  loss_scale: 131072.0000 (115215.3430)  weight_decay: 0.0500 (0.0500)  time: 0.3080  data: 0.0001  max mem: 41808
Epoch: [17]  [ 910/1349]  eta: 0:02:17  lr: 0.000409  min_lr: 0.000010  loss: 0.9096 (0.8292)  loss_scale: 131072.0000 (115389.4007)  weight_decay: 0.0500 (0.0500)  time: 0.3083  data: 0.0001  max mem: 41808
Epoch: [17]  [ 920/1349]  eta: 0:02:14  lr: 0.000408  min_lr: 0.000010  loss: 0.8191 (0.8289)  loss_scale: 131072.0000 (115559.6786)  weight_decay: 0.0500 (0.0500)  time: 0.3091  data: 0.0001  max mem: 41808
Epoch: [17]  [ 930/1349]  eta: 0:02:11  lr: 0.000408  min_lr: 0.000010  loss: 0.8335 (0.8287)  loss_scale: 131072.0000 (115726.2986)  weight_decay: 0.0500 (0.0500)  time: 0.3090  data: 0.0002  max mem: 41808
Epoch: [17]  [ 940/1349]  eta: 0:02:07  lr: 0.000408  min_lr: 0.000010  loss: 0.8646 (0.8293)  loss_scale: 131072.0000 (115889.3773)  weight_decay: 0.0500 (0.0500)  time: 0.3079  data: 0.0002  max mem: 41808
Epoch: [17]  [ 950/1349]  eta: 0:02:04  lr: 0.000408  min_lr: 0.000010  loss: 0.8058 (0.8284)  loss_scale: 131072.0000 (116049.0263)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0002  max mem: 41808
[2025-05-23 18:36:36,345] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 23891
[2025-05-23 18:36:36,345] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 23891
[2025-05-23 18:36:36,345] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 18:36:36,345] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 18:36:36,345] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [17]  [ 960/1349]  eta: 0:02:01  lr: 0.000408  min_lr: 0.000010  loss: 0.7774 (0.8283)  loss_scale: 131072.0000 (116000.7659)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0002  max mem: 41808
Epoch: [17]  [ 970/1349]  eta: 0:01:58  lr: 0.000408  min_lr: 0.000010  loss: 0.8393 (0.8280)  loss_scale: 65536.0000 (115481.0463)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0002  max mem: 41808
Epoch: [17]  [ 980/1349]  eta: 0:01:55  lr: 0.000408  min_lr: 0.000010  loss: 0.8526 (0.8279)  loss_scale: 65536.0000 (114971.9225)  weight_decay: 0.0500 (0.0500)  time: 0.3079  data: 0.0001  max mem: 41808
Epoch: [17]  [ 990/1349]  eta: 0:01:52  lr: 0.000408  min_lr: 0.000010  loss: 0.7587 (0.8274)  loss_scale: 65536.0000 (114473.0737)  weight_decay: 0.0500 (0.0500)  time: 0.3081  data: 0.0001  max mem: 41808
Epoch: [17]  [1000/1349]  eta: 0:01:48  lr: 0.000408  min_lr: 0.000010  loss: 0.8515 (0.8278)  loss_scale: 65536.0000 (113984.1918)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
Epoch: [17]  [1010/1349]  eta: 0:01:45  lr: 0.000408  min_lr: 0.000010  loss: 0.8515 (0.8277)  loss_scale: 65536.0000 (113504.9812)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [17]  [1020/1349]  eta: 0:01:42  lr: 0.000407  min_lr: 0.000010  loss: 0.8535 (0.8280)  loss_scale: 65536.0000 (113035.1577)  weight_decay: 0.0500 (0.0500)  time: 0.3080  data: 0.0001  max mem: 41808
Epoch: [17]  [1030/1349]  eta: 0:01:39  lr: 0.000407  min_lr: 0.000010  loss: 0.8770 (0.8279)  loss_scale: 65536.0000 (112574.4481)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
Epoch: [17]  [1040/1349]  eta: 0:01:36  lr: 0.000407  min_lr: 0.000010  loss: 0.8548 (0.8274)  loss_scale: 65536.0000 (112122.5898)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [17]  [1050/1349]  eta: 0:01:33  lr: 0.000407  min_lr: 0.000010  loss: 0.8548 (0.8277)  loss_scale: 65536.0000 (111679.3302)  weight_decay: 0.0500 (0.0500)  time: 0.3080  data: 0.0001  max mem: 41808
Epoch: [17]  [1060/1349]  eta: 0:01:30  lr: 0.000407  min_lr: 0.000010  loss: 0.8486 (0.8274)  loss_scale: 65536.0000 (111244.4260)  weight_decay: 0.0500 (0.0500)  time: 0.3103  data: 0.0001  max mem: 41808
[2025-05-23 18:37:09,618] [INFO] [logging.py:96:log_dist] [Rank 0] step=24000, skipped=145, lr=[9.668504293010728e-06, 9.668504293010728e-06, 1.2891339057347637e-05, 1.2891339057347637e-05, 1.7188452076463516e-05, 1.7188452076463516e-05, 2.2917936101951353e-05, 2.2917936101951353e-05, 3.055724813593514e-05, 3.055724813593514e-05, 4.0742997514580184e-05, 4.0742997514580184e-05, 5.432399668610692e-05, 5.432399668610692e-05, 7.243199558147589e-05, 7.243199558147589e-05, 9.657599410863451e-05, 9.657599410863451e-05, 0.00012876799214484602, 0.00012876799214484602, 0.00017169065619312802, 0.00017169065619312802, 0.0002289208749241707, 0.0002289208749241707, 0.0003052278332322276, 0.0003052278332322276, 0.0004069704443096368, 0.0004069704443096368], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-05-23 18:37:09,618] [INFO] [timer.py:260:stop] epoch=0/micro_step=24000/global_step=24000, RunningAvgSamplesPerSec=207.41729111802434, CurrSamplesPerSec=214.22428650549296, MemAllocated=1.37GB, MaxMemAllocated=40.83GB
Epoch: [17]  [1070/1349]  eta: 0:01:27  lr: 0.000407  min_lr: 0.000010  loss: 0.8432 (0.8279)  loss_scale: 65536.0000 (110817.6433)  weight_decay: 0.0500 (0.0500)  time: 0.3086  data: 0.0001  max mem: 41808
Epoch: [17]  [1080/1349]  eta: 0:01:23  lr: 0.000407  min_lr: 0.000010  loss: 0.8271 (0.8278)  loss_scale: 65536.0000 (110398.7567)  weight_decay: 0.0500 (0.0500)  time: 0.3078  data: 0.0001  max mem: 41808
[2025-05-23 18:37:16,113] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 18:37:16,113] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-05-23 18:37:16,113] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 18:37:16,113] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [17]  [1090/1349]  eta: 0:01:20  lr: 0.000407  min_lr: 0.000010  loss: 0.8597 (0.8287)  loss_scale: 65536.0000 (110227.8277)  weight_decay: 0.0500 (0.0500)  time: 0.3098  data: 0.0001  max mem: 41808
Epoch: [17]  [1100/1349]  eta: 0:01:17  lr: 0.000407  min_lr: 0.000010  loss: 0.8484 (0.8275)  loss_scale: 131072.0000 (110417.1480)  weight_decay: 0.0500 (0.0500)  time: 0.3083  data: 0.0001  max mem: 41808
[2025-05-23 18:37:20,722] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 24035
[2025-05-23 18:37:20,722] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 18:37:20,722] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 24035
[2025-05-23 18:37:20,722] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 18:37:20,722] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [17]  [1110/1349]  eta: 0:01:14  lr: 0.000407  min_lr: 0.000010  loss: 0.7442 (0.8273)  loss_scale: 131072.0000 (110072.1656)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
Epoch: [17]  [1120/1349]  eta: 0:01:11  lr: 0.000406  min_lr: 0.000010  loss: 0.8321 (0.8278)  loss_scale: 65536.0000 (109674.8760)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0002  max mem: 41808
Epoch: [17]  [1130/1349]  eta: 0:01:08  lr: 0.000406  min_lr: 0.000010  loss: 0.9021 (0.8283)  loss_scale: 65536.0000 (109284.6118)  weight_decay: 0.0500 (0.0500)  time: 0.3086  data: 0.0002  max mem: 41808
Epoch: [17]  [1140/1349]  eta: 0:01:05  lr: 0.000406  min_lr: 0.000010  loss: 0.8484 (0.8279)  loss_scale: 65536.0000 (108901.1884)  weight_decay: 0.0500 (0.0500)  time: 0.3085  data: 0.0002  max mem: 41808
Epoch: [17]  [1150/1349]  eta: 0:01:02  lr: 0.000406  min_lr: 0.000010  loss: 0.8025 (0.8282)  loss_scale: 65536.0000 (108524.4275)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [17]  [1160/1349]  eta: 0:00:58  lr: 0.000406  min_lr: 0.000010  loss: 0.8347 (0.8280)  loss_scale: 65536.0000 (108154.1568)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
Epoch: [17]  [1170/1349]  eta: 0:00:55  lr: 0.000406  min_lr: 0.000010  loss: 0.8244 (0.8275)  loss_scale: 65536.0000 (107790.2101)  weight_decay: 0.0500 (0.0500)  time: 0.3092  data: 0.0001  max mem: 41808
Epoch: [17]  [1180/1349]  eta: 0:00:52  lr: 0.000406  min_lr: 0.000010  loss: 0.8892 (0.8285)  loss_scale: 65536.0000 (107432.4268)  weight_decay: 0.0500 (0.0500)  time: 0.3091  data: 0.0001  max mem: 41808
Epoch: [17]  [1190/1349]  eta: 0:00:49  lr: 0.000406  min_lr: 0.000010  loss: 0.9172 (0.8286)  loss_scale: 65536.0000 (107080.6516)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
Epoch: [17]  [1200/1349]  eta: 0:00:46  lr: 0.000406  min_lr: 0.000010  loss: 0.7731 (0.8281)  loss_scale: 65536.0000 (106734.7344)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [17]  [1210/1349]  eta: 0:00:43  lr: 0.000406  min_lr: 0.000010  loss: 0.7606 (0.8280)  loss_scale: 65536.0000 (106394.5301)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [17]  [1220/1349]  eta: 0:00:40  lr: 0.000405  min_lr: 0.000010  loss: 0.8684 (0.8279)  loss_scale: 65536.0000 (106059.8984)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
Epoch: [17]  [1230/1349]  eta: 0:00:37  lr: 0.000405  min_lr: 0.000010  loss: 0.7708 (0.8275)  loss_scale: 65536.0000 (105730.7035)  weight_decay: 0.0500 (0.0500)  time: 0.3078  data: 0.0001  max mem: 41808
[2025-05-23 18:38:00,453] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 18:38:00,454] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-05-23 18:38:00,453] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 18:38:00,454] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-05-23 18:38:02,594] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 24171
[2025-05-23 18:38:02,594] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 24171
[2025-05-23 18:38:02,594] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 18:38:02,594] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 18:38:02,594] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [17]  [1240/1349]  eta: 0:00:33  lr: 0.000405  min_lr: 0.000010  loss: 0.8339 (0.8275)  loss_scale: 65536.0000 (105776.4770)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
Epoch: [17]  [1250/1349]  eta: 0:00:30  lr: 0.000405  min_lr: 0.000010  loss: 0.8709 (0.8274)  loss_scale: 65536.0000 (105454.8106)  weight_decay: 0.0500 (0.0500)  time: 0.3060  data: 0.0001  max mem: 41808
Epoch: [17]  [1260/1349]  eta: 0:00:27  lr: 0.000405  min_lr: 0.000010  loss: 0.8357 (0.8272)  loss_scale: 65536.0000 (105138.2458)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [17]  [1270/1349]  eta: 0:00:24  lr: 0.000405  min_lr: 0.000010  loss: 0.8091 (0.8270)  loss_scale: 65536.0000 (104826.6625)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
Epoch: [17]  [1280/1349]  eta: 0:00:21  lr: 0.000405  min_lr: 0.000010  loss: 0.8212 (0.8269)  loss_scale: 65536.0000 (104519.9438)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [17]  [1290/1349]  eta: 0:00:18  lr: 0.000405  min_lr: 0.000010  loss: 0.8117 (0.8266)  loss_scale: 65536.0000 (104217.9768)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0001  max mem: 41808
Epoch: [17]  [1300/1349]  eta: 0:00:15  lr: 0.000405  min_lr: 0.000010  loss: 0.7194 (0.8265)  loss_scale: 65536.0000 (103920.6518)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
Epoch: [17]  [1310/1349]  eta: 0:00:12  lr: 0.000405  min_lr: 0.000010  loss: 0.7308 (0.8264)  loss_scale: 65536.0000 (103627.8627)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
Epoch: [17]  [1320/1349]  eta: 0:00:09  lr: 0.000404  min_lr: 0.000010  loss: 0.8240 (0.8265)  loss_scale: 65536.0000 (103339.5064)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
Epoch: [17]  [1330/1349]  eta: 0:00:05  lr: 0.000404  min_lr: 0.000010  loss: 0.8879 (0.8269)  loss_scale: 65536.0000 (103055.4831)  weight_decay: 0.0500 (0.0500)  time: 0.3047  data: 0.0001  max mem: 41808
Epoch: [17]  [1340/1349]  eta: 0:00:02  lr: 0.000404  min_lr: 0.000010  loss: 0.8879 (0.8267)  loss_scale: 65536.0000 (102775.6957)  weight_decay: 0.0500 (0.0500)  time: 0.3025  data: 0.0001  max mem: 41808
Epoch: [17]  [1348/1349]  eta: 0:00:00  lr: 0.000404  min_lr: 0.000010  loss: 0.8045 (0.8265)  loss_scale: 65536.0000 (102554.8525)  weight_decay: 0.0500 (0.0500)  time: 0.3020  data: 0.0001  max mem: 41808
Epoch: [17] Total time: 0:07:00 (0.3114 s / it)
Averaged stats: lr: 0.000404  min_lr: 0.000010  loss: 0.8045 (0.8265)  loss_scale: 65536.0000 (102554.8525)  weight_decay: 0.0500 (0.0500)  total_time: 420.0342 (420.0303)
Val:  [  0/346]  eta: 1:27:00  loss: 3.4289 (3.4289)  acc1: 0.0000 (0.0000)  acc5: 65.6250 (65.6250)  time: 15.0872  data: 14.2076  max mem: 41808
Val:  [ 10/346]  eta: 0:11:45  loss: 0.1275 (0.6020)  acc1: 100.0000 (85.0142)  acc5: 100.0000 (96.8750)  time: 2.0998  data: 1.2919  max mem: 41808
Val:  [ 20/346]  eta: 0:07:57  loss: 0.1255 (0.4570)  acc1: 100.0000 (89.0997)  acc5: 100.0000 (98.3631)  time: 0.7842  data: 0.0003  max mem: 41808
Val:  [ 30/346]  eta: 0:06:34  loss: 0.1165 (0.3848)  acc1: 100.0000 (91.1542)  acc5: 100.0000 (98.8911)  time: 0.7807  data: 0.0070  max mem: 41808
Val:  [ 40/346]  eta: 0:05:43  loss: 0.1208 (0.4295)  acc1: 100.0000 (89.8628)  acc5: 100.0000 (99.0473)  time: 0.7626  data: 0.0070  max mem: 41808
Val:  [ 50/346]  eta: 0:05:12  loss: 0.1215 (0.3817)  acc1: 99.2188 (91.3143)  acc5: 100.0000 (99.2341)  time: 0.7535  data: 0.0136  max mem: 41808
Val:  [ 60/346]  eta: 0:04:53  loss: 0.1681 (0.3710)  acc1: 96.0938 (91.3934)  acc5: 100.0000 (99.3596)  time: 0.8309  data: 0.0856  max mem: 41808
Val:  [ 70/346]  eta: 0:04:36  loss: 0.2228 (0.3895)  acc1: 94.5312 (90.7460)  acc5: 100.0000 (99.4498)  time: 0.8682  data: 0.1410  max mem: 41808
Val:  [ 80/346]  eta: 0:04:22  loss: 0.2626 (0.3944)  acc1: 86.7188 (90.5093)  acc5: 100.0000 (99.5177)  time: 0.8592  data: 0.1410  max mem: 41808
Val:  [ 90/346]  eta: 0:04:10  loss: 0.3308 (0.3938)  acc1: 87.5000 (90.4104)  acc5: 100.0000 (99.5536)  time: 0.8862  data: 0.1423  max mem: 41808
Val:  [100/346]  eta: 0:03:57  loss: 0.2117 (0.3720)  acc1: 97.6562 (91.2515)  acc5: 100.0000 (99.5978)  time: 0.8901  data: 0.1325  max mem: 41808
Val:  [110/346]  eta: 0:03:46  loss: 0.1942 (0.3845)  acc1: 98.4375 (90.9417)  acc5: 100.0000 (99.4510)  time: 0.8875  data: 0.1344  max mem: 41808
Val:  [120/346]  eta: 0:03:35  loss: 0.1937 (0.3808)  acc1: 96.8750 (90.9349)  acc5: 100.0000 (99.4899)  time: 0.8876  data: 0.1463  max mem: 41808
Val:  [130/346]  eta: 0:03:25  loss: 0.1352 (0.3942)  acc1: 97.6562 (90.3447)  acc5: 100.0000 (99.5110)  time: 0.8887  data: 0.1425  max mem: 41808
Val:  [140/346]  eta: 0:03:15  loss: 0.2008 (0.3969)  acc1: 96.0938 (90.2593)  acc5: 100.0000 (99.5401)  time: 0.9072  data: 0.1442  max mem: 41808
Val:  [150/346]  eta: 0:03:04  loss: 0.3338 (0.3971)  acc1: 92.9688 (90.3404)  acc5: 100.0000 (99.5550)  time: 0.9004  data: 0.1399  max mem: 41808
Val:  [160/346]  eta: 0:02:54  loss: 0.2284 (0.3892)  acc1: 93.7500 (90.5425)  acc5: 100.0000 (99.5827)  time: 0.8857  data: 0.1368  max mem: 41808
Val:  [170/346]  eta: 0:02:44  loss: 0.1410 (0.3848)  acc1: 96.0938 (90.5793)  acc5: 100.0000 (99.6071)  time: 0.8861  data: 0.1533  max mem: 41808
Val:  [180/346]  eta: 0:02:35  loss: 0.3023 (0.3936)  acc1: 90.6250 (90.0552)  acc5: 100.0000 (99.6288)  time: 0.8941  data: 0.1584  max mem: 41808
Val:  [190/346]  eta: 0:02:25  loss: 0.2878 (0.3936)  acc1: 90.6250 (90.0646)  acc5: 100.0000 (99.6482)  time: 0.9036  data: 0.1531  max mem: 41808
Val:  [200/346]  eta: 0:02:16  loss: 0.2886 (0.4062)  acc1: 89.8438 (89.5756)  acc5: 100.0000 (99.6308)  time: 0.9060  data: 0.1627  max mem: 41808
Val:  [210/346]  eta: 0:02:06  loss: 0.2711 (0.3996)  acc1: 90.6250 (89.8104)  acc5: 100.0000 (99.6483)  time: 0.9132  data: 0.1637  max mem: 41808
Val:  [220/346]  eta: 0:01:57  loss: 0.2018 (0.3925)  acc1: 95.3125 (90.0771)  acc5: 100.0000 (99.6571)  time: 0.9229  data: 0.1540  max mem: 41808
Val:  [230/346]  eta: 0:01:47  loss: 0.1382 (0.3830)  acc1: 99.2188 (90.4288)  acc5: 100.0000 (99.6719)  time: 0.9244  data: 0.1597  max mem: 41808
Val:  [240/346]  eta: 0:01:38  loss: 0.1664 (0.3909)  acc1: 98.4375 (90.2717)  acc5: 100.0000 (99.6726)  time: 0.9177  data: 0.1658  max mem: 41808
Val:  [250/346]  eta: 0:01:29  loss: 0.1654 (0.3860)  acc1: 97.6562 (90.4165)  acc5: 100.0000 (99.6856)  time: 0.9038  data: 0.1604  max mem: 41808
Val:  [260/346]  eta: 0:01:19  loss: 0.1460 (0.3839)  acc1: 99.2188 (90.4963)  acc5: 100.0000 (99.6618)  time: 0.9238  data: 0.1503  max mem: 41808
Val:  [270/346]  eta: 0:01:10  loss: 0.1460 (0.3792)  acc1: 98.4375 (90.6596)  acc5: 100.0000 (99.6541)  time: 0.9494  data: 0.1576  max mem: 41808
Val:  [280/346]  eta: 0:01:01  loss: 0.1205 (0.3775)  acc1: 98.4375 (90.7362)  acc5: 100.0000 (99.6580)  time: 0.9244  data: 0.1621  max mem: 41808
Val:  [290/346]  eta: 0:00:52  loss: 0.1083 (0.3685)  acc1: 100.0000 (91.0304)  acc5: 100.0000 (99.6698)  time: 0.9130  data: 0.1575  max mem: 41808
Val:  [300/346]  eta: 0:00:42  loss: 0.1090 (0.3679)  acc1: 100.0000 (91.0922)  acc5: 100.0000 (99.6730)  time: 0.9123  data: 0.1545  max mem: 41808
Val:  [310/346]  eta: 0:00:33  loss: 0.1195 (0.3694)  acc1: 100.0000 (91.0144)  acc5: 100.0000 (99.6659)  time: 0.9145  data: 0.1483  max mem: 41808
Val:  [320/346]  eta: 0:00:24  loss: 0.1214 (0.3689)  acc1: 100.0000 (91.0047)  acc5: 100.0000 (99.6763)  time: 0.9000  data: 0.1544  max mem: 41808
Val:  [330/346]  eta: 0:00:14  loss: 0.2904 (0.3778)  acc1: 89.8438 (90.7359)  acc5: 100.0000 (99.6648)  time: 0.8901  data: 0.1614  max mem: 41808
Val:  [340/346]  eta: 0:00:05  loss: 0.4628 (0.3853)  acc1: 85.9375 (90.5104)  acc5: 100.0000 (99.6747)  time: 0.8933  data: 0.1669  max mem: 41808
Val:  [345/346]  eta: 0:00:00  loss: 0.1679 (0.3816)  acc1: 96.5517 (90.6276)  acc5: 100.0000 (99.6791)  time: 0.8944  data: 0.1844  max mem: 41808
Val: Total time: 0:05:20 (0.9254 s / it)
* Acc@1 90.734 Acc@5 99.652 loss 0.379
Accuracy of the network on the 88494 val videos: 90.7%
Max accuracy: 91.25%   Max Epoch: 3
Epoch: [18]  [   0/1349]  eta: 1:48:58  lr: 0.000404  min_lr: 0.000010  loss: 0.9581 (0.9581)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 4.8471  data: 4.5023  max mem: 41808
Epoch: [18]  [  10/1349]  eta: 0:17:01  lr: 0.000404  min_lr: 0.000010  loss: 0.9250 (0.9272)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7631  data: 0.4094  max mem: 41808
[2025-05-23 18:44:07,607] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 18:44:07,607] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 18:44:07,608] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-05-23 18:44:07,608] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [18]  [  20/1349]  eta: 0:12:07  lr: 0.000404  min_lr: 0.000010  loss: 0.9083 (0.8921)  loss_scale: 65536.0000 (74898.2857)  weight_decay: 0.0500 (0.0500)  time: 0.3323  data: 0.0001  max mem: 41808
Epoch: [18]  [  30/1349]  eta: 0:10:20  lr: 0.000404  min_lr: 0.000010  loss: 0.9083 (0.8700)  loss_scale: 131072.0000 (93018.8387)  weight_decay: 0.0500 (0.0500)  time: 0.3090  data: 0.0001  max mem: 41808
Epoch: [18]  [  40/1349]  eta: 0:09:23  lr: 0.000404  min_lr: 0.000010  loss: 0.8220 (0.8656)  loss_scale: 131072.0000 (102300.0976)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0002  max mem: 41808
Epoch: [18]  [  50/1349]  eta: 0:08:48  lr: 0.000404  min_lr: 0.000010  loss: 0.8386 (0.8641)  loss_scale: 131072.0000 (107941.6471)  weight_decay: 0.0500 (0.0500)  time: 0.3084  data: 0.0003  max mem: 41808
Epoch: [18]  [  60/1349]  eta: 0:08:23  lr: 0.000403  min_lr: 0.000010  loss: 0.8106 (0.8511)  loss_scale: 131072.0000 (111733.5082)  weight_decay: 0.0500 (0.0500)  time: 0.3082  data: 0.0002  max mem: 41808
Epoch: [18]  [  70/1349]  eta: 0:08:04  lr: 0.000403  min_lr: 0.000010  loss: 0.8807 (0.8573)  loss_scale: 131072.0000 (114457.2394)  weight_decay: 0.0500 (0.0500)  time: 0.3064  data: 0.0002  max mem: 41808
Epoch: [18]  [  80/1349]  eta: 0:07:49  lr: 0.000403  min_lr: 0.000010  loss: 0.8807 (0.8434)  loss_scale: 131072.0000 (116508.4444)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0002  max mem: 41808
Epoch: [18]  [  90/1349]  eta: 0:07:36  lr: 0.000403  min_lr: 0.000010  loss: 0.8510 (0.8473)  loss_scale: 131072.0000 (118108.8352)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
Epoch: [18]  [ 100/1349]  eta: 0:07:26  lr: 0.000403  min_lr: 0.000010  loss: 0.8893 (0.8502)  loss_scale: 131072.0000 (119392.3168)  weight_decay: 0.0500 (0.0500)  time: 0.3078  data: 0.0001  max mem: 41808
Epoch: [18]  [ 110/1349]  eta: 0:07:17  lr: 0.000403  min_lr: 0.000010  loss: 0.8628 (0.8468)  loss_scale: 131072.0000 (120444.5405)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0002  max mem: 41808
Epoch: [18]  [ 120/1349]  eta: 0:07:08  lr: 0.000403  min_lr: 0.000010  loss: 0.7835 (0.8398)  loss_scale: 131072.0000 (121322.8430)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0003  max mem: 41808
Epoch: [18]  [ 130/1349]  eta: 0:07:01  lr: 0.000403  min_lr: 0.000010  loss: 0.7941 (0.8372)  loss_scale: 131072.0000 (122067.0534)  weight_decay: 0.0500 (0.0500)  time: 0.3066  data: 0.0003  max mem: 41808
Epoch: [18]  [ 140/1349]  eta: 0:06:54  lr: 0.000403  min_lr: 0.000010  loss: 0.8221 (0.8339)  loss_scale: 131072.0000 (122705.7021)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0003  max mem: 41808
[2025-05-23 18:44:46,972] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 18:44:46,972] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 18:44:46,972] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 18:44:46,972] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 18:44:48,198] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 24432
[2025-05-23 18:44:48,198] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 24432
[2025-05-23 18:44:48,198] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 18:44:48,198] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 18:44:48,198] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [18]  [ 150/1349]  eta: 0:06:48  lr: 0.000403  min_lr: 0.000010  loss: 0.8596 (0.8371)  loss_scale: 131072.0000 (126731.8675)  weight_decay: 0.0500 (0.0500)  time: 0.3063  data: 0.0002  max mem: 41808
Epoch: [18]  [ 160/1349]  eta: 0:06:42  lr: 0.000402  min_lr: 0.000010  loss: 0.8826 (0.8398)  loss_scale: 131072.0000 (127001.4410)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0002  max mem: 41808
Epoch: [18]  [ 170/1349]  eta: 0:06:36  lr: 0.000402  min_lr: 0.000010  loss: 0.8587 (0.8369)  loss_scale: 131072.0000 (127239.4854)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0002  max mem: 41808
Epoch: [18]  [ 180/1349]  eta: 0:06:31  lr: 0.000402  min_lr: 0.000010  loss: 0.8481 (0.8353)  loss_scale: 131072.0000 (127451.2265)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0002  max mem: 41808
Epoch: [18]  [ 190/1349]  eta: 0:06:26  lr: 0.000402  min_lr: 0.000010  loss: 0.8481 (0.8353)  loss_scale: 131072.0000 (127640.7958)  weight_decay: 0.0500 (0.0500)  time: 0.3090  data: 0.0002  max mem: 41808
Epoch: [18]  [ 200/1349]  eta: 0:06:22  lr: 0.000402  min_lr: 0.000010  loss: 0.8364 (0.8341)  loss_scale: 131072.0000 (127811.5025)  weight_decay: 0.0500 (0.0500)  time: 0.3089  data: 0.0002  max mem: 41808
Epoch: [18]  [ 210/1349]  eta: 0:06:17  lr: 0.000402  min_lr: 0.000010  loss: 0.8364 (0.8356)  loss_scale: 131072.0000 (127966.0284)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
Epoch: [18]  [ 220/1349]  eta: 0:06:12  lr: 0.000402  min_lr: 0.000010  loss: 0.8830 (0.8362)  loss_scale: 131072.0000 (128106.5701)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [18]  [ 230/1349]  eta: 0:06:08  lr: 0.000402  min_lr: 0.000010  loss: 0.8258 (0.8368)  loss_scale: 131072.0000 (128234.9437)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [18]  [ 240/1349]  eta: 0:06:04  lr: 0.000402  min_lr: 0.000010  loss: 0.8564 (0.8381)  loss_scale: 131072.0000 (128352.6639)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [18]  [ 250/1349]  eta: 0:05:59  lr: 0.000402  min_lr: 0.000010  loss: 0.9094 (0.8426)  loss_scale: 131072.0000 (128461.0040)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [18]  [ 260/1349]  eta: 0:05:55  lr: 0.000401  min_lr: 0.000010  loss: 0.9069 (0.8392)  loss_scale: 131072.0000 (128561.0421)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
Epoch: [18]  [ 270/1349]  eta: 0:05:51  lr: 0.000401  min_lr: 0.000010  loss: 0.7955 (0.8380)  loss_scale: 131072.0000 (128653.6974)  weight_decay: 0.0500 (0.0500)  time: 0.3066  data: 0.0001  max mem: 41808
[2025-05-23 18:45:27,865] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 18:45:27,865] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 18:45:27,865] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 18:45:27,865] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [18]  [ 280/1349]  eta: 0:05:47  lr: 0.000401  min_lr: 0.000010  loss: 0.8370 (0.8388)  loss_scale: 131072.0000 (129672.6548)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
[2025-05-23 18:45:31,252] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 24572
[2025-05-23 18:45:31,252] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 24572
[2025-05-23 18:45:31,253] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 18:45:31,253] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 18:45:31,253] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [18]  [ 290/1349]  eta: 0:05:43  lr: 0.000401  min_lr: 0.000010  loss: 0.8370 (0.8371)  loss_scale: 262144.0000 (133774.5155)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [18]  [ 300/1349]  eta: 0:05:39  lr: 0.000401  min_lr: 0.000010  loss: 0.8047 (0.8360)  loss_scale: 131072.0000 (133684.7309)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
Epoch: [18]  [ 310/1349]  eta: 0:05:36  lr: 0.000401  min_lr: 0.000010  loss: 0.8085 (0.8355)  loss_scale: 131072.0000 (133600.7203)  weight_decay: 0.0500 (0.0500)  time: 0.3080  data: 0.0001  max mem: 41808
Epoch: [18]  [ 320/1349]  eta: 0:05:32  lr: 0.000401  min_lr: 0.000010  loss: 0.8100 (0.8349)  loss_scale: 131072.0000 (133521.9439)  weight_decay: 0.0500 (0.0500)  time: 0.3083  data: 0.0001  max mem: 41808
Epoch: [18]  [ 330/1349]  eta: 0:05:28  lr: 0.000401  min_lr: 0.000010  loss: 0.8042 (0.8342)  loss_scale: 131072.0000 (133447.9275)  weight_decay: 0.0500 (0.0500)  time: 0.3080  data: 0.0001  max mem: 41808
Epoch: [18]  [ 340/1349]  eta: 0:05:25  lr: 0.000401  min_lr: 0.000010  loss: 0.7935 (0.8321)  loss_scale: 131072.0000 (133378.2522)  weight_decay: 0.0500 (0.0500)  time: 0.3079  data: 0.0001  max mem: 41808
Epoch: [18]  [ 350/1349]  eta: 0:05:21  lr: 0.000401  min_lr: 0.000010  loss: 0.8562 (0.8348)  loss_scale: 131072.0000 (133312.5470)  weight_decay: 0.0500 (0.0500)  time: 0.3079  data: 0.0001  max mem: 41808
[2025-05-23 18:45:51,568] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 24638
[2025-05-23 18:45:51,569] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 18:45:51,569] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
[2025-05-23 18:45:51,569] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 24638
[2025-05-23 18:45:51,569] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
Epoch: [18]  [ 360/1349]  eta: 0:05:17  lr: 0.000400  min_lr: 0.000010  loss: 0.8885 (0.8353)  loss_scale: 131072.0000 (132342.7812)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [18]  [ 370/1349]  eta: 0:05:14  lr: 0.000400  min_lr: 0.000010  loss: 0.8487 (0.8351)  loss_scale: 65536.0000 (130542.0593)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
Epoch: [18]  [ 380/1349]  eta: 0:05:10  lr: 0.000400  min_lr: 0.000010  loss: 0.8487 (0.8324)  loss_scale: 65536.0000 (128835.8635)  weight_decay: 0.0500 (0.0500)  time: 0.3089  data: 0.0001  max mem: 41808
Epoch: [18]  [ 390/1349]  eta: 0:05:07  lr: 0.000400  min_lr: 0.000010  loss: 0.8128 (0.8324)  loss_scale: 65536.0000 (127216.9412)  weight_decay: 0.0500 (0.0500)  time: 0.3092  data: 0.0001  max mem: 41808
Epoch: [18]  [ 400/1349]  eta: 0:05:03  lr: 0.000400  min_lr: 0.000010  loss: 0.8256 (0.8340)  loss_scale: 65536.0000 (125678.7631)  weight_decay: 0.0500 (0.0500)  time: 0.3086  data: 0.0001  max mem: 41808
Epoch: [18]  [ 410/1349]  eta: 0:05:00  lr: 0.000400  min_lr: 0.000010  loss: 0.8442 (0.8346)  loss_scale: 65536.0000 (124215.4355)  weight_decay: 0.0500 (0.0500)  time: 0.3080  data: 0.0001  max mem: 41808
Epoch: [18]  [ 420/1349]  eta: 0:04:56  lr: 0.000400  min_lr: 0.000009  loss: 0.8312 (0.8352)  loss_scale: 65536.0000 (122821.6247)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
Epoch: [18]  [ 430/1349]  eta: 0:04:53  lr: 0.000400  min_lr: 0.000009  loss: 0.8312 (0.8341)  loss_scale: 65536.0000 (121492.4919)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
Epoch: [18]  [ 440/1349]  eta: 0:04:49  lr: 0.000400  min_lr: 0.000009  loss: 0.8117 (0.8333)  loss_scale: 65536.0000 (120223.6372)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0001  max mem: 41808
Epoch: [18]  [ 450/1349]  eta: 0:04:46  lr: 0.000399  min_lr: 0.000009  loss: 0.8117 (0.8321)  loss_scale: 65536.0000 (119011.0510)  weight_decay: 0.0500 (0.0500)  time: 0.3080  data: 0.0001  max mem: 41808
Epoch: [18]  [ 460/1349]  eta: 0:04:43  lr: 0.000399  min_lr: 0.000009  loss: 0.8240 (0.8330)  loss_scale: 65536.0000 (117851.0716)  weight_decay: 0.0500 (0.0500)  time: 0.3079  data: 0.0001  max mem: 41808
Epoch: [18]  [ 470/1349]  eta: 0:04:39  lr: 0.000399  min_lr: 0.000009  loss: 0.8627 (0.8322)  loss_scale: 65536.0000 (116740.3482)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
Epoch: [18]  [ 480/1349]  eta: 0:04:36  lr: 0.000399  min_lr: 0.000009  loss: 0.8329 (0.8307)  loss_scale: 65536.0000 (115675.8087)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
[2025-05-23 18:46:31,298] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 18:46:31,298] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-05-23 18:46:31,298] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 18:46:31,298] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [18]  [ 490/1349]  eta: 0:04:32  lr: 0.000399  min_lr: 0.000009  loss: 0.8552 (0.8311)  loss_scale: 65536.0000 (115455.4786)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
Epoch: [18]  [ 500/1349]  eta: 0:04:29  lr: 0.000399  min_lr: 0.000009  loss: 0.8735 (0.8319)  loss_scale: 131072.0000 (115767.1856)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
Epoch: [18]  [ 510/1349]  eta: 0:04:26  lr: 0.000399  min_lr: 0.000009  loss: 0.8771 (0.8318)  loss_scale: 131072.0000 (116066.6928)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [18]  [ 520/1349]  eta: 0:04:22  lr: 0.000399  min_lr: 0.000009  loss: 0.8286 (0.8317)  loss_scale: 131072.0000 (116354.7025)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [18]  [ 530/1349]  eta: 0:04:19  lr: 0.000399  min_lr: 0.000009  loss: 0.8544 (0.8326)  loss_scale: 131072.0000 (116631.8644)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
Epoch: [18]  [ 540/1349]  eta: 0:04:16  lr: 0.000399  min_lr: 0.000009  loss: 0.8297 (0.8312)  loss_scale: 131072.0000 (116898.7800)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0001  max mem: 41808
Epoch: [18]  [ 550/1349]  eta: 0:04:13  lr: 0.000398  min_lr: 0.000009  loss: 0.7752 (0.8309)  loss_scale: 131072.0000 (117156.0073)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
Epoch: [18]  [ 560/1349]  eta: 0:04:09  lr: 0.000398  min_lr: 0.000009  loss: 0.8315 (0.8311)  loss_scale: 131072.0000 (117404.0642)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
Epoch: [18]  [ 570/1349]  eta: 0:04:06  lr: 0.000398  min_lr: 0.000009  loss: 0.8871 (0.8316)  loss_scale: 131072.0000 (117643.4326)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
Epoch: [18]  [ 580/1349]  eta: 0:04:03  lr: 0.000398  min_lr: 0.000009  loss: 0.9290 (0.8312)  loss_scale: 131072.0000 (117874.5611)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
Epoch: [18]  [ 590/1349]  eta: 0:03:59  lr: 0.000398  min_lr: 0.000009  loss: 0.8506 (0.8318)  loss_scale: 131072.0000 (118097.8680)  weight_decay: 0.0500 (0.0500)  time: 0.3080  data: 0.0001  max mem: 41808
Epoch: [18]  [ 600/1349]  eta: 0:03:56  lr: 0.000398  min_lr: 0.000009  loss: 0.8606 (0.8313)  loss_scale: 131072.0000 (118313.7438)  weight_decay: 0.0500 (0.0500)  time: 0.3081  data: 0.0001  max mem: 41808
Epoch: [18]  [ 610/1349]  eta: 0:03:53  lr: 0.000398  min_lr: 0.000009  loss: 0.8334 (0.8315)  loss_scale: 131072.0000 (118522.5532)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
[2025-05-23 18:47:10,696] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 18:47:10,696] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 18:47:10,696] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 18:47:10,696] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 18:47:11,615] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 24898
[2025-05-23 18:47:11,615] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 24898
[2025-05-23 18:47:11,615] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 18:47:11,615] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 18:47:11,615] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [18]  [ 620/1349]  eta: 0:03:50  lr: 0.000398  min_lr: 0.000009  loss: 0.8334 (0.8316)  loss_scale: 131072.0000 (119357.8357)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [18]  [ 630/1349]  eta: 0:03:46  lr: 0.000398  min_lr: 0.000009  loss: 0.9079 (0.8326)  loss_scale: 131072.0000 (119543.4802)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
Epoch: [18]  [ 640/1349]  eta: 0:03:43  lr: 0.000398  min_lr: 0.000009  loss: 0.8669 (0.8324)  loss_scale: 131072.0000 (119723.3323)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0001  max mem: 41808
Epoch: [18]  [ 650/1349]  eta: 0:03:40  lr: 0.000397  min_lr: 0.000009  loss: 0.7647 (0.8317)  loss_scale: 131072.0000 (119897.6590)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
Epoch: [18]  [ 660/1349]  eta: 0:03:37  lr: 0.000397  min_lr: 0.000009  loss: 0.8108 (0.8323)  loss_scale: 131072.0000 (120066.7110)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0001  max mem: 41808
Epoch: [18]  [ 670/1349]  eta: 0:03:33  lr: 0.000397  min_lr: 0.000009  loss: 0.9228 (0.8334)  loss_scale: 131072.0000 (120230.7243)  weight_decay: 0.0500 (0.0500)  time: 0.3078  data: 0.0001  max mem: 41808
Epoch: [18]  [ 680/1349]  eta: 0:03:30  lr: 0.000397  min_lr: 0.000009  loss: 0.8886 (0.8333)  loss_scale: 131072.0000 (120389.9207)  weight_decay: 0.0500 (0.0500)  time: 0.3079  data: 0.0001  max mem: 41808
Epoch: [18]  [ 690/1349]  eta: 0:03:27  lr: 0.000397  min_lr: 0.000009  loss: 0.8798 (0.8346)  loss_scale: 131072.0000 (120544.5094)  weight_decay: 0.0500 (0.0500)  time: 0.3082  data: 0.0001  max mem: 41808
Epoch: [18]  [ 700/1349]  eta: 0:03:24  lr: 0.000397  min_lr: 0.000009  loss: 0.8786 (0.8341)  loss_scale: 131072.0000 (120694.6876)  weight_decay: 0.0500 (0.0500)  time: 0.3079  data: 0.0001  max mem: 41808
Epoch: [18]  [ 710/1349]  eta: 0:03:21  lr: 0.000397  min_lr: 0.000009  loss: 0.8134 (0.8333)  loss_scale: 131072.0000 (120840.6414)  weight_decay: 0.0500 (0.0500)  time: 0.3087  data: 0.0001  max mem: 41808
[2025-05-23 18:47:42,733] [INFO] [logging.py:96:log_dist] [Rank 0] step=25000, skipped=151, lr=[9.424699246110819e-06, 9.424699246110819e-06, 1.2566265661481093e-05, 1.2566265661481093e-05, 1.675502088197479e-05, 1.675502088197479e-05, 2.2340027842633055e-05, 2.2340027842633055e-05, 2.9786703790177404e-05, 2.9786703790177404e-05, 3.971560505356987e-05, 3.971560505356987e-05, 5.29541400714265e-05, 5.29541400714265e-05, 7.060552009523533e-05, 7.060552009523533e-05, 9.414069346031378e-05, 9.414069346031378e-05, 0.0001255209246137517, 0.0001255209246137517, 0.0001673612328183356, 0.0001673612328183356, 0.00022314831042444746, 0.00022314831042444746, 0.00029753108056593, 0.00029753108056593, 0.00039670810742123994, 0.00039670810742123994], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-05-23 18:47:42,734] [INFO] [timer.py:260:stop] epoch=0/micro_step=25000/global_step=25000, RunningAvgSamplesPerSec=207.63063073402657, CurrSamplesPerSec=212.53897959926968, MemAllocated=1.37GB, MaxMemAllocated=40.83GB
Epoch: [18]  [ 720/1349]  eta: 0:03:17  lr: 0.000397  min_lr: 0.000009  loss: 0.9048 (0.8349)  loss_scale: 131072.0000 (120982.5465)  weight_decay: 0.0500 (0.0500)  time: 0.3089  data: 0.0002  max mem: 41808
Epoch: [18]  [ 730/1349]  eta: 0:03:14  lr: 0.000397  min_lr: 0.000009  loss: 0.8588 (0.8340)  loss_scale: 131072.0000 (121120.5691)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
Epoch: [18]  [ 740/1349]  eta: 0:03:11  lr: 0.000396  min_lr: 0.000009  loss: 0.7392 (0.8332)  loss_scale: 131072.0000 (121254.8664)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
[2025-05-23 18:47:51,333] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 18:47:51,333] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 18:47:51,333] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 18:47:51,333] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 18:47:51,946] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 25029
[2025-05-23 18:47:51,946] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 25029
[2025-05-23 18:47:51,946] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 18:47:51,946] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 18:47:51,946] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [18]  [ 750/1349]  eta: 0:03:08  lr: 0.000396  min_lr: 0.000009  loss: 0.8004 (0.8337)  loss_scale: 131072.0000 (121734.6471)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
Epoch: [18]  [ 760/1349]  eta: 0:03:05  lr: 0.000396  min_lr: 0.000009  loss: 0.8500 (0.8338)  loss_scale: 131072.0000 (121857.3456)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [18]  [ 770/1349]  eta: 0:03:01  lr: 0.000396  min_lr: 0.000009  loss: 0.8137 (0.8335)  loss_scale: 131072.0000 (121976.8612)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
Epoch: [18]  [ 780/1349]  eta: 0:02:58  lr: 0.000396  min_lr: 0.000009  loss: 0.7160 (0.8321)  loss_scale: 131072.0000 (122093.3163)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
Epoch: [18]  [ 790/1349]  eta: 0:02:55  lr: 0.000396  min_lr: 0.000009  loss: 0.7773 (0.8318)  loss_scale: 131072.0000 (122206.8268)  weight_decay: 0.0500 (0.0500)  time: 0.3080  data: 0.0002  max mem: 41808
Epoch: [18]  [ 800/1349]  eta: 0:02:52  lr: 0.000396  min_lr: 0.000009  loss: 0.8432 (0.8321)  loss_scale: 131072.0000 (122317.5031)  weight_decay: 0.0500 (0.0500)  time: 0.3080  data: 0.0002  max mem: 41808
Epoch: [18]  [ 810/1349]  eta: 0:02:49  lr: 0.000396  min_lr: 0.000009  loss: 0.8617 (0.8320)  loss_scale: 131072.0000 (122425.4501)  weight_decay: 0.0500 (0.0500)  time: 0.3080  data: 0.0001  max mem: 41808
Epoch: [18]  [ 820/1349]  eta: 0:02:45  lr: 0.000396  min_lr: 0.000009  loss: 0.8820 (0.8324)  loss_scale: 131072.0000 (122530.7674)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [18]  [ 830/1349]  eta: 0:02:42  lr: 0.000396  min_lr: 0.000009  loss: 0.8615 (0.8319)  loss_scale: 131072.0000 (122633.5499)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [18]  [ 840/1349]  eta: 0:02:39  lr: 0.000395  min_lr: 0.000009  loss: 0.8288 (0.8315)  loss_scale: 131072.0000 (122733.8882)  weight_decay: 0.0500 (0.0500)  time: 0.3085  data: 0.0001  max mem: 41808
Epoch: [18]  [ 850/1349]  eta: 0:02:36  lr: 0.000395  min_lr: 0.000009  loss: 0.8242 (0.8311)  loss_scale: 131072.0000 (122831.8684)  weight_decay: 0.0500 (0.0500)  time: 0.3088  data: 0.0001  max mem: 41808
Epoch: [18]  [ 860/1349]  eta: 0:02:33  lr: 0.000395  min_lr: 0.000009  loss: 0.8148 (0.8308)  loss_scale: 131072.0000 (122927.5726)  weight_decay: 0.0500 (0.0500)  time: 0.3081  data: 0.0001  max mem: 41808
Epoch: [18]  [ 870/1349]  eta: 0:02:30  lr: 0.000395  min_lr: 0.000009  loss: 0.8148 (0.8307)  loss_scale: 131072.0000 (123021.0792)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0001  max mem: 41808
[2025-05-23 18:48:31,665] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 18:48:31,665] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 18:48:31,665] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 18:48:31,665] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [18]  [ 880/1349]  eta: 0:02:26  lr: 0.000395  min_lr: 0.000009  loss: 0.7888 (0.8300)  loss_scale: 131072.0000 (123856.3451)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
[2025-05-23 18:48:35,664] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 25171
[2025-05-23 18:48:35,664] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 18:48:35,664] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 25171
[2025-05-23 18:48:35,664] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 18:48:35,664] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [18]  [ 890/1349]  eta: 0:02:23  lr: 0.000395  min_lr: 0.000009  loss: 0.7888 (0.8305)  loss_scale: 262144.0000 (125114.1818)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [18]  [ 900/1349]  eta: 0:02:20  lr: 0.000395  min_lr: 0.000009  loss: 0.9045 (0.8310)  loss_scale: 131072.0000 (125180.3063)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
Epoch: [18]  [ 910/1349]  eta: 0:02:17  lr: 0.000395  min_lr: 0.000009  loss: 0.8896 (0.8313)  loss_scale: 131072.0000 (125244.9791)  weight_decay: 0.0500 (0.0500)  time: 0.3078  data: 0.0001  max mem: 41808
Epoch: [18]  [ 920/1349]  eta: 0:02:14  lr: 0.000395  min_lr: 0.000009  loss: 0.8050 (0.8300)  loss_scale: 131072.0000 (125308.2476)  weight_decay: 0.0500 (0.0500)  time: 0.3079  data: 0.0002  max mem: 41808
Epoch: [18]  [ 930/1349]  eta: 0:02:11  lr: 0.000394  min_lr: 0.000009  loss: 0.7551 (0.8295)  loss_scale: 131072.0000 (125370.1568)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
Epoch: [18]  [ 940/1349]  eta: 0:02:07  lr: 0.000394  min_lr: 0.000009  loss: 0.8442 (0.8301)  loss_scale: 131072.0000 (125430.7503)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0002  max mem: 41808
Epoch: [18]  [ 950/1349]  eta: 0:02:04  lr: 0.000394  min_lr: 0.000009  loss: 0.8181 (0.8299)  loss_scale: 131072.0000 (125490.0694)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0002  max mem: 41808
Epoch: [18]  [ 960/1349]  eta: 0:02:01  lr: 0.000394  min_lr: 0.000009  loss: 0.8181 (0.8296)  loss_scale: 131072.0000 (125548.1540)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
Epoch: [18]  [ 970/1349]  eta: 0:01:58  lr: 0.000394  min_lr: 0.000009  loss: 0.8346 (0.8298)  loss_scale: 131072.0000 (125605.0422)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0002  max mem: 41808
Epoch: [18]  [ 980/1349]  eta: 0:01:55  lr: 0.000394  min_lr: 0.000009  loss: 0.8924 (0.8305)  loss_scale: 131072.0000 (125660.7706)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [18]  [ 990/1349]  eta: 0:01:52  lr: 0.000394  min_lr: 0.000009  loss: 0.8863 (0.8300)  loss_scale: 131072.0000 (125715.3744)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [18]  [1000/1349]  eta: 0:01:49  lr: 0.000394  min_lr: 0.000009  loss: 0.7464 (0.8297)  loss_scale: 131072.0000 (125768.8871)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0002  max mem: 41808
Epoch: [18]  [1010/1349]  eta: 0:01:45  lr: 0.000394  min_lr: 0.000009  loss: 0.8283 (0.8300)  loss_scale: 131072.0000 (125821.3412)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
[2025-05-23 18:49:15,311] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 18:49:15,311] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 18:49:15,311] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 18:49:15,311] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 18:49:15,920] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 25302
[2025-05-23 18:49:15,920] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 25302
[2025-05-23 18:49:15,920] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 18:49:15,920] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 18:49:15,920] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [18]  [1020/1349]  eta: 0:01:42  lr: 0.000394  min_lr: 0.000009  loss: 0.8711 (0.8303)  loss_scale: 131072.0000 (126129.5201)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [18]  [1030/1349]  eta: 0:01:39  lr: 0.000393  min_lr: 0.000009  loss: 0.8711 (0.8309)  loss_scale: 131072.0000 (126177.4588)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
Epoch: [18]  [1040/1349]  eta: 0:01:36  lr: 0.000393  min_lr: 0.000009  loss: 0.8124 (0.8302)  loss_scale: 131072.0000 (126224.4765)  weight_decay: 0.0500 (0.0500)  time: 0.3080  data: 0.0001  max mem: 41808
Epoch: [18]  [1050/1349]  eta: 0:01:33  lr: 0.000393  min_lr: 0.000009  loss: 0.7297 (0.8298)  loss_scale: 131072.0000 (126270.5994)  weight_decay: 0.0500 (0.0500)  time: 0.3096  data: 0.0001  max mem: 41808
Epoch: [18]  [1060/1349]  eta: 0:01:30  lr: 0.000393  min_lr: 0.000009  loss: 0.7821 (0.8300)  loss_scale: 131072.0000 (126315.8530)  weight_decay: 0.0500 (0.0500)  time: 0.3087  data: 0.0001  max mem: 41808
Epoch: [18]  [1070/1349]  eta: 0:01:27  lr: 0.000393  min_lr: 0.000009  loss: 0.8702 (0.8301)  loss_scale: 131072.0000 (126360.2614)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0001  max mem: 41808
Epoch: [18]  [1080/1349]  eta: 0:01:23  lr: 0.000393  min_lr: 0.000009  loss: 0.8505 (0.8297)  loss_scale: 131072.0000 (126403.8483)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
Epoch: [18]  [1090/1349]  eta: 0:01:20  lr: 0.000393  min_lr: 0.000009  loss: 0.8784 (0.8303)  loss_scale: 131072.0000 (126446.6361)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
Epoch: [18]  [1100/1349]  eta: 0:01:17  lr: 0.000393  min_lr: 0.000009  loss: 0.8651 (0.8303)  loss_scale: 131072.0000 (126488.6467)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
Epoch: [18]  [1110/1349]  eta: 0:01:14  lr: 0.000393  min_lr: 0.000009  loss: 0.8169 (0.8304)  loss_scale: 131072.0000 (126529.9010)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [18]  [1120/1349]  eta: 0:01:11  lr: 0.000392  min_lr: 0.000009  loss: 0.8750 (0.8310)  loss_scale: 131072.0000 (126570.4193)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [18]  [1130/1349]  eta: 0:01:08  lr: 0.000392  min_lr: 0.000009  loss: 0.9226 (0.8317)  loss_scale: 131072.0000 (126610.2210)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [18]  [1140/1349]  eta: 0:01:05  lr: 0.000392  min_lr: 0.000009  loss: 0.8962 (0.8317)  loss_scale: 131072.0000 (126649.3252)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
[2025-05-23 18:49:55,630] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 18:49:55,630] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 18:49:55,630] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 18:49:55,630] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 18:49:55,932] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 25432
[2025-05-23 18:49:55,932] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 25432
[2025-05-23 18:49:55,932] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 18:49:55,932] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 18:49:55,932] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [18]  [1150/1349]  eta: 0:01:02  lr: 0.000392  min_lr: 0.000009  loss: 0.8865 (0.8323)  loss_scale: 131072.0000 (126801.6264)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
Epoch: [18]  [1160/1349]  eta: 0:00:58  lr: 0.000392  min_lr: 0.000009  loss: 0.8798 (0.8324)  loss_scale: 131072.0000 (126838.4083)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
Epoch: [18]  [1170/1349]  eta: 0:00:55  lr: 0.000392  min_lr: 0.000009  loss: 0.8653 (0.8330)  loss_scale: 131072.0000 (126874.5619)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [18]  [1180/1349]  eta: 0:00:52  lr: 0.000392  min_lr: 0.000009  loss: 0.8737 (0.8330)  loss_scale: 131072.0000 (126910.1033)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0001  max mem: 41808
Epoch: [18]  [1190/1349]  eta: 0:00:49  lr: 0.000392  min_lr: 0.000009  loss: 0.8737 (0.8336)  loss_scale: 131072.0000 (126945.0479)  weight_decay: 0.0500 (0.0500)  time: 0.3079  data: 0.0001  max mem: 41808
Epoch: [18]  [1200/1349]  eta: 0:00:46  lr: 0.000392  min_lr: 0.000009  loss: 0.8590 (0.8333)  loss_scale: 131072.0000 (126979.4105)  weight_decay: 0.0500 (0.0500)  time: 0.3155  data: 0.0001  max mem: 41808
Epoch: [18]  [1210/1349]  eta: 0:00:43  lr: 0.000392  min_lr: 0.000009  loss: 0.8121 (0.8335)  loss_scale: 131072.0000 (127013.2056)  weight_decay: 0.0500 (0.0500)  time: 0.3157  data: 0.0001  max mem: 41808
Epoch: [18]  [1220/1349]  eta: 0:00:40  lr: 0.000391  min_lr: 0.000009  loss: 0.9153 (0.8343)  loss_scale: 131072.0000 (127046.4472)  weight_decay: 0.0500 (0.0500)  time: 0.3078  data: 0.0001  max mem: 41808
Epoch: [18]  [1230/1349]  eta: 0:00:37  lr: 0.000391  min_lr: 0.000009  loss: 0.9547 (0.8347)  loss_scale: 131072.0000 (127079.1487)  weight_decay: 0.0500 (0.0500)  time: 0.3079  data: 0.0001  max mem: 41808
Epoch: [18]  [1240/1349]  eta: 0:00:33  lr: 0.000391  min_lr: 0.000009  loss: 0.7827 (0.8337)  loss_scale: 131072.0000 (127111.3231)  weight_decay: 0.0500 (0.0500)  time: 0.3079  data: 0.0001  max mem: 41808
Epoch: [18]  [1250/1349]  eta: 0:00:30  lr: 0.000391  min_lr: 0.000009  loss: 0.7349 (0.8332)  loss_scale: 131072.0000 (127142.9832)  weight_decay: 0.0500 (0.0500)  time: 0.3082  data: 0.0001  max mem: 41808
Epoch: [18]  [1260/1349]  eta: 0:00:27  lr: 0.000391  min_lr: 0.000009  loss: 0.8708 (0.8334)  loss_scale: 131072.0000 (127174.1412)  weight_decay: 0.0500 (0.0500)  time: 0.3079  data: 0.0001  max mem: 41808
Epoch: [18]  [1270/1349]  eta: 0:00:24  lr: 0.000391  min_lr: 0.000009  loss: 0.7392 (0.8319)  loss_scale: 131072.0000 (127204.8088)  weight_decay: 0.0500 (0.0500)  time: 0.3079  data: 0.0001  max mem: 41808
[2025-05-23 18:50:35,812] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 18:50:35,812] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 18:50:35,812] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 18:50:35,813] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [18]  [1280/1349]  eta: 0:00:21  lr: 0.000391  min_lr: 0.000009  loss: 0.7392 (0.8317)  loss_scale: 131072.0000 (127439.6378)  weight_decay: 0.0500 (0.0500)  time: 0.3081  data: 0.0001  max mem: 41808
[2025-05-23 18:50:36,746] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 25564
[2025-05-23 18:50:36,746] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 18:50:36,746] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 25564
[2025-05-23 18:50:36,746] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 18:50:36,746] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [18]  [1290/1349]  eta: 0:00:18  lr: 0.000391  min_lr: 0.000009  loss: 0.8540 (0.8317)  loss_scale: 131072.0000 (127569.3013)  weight_decay: 0.0500 (0.0500)  time: 0.3081  data: 0.0001  max mem: 41808
Epoch: [18]  [1300/1349]  eta: 0:00:15  lr: 0.000391  min_lr: 0.000009  loss: 0.9112 (0.8315)  loss_scale: 131072.0000 (127596.2244)  weight_decay: 0.0500 (0.0500)  time: 0.3078  data: 0.0002  max mem: 41808
Epoch: [18]  [1310/1349]  eta: 0:00:12  lr: 0.000390  min_lr: 0.000009  loss: 0.7940 (0.8311)  loss_scale: 131072.0000 (127622.7368)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0002  max mem: 41808
Epoch: [18]  [1320/1349]  eta: 0:00:09  lr: 0.000390  min_lr: 0.000009  loss: 0.7794 (0.8308)  loss_scale: 131072.0000 (127648.8478)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0002  max mem: 41808
Epoch: [18]  [1330/1349]  eta: 0:00:05  lr: 0.000390  min_lr: 0.000009  loss: 0.7956 (0.8305)  loss_scale: 131072.0000 (127674.5665)  weight_decay: 0.0500 (0.0500)  time: 0.3053  data: 0.0002  max mem: 41808
Epoch: [18]  [1340/1349]  eta: 0:00:02  lr: 0.000390  min_lr: 0.000009  loss: 0.8965 (0.8313)  loss_scale: 131072.0000 (127699.9016)  weight_decay: 0.0500 (0.0500)  time: 0.3026  data: 0.0001  max mem: 41808
Epoch: [18]  [1348/1349]  eta: 0:00:00  lr: 0.000390  min_lr: 0.000009  loss: 0.8999 (0.8315)  loss_scale: 131072.0000 (127719.8992)  weight_decay: 0.0500 (0.0500)  time: 0.3020  data: 0.0001  max mem: 41808
Epoch: [18] Total time: 0:07:00 (0.3117 s / it)
Averaged stats: lr: 0.000390  min_lr: 0.000009  loss: 0.8999 (0.8301)  loss_scale: 131072.0000 (127719.8992)  weight_decay: 0.0500 (0.0500)  total_time: 420.4899 (420.4856)
Val:  [  0/346]  eta: 0:57:12  loss: 3.2684 (3.2684)  acc1: 0.7812 (0.7812)  acc5: 72.6562 (72.6562)  time: 9.9193  data: 8.8391  max mem: 41808
Val:  [ 10/346]  eta: 0:11:10  loss: 0.1307 (0.6425)  acc1: 100.0000 (83.8068)  acc5: 100.0000 (96.8040)  time: 1.9948  data: 1.1688  max mem: 41808
Val:  [ 20/346]  eta: 0:08:07  loss: 0.1235 (0.4809)  acc1: 100.0000 (88.4673)  acc5: 100.0000 (98.2143)  time: 1.0731  data: 0.2822  max mem: 41808
Val:  [ 30/346]  eta: 0:06:32  loss: 0.1105 (0.3976)  acc1: 100.0000 (90.9526)  acc5: 100.0000 (98.7399)  time: 0.8293  data: 0.0815  max mem: 41808
Val:  [ 40/346]  eta: 0:05:47  loss: 0.1093 (0.4165)  acc1: 100.0000 (90.3963)  acc5: 100.0000 (98.4375)  time: 0.7603  data: 0.0003  max mem: 41808
Val:  [ 50/346]  eta: 0:05:15  loss: 0.1195 (0.3681)  acc1: 99.2188 (91.8658)  acc5: 100.0000 (98.7132)  time: 0.7958  data: 0.0003  max mem: 41808
Val:  [ 60/346]  eta: 0:04:52  loss: 0.1195 (0.3497)  acc1: 98.4375 (92.4693)  acc5: 100.0000 (98.9242)  time: 0.7883  data: 0.0003  max mem: 41808
Val:  [ 70/346]  eta: 0:04:36  loss: 0.2221 (0.3680)  acc1: 95.3125 (91.7143)  acc5: 100.0000 (99.0757)  time: 0.8296  data: 0.0585  max mem: 41808
Val:  [ 80/346]  eta: 0:04:22  loss: 0.1990 (0.3637)  acc1: 96.0938 (91.9174)  acc5: 100.0000 (98.9294)  time: 0.8743  data: 0.1230  max mem: 41808
Val:  [ 90/346]  eta: 0:04:09  loss: 0.2426 (0.3676)  acc1: 94.5312 (91.6896)  acc5: 100.0000 (99.0470)  time: 0.8789  data: 0.1380  max mem: 41808
Val:  [100/346]  eta: 0:03:57  loss: 0.2135 (0.3482)  acc1: 96.8750 (92.3422)  acc5: 100.0000 (99.1414)  time: 0.8923  data: 0.1480  max mem: 41808
Val:  [110/346]  eta: 0:03:46  loss: 0.1660 (0.3610)  acc1: 97.6562 (91.8497)  acc5: 100.0000 (99.1906)  time: 0.8936  data: 0.1448  max mem: 41808
Val:  [120/346]  eta: 0:03:35  loss: 0.1660 (0.3618)  acc1: 96.8750 (91.6968)  acc5: 100.0000 (99.2575)  time: 0.8905  data: 0.1404  max mem: 41808
Val:  [130/346]  eta: 0:03:25  loss: 0.1215 (0.3757)  acc1: 99.2188 (91.1975)  acc5: 100.0000 (99.2903)  time: 0.9048  data: 0.1461  max mem: 41808
Val:  [140/346]  eta: 0:03:15  loss: 0.2064 (0.3693)  acc1: 96.8750 (91.3398)  acc5: 100.0000 (99.3406)  time: 0.9018  data: 0.1483  max mem: 41808
Val:  [150/346]  eta: 0:03:04  loss: 0.2849 (0.3663)  acc1: 92.9688 (91.4632)  acc5: 100.0000 (99.3791)  time: 0.8951  data: 0.1488  max mem: 41808
Val:  [160/346]  eta: 0:02:54  loss: 0.2161 (0.3584)  acc1: 96.8750 (91.6974)  acc5: 100.0000 (99.4031)  time: 0.8844  data: 0.1494  max mem: 41808
Val:  [170/346]  eta: 0:02:44  loss: 0.2397 (0.3697)  acc1: 92.1875 (91.2007)  acc5: 100.0000 (99.3878)  time: 0.8902  data: 0.1459  max mem: 41808
Val:  [180/346]  eta: 0:02:35  loss: 0.4630 (0.3894)  acc1: 85.1562 (90.3833)  acc5: 100.0000 (99.4087)  time: 0.9023  data: 0.1436  max mem: 41808
Val:  [190/346]  eta: 0:02:25  loss: 0.3539 (0.3954)  acc1: 89.8438 (90.1996)  acc5: 100.0000 (99.4192)  time: 0.9057  data: 0.1523  max mem: 41808
Val:  [200/346]  eta: 0:02:16  loss: 0.5166 (0.4223)  acc1: 83.5938 (89.1480)  acc5: 100.0000 (99.3859)  time: 0.9116  data: 0.1586  max mem: 41808
Val:  [210/346]  eta: 0:02:06  loss: 0.2724 (0.4146)  acc1: 89.0625 (89.4365)  acc5: 100.0000 (99.4150)  time: 0.8971  data: 0.1506  max mem: 41808
Val:  [220/346]  eta: 0:01:57  loss: 0.1423 (0.4086)  acc1: 96.8750 (89.6281)  acc5: 100.0000 (99.4344)  time: 0.9005  data: 0.1465  max mem: 41808
Val:  [230/346]  eta: 0:01:47  loss: 0.1284 (0.3981)  acc1: 96.0938 (89.9824)  acc5: 100.0000 (99.4420)  time: 0.9015  data: 0.1498  max mem: 41808
Val:  [240/346]  eta: 0:01:38  loss: 0.1630 (0.4015)  acc1: 98.4375 (89.9118)  acc5: 100.0000 (99.4554)  time: 0.8949  data: 0.1594  max mem: 41808
Val:  [250/346]  eta: 0:01:28  loss: 0.1791 (0.3994)  acc1: 96.8750 (90.0181)  acc5: 100.0000 (99.4678)  time: 0.8978  data: 0.1570  max mem: 41808
Val:  [260/346]  eta: 0:01:19  loss: 0.1680 (0.3978)  acc1: 96.8750 (90.0802)  acc5: 100.0000 (99.4582)  time: 0.8964  data: 0.1504  max mem: 41808
Val:  [270/346]  eta: 0:01:10  loss: 0.1557 (0.3935)  acc1: 96.8750 (90.2272)  acc5: 100.0000 (99.4782)  time: 0.9000  data: 0.1455  max mem: 41808
Val:  [280/346]  eta: 0:01:00  loss: 0.1557 (0.3903)  acc1: 96.8750 (90.3414)  acc5: 100.0000 (99.4690)  time: 0.9066  data: 0.1419  max mem: 41808
Val:  [290/346]  eta: 0:00:51  loss: 0.1097 (0.3806)  acc1: 100.0000 (90.6653)  acc5: 100.0000 (99.4872)  time: 0.9009  data: 0.1595  max mem: 41808
Val:  [300/346]  eta: 0:00:42  loss: 0.1090 (0.3794)  acc1: 100.0000 (90.7548)  acc5: 100.0000 (99.4134)  time: 0.9043  data: 0.1589  max mem: 41808
Val:  [310/346]  eta: 0:00:33  loss: 0.1216 (0.3846)  acc1: 100.0000 (90.6049)  acc5: 100.0000 (99.4298)  time: 0.9052  data: 0.1445  max mem: 41808
Val:  [320/346]  eta: 0:00:23  loss: 0.1236 (0.3831)  acc1: 99.2188 (90.6591)  acc5: 100.0000 (99.4475)  time: 0.9141  data: 0.1560  max mem: 41808
Val:  [330/346]  eta: 0:00:14  loss: 0.2878 (0.3948)  acc1: 90.6250 (90.2993)  acc5: 100.0000 (99.3651)  time: 0.9037  data: 0.1607  max mem: 41808
Val:  [340/346]  eta: 0:00:05  loss: 0.4682 (0.4026)  acc1: 87.5000 (90.0637)  acc5: 100.0000 (99.3791)  time: 0.8985  data: 0.1600  max mem: 41808
Val:  [345/346]  eta: 0:00:00  loss: 0.1771 (0.3986)  acc1: 95.3125 (90.1914)  acc5: 100.0000 (99.3875)  time: 0.8960  data: 0.1735  max mem: 41808
Val: Total time: 0:05:18 (0.9209 s / it)
* Acc@1 90.203 Acc@5 99.430 loss 0.397
Accuracy of the network on the 88494 val videos: 90.2%
Max accuracy: 91.25%   Max Epoch: 3
Epoch: [19]  [   0/1349]  eta: 1:49:50  lr: 0.000390  min_lr: 0.000009  loss: 0.6354 (0.6354)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 4.8856  data: 4.5427  max mem: 41808
Epoch: [19]  [  10/1349]  eta: 0:16:18  lr: 0.000390  min_lr: 0.000009  loss: 0.8230 (0.8427)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7308  data: 0.4131  max mem: 41808
Epoch: [19]  [  20/1349]  eta: 0:11:44  lr: 0.000390  min_lr: 0.000009  loss: 0.8850 (0.8675)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3127  data: 0.0001  max mem: 41808
Epoch: [19]  [  30/1349]  eta: 0:10:06  lr: 0.000390  min_lr: 0.000009  loss: 0.8067 (0.8293)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3105  data: 0.0001  max mem: 41808
Epoch: [19]  [  40/1349]  eta: 0:09:12  lr: 0.000390  min_lr: 0.000009  loss: 0.7239 (0.8046)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3088  data: 0.0001  max mem: 41808
Epoch: [19]  [  50/1349]  eta: 0:08:39  lr: 0.000389  min_lr: 0.000009  loss: 0.8125 (0.8163)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
[2025-05-23 18:56:38,406] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 25688
[2025-05-23 18:56:38,406] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 18:56:38,406] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 25688
[2025-05-23 18:56:38,406] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 18:56:38,406] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [19]  [  60/1349]  eta: 0:08:15  lr: 0.000389  min_lr: 0.000009  loss: 0.8817 (0.8271)  loss_scale: 131072.0000 (126774.5574)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0002  max mem: 41808
Epoch: [19]  [  70/1349]  eta: 0:07:58  lr: 0.000389  min_lr: 0.000009  loss: 0.8623 (0.8239)  loss_scale: 65536.0000 (118149.4085)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0002  max mem: 41808
Epoch: [19]  [  80/1349]  eta: 0:07:44  lr: 0.000389  min_lr: 0.000009  loss: 0.8769 (0.8343)  loss_scale: 65536.0000 (111653.9259)  weight_decay: 0.0500 (0.0500)  time: 0.3089  data: 0.0002  max mem: 41808
Epoch: [19]  [  90/1349]  eta: 0:07:32  lr: 0.000389  min_lr: 0.000009  loss: 0.8879 (0.8337)  loss_scale: 65536.0000 (106586.0220)  weight_decay: 0.0500 (0.0500)  time: 0.3097  data: 0.0002  max mem: 41808
Epoch: [19]  [ 100/1349]  eta: 0:07:22  lr: 0.000389  min_lr: 0.000009  loss: 0.7732 (0.8251)  loss_scale: 65536.0000 (102521.6634)  weight_decay: 0.0500 (0.0500)  time: 0.3090  data: 0.0002  max mem: 41808
Epoch: [19]  [ 110/1349]  eta: 0:07:14  lr: 0.000389  min_lr: 0.000009  loss: 0.8047 (0.8261)  loss_scale: 65536.0000 (99189.6216)  weight_decay: 0.0500 (0.0500)  time: 0.3089  data: 0.0001  max mem: 41808
Epoch: [19]  [ 120/1349]  eta: 0:07:06  lr: 0.000389  min_lr: 0.000009  loss: 0.8811 (0.8281)  loss_scale: 65536.0000 (96408.3306)  weight_decay: 0.0500 (0.0500)  time: 0.3088  data: 0.0001  max mem: 41808
Epoch: [19]  [ 130/1349]  eta: 0:06:59  lr: 0.000389  min_lr: 0.000009  loss: 0.8730 (0.8285)  loss_scale: 65536.0000 (94051.6641)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
Epoch: [19]  [ 140/1349]  eta: 0:06:52  lr: 0.000389  min_lr: 0.000009  loss: 0.8395 (0.8257)  loss_scale: 65536.0000 (92029.2766)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [19]  [ 150/1349]  eta: 0:06:46  lr: 0.000388  min_lr: 0.000009  loss: 0.8447 (0.8275)  loss_scale: 65536.0000 (90274.7550)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [19]  [ 160/1349]  eta: 0:06:40  lr: 0.000388  min_lr: 0.000009  loss: 0.8969 (0.8292)  loss_scale: 65536.0000 (88738.1863)  weight_decay: 0.0500 (0.0500)  time: 0.3066  data: 0.0001  max mem: 41808
Epoch: [19]  [ 170/1349]  eta: 0:06:35  lr: 0.000388  min_lr: 0.000009  loss: 0.8697 (0.8292)  loss_scale: 65536.0000 (87381.3333)  weight_decay: 0.0500 (0.0500)  time: 0.3066  data: 0.0001  max mem: 41808
Epoch: [19]  [ 180/1349]  eta: 0:06:30  lr: 0.000388  min_lr: 0.000009  loss: 0.8785 (0.8312)  loss_scale: 65536.0000 (86174.4088)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
[2025-05-23 18:57:18,134] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 18:57:18,134] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-05-23 18:57:18,134] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 18:57:18,134] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [19]  [ 190/1349]  eta: 0:06:25  lr: 0.000388  min_lr: 0.000009  loss: 0.8649 (0.8304)  loss_scale: 65536.0000 (86809.4660)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [19]  [ 200/1349]  eta: 0:06:20  lr: 0.000388  min_lr: 0.000009  loss: 0.8237 (0.8309)  loss_scale: 131072.0000 (89011.5821)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
Epoch: [19]  [ 210/1349]  eta: 0:06:15  lr: 0.000388  min_lr: 0.000009  loss: 0.8400 (0.8311)  loss_scale: 131072.0000 (91004.9668)  weight_decay: 0.0500 (0.0500)  time: 0.3066  data: 0.0001  max mem: 41808
Epoch: [19]  [ 220/1349]  eta: 0:06:11  lr: 0.000388  min_lr: 0.000009  loss: 0.8400 (0.8310)  loss_scale: 131072.0000 (92817.9548)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
Epoch: [19]  [ 230/1349]  eta: 0:06:07  lr: 0.000388  min_lr: 0.000009  loss: 0.8545 (0.8311)  loss_scale: 131072.0000 (94473.9740)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
Epoch: [19]  [ 240/1349]  eta: 0:06:02  lr: 0.000387  min_lr: 0.000009  loss: 0.8719 (0.8309)  loss_scale: 131072.0000 (95992.5643)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0002  max mem: 41808
Epoch: [19]  [ 250/1349]  eta: 0:05:58  lr: 0.000387  min_lr: 0.000009  loss: 0.8610 (0.8311)  loss_scale: 131072.0000 (97390.1514)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0002  max mem: 41808
Epoch: [19]  [ 260/1349]  eta: 0:05:54  lr: 0.000387  min_lr: 0.000009  loss: 0.9030 (0.8313)  loss_scale: 131072.0000 (98680.6437)  weight_decay: 0.0500 (0.0500)  time: 0.3078  data: 0.0002  max mem: 41808
Epoch: [19]  [ 270/1349]  eta: 0:05:50  lr: 0.000387  min_lr: 0.000009  loss: 0.8667 (0.8294)  loss_scale: 131072.0000 (99875.8967)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0002  max mem: 41808
Epoch: [19]  [ 280/1349]  eta: 0:05:46  lr: 0.000387  min_lr: 0.000009  loss: 0.8554 (0.8295)  loss_scale: 131072.0000 (100986.0783)  weight_decay: 0.0500 (0.0500)  time: 0.3080  data: 0.0002  max mem: 41808
Epoch: [19]  [ 290/1349]  eta: 0:05:42  lr: 0.000387  min_lr: 0.000009  loss: 0.8470 (0.8301)  loss_scale: 131072.0000 (102019.9588)  weight_decay: 0.0500 (0.0500)  time: 0.3091  data: 0.0002  max mem: 41808
Epoch: [19]  [ 300/1349]  eta: 0:05:39  lr: 0.000387  min_lr: 0.000009  loss: 0.8055 (0.8272)  loss_scale: 131072.0000 (102985.1429)  weight_decay: 0.0500 (0.0500)  time: 0.3086  data: 0.0001  max mem: 41808
Epoch: [19]  [ 310/1349]  eta: 0:05:35  lr: 0.000387  min_lr: 0.000009  loss: 0.7401 (0.8238)  loss_scale: 131072.0000 (103888.2572)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
[2025-05-23 18:57:57,525] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 18:57:57,525] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 18:57:57,525] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 18:57:57,525] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [19]  [ 320/1349]  eta: 0:05:31  lr: 0.000387  min_lr: 0.000009  loss: 0.8040 (0.8238)  loss_scale: 131072.0000 (107593.3707)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
[2025-05-23 18:58:00,592] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 25955
[2025-05-23 18:58:00,592] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 25955
[2025-05-23 18:58:00,592] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 18:58:00,592] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 18:58:00,592] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [19]  [ 330/1349]  eta: 0:05:27  lr: 0.000386  min_lr: 0.000009  loss: 0.8774 (0.8253)  loss_scale: 131072.0000 (109490.6586)  weight_decay: 0.0500 (0.0500)  time: 0.3063  data: 0.0001  max mem: 41808
Epoch: [19]  [ 340/1349]  eta: 0:05:24  lr: 0.000386  min_lr: 0.000009  loss: 0.8897 (0.8256)  loss_scale: 131072.0000 (110123.5425)  weight_decay: 0.0500 (0.0500)  time: 0.3061  data: 0.0001  max mem: 41808
Epoch: [19]  [ 350/1349]  eta: 0:05:20  lr: 0.000386  min_lr: 0.000009  loss: 0.7983 (0.8241)  loss_scale: 131072.0000 (110720.3647)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0001  max mem: 41808
Epoch: [19]  [ 360/1349]  eta: 0:05:17  lr: 0.000386  min_lr: 0.000009  loss: 0.7517 (0.8224)  loss_scale: 131072.0000 (111284.1219)  weight_decay: 0.0500 (0.0500)  time: 0.3099  data: 0.0002  max mem: 41808
[2025-05-23 18:58:14,181] [INFO] [logging.py:96:log_dist] [Rank 0] step=26000, skipped=158, lr=[9.17159336791723e-06, 9.17159336791723e-06, 1.2228791157222975e-05, 1.2228791157222975e-05, 1.63050548762973e-05, 1.63050548762973e-05, 2.17400731683964e-05, 2.17400731683964e-05, 2.8986764224528532e-05, 2.8986764224528532e-05, 3.8649018966038045e-05, 3.8649018966038045e-05, 5.153202528805073e-05, 5.153202528805073e-05, 6.87093670507343e-05, 6.87093670507343e-05, 9.161248940097907e-05, 9.161248940097907e-05, 0.00012214998586797208, 0.00012214998586797208, 0.00016286664782396278, 0.00016286664782396278, 0.0002171555304319504, 0.0002171555304319504, 0.0002895407072426005, 0.0002895407072426005, 0.00038605427632346735, 0.00038605427632346735], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-05-23 18:58:14,181] [INFO] [timer.py:260:stop] epoch=0/micro_step=26000/global_step=26000, RunningAvgSamplesPerSec=207.83239529920994, CurrSamplesPerSec=212.93855741283681, MemAllocated=1.37GB, MaxMemAllocated=40.83GB
Epoch: [19]  [ 370/1349]  eta: 0:05:13  lr: 0.000386  min_lr: 0.000009  loss: 0.8833 (0.8249)  loss_scale: 131072.0000 (111817.4879)  weight_decay: 0.0500 (0.0500)  time: 0.3107  data: 0.0002  max mem: 41808
Epoch: [19]  [ 380/1349]  eta: 0:05:10  lr: 0.000386  min_lr: 0.000009  loss: 0.9021 (0.8266)  loss_scale: 131072.0000 (112322.8556)  weight_decay: 0.0500 (0.0500)  time: 0.3086  data: 0.0001  max mem: 41808
Epoch: [19]  [ 390/1349]  eta: 0:05:06  lr: 0.000386  min_lr: 0.000009  loss: 0.8684 (0.8269)  loss_scale: 131072.0000 (112802.3734)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [19]  [ 400/1349]  eta: 0:05:03  lr: 0.000386  min_lr: 0.000009  loss: 0.8364 (0.8263)  loss_scale: 131072.0000 (113257.9751)  weight_decay: 0.0500 (0.0500)  time: 0.3083  data: 0.0002  max mem: 41808
Epoch: [19]  [ 410/1349]  eta: 0:04:59  lr: 0.000386  min_lr: 0.000009  loss: 0.7559 (0.8240)  loss_scale: 131072.0000 (113691.4063)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0002  max mem: 41808
Epoch: [19]  [ 420/1349]  eta: 0:04:56  lr: 0.000385  min_lr: 0.000009  loss: 0.8625 (0.8250)  loss_scale: 131072.0000 (114104.2470)  weight_decay: 0.0500 (0.0500)  time: 0.3063  data: 0.0001  max mem: 41808
Epoch: [19]  [ 430/1349]  eta: 0:04:52  lr: 0.000385  min_lr: 0.000009  loss: 0.8817 (0.8243)  loss_scale: 131072.0000 (114497.9304)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
[2025-05-23 18:58:36,291] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 26071
[2025-05-23 18:58:36,291] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 26071
[2025-05-23 18:58:36,291] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 18:58:36,291] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 18:58:36,291] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [19]  [ 440/1349]  eta: 0:04:49  lr: 0.000385  min_lr: 0.000009  loss: 0.8830 (0.8249)  loss_scale: 131072.0000 (114725.1519)  weight_decay: 0.0500 (0.0500)  time: 0.3060  data: 0.0001  max mem: 41808
Epoch: [19]  [ 450/1349]  eta: 0:04:45  lr: 0.000385  min_lr: 0.000009  loss: 0.8327 (0.8245)  loss_scale: 65536.0000 (113634.4834)  weight_decay: 0.0500 (0.0500)  time: 0.3061  data: 0.0001  max mem: 41808
Epoch: [19]  [ 460/1349]  eta: 0:04:42  lr: 0.000385  min_lr: 0.000009  loss: 0.8463 (0.8250)  loss_scale: 65536.0000 (112591.1323)  weight_decay: 0.0500 (0.0500)  time: 0.3079  data: 0.0001  max mem: 41808
Epoch: [19]  [ 470/1349]  eta: 0:04:39  lr: 0.000385  min_lr: 0.000009  loss: 0.8538 (0.8250)  loss_scale: 65536.0000 (111592.0849)  weight_decay: 0.0500 (0.0500)  time: 0.3081  data: 0.0001  max mem: 41808
Epoch: [19]  [ 480/1349]  eta: 0:04:35  lr: 0.000385  min_lr: 0.000009  loss: 0.7548 (0.8233)  loss_scale: 65536.0000 (110634.5780)  weight_decay: 0.0500 (0.0500)  time: 0.3086  data: 0.0001  max mem: 41808
Epoch: [19]  [ 490/1349]  eta: 0:04:32  lr: 0.000385  min_lr: 0.000009  loss: 0.7340 (0.8223)  loss_scale: 65536.0000 (109716.0733)  weight_decay: 0.0500 (0.0500)  time: 0.3086  data: 0.0002  max mem: 41808
Epoch: [19]  [ 500/1349]  eta: 0:04:29  lr: 0.000385  min_lr: 0.000009  loss: 0.8542 (0.8233)  loss_scale: 65536.0000 (108834.2355)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0001  max mem: 41808
Epoch: [19]  [ 510/1349]  eta: 0:04:25  lr: 0.000385  min_lr: 0.000009  loss: 0.8627 (0.8240)  loss_scale: 65536.0000 (107986.9119)  weight_decay: 0.0500 (0.0500)  time: 0.3079  data: 0.0001  max mem: 41808
Epoch: [19]  [ 520/1349]  eta: 0:04:22  lr: 0.000384  min_lr: 0.000009  loss: 0.8754 (0.8254)  loss_scale: 65536.0000 (107172.1152)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
Epoch: [19]  [ 530/1349]  eta: 0:04:19  lr: 0.000384  min_lr: 0.000009  loss: 0.8235 (0.8242)  loss_scale: 65536.0000 (106388.0075)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
Epoch: [19]  [ 540/1349]  eta: 0:04:15  lr: 0.000384  min_lr: 0.000009  loss: 0.7464 (0.8239)  loss_scale: 65536.0000 (105632.8872)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
Epoch: [19]  [ 550/1349]  eta: 0:04:12  lr: 0.000384  min_lr: 0.000009  loss: 0.8126 (0.8238)  loss_scale: 65536.0000 (104905.1760)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [19]  [ 560/1349]  eta: 0:04:09  lr: 0.000384  min_lr: 0.000009  loss: 0.8126 (0.8226)  loss_scale: 65536.0000 (104203.4082)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0002  max mem: 41808
[2025-05-23 18:59:15,996] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 18:59:15,996] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 18:59:15,997] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-05-23 18:59:15,997] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [19]  [ 570/1349]  eta: 0:04:06  lr: 0.000384  min_lr: 0.000009  loss: 0.8304 (0.8235)  loss_scale: 65536.0000 (103755.7688)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0001  max mem: 41808
Epoch: [19]  [ 580/1349]  eta: 0:04:02  lr: 0.000384  min_lr: 0.000009  loss: 0.8239 (0.8226)  loss_scale: 131072.0000 (104225.9277)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0002  max mem: 41808
Epoch: [19]  [ 590/1349]  eta: 0:03:59  lr: 0.000384  min_lr: 0.000009  loss: 0.7711 (0.8219)  loss_scale: 131072.0000 (104680.1760)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0002  max mem: 41808
Epoch: [19]  [ 600/1349]  eta: 0:03:56  lr: 0.000384  min_lr: 0.000009  loss: 0.7959 (0.8210)  loss_scale: 131072.0000 (105119.3078)  weight_decay: 0.0500 (0.0500)  time: 0.3079  data: 0.0001  max mem: 41808
Epoch: [19]  [ 610/1349]  eta: 0:03:53  lr: 0.000383  min_lr: 0.000009  loss: 0.8224 (0.8211)  loss_scale: 131072.0000 (105544.0655)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
Epoch: [19]  [ 620/1349]  eta: 0:03:49  lr: 0.000383  min_lr: 0.000009  loss: 0.8548 (0.8209)  loss_scale: 131072.0000 (105955.1433)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
[2025-05-23 18:59:33,809] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 26258
[2025-05-23 18:59:33,809] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 26258
[2025-05-23 18:59:33,809] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 18:59:33,809] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 18:59:33,810] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [19]  [ 630/1349]  eta: 0:03:46  lr: 0.000383  min_lr: 0.000009  loss: 0.7983 (0.8208)  loss_scale: 131072.0000 (105937.7496)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0002  max mem: 41808
Epoch: [19]  [ 640/1349]  eta: 0:03:43  lr: 0.000383  min_lr: 0.000009  loss: 0.7983 (0.8212)  loss_scale: 65536.0000 (105307.4571)  weight_decay: 0.0500 (0.0500)  time: 0.3062  data: 0.0002  max mem: 41808
Epoch: [19]  [ 650/1349]  eta: 0:03:40  lr: 0.000383  min_lr: 0.000009  loss: 0.8923 (0.8222)  loss_scale: 65536.0000 (104696.5284)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0002  max mem: 41808
Epoch: [19]  [ 660/1349]  eta: 0:03:36  lr: 0.000383  min_lr: 0.000009  loss: 0.8835 (0.8221)  loss_scale: 65536.0000 (104104.0847)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0002  max mem: 41808
Epoch: [19]  [ 670/1349]  eta: 0:03:33  lr: 0.000383  min_lr: 0.000009  loss: 0.8110 (0.8213)  loss_scale: 65536.0000 (103529.2996)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [19]  [ 680/1349]  eta: 0:03:30  lr: 0.000383  min_lr: 0.000009  loss: 0.7712 (0.8213)  loss_scale: 65536.0000 (102971.3950)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
Epoch: [19]  [ 690/1349]  eta: 0:03:27  lr: 0.000383  min_lr: 0.000009  loss: 0.7712 (0.8215)  loss_scale: 65536.0000 (102429.6382)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [19]  [ 700/1349]  eta: 0:03:23  lr: 0.000382  min_lr: 0.000009  loss: 0.8115 (0.8224)  loss_scale: 65536.0000 (101903.3381)  weight_decay: 0.0500 (0.0500)  time: 0.3081  data: 0.0001  max mem: 41808
Epoch: [19]  [ 710/1349]  eta: 0:03:20  lr: 0.000382  min_lr: 0.000009  loss: 0.8955 (0.8235)  loss_scale: 65536.0000 (101391.8425)  weight_decay: 0.0500 (0.0500)  time: 0.3080  data: 0.0001  max mem: 41808
Epoch: [19]  [ 720/1349]  eta: 0:03:17  lr: 0.000382  min_lr: 0.000009  loss: 0.8955 (0.8244)  loss_scale: 65536.0000 (100894.5354)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
Epoch: [19]  [ 730/1349]  eta: 0:03:14  lr: 0.000382  min_lr: 0.000009  loss: 0.8500 (0.8243)  loss_scale: 65536.0000 (100410.8345)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [19]  [ 740/1349]  eta: 0:03:11  lr: 0.000382  min_lr: 0.000009  loss: 0.8836 (0.8253)  loss_scale: 65536.0000 (99940.1889)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [19]  [ 750/1349]  eta: 0:03:08  lr: 0.000382  min_lr: 0.000009  loss: 0.8950 (0.8254)  loss_scale: 65536.0000 (99482.0772)  weight_decay: 0.0500 (0.0500)  time: 0.3117  data: 0.0001  max mem: 41808
[2025-05-23 19:00:13,547] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 19:00:13,547] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-05-23 19:00:13,547] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 19:00:13,547] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [19]  [ 760/1349]  eta: 0:03:04  lr: 0.000382  min_lr: 0.000009  loss: 0.8150 (0.8254)  loss_scale: 65536.0000 (99466.5966)  weight_decay: 0.0500 (0.0500)  time: 0.3117  data: 0.0001  max mem: 41808
Epoch: [19]  [ 770/1349]  eta: 0:03:01  lr: 0.000382  min_lr: 0.000009  loss: 0.7983 (0.8247)  loss_scale: 131072.0000 (99876.5240)  weight_decay: 0.0500 (0.0500)  time: 0.3061  data: 0.0001  max mem: 41808
Epoch: [19]  [ 780/1349]  eta: 0:02:58  lr: 0.000382  min_lr: 0.000009  loss: 0.8798 (0.8250)  loss_scale: 131072.0000 (100275.9539)  weight_decay: 0.0500 (0.0500)  time: 0.3062  data: 0.0002  max mem: 41808
Epoch: [19]  [ 790/1349]  eta: 0:02:55  lr: 0.000381  min_lr: 0.000009  loss: 0.8798 (0.8252)  loss_scale: 131072.0000 (100665.2845)  weight_decay: 0.0500 (0.0500)  time: 0.3064  data: 0.0002  max mem: 41808
[2025-05-23 19:00:26,420] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 26429
[2025-05-23 19:00:26,420] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 26429
[2025-05-23 19:00:26,420] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 19:00:26,420] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 19:00:26,420] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [19]  [ 800/1349]  eta: 0:02:52  lr: 0.000381  min_lr: 0.000009  loss: 0.8489 (0.8251)  loss_scale: 131072.0000 (100799.4407)  weight_decay: 0.0500 (0.0500)  time: 0.3064  data: 0.0002  max mem: 41808
Epoch: [19]  [ 810/1349]  eta: 0:02:48  lr: 0.000381  min_lr: 0.000009  loss: 0.8779 (0.8255)  loss_scale: 65536.0000 (100364.6264)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
Epoch: [19]  [ 820/1349]  eta: 0:02:45  lr: 0.000381  min_lr: 0.000009  loss: 0.8070 (0.8252)  loss_scale: 65536.0000 (99940.4044)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
Epoch: [19]  [ 830/1349]  eta: 0:02:42  lr: 0.000381  min_lr: 0.000009  loss: 0.7954 (0.8251)  loss_scale: 65536.0000 (99526.3923)  weight_decay: 0.0500 (0.0500)  time: 0.3080  data: 0.0001  max mem: 41808
Epoch: [19]  [ 840/1349]  eta: 0:02:39  lr: 0.000381  min_lr: 0.000009  loss: 0.8938 (0.8254)  loss_scale: 65536.0000 (99122.2259)  weight_decay: 0.0500 (0.0500)  time: 0.3078  data: 0.0001  max mem: 41808
Epoch: [19]  [ 850/1349]  eta: 0:02:36  lr: 0.000381  min_lr: 0.000009  loss: 0.8629 (0.8257)  loss_scale: 65536.0000 (98727.5582)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
Epoch: [19]  [ 860/1349]  eta: 0:02:33  lr: 0.000381  min_lr: 0.000009  loss: 0.8714 (0.8260)  loss_scale: 65536.0000 (98342.0581)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [19]  [ 870/1349]  eta: 0:02:29  lr: 0.000381  min_lr: 0.000009  loss: 0.8714 (0.8257)  loss_scale: 65536.0000 (97965.4099)  weight_decay: 0.0500 (0.0500)  time: 0.3066  data: 0.0001  max mem: 41808
Epoch: [19]  [ 880/1349]  eta: 0:02:26  lr: 0.000380  min_lr: 0.000009  loss: 0.7238 (0.8248)  loss_scale: 65536.0000 (97597.3121)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
Epoch: [19]  [ 890/1349]  eta: 0:02:23  lr: 0.000380  min_lr: 0.000009  loss: 0.8403 (0.8255)  loss_scale: 65536.0000 (97237.4770)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
Epoch: [19]  [ 900/1349]  eta: 0:02:20  lr: 0.000380  min_lr: 0.000009  loss: 0.8719 (0.8253)  loss_scale: 65536.0000 (96885.6293)  weight_decay: 0.0500 (0.0500)  time: 0.3066  data: 0.0001  max mem: 41808
Epoch: [19]  [ 910/1349]  eta: 0:02:17  lr: 0.000380  min_lr: 0.000009  loss: 0.8011 (0.8254)  loss_scale: 65536.0000 (96541.5060)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [19]  [ 920/1349]  eta: 0:02:14  lr: 0.000380  min_lr: 0.000009  loss: 0.7925 (0.8246)  loss_scale: 65536.0000 (96204.8556)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
[2025-05-23 19:01:06,039] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 19:01:06,039] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 19:01:06,039] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-05-23 19:01:06,039] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [19]  [ 930/1349]  eta: 0:02:10  lr: 0.000380  min_lr: 0.000009  loss: 0.8409 (0.8248)  loss_scale: 65536.0000 (96157.0097)  weight_decay: 0.0500 (0.0500)  time: 0.3066  data: 0.0001  max mem: 41808
Epoch: [19]  [ 940/1349]  eta: 0:02:07  lr: 0.000380  min_lr: 0.000009  loss: 0.8620 (0.8248)  loss_scale: 131072.0000 (96528.0510)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
Epoch: [19]  [ 950/1349]  eta: 0:02:04  lr: 0.000380  min_lr: 0.000009  loss: 0.8854 (0.8245)  loss_scale: 131072.0000 (96891.2892)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
[2025-05-23 19:01:15,851] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 26590
[2025-05-23 19:01:15,851] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 19:01:15,851] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 26590
[2025-05-23 19:01:15,851] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 19:01:15,852] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [19]  [ 960/1349]  eta: 0:02:01  lr: 0.000380  min_lr: 0.000009  loss: 0.7987 (0.8246)  loss_scale: 131072.0000 (97110.5765)  weight_decay: 0.0500 (0.0500)  time: 0.3060  data: 0.0002  max mem: 41808
Epoch: [19]  [ 970/1349]  eta: 0:01:58  lr: 0.000379  min_lr: 0.000009  loss: 0.8445 (0.8252)  loss_scale: 65536.0000 (96785.4006)  weight_decay: 0.0500 (0.0500)  time: 0.3064  data: 0.0001  max mem: 41808
Epoch: [19]  [ 980/1349]  eta: 0:01:55  lr: 0.000379  min_lr: 0.000009  loss: 0.8445 (0.8249)  loss_scale: 65536.0000 (96466.8542)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
Epoch: [19]  [ 990/1349]  eta: 0:01:52  lr: 0.000379  min_lr: 0.000009  loss: 0.8773 (0.8261)  loss_scale: 65536.0000 (96154.7366)  weight_decay: 0.0500 (0.0500)  time: 0.3062  data: 0.0001  max mem: 41808
Epoch: [19]  [1000/1349]  eta: 0:01:48  lr: 0.000379  min_lr: 0.000009  loss: 0.8835 (0.8259)  loss_scale: 65536.0000 (95848.8551)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
Epoch: [19]  [1010/1349]  eta: 0:01:45  lr: 0.000379  min_lr: 0.000009  loss: 0.8551 (0.8258)  loss_scale: 65536.0000 (95549.0247)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [19]  [1020/1349]  eta: 0:01:42  lr: 0.000379  min_lr: 0.000009  loss: 0.7938 (0.8249)  loss_scale: 65536.0000 (95255.0676)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [19]  [1030/1349]  eta: 0:01:39  lr: 0.000379  min_lr: 0.000009  loss: 0.8050 (0.8259)  loss_scale: 65536.0000 (94966.8128)  weight_decay: 0.0500 (0.0500)  time: 0.3088  data: 0.0001  max mem: 41808
Epoch: [19]  [1040/1349]  eta: 0:01:36  lr: 0.000379  min_lr: 0.000009  loss: 0.9418 (0.8263)  loss_scale: 65536.0000 (94684.0961)  weight_decay: 0.0500 (0.0500)  time: 0.3087  data: 0.0002  max mem: 41808
Epoch: [19]  [1050/1349]  eta: 0:01:33  lr: 0.000379  min_lr: 0.000009  loss: 0.8410 (0.8261)  loss_scale: 65536.0000 (94406.7593)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0002  max mem: 41808
Epoch: [19]  [1060/1349]  eta: 0:01:30  lr: 0.000378  min_lr: 0.000009  loss: 0.8410 (0.8262)  loss_scale: 65536.0000 (94134.6503)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [19]  [1070/1349]  eta: 0:01:27  lr: 0.000378  min_lr: 0.000009  loss: 0.8130 (0.8256)  loss_scale: 65536.0000 (93867.6228)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
Epoch: [19]  [1080/1349]  eta: 0:01:23  lr: 0.000378  min_lr: 0.000009  loss: 0.7185 (0.8248)  loss_scale: 65536.0000 (93605.5356)  weight_decay: 0.0500 (0.0500)  time: 0.3082  data: 0.0001  max mem: 41808
[2025-05-23 19:01:55,516] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 19:01:55,516] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 19:01:55,516] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-05-23 19:01:55,516] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-05-23 19:01:56,127] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 26721
[2025-05-23 19:01:56,127] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 26721
[2025-05-23 19:01:56,128] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 19:01:56,128] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 19:01:56,128] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [19]  [1090/1349]  eta: 0:01:20  lr: 0.000378  min_lr: 0.000009  loss: 0.7241 (0.8249)  loss_scale: 65536.0000 (93468.3923)  weight_decay: 0.0500 (0.0500)  time: 0.3080  data: 0.0001  max mem: 41808
Epoch: [19]  [1100/1349]  eta: 0:01:17  lr: 0.000378  min_lr: 0.000009  loss: 0.8131 (0.8245)  loss_scale: 65536.0000 (93214.6921)  weight_decay: 0.0500 (0.0500)  time: 0.3064  data: 0.0001  max mem: 41808
Epoch: [19]  [1110/1349]  eta: 0:01:14  lr: 0.000378  min_lr: 0.000009  loss: 0.7632 (0.8239)  loss_scale: 65536.0000 (92965.5590)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
Epoch: [19]  [1120/1349]  eta: 0:01:11  lr: 0.000378  min_lr: 0.000009  loss: 0.7774 (0.8241)  loss_scale: 65536.0000 (92720.8707)  weight_decay: 0.0500 (0.0500)  time: 0.3066  data: 0.0001  max mem: 41808
Epoch: [19]  [1130/1349]  eta: 0:01:08  lr: 0.000378  min_lr: 0.000009  loss: 0.8878 (0.8244)  loss_scale: 65536.0000 (92480.5093)  weight_decay: 0.0500 (0.0500)  time: 0.3063  data: 0.0001  max mem: 41808
Epoch: [19]  [1140/1349]  eta: 0:01:05  lr: 0.000378  min_lr: 0.000009  loss: 0.8883 (0.8247)  loss_scale: 65536.0000 (92244.3611)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0001  max mem: 41808
Epoch: [19]  [1150/1349]  eta: 0:01:01  lr: 0.000377  min_lr: 0.000009  loss: 0.8788 (0.8250)  loss_scale: 65536.0000 (92012.3162)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
Epoch: [19]  [1160/1349]  eta: 0:00:58  lr: 0.000377  min_lr: 0.000009  loss: 0.8601 (0.8247)  loss_scale: 65536.0000 (91784.2687)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
Epoch: [19]  [1170/1349]  eta: 0:00:55  lr: 0.000377  min_lr: 0.000009  loss: 0.7864 (0.8245)  loss_scale: 65536.0000 (91560.1161)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
Epoch: [19]  [1180/1349]  eta: 0:00:52  lr: 0.000377  min_lr: 0.000009  loss: 0.8112 (0.8247)  loss_scale: 65536.0000 (91339.7595)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
Epoch: [19]  [1190/1349]  eta: 0:00:49  lr: 0.000377  min_lr: 0.000009  loss: 0.8112 (0.8246)  loss_scale: 65536.0000 (91123.1033)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
Epoch: [19]  [1200/1349]  eta: 0:00:46  lr: 0.000377  min_lr: 0.000009  loss: 0.8036 (0.8245)  loss_scale: 65536.0000 (90910.0550)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
Epoch: [19]  [1210/1349]  eta: 0:00:43  lr: 0.000377  min_lr: 0.000009  loss: 0.7911 (0.8242)  loss_scale: 65536.0000 (90700.5252)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
[2025-05-23 19:02:35,702] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 19:02:35,702] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 19:02:35,702] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-05-23 19:02:35,702] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [19]  [1220/1349]  eta: 0:00:40  lr: 0.000377  min_lr: 0.000009  loss: 0.7944 (0.8244)  loss_scale: 65536.0000 (90601.7756)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
Epoch: [19]  [1230/1349]  eta: 0:00:37  lr: 0.000377  min_lr: 0.000009  loss: 0.8339 (0.8243)  loss_scale: 131072.0000 (90930.5345)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [19]  [1240/1349]  eta: 0:00:33  lr: 0.000376  min_lr: 0.000009  loss: 0.8068 (0.8240)  loss_scale: 131072.0000 (91253.9952)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0002  max mem: 41808
[2025-05-23 19:02:43,075] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 26874
[2025-05-23 19:02:43,075] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 19:02:43,075] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 26874
[2025-05-23 19:02:43,075] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 19:02:43,075] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [19]  [1250/1349]  eta: 0:00:30  lr: 0.000376  min_lr: 0.000009  loss: 0.8068 (0.8236)  loss_scale: 131072.0000 (91153.1894)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
Epoch: [19]  [1260/1349]  eta: 0:00:27  lr: 0.000376  min_lr: 0.000009  loss: 0.8371 (0.8238)  loss_scale: 65536.0000 (90950.0397)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
Epoch: [19]  [1270/1349]  eta: 0:00:24  lr: 0.000376  min_lr: 0.000009  loss: 0.7951 (0.8231)  loss_scale: 65536.0000 (90750.0865)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
Epoch: [19]  [1280/1349]  eta: 0:00:21  lr: 0.000376  min_lr: 0.000009  loss: 0.8084 (0.8233)  loss_scale: 65536.0000 (90553.2553)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [19]  [1290/1349]  eta: 0:00:18  lr: 0.000376  min_lr: 0.000009  loss: 0.8922 (0.8234)  loss_scale: 65536.0000 (90359.4733)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
Epoch: [19]  [1300/1349]  eta: 0:00:15  lr: 0.000376  min_lr: 0.000009  loss: 0.8389 (0.8230)  loss_scale: 65536.0000 (90168.6703)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
Epoch: [19]  [1310/1349]  eta: 0:00:12  lr: 0.000376  min_lr: 0.000009  loss: 0.8252 (0.8231)  loss_scale: 65536.0000 (89980.7780)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [19]  [1320/1349]  eta: 0:00:09  lr: 0.000376  min_lr: 0.000009  loss: 0.8552 (0.8236)  loss_scale: 65536.0000 (89795.7305)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0002  max mem: 41808
Epoch: [19]  [1330/1349]  eta: 0:00:05  lr: 0.000375  min_lr: 0.000009  loss: 0.9224 (0.8244)  loss_scale: 65536.0000 (89613.4636)  weight_decay: 0.0500 (0.0500)  time: 0.3049  data: 0.0001  max mem: 41808
Epoch: [19]  [1340/1349]  eta: 0:00:02  lr: 0.000375  min_lr: 0.000009  loss: 0.8642 (0.8242)  loss_scale: 65536.0000 (89433.9150)  weight_decay: 0.0500 (0.0500)  time: 0.3024  data: 0.0001  max mem: 41808
Epoch: [19]  [1348/1349]  eta: 0:00:00  lr: 0.000375  min_lr: 0.000009  loss: 0.8576 (0.8246)  loss_scale: 65536.0000 (89292.1927)  weight_decay: 0.0500 (0.0500)  time: 0.3019  data: 0.0001  max mem: 41808
Epoch: [19] Total time: 0:06:59 (0.3111 s / it)
Averaged stats: lr: 0.000375  min_lr: 0.000009  loss: 0.8576 (0.8253)  loss_scale: 65536.0000 (89292.1927)  weight_decay: 0.0500 (0.0500)  total_time: 419.6594 (419.6535)
[2025-05-23 19:03:15,527] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-19 is about to be saved!
[2025-05-23 19:03:15,531] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /project/mmendoscope/Natural_Comparison/Cholec75/unified_base_st_settingE_videomae-st_Cholec80_0.0005_0.75_online_key_frame_frame16_Fixed_Stride_4/checkpoint-19/mp_rank_00_model_states.pt
[2025-05-23 19:03:15,531] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-19 is ready now!
[2025-05-23 19:03:15,531] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /project/mmendoscope/Natural_Comparison/Cholec75/unified_base_st_settingE_videomae-st_Cholec80_0.0005_0.75_online_key_frame_frame16_Fixed_Stride_4/checkpoint-19/mp_rank_00_model_states.pt...
[2025-05-23 19:03:18,415] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /project/mmendoscope/Natural_Comparison/Cholec75/unified_base_st_settingE_videomae-st_Cholec80_0.0005_0.75_online_key_frame_frame16_Fixed_Stride_4/checkpoint-19/mp_rank_00_model_states.pt.
[2025-05-23 19:03:18,415] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-19 is ready now!
Val:  [  0/346]  eta: 0:45:59  loss: 2.4228 (2.4228)  acc1: 14.0625 (14.0625)  acc5: 84.3750 (84.3750)  time: 7.9753  data: 7.0708  max mem: 41808
Val:  [ 10/346]  eta: 0:11:49  loss: 0.1204 (0.5121)  acc1: 100.0000 (85.8665)  acc5: 100.0000 (98.0824)  time: 2.1102  data: 1.3276  max mem: 41808
Val:  [ 20/346]  eta: 0:08:11  loss: 0.1168 (0.4134)  acc1: 100.0000 (89.4345)  acc5: 100.0000 (98.8467)  time: 1.1833  data: 0.4177  max mem: 41808
Val:  [ 30/346]  eta: 0:06:39  loss: 0.1016 (0.3553)  acc1: 100.0000 (91.5323)  acc5: 100.0000 (98.9667)  time: 0.7994  data: 0.0413  max mem: 41808
Val:  [ 40/346]  eta: 0:05:52  loss: 0.1239 (0.4025)  acc1: 99.2188 (90.1486)  acc5: 100.0000 (99.1616)  time: 0.7799  data: 0.0003  max mem: 41808
Val:  [ 50/346]  eta: 0:05:20  loss: 0.1220 (0.3529)  acc1: 99.2188 (91.8352)  acc5: 100.0000 (99.3107)  time: 0.8002  data: 0.0003  max mem: 41808
Val:  [ 60/346]  eta: 0:04:55  loss: 0.1438 (0.3423)  acc1: 98.4375 (92.0210)  acc5: 100.0000 (99.4237)  time: 0.7924  data: 0.0003  max mem: 41808
Val:  [ 70/346]  eta: 0:04:36  loss: 0.2086 (0.3598)  acc1: 96.0938 (91.4062)  acc5: 100.0000 (99.4938)  time: 0.8001  data: 0.0003  max mem: 41808
Val:  [ 80/346]  eta: 0:04:19  loss: 0.2218 (0.3629)  acc1: 96.0938 (91.3580)  acc5: 100.0000 (99.5467)  time: 0.7932  data: 0.0003  max mem: 41808
Val:  [ 90/346]  eta: 0:04:05  loss: 0.2776 (0.3627)  acc1: 91.4062 (91.3118)  acc5: 100.0000 (99.5965)  time: 0.8054  data: 0.0525  max mem: 41808
Val:  [100/346]  eta: 0:03:55  loss: 0.1472 (0.3409)  acc1: 96.8750 (92.0947)  acc5: 100.0000 (99.6364)  time: 0.8783  data: 0.1317  max mem: 41808
Val:  [110/346]  eta: 0:03:44  loss: 0.1329 (0.3602)  acc1: 99.2188 (91.5541)  acc5: 100.0000 (99.4017)  time: 0.9102  data: 0.1539  max mem: 41808
Val:  [120/346]  eta: 0:03:34  loss: 0.2309 (0.3641)  acc1: 94.5312 (91.3740)  acc5: 100.0000 (99.3543)  time: 0.9041  data: 0.1452  max mem: 41808
Val:  [130/346]  eta: 0:03:23  loss: 0.1400 (0.3634)  acc1: 98.4375 (91.2810)  acc5: 100.0000 (99.3977)  time: 0.8950  data: 0.1442  max mem: 41808
Val:  [140/346]  eta: 0:03:12  loss: 0.2195 (0.3677)  acc1: 96.0938 (91.1403)  acc5: 100.0000 (99.4404)  time: 0.8736  data: 0.1395  max mem: 41808
Val:  [150/346]  eta: 0:03:02  loss: 0.3001 (0.3656)  acc1: 94.5312 (91.2459)  acc5: 100.0000 (99.4567)  time: 0.8687  data: 0.1402  max mem: 41808
Val:  [160/346]  eta: 0:02:53  loss: 0.1689 (0.3577)  acc1: 96.0938 (91.5276)  acc5: 100.0000 (99.4759)  time: 0.8889  data: 0.1465  max mem: 41808
Val:  [170/346]  eta: 0:02:43  loss: 0.1610 (0.3561)  acc1: 96.8750 (91.5296)  acc5: 100.0000 (99.5066)  time: 0.8944  data: 0.1447  max mem: 41808
Val:  [180/346]  eta: 0:02:33  loss: 0.1933 (0.3709)  acc1: 96.0938 (90.8408)  acc5: 100.0000 (99.5209)  time: 0.8948  data: 0.1546  max mem: 41808
Val:  [190/346]  eta: 0:02:24  loss: 0.2350 (0.3683)  acc1: 92.1875 (90.9440)  acc5: 100.0000 (99.5460)  time: 0.9079  data: 0.1546  max mem: 41808
Val:  [200/346]  eta: 0:02:14  loss: 0.3221 (0.3767)  acc1: 92.1875 (90.6328)  acc5: 100.0000 (99.5025)  time: 0.9051  data: 0.1430  max mem: 41808
Val:  [210/346]  eta: 0:02:05  loss: 0.1673 (0.3701)  acc1: 96.8750 (90.8768)  acc5: 100.0000 (99.5261)  time: 0.9100  data: 0.1510  max mem: 41808
Val:  [220/346]  eta: 0:01:56  loss: 0.1438 (0.3669)  acc1: 98.4375 (91.0209)  acc5: 100.0000 (99.5192)  time: 0.9105  data: 0.1598  max mem: 41808
Val:  [230/346]  eta: 0:01:46  loss: 0.1408 (0.3591)  acc1: 99.2188 (91.2946)  acc5: 100.0000 (99.5198)  time: 0.8943  data: 0.1646  max mem: 41808
Val:  [240/346]  eta: 0:01:37  loss: 0.1574 (0.3677)  acc1: 95.3125 (91.0626)  acc5: 100.0000 (99.5202)  time: 0.9008  data: 0.1658  max mem: 41808
Val:  [250/346]  eta: 0:01:28  loss: 0.1977 (0.3651)  acc1: 96.0938 (91.1510)  acc5: 100.0000 (99.5362)  time: 0.8944  data: 0.1593  max mem: 41808
Val:  [260/346]  eta: 0:01:19  loss: 0.1602 (0.3646)  acc1: 98.4375 (91.1518)  acc5: 100.0000 (99.5480)  time: 0.9110  data: 0.1742  max mem: 41808
Val:  [270/346]  eta: 0:01:09  loss: 0.1416 (0.3599)  acc1: 98.4375 (91.3111)  acc5: 100.0000 (99.5589)  time: 0.9192  data: 0.1677  max mem: 41808
Val:  [280/346]  eta: 0:01:00  loss: 0.1098 (0.3577)  acc1: 100.0000 (91.4035)  acc5: 100.0000 (99.5691)  time: 0.9034  data: 0.1529  max mem: 41808
Val:  [290/346]  eta: 0:00:51  loss: 0.0993 (0.3490)  acc1: 100.0000 (91.6881)  acc5: 100.0000 (99.5839)  time: 0.8994  data: 0.1530  max mem: 41808
Val:  [300/346]  eta: 0:00:42  loss: 0.1019 (0.3495)  acc1: 100.0000 (91.7125)  acc5: 100.0000 (99.5224)  time: 0.8834  data: 0.1474  max mem: 41808
Val:  [310/346]  eta: 0:00:32  loss: 0.1303 (0.3517)  acc1: 98.4375 (91.6198)  acc5: 100.0000 (99.5353)  time: 0.8849  data: 0.1447  max mem: 41808
Val:  [320/346]  eta: 0:00:23  loss: 0.1288 (0.3538)  acc1: 99.2188 (91.5547)  acc5: 100.0000 (99.5497)  time: 0.8879  data: 0.1452  max mem: 41808
Val:  [330/346]  eta: 0:00:14  loss: 0.4424 (0.3690)  acc1: 86.7188 (91.0923)  acc5: 100.0000 (99.4666)  time: 0.8949  data: 0.1500  max mem: 41808
Val:  [340/346]  eta: 0:00:05  loss: 0.4807 (0.3774)  acc1: 86.7188 (90.8862)  acc5: 100.0000 (99.4135)  time: 0.9052  data: 0.1599  max mem: 41808
Val:  [345/346]  eta: 0:00:00  loss: 0.1756 (0.3739)  acc1: 96.8750 (90.9960)  acc5: 100.0000 (99.4214)  time: 0.9038  data: 0.1694  max mem: 41808
Val: Total time: 0:05:16 (0.9151 s / it)
* Acc@1 91.013 Acc@5 99.437 loss 0.372
Accuracy of the network on the 88494 val videos: 91.0%
Max accuracy: 91.25%   Max Epoch: 3
Epoch: [20]  [   0/1349]  eta: 1:54:57  lr: 0.000375  min_lr: 0.000009  loss: 1.0550 (1.0550)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 5.1131  data: 3.4445  max mem: 41808
Epoch: [20]  [  10/1349]  eta: 0:16:50  lr: 0.000375  min_lr: 0.000009  loss: 0.7094 (0.7947)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7545  data: 0.3133  max mem: 41808
[2025-05-23 19:08:46,121] [INFO] [logging.py:96:log_dist] [Rank 0] step=27000, skipped=164, lr=[8.909864387506818e-06, 8.909864387506818e-06, 1.1879819183342424e-05, 1.1879819183342424e-05, 1.5839758911123233e-05, 1.5839758911123233e-05, 2.1119678548164308e-05, 2.1119678548164308e-05, 2.815957139755241e-05, 2.815957139755241e-05, 3.754609519673655e-05, 3.754609519673655e-05, 5.0061460262315396e-05, 5.0061460262315396e-05, 6.67486136830872e-05, 6.67486136830872e-05, 8.89981515774496e-05, 8.89981515774496e-05, 0.00011866420210326613, 0.00011866420210326613, 0.00015821893613768818, 0.00015821893613768818, 0.00021095858151691757, 0.00021095858151691757, 0.0002812781086892234, 0.0002812781086892234, 0.0003750374782522979, 0.0003750374782522979], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-05-23 19:08:46,122] [INFO] [timer.py:260:stop] epoch=0/micro_step=27000/global_step=27000, RunningAvgSamplesPerSec=208.00121463825363, CurrSamplesPerSec=213.29200697636537, MemAllocated=1.37GB, MaxMemAllocated=40.83GB
Epoch: [20]  [  20/1349]  eta: 0:12:00  lr: 0.000375  min_lr: 0.000009  loss: 0.7638 (0.7987)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3138  data: 0.0002  max mem: 41808
[2025-05-23 19:08:47,340] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 19:08:47,340] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-05-23 19:08:47,340] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 19:08:47,341] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [20]  [  30/1349]  eta: 0:10:16  lr: 0.000375  min_lr: 0.000009  loss: 0.7981 (0.7767)  loss_scale: 65536.0000 (82448.5161)  weight_decay: 0.0500 (0.0500)  time: 0.3089  data: 0.0001  max mem: 41808
Epoch: [20]  [  40/1349]  eta: 0:09:20  lr: 0.000375  min_lr: 0.000009  loss: 0.8362 (0.7996)  loss_scale: 131072.0000 (94307.9024)  weight_decay: 0.0500 (0.0500)  time: 0.3087  data: 0.0001  max mem: 41808
[2025-05-23 19:08:53,824] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 27024
[2025-05-23 19:08:53,824] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 19:08:53,824] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 27024
[2025-05-23 19:08:53,824] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 19:08:53,824] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [20]  [  50/1349]  eta: 0:08:45  lr: 0.000375  min_lr: 0.000009  loss: 0.8383 (0.8018)  loss_scale: 131072.0000 (92521.4118)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0002  max mem: 41808
Epoch: [20]  [  60/1349]  eta: 0:08:20  lr: 0.000375  min_lr: 0.000009  loss: 0.8732 (0.8148)  loss_scale: 65536.0000 (88097.5738)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0002  max mem: 41808
Epoch: [20]  [  70/1349]  eta: 0:08:02  lr: 0.000374  min_lr: 0.000009  loss: 0.8937 (0.8260)  loss_scale: 65536.0000 (84919.8873)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
Epoch: [20]  [  80/1349]  eta: 0:07:47  lr: 0.000374  min_lr: 0.000009  loss: 0.9085 (0.8371)  loss_scale: 65536.0000 (82526.8148)  weight_decay: 0.0500 (0.0500)  time: 0.3064  data: 0.0001  max mem: 41808
Epoch: [20]  [  90/1349]  eta: 0:07:35  lr: 0.000374  min_lr: 0.000009  loss: 0.9085 (0.8374)  loss_scale: 65536.0000 (80659.6923)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [20]  [ 100/1349]  eta: 0:07:24  lr: 0.000374  min_lr: 0.000009  loss: 0.8992 (0.8348)  loss_scale: 65536.0000 (79162.2970)  weight_decay: 0.0500 (0.0500)  time: 0.3078  data: 0.0002  max mem: 41808
Epoch: [20]  [ 110/1349]  eta: 0:07:16  lr: 0.000374  min_lr: 0.000009  loss: 0.7372 (0.8308)  loss_scale: 65536.0000 (77934.7027)  weight_decay: 0.0500 (0.0500)  time: 0.3079  data: 0.0001  max mem: 41808
Epoch: [20]  [ 120/1349]  eta: 0:07:08  lr: 0.000374  min_lr: 0.000009  loss: 0.7721 (0.8309)  loss_scale: 65536.0000 (76910.0165)  weight_decay: 0.0500 (0.0500)  time: 0.3088  data: 0.0002  max mem: 41808
Epoch: [20]  [ 130/1349]  eta: 0:07:00  lr: 0.000374  min_lr: 0.000009  loss: 0.8387 (0.8306)  loss_scale: 65536.0000 (76041.7710)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
Epoch: [20]  [ 140/1349]  eta: 0:06:54  lr: 0.000374  min_lr: 0.000009  loss: 0.8910 (0.8352)  loss_scale: 65536.0000 (75296.6809)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
Epoch: [20]  [ 150/1349]  eta: 0:06:47  lr: 0.000374  min_lr: 0.000009  loss: 0.9118 (0.8354)  loss_scale: 65536.0000 (74650.2781)  weight_decay: 0.0500 (0.0500)  time: 0.3066  data: 0.0002  max mem: 41808
Epoch: [20]  [ 160/1349]  eta: 0:06:41  lr: 0.000373  min_lr: 0.000009  loss: 0.9142 (0.8371)  loss_scale: 65536.0000 (74084.1739)  weight_decay: 0.0500 (0.0500)  time: 0.3062  data: 0.0001  max mem: 41808
Epoch: [20]  [ 170/1349]  eta: 0:06:36  lr: 0.000373  min_lr: 0.000009  loss: 0.8359 (0.8304)  loss_scale: 65536.0000 (73584.2807)  weight_decay: 0.0500 (0.0500)  time: 0.3066  data: 0.0001  max mem: 41808
[2025-05-23 19:09:33,460] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 19:09:33,460] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-05-23 19:09:33,460] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 19:09:33,460] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [20]  [ 180/1349]  eta: 0:06:31  lr: 0.000373  min_lr: 0.000009  loss: 0.7505 (0.8280)  loss_scale: 65536.0000 (76036.2431)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [20]  [ 190/1349]  eta: 0:06:26  lr: 0.000373  min_lr: 0.000009  loss: 0.7988 (0.8286)  loss_scale: 131072.0000 (78917.6963)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0002  max mem: 41808
Epoch: [20]  [ 200/1349]  eta: 0:06:21  lr: 0.000373  min_lr: 0.000009  loss: 0.8093 (0.8268)  loss_scale: 131072.0000 (81512.4378)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0002  max mem: 41808
Epoch: [20]  [ 210/1349]  eta: 0:06:16  lr: 0.000373  min_lr: 0.000009  loss: 0.8532 (0.8266)  loss_scale: 131072.0000 (83861.2322)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0002  max mem: 41808
Epoch: [20]  [ 220/1349]  eta: 0:06:12  lr: 0.000373  min_lr: 0.000009  loss: 0.8733 (0.8289)  loss_scale: 131072.0000 (85997.4661)  weight_decay: 0.0500 (0.0500)  time: 0.3066  data: 0.0002  max mem: 41808
Epoch: [20]  [ 230/1349]  eta: 0:06:07  lr: 0.000373  min_lr: 0.000009  loss: 0.8964 (0.8294)  loss_scale: 131072.0000 (87948.7446)  weight_decay: 0.0500 (0.0500)  time: 0.3064  data: 0.0002  max mem: 41808
[2025-05-23 19:09:51,271] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 27211
[2025-05-23 19:09:51,271] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 27211
[2025-05-23 19:09:51,271] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 19:09:51,271] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 19:09:51,271] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [20]  [ 240/1349]  eta: 0:06:03  lr: 0.000373  min_lr: 0.000009  loss: 0.8964 (0.8303)  loss_scale: 65536.0000 (87018.7552)  weight_decay: 0.0500 (0.0500)  time: 0.3063  data: 0.0002  max mem: 41808
Epoch: [20]  [ 250/1349]  eta: 0:05:59  lr: 0.000372  min_lr: 0.000009  loss: 0.8485 (0.8302)  loss_scale: 65536.0000 (86162.8685)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0002  max mem: 41808
Epoch: [20]  [ 260/1349]  eta: 0:05:55  lr: 0.000372  min_lr: 0.000009  loss: 0.8819 (0.8334)  loss_scale: 65536.0000 (85372.5670)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
Epoch: [20]  [ 270/1349]  eta: 0:05:51  lr: 0.000372  min_lr: 0.000009  loss: 0.8327 (0.8328)  loss_scale: 65536.0000 (84640.5904)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0002  max mem: 41808
Epoch: [20]  [ 280/1349]  eta: 0:05:47  lr: 0.000372  min_lr: 0.000009  loss: 0.8049 (0.8327)  loss_scale: 65536.0000 (83960.7117)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0002  max mem: 41808
Epoch: [20]  [ 290/1349]  eta: 0:05:43  lr: 0.000372  min_lr: 0.000009  loss: 0.8123 (0.8304)  loss_scale: 65536.0000 (83327.5601)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [20]  [ 300/1349]  eta: 0:05:39  lr: 0.000372  min_lr: 0.000009  loss: 0.8513 (0.8327)  loss_scale: 65536.0000 (82736.4784)  weight_decay: 0.0500 (0.0500)  time: 0.3138  data: 0.0001  max mem: 41808
Epoch: [20]  [ 310/1349]  eta: 0:05:36  lr: 0.000372  min_lr: 0.000009  loss: 0.8718 (0.8336)  loss_scale: 65536.0000 (82183.4084)  weight_decay: 0.0500 (0.0500)  time: 0.3137  data: 0.0001  max mem: 41808
Epoch: [20]  [ 320/1349]  eta: 0:05:32  lr: 0.000372  min_lr: 0.000009  loss: 0.8484 (0.8315)  loss_scale: 65536.0000 (81664.7975)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [20]  [ 330/1349]  eta: 0:05:28  lr: 0.000372  min_lr: 0.000009  loss: 0.7850 (0.8310)  loss_scale: 65536.0000 (81177.5227)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [20]  [ 340/1349]  eta: 0:05:24  lr: 0.000371  min_lr: 0.000009  loss: 0.7485 (0.8296)  loss_scale: 65536.0000 (80718.8270)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0002  max mem: 41808
Epoch: [20]  [ 350/1349]  eta: 0:05:21  lr: 0.000371  min_lr: 0.000009  loss: 0.7666 (0.8288)  loss_scale: 65536.0000 (80286.2678)  weight_decay: 0.0500 (0.0500)  time: 0.3087  data: 0.0002  max mem: 41808
[2025-05-23 19:10:31,046] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 19:10:31,046] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-05-23 19:10:31,046] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 19:10:31,046] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [20]  [ 360/1349]  eta: 0:05:17  lr: 0.000371  min_lr: 0.000009  loss: 0.8108 (0.8297)  loss_scale: 65536.0000 (80059.2133)  weight_decay: 0.0500 (0.0500)  time: 0.3086  data: 0.0002  max mem: 41808
Epoch: [20]  [ 370/1349]  eta: 0:05:14  lr: 0.000371  min_lr: 0.000009  loss: 0.8631 (0.8303)  loss_scale: 131072.0000 (81434.2210)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0001  max mem: 41808
Epoch: [20]  [ 380/1349]  eta: 0:05:10  lr: 0.000371  min_lr: 0.000009  loss: 0.7853 (0.8278)  loss_scale: 131072.0000 (82737.0499)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
Epoch: [20]  [ 390/1349]  eta: 0:05:07  lr: 0.000371  min_lr: 0.000009  loss: 0.7785 (0.8282)  loss_scale: 131072.0000 (83973.2379)  weight_decay: 0.0500 (0.0500)  time: 0.3080  data: 0.0001  max mem: 41808
Epoch: [20]  [ 400/1349]  eta: 0:05:03  lr: 0.000371  min_lr: 0.000009  loss: 0.7792 (0.8274)  loss_scale: 131072.0000 (85147.7706)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [20]  [ 410/1349]  eta: 0:05:00  lr: 0.000371  min_lr: 0.000009  loss: 0.8134 (0.8276)  loss_scale: 131072.0000 (86265.1484)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
Epoch: [20]  [ 420/1349]  eta: 0:04:56  lr: 0.000371  min_lr: 0.000009  loss: 0.8134 (0.8260)  loss_scale: 131072.0000 (87329.4442)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0001  max mem: 41808
Epoch: [20]  [ 430/1349]  eta: 0:04:53  lr: 0.000370  min_lr: 0.000009  loss: 0.7434 (0.8240)  loss_scale: 131072.0000 (88344.3527)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0001  max mem: 41808
Epoch: [20]  [ 440/1349]  eta: 0:04:49  lr: 0.000370  min_lr: 0.000009  loss: 0.7485 (0.8223)  loss_scale: 131072.0000 (89313.2336)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
Epoch: [20]  [ 450/1349]  eta: 0:04:46  lr: 0.000370  min_lr: 0.000009  loss: 0.7725 (0.8226)  loss_scale: 131072.0000 (90239.1486)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
Epoch: [20]  [ 460/1349]  eta: 0:04:42  lr: 0.000370  min_lr: 0.000009  loss: 0.7822 (0.8209)  loss_scale: 131072.0000 (91124.8937)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [20]  [ 470/1349]  eta: 0:04:39  lr: 0.000370  min_lr: 0.000009  loss: 0.7942 (0.8227)  loss_scale: 131072.0000 (91973.0276)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
Epoch: [20]  [ 480/1349]  eta: 0:04:36  lr: 0.000370  min_lr: 0.000009  loss: 0.9091 (0.8229)  loss_scale: 131072.0000 (92785.8960)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
[2025-05-23 19:11:10,391] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 19:11:10,392] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 19:11:10,392] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 19:11:10,392] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [20]  [ 490/1349]  eta: 0:04:32  lr: 0.000370  min_lr: 0.000009  loss: 0.8080 (0.8226)  loss_scale: 131072.0000 (94366.5010)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
[2025-05-23 19:11:11,310] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 27471
[2025-05-23 19:11:11,310] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 27471
[2025-05-23 19:11:11,310] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 19:11:11,310] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 19:11:11,310] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [20]  [ 500/1349]  eta: 0:04:29  lr: 0.000370  min_lr: 0.000009  loss: 0.8017 (0.8226)  loss_scale: 131072.0000 (95099.1457)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
Epoch: [20]  [ 510/1349]  eta: 0:04:26  lr: 0.000370  min_lr: 0.000009  loss: 0.8297 (0.8229)  loss_scale: 131072.0000 (95803.1155)  weight_decay: 0.0500 (0.0500)  time: 0.3064  data: 0.0001  max mem: 41808
Epoch: [20]  [ 520/1349]  eta: 0:04:22  lr: 0.000369  min_lr: 0.000009  loss: 0.7735 (0.8210)  loss_scale: 131072.0000 (96480.0614)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0001  max mem: 41808
Epoch: [20]  [ 530/1349]  eta: 0:04:19  lr: 0.000369  min_lr: 0.000009  loss: 0.7238 (0.8190)  loss_scale: 131072.0000 (97131.5104)  weight_decay: 0.0500 (0.0500)  time: 0.3066  data: 0.0001  max mem: 41808
Epoch: [20]  [ 540/1349]  eta: 0:04:16  lr: 0.000369  min_lr: 0.000009  loss: 0.7110 (0.8182)  loss_scale: 131072.0000 (97758.8762)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0001  max mem: 41808
Epoch: [20]  [ 550/1349]  eta: 0:04:12  lr: 0.000369  min_lr: 0.000009  loss: 0.7782 (0.8175)  loss_scale: 131072.0000 (98363.4701)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
Epoch: [20]  [ 560/1349]  eta: 0:04:09  lr: 0.000369  min_lr: 0.000009  loss: 0.8336 (0.8186)  loss_scale: 131072.0000 (98946.5098)  weight_decay: 0.0500 (0.0500)  time: 0.3064  data: 0.0001  max mem: 41808
Epoch: [20]  [ 570/1349]  eta: 0:04:06  lr: 0.000369  min_lr: 0.000009  loss: 0.8336 (0.8185)  loss_scale: 131072.0000 (99509.1278)  weight_decay: 0.0500 (0.0500)  time: 0.3061  data: 0.0001  max mem: 41808
Epoch: [20]  [ 580/1349]  eta: 0:04:02  lr: 0.000369  min_lr: 0.000009  loss: 0.7607 (0.8172)  loss_scale: 131072.0000 (100052.3787)  weight_decay: 0.0500 (0.0500)  time: 0.3059  data: 0.0001  max mem: 41808
Epoch: [20]  [ 590/1349]  eta: 0:03:59  lr: 0.000369  min_lr: 0.000009  loss: 0.8159 (0.8176)  loss_scale: 131072.0000 (100577.2453)  weight_decay: 0.0500 (0.0500)  time: 0.3056  data: 0.0001  max mem: 41808
[2025-05-23 19:11:43,170] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 27575
[2025-05-23 19:11:43,170] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 19:11:43,170] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 27575
[2025-05-23 19:11:43,170] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 19:11:43,170] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [20]  [ 600/1349]  eta: 0:03:56  lr: 0.000368  min_lr: 0.000009  loss: 0.8082 (0.8167)  loss_scale: 131072.0000 (100430.3760)  weight_decay: 0.0500 (0.0500)  time: 0.3057  data: 0.0001  max mem: 41808
Epoch: [20]  [ 610/1349]  eta: 0:03:53  lr: 0.000368  min_lr: 0.000009  loss: 0.8022 (0.8167)  loss_scale: 65536.0000 (99859.2733)  weight_decay: 0.0500 (0.0500)  time: 0.3061  data: 0.0001  max mem: 41808
Epoch: [20]  [ 620/1349]  eta: 0:03:49  lr: 0.000368  min_lr: 0.000009  loss: 0.8593 (0.8176)  loss_scale: 65536.0000 (99306.5636)  weight_decay: 0.0500 (0.0500)  time: 0.3064  data: 0.0001  max mem: 41808
Epoch: [20]  [ 630/1349]  eta: 0:03:46  lr: 0.000368  min_lr: 0.000009  loss: 0.8400 (0.8169)  loss_scale: 65536.0000 (98771.3724)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0001  max mem: 41808
Epoch: [20]  [ 640/1349]  eta: 0:03:43  lr: 0.000368  min_lr: 0.000009  loss: 0.8755 (0.8171)  loss_scale: 65536.0000 (98252.8799)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0001  max mem: 41808
Epoch: [20]  [ 650/1349]  eta: 0:03:40  lr: 0.000368  min_lr: 0.000009  loss: 0.8755 (0.8173)  loss_scale: 65536.0000 (97750.3164)  weight_decay: 0.0500 (0.0500)  time: 0.3079  data: 0.0001  max mem: 41808
Epoch: [20]  [ 660/1349]  eta: 0:03:36  lr: 0.000368  min_lr: 0.000009  loss: 0.8056 (0.8176)  loss_scale: 65536.0000 (97262.9592)  weight_decay: 0.0500 (0.0500)  time: 0.3078  data: 0.0001  max mem: 41808
Epoch: [20]  [ 670/1349]  eta: 0:03:33  lr: 0.000368  min_lr: 0.000009  loss: 0.7692 (0.8176)  loss_scale: 65536.0000 (96790.1282)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [20]  [ 680/1349]  eta: 0:03:30  lr: 0.000368  min_lr: 0.000009  loss: 0.8256 (0.8183)  loss_scale: 65536.0000 (96331.1836)  weight_decay: 0.0500 (0.0500)  time: 0.3092  data: 0.0001  max mem: 41808
Epoch: [20]  [ 690/1349]  eta: 0:03:27  lr: 0.000367  min_lr: 0.000009  loss: 0.8517 (0.8179)  loss_scale: 65536.0000 (95885.5224)  weight_decay: 0.0500 (0.0500)  time: 0.3086  data: 0.0001  max mem: 41808
Epoch: [20]  [ 700/1349]  eta: 0:03:23  lr: 0.000367  min_lr: 0.000009  loss: 0.8160 (0.8178)  loss_scale: 65536.0000 (95452.5763)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
Epoch: [20]  [ 710/1349]  eta: 0:03:20  lr: 0.000367  min_lr: 0.000009  loss: 0.8160 (0.8178)  loss_scale: 65536.0000 (95031.8087)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
Epoch: [20]  [ 720/1349]  eta: 0:03:17  lr: 0.000367  min_lr: 0.000009  loss: 0.8383 (0.8183)  loss_scale: 65536.0000 (94622.7129)  weight_decay: 0.0500 (0.0500)  time: 0.3063  data: 0.0001  max mem: 41808
[2025-05-23 19:12:22,808] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 19:12:22,808] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-05-23 19:12:22,808] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 19:12:22,808] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [20]  [ 730/1349]  eta: 0:03:14  lr: 0.000367  min_lr: 0.000009  loss: 0.8656 (0.8190)  loss_scale: 65536.0000 (94852.3776)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0001  max mem: 41808
Epoch: [20]  [ 740/1349]  eta: 0:03:11  lr: 0.000367  min_lr: 0.000009  loss: 0.8911 (0.8196)  loss_scale: 131072.0000 (95341.1714)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
Epoch: [20]  [ 750/1349]  eta: 0:03:07  lr: 0.000367  min_lr: 0.000009  loss: 0.8911 (0.8203)  loss_scale: 131072.0000 (95816.9481)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [20]  [ 760/1349]  eta: 0:03:04  lr: 0.000367  min_lr: 0.000009  loss: 0.8345 (0.8204)  loss_scale: 131072.0000 (96280.2208)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
Epoch: [20]  [ 770/1349]  eta: 0:03:01  lr: 0.000367  min_lr: 0.000009  loss: 0.8276 (0.8207)  loss_scale: 131072.0000 (96731.4760)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
Epoch: [20]  [ 780/1349]  eta: 0:02:58  lr: 0.000366  min_lr: 0.000009  loss: 0.7791 (0.8199)  loss_scale: 131072.0000 (97171.1754)  weight_decay: 0.0500 (0.0500)  time: 0.3066  data: 0.0002  max mem: 41808
Epoch: [20]  [ 790/1349]  eta: 0:02:55  lr: 0.000366  min_lr: 0.000009  loss: 0.7609 (0.8195)  loss_scale: 131072.0000 (97599.7573)  weight_decay: 0.0500 (0.0500)  time: 0.3064  data: 0.0001  max mem: 41808
Epoch: [20]  [ 800/1349]  eta: 0:02:52  lr: 0.000366  min_lr: 0.000009  loss: 0.8726 (0.8204)  loss_scale: 131072.0000 (98017.6380)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0001  max mem: 41808
Epoch: [20]  [ 810/1349]  eta: 0:02:48  lr: 0.000366  min_lr: 0.000009  loss: 0.8703 (0.8193)  loss_scale: 131072.0000 (98425.2133)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
Epoch: [20]  [ 820/1349]  eta: 0:02:45  lr: 0.000366  min_lr: 0.000009  loss: 0.7830 (0.8198)  loss_scale: 131072.0000 (98822.8599)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [20]  [ 830/1349]  eta: 0:02:42  lr: 0.000366  min_lr: 0.000009  loss: 0.8822 (0.8209)  loss_scale: 131072.0000 (99210.9362)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
Epoch: [20]  [ 840/1349]  eta: 0:02:39  lr: 0.000366  min_lr: 0.000009  loss: 0.8319 (0.8203)  loss_scale: 131072.0000 (99589.7836)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [20]  [ 850/1349]  eta: 0:02:36  lr: 0.000366  min_lr: 0.000009  loss: 0.8779 (0.8209)  loss_scale: 131072.0000 (99959.7274)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
[2025-05-23 19:13:02,113] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 19:13:02,113] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 19:13:02,113] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 19:13:02,113] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 19:13:02,723] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 27834
[2025-05-23 19:13:02,723] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 27834
[2025-05-23 19:13:02,723] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 19:13:02,723] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 19:13:02,723] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [20]  [ 860/1349]  eta: 0:02:33  lr: 0.000366  min_lr: 0.000009  loss: 0.8647 (0.8207)  loss_scale: 131072.0000 (100625.5424)  weight_decay: 0.0500 (0.0500)  time: 0.3066  data: 0.0001  max mem: 41808
Epoch: [20]  [ 870/1349]  eta: 0:02:29  lr: 0.000365  min_lr: 0.000009  loss: 0.8572 (0.8213)  loss_scale: 131072.0000 (100975.0999)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
Epoch: [20]  [ 880/1349]  eta: 0:02:26  lr: 0.000365  min_lr: 0.000009  loss: 0.8572 (0.8218)  loss_scale: 131072.0000 (101316.7219)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [20]  [ 890/1349]  eta: 0:02:23  lr: 0.000365  min_lr: 0.000009  loss: 0.8426 (0.8216)  loss_scale: 131072.0000 (101650.6756)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
Epoch: [20]  [ 900/1349]  eta: 0:02:20  lr: 0.000365  min_lr: 0.000009  loss: 0.7801 (0.8210)  loss_scale: 131072.0000 (101977.2164)  weight_decay: 0.0500 (0.0500)  time: 0.3059  data: 0.0001  max mem: 41808
[2025-05-23 19:13:18,045] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 27884
[2025-05-23 19:13:18,045] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 27884
[2025-05-23 19:13:18,045] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 19:13:18,045] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 19:13:18,045] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [20]  [ 910/1349]  eta: 0:02:17  lr: 0.000365  min_lr: 0.000009  loss: 0.8262 (0.8218)  loss_scale: 131072.0000 (101793.0187)  weight_decay: 0.0500 (0.0500)  time: 0.3055  data: 0.0001  max mem: 41808
Epoch: [20]  [ 920/1349]  eta: 0:02:14  lr: 0.000365  min_lr: 0.000009  loss: 0.8880 (0.8219)  loss_scale: 65536.0000 (101399.3485)  weight_decay: 0.0500 (0.0500)  time: 0.3063  data: 0.0001  max mem: 41808
Epoch: [20]  [ 930/1349]  eta: 0:02:10  lr: 0.000365  min_lr: 0.000009  loss: 0.8861 (0.8227)  loss_scale: 65536.0000 (101014.1353)  weight_decay: 0.0500 (0.0500)  time: 0.3061  data: 0.0001  max mem: 41808
Epoch: [20]  [ 940/1349]  eta: 0:02:07  lr: 0.000365  min_lr: 0.000009  loss: 0.8723 (0.8228)  loss_scale: 65536.0000 (100637.1095)  weight_decay: 0.0500 (0.0500)  time: 0.3052  data: 0.0001  max mem: 41808
Epoch: [20]  [ 950/1349]  eta: 0:02:04  lr: 0.000364  min_lr: 0.000009  loss: 0.8723 (0.8238)  loss_scale: 65536.0000 (100268.0126)  weight_decay: 0.0500 (0.0500)  time: 0.3056  data: 0.0001  max mem: 41808
Epoch: [20]  [ 960/1349]  eta: 0:02:01  lr: 0.000364  min_lr: 0.000009  loss: 0.8937 (0.8240)  loss_scale: 65536.0000 (99906.5973)  weight_decay: 0.0500 (0.0500)  time: 0.3055  data: 0.0001  max mem: 41808
Epoch: [20]  [ 970/1349]  eta: 0:01:58  lr: 0.000364  min_lr: 0.000009  loss: 0.8367 (0.8239)  loss_scale: 65536.0000 (99552.6262)  weight_decay: 0.0500 (0.0500)  time: 0.3060  data: 0.0001  max mem: 41808
Epoch: [20]  [ 980/1349]  eta: 0:01:55  lr: 0.000364  min_lr: 0.000009  loss: 0.8691 (0.8238)  loss_scale: 65536.0000 (99205.8716)  weight_decay: 0.0500 (0.0500)  time: 0.3064  data: 0.0001  max mem: 41808
Epoch: [20]  [ 990/1349]  eta: 0:01:52  lr: 0.000364  min_lr: 0.000009  loss: 0.8766 (0.8249)  loss_scale: 65536.0000 (98866.1150)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
Epoch: [20]  [1000/1349]  eta: 0:01:48  lr: 0.000364  min_lr: 0.000009  loss: 0.8930 (0.8255)  loss_scale: 65536.0000 (98533.1469)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [20]  [1010/1349]  eta: 0:01:45  lr: 0.000364  min_lr: 0.000009  loss: 0.8882 (0.8256)  loss_scale: 65536.0000 (98206.7656)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
[2025-05-23 19:13:53,286] [INFO] [logging.py:96:log_dist] [Rank 0] step=28000, skipped=170, lr=[8.640213123610254e-06, 8.640213123610254e-06, 1.1520284164813672e-05, 1.1520284164813672e-05, 1.5360378886418227e-05, 1.5360378886418227e-05, 2.0480505181890973e-05, 2.0480505181890973e-05, 2.7307340242521296e-05, 2.7307340242521296e-05, 3.640978699002839e-05, 3.640978699002839e-05, 4.8546382653371194e-05, 4.8546382653371194e-05, 6.472851020449493e-05, 6.472851020449493e-05, 8.630468027265989e-05, 8.630468027265989e-05, 0.0001150729070302132, 0.0001150729070302132, 0.00015343054270695094, 0.00015343054270695094, 0.00020457405694260122, 0.00020457405694260122, 0.00027276540925680163, 0.00027276540925680163, 0.0003636872123424022, 0.0003636872123424022], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-05-23 19:13:53,287] [INFO] [timer.py:260:stop] epoch=0/micro_step=28000/global_step=28000, RunningAvgSamplesPerSec=208.19396904242416, CurrSamplesPerSec=214.38973365354434, MemAllocated=1.37GB, MaxMemAllocated=40.83GB
Epoch: [20]  [1020/1349]  eta: 0:01:42  lr: 0.000364  min_lr: 0.000009  loss: 0.8882 (0.8255)  loss_scale: 65536.0000 (97886.7777)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [20]  [1030/1349]  eta: 0:01:39  lr: 0.000364  min_lr: 0.000009  loss: 0.8861 (0.8263)  loss_scale: 65536.0000 (97572.9971)  weight_decay: 0.0500 (0.0500)  time: 0.3056  data: 0.0001  max mem: 41808
[2025-05-23 19:13:57,562] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 19:13:57,562] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 19:13:57,562] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-05-23 19:13:57,562] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [20]  [1040/1349]  eta: 0:01:36  lr: 0.000363  min_lr: 0.000009  loss: 0.8681 (0.8270)  loss_scale: 65536.0000 (97768.8838)  weight_decay: 0.0500 (0.0500)  time: 0.3055  data: 0.0001  max mem: 41808
Epoch: [20]  [1050/1349]  eta: 0:01:33  lr: 0.000363  min_lr: 0.000009  loss: 0.8359 (0.8270)  loss_scale: 131072.0000 (98085.7545)  weight_decay: 0.0500 (0.0500)  time: 0.3055  data: 0.0001  max mem: 41808
Epoch: [20]  [1060/1349]  eta: 0:01:30  lr: 0.000363  min_lr: 0.000009  loss: 0.8175 (0.8272)  loss_scale: 131072.0000 (98396.6522)  weight_decay: 0.0500 (0.0500)  time: 0.3054  data: 0.0001  max mem: 41808
Epoch: [20]  [1070/1349]  eta: 0:01:26  lr: 0.000363  min_lr: 0.000009  loss: 0.7905 (0.8263)  loss_scale: 131072.0000 (98701.7442)  weight_decay: 0.0500 (0.0500)  time: 0.3054  data: 0.0001  max mem: 41808
Epoch: [20]  [1080/1349]  eta: 0:01:23  lr: 0.000363  min_lr: 0.000009  loss: 0.7815 (0.8267)  loss_scale: 131072.0000 (99001.1915)  weight_decay: 0.0500 (0.0500)  time: 0.3054  data: 0.0001  max mem: 41808
Epoch: [20]  [1090/1349]  eta: 0:01:20  lr: 0.000363  min_lr: 0.000009  loss: 0.9220 (0.8278)  loss_scale: 131072.0000 (99295.1494)  weight_decay: 0.0500 (0.0500)  time: 0.3055  data: 0.0001  max mem: 41808
Epoch: [20]  [1100/1349]  eta: 0:01:17  lr: 0.000363  min_lr: 0.000009  loss: 0.8862 (0.8273)  loss_scale: 131072.0000 (99583.7675)  weight_decay: 0.0500 (0.0500)  time: 0.3057  data: 0.0001  max mem: 41808
Epoch: [20]  [1110/1349]  eta: 0:01:14  lr: 0.000363  min_lr: 0.000009  loss: 0.8371 (0.8277)  loss_scale: 131072.0000 (99867.1899)  weight_decay: 0.0500 (0.0500)  time: 0.3056  data: 0.0001  max mem: 41808
Epoch: [20]  [1120/1349]  eta: 0:01:11  lr: 0.000363  min_lr: 0.000009  loss: 0.8371 (0.8270)  loss_scale: 131072.0000 (100145.5558)  weight_decay: 0.0500 (0.0500)  time: 0.3054  data: 0.0001  max mem: 41808
[2025-05-23 19:14:25,673] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 28105
[2025-05-23 19:14:25,674] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 28105
[2025-05-23 19:14:25,674] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 19:14:25,674] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 19:14:25,674] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [20]  [1130/1349]  eta: 0:01:08  lr: 0.000362  min_lr: 0.000009  loss: 0.7728 (0.8261)  loss_scale: 131072.0000 (100071.3280)  weight_decay: 0.0500 (0.0500)  time: 0.3051  data: 0.0001  max mem: 41808
Epoch: [20]  [1140/1349]  eta: 0:01:05  lr: 0.000362  min_lr: 0.000009  loss: 0.8089 (0.8266)  loss_scale: 65536.0000 (99768.6521)  weight_decay: 0.0500 (0.0500)  time: 0.3054  data: 0.0001  max mem: 41808
Epoch: [20]  [1150/1349]  eta: 0:01:01  lr: 0.000362  min_lr: 0.000009  loss: 0.8433 (0.8262)  loss_scale: 65536.0000 (99471.2354)  weight_decay: 0.0500 (0.0500)  time: 0.3058  data: 0.0001  max mem: 41808
Epoch: [20]  [1160/1349]  eta: 0:00:58  lr: 0.000362  min_lr: 0.000009  loss: 0.8311 (0.8264)  loss_scale: 65536.0000 (99178.9423)  weight_decay: 0.0500 (0.0500)  time: 0.3055  data: 0.0001  max mem: 41808
Epoch: [20]  [1170/1349]  eta: 0:00:55  lr: 0.000362  min_lr: 0.000009  loss: 0.8311 (0.8260)  loss_scale: 65536.0000 (98891.6413)  weight_decay: 0.0500 (0.0500)  time: 0.3053  data: 0.0001  max mem: 41808
Epoch: [20]  [1180/1349]  eta: 0:00:52  lr: 0.000362  min_lr: 0.000009  loss: 0.7663 (0.8254)  loss_scale: 65536.0000 (98609.2058)  weight_decay: 0.0500 (0.0500)  time: 0.3053  data: 0.0001  max mem: 41808
Epoch: [20]  [1190/1349]  eta: 0:00:49  lr: 0.000362  min_lr: 0.000009  loss: 0.7192 (0.8251)  loss_scale: 65536.0000 (98331.5130)  weight_decay: 0.0500 (0.0500)  time: 0.3056  data: 0.0001  max mem: 41808
Epoch: [20]  [1200/1349]  eta: 0:00:46  lr: 0.000362  min_lr: 0.000009  loss: 0.8407 (0.8252)  loss_scale: 65536.0000 (98058.4446)  weight_decay: 0.0500 (0.0500)  time: 0.3057  data: 0.0001  max mem: 41808
Epoch: [20]  [1210/1349]  eta: 0:00:43  lr: 0.000361  min_lr: 0.000009  loss: 0.8100 (0.8250)  loss_scale: 65536.0000 (97789.8860)  weight_decay: 0.0500 (0.0500)  time: 0.3057  data: 0.0001  max mem: 41808
Epoch: [20]  [1220/1349]  eta: 0:00:40  lr: 0.000361  min_lr: 0.000009  loss: 0.7865 (0.8247)  loss_scale: 65536.0000 (97525.7265)  weight_decay: 0.0500 (0.0500)  time: 0.3066  data: 0.0001  max mem: 41808
Epoch: [20]  [1230/1349]  eta: 0:00:36  lr: 0.000361  min_lr: 0.000009  loss: 0.8735 (0.8248)  loss_scale: 65536.0000 (97265.8587)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [20]  [1240/1349]  eta: 0:00:33  lr: 0.000361  min_lr: 0.000009  loss: 0.8523 (0.8245)  loss_scale: 65536.0000 (97010.1789)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
Epoch: [20]  [1250/1349]  eta: 0:00:30  lr: 0.000361  min_lr: 0.000009  loss: 0.8266 (0.8245)  loss_scale: 65536.0000 (96758.5867)  weight_decay: 0.0500 (0.0500)  time: 0.3082  data: 0.0001  max mem: 41808
[2025-05-23 19:15:05,196] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 19:15:05,196] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-05-23 19:15:05,196] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 19:15:05,196] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [20]  [1260/1349]  eta: 0:00:27  lr: 0.000361  min_lr: 0.000009  loss: 0.8359 (0.8242)  loss_scale: 65536.0000 (96874.7851)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
Epoch: [20]  [1270/1349]  eta: 0:00:24  lr: 0.000361  min_lr: 0.000009  loss: 0.7948 (0.8243)  loss_scale: 131072.0000 (97143.8426)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
Epoch: [20]  [1280/1349]  eta: 0:00:21  lr: 0.000361  min_lr: 0.000009  loss: 0.8333 (0.8246)  loss_scale: 131072.0000 (97408.6995)  weight_decay: 0.0500 (0.0500)  time: 0.3066  data: 0.0001  max mem: 41808
Epoch: [20]  [1290/1349]  eta: 0:00:18  lr: 0.000361  min_lr: 0.000009  loss: 0.8428 (0.8242)  loss_scale: 131072.0000 (97669.4531)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0001  max mem: 41808
Epoch: [20]  [1300/1349]  eta: 0:00:15  lr: 0.000360  min_lr: 0.000009  loss: 0.8140 (0.8246)  loss_scale: 131072.0000 (97926.1983)  weight_decay: 0.0500 (0.0500)  time: 0.3064  data: 0.0001  max mem: 41808
Epoch: [20]  [1310/1349]  eta: 0:00:12  lr: 0.000360  min_lr: 0.000009  loss: 0.8421 (0.8247)  loss_scale: 131072.0000 (98179.0267)  weight_decay: 0.0500 (0.0500)  time: 0.3062  data: 0.0001  max mem: 41808
Epoch: [20]  [1320/1349]  eta: 0:00:09  lr: 0.000360  min_lr: 0.000009  loss: 0.8372 (0.8246)  loss_scale: 131072.0000 (98428.0273)  weight_decay: 0.0500 (0.0500)  time: 0.3064  data: 0.0001  max mem: 41808
Epoch: [20]  [1330/1349]  eta: 0:00:05  lr: 0.000360  min_lr: 0.000009  loss: 0.8390 (0.8247)  loss_scale: 131072.0000 (98673.2863)  weight_decay: 0.0500 (0.0500)  time: 0.3049  data: 0.0001  max mem: 41808
Epoch: [20]  [1340/1349]  eta: 0:00:02  lr: 0.000360  min_lr: 0.000009  loss: 0.8248 (0.8243)  loss_scale: 131072.0000 (98914.8874)  weight_decay: 0.0500 (0.0500)  time: 0.3026  data: 0.0001  max mem: 41808
Epoch: [20]  [1348/1349]  eta: 0:00:00  lr: 0.000360  min_lr: 0.000009  loss: 0.7554 (0.8236)  loss_scale: 131072.0000 (99105.5893)  weight_decay: 0.0500 (0.0500)  time: 0.3034  data: 0.0001  max mem: 41808
Epoch: [20] Total time: 0:06:59 (0.3108 s / it)
Averaged stats: lr: 0.000360  min_lr: 0.000009  loss: 0.7554 (0.8205)  loss_scale: 131072.0000 (99105.5893)  weight_decay: 0.0500 (0.0500)  total_time: 419.2319 (419.2270)
Val:  [  0/346]  eta: 0:45:26  loss: 3.1481 (3.1481)  acc1: 0.0000 (0.0000)  acc5: 91.4062 (91.4062)  time: 7.8789  data: 6.8971  max mem: 41808
Val:  [ 10/346]  eta: 0:11:11  loss: 0.1461 (0.5782)  acc1: 100.0000 (85.0852)  acc5: 100.0000 (98.7926)  time: 1.9993  data: 1.2053  max mem: 41808
Val:  [ 20/346]  eta: 0:07:39  loss: 0.1420 (0.4475)  acc1: 100.0000 (89.0625)  acc5: 100.0000 (99.1815)  time: 1.0849  data: 0.3318  max mem: 41808
Val:  [ 30/346]  eta: 0:06:31  loss: 0.1235 (0.3821)  acc1: 100.0000 (91.0534)  acc5: 100.0000 (99.3952)  time: 0.8198  data: 0.0773  max mem: 41808
Val:  [ 40/346]  eta: 0:05:46  loss: 0.1284 (0.4150)  acc1: 99.2188 (90.2439)  acc5: 100.0000 (98.8567)  time: 0.8416  data: 0.0637  max mem: 41808
Val:  [ 50/346]  eta: 0:05:16  loss: 0.1231 (0.3670)  acc1: 99.2188 (91.7892)  acc5: 100.0000 (99.0502)  time: 0.8066  data: 0.0003  max mem: 41808
Val:  [ 60/346]  eta: 0:04:49  loss: 0.1658 (0.3533)  acc1: 96.8750 (92.0338)  acc5: 100.0000 (99.1931)  time: 0.7718  data: 0.0003  max mem: 41808
Val:  [ 70/346]  eta: 0:04:33  loss: 0.2543 (0.3824)  acc1: 93.7500 (91.2632)  acc5: 100.0000 (98.8996)  time: 0.7954  data: 0.0572  max mem: 41808
Val:  [ 80/346]  eta: 0:04:18  loss: 0.2401 (0.3779)  acc1: 96.8750 (91.5606)  acc5: 100.0000 (99.0066)  time: 0.8489  data: 0.1267  max mem: 41808
Val:  [ 90/346]  eta: 0:04:06  loss: 0.2578 (0.3852)  acc1: 92.1875 (91.2775)  acc5: 100.0000 (99.0814)  time: 0.8618  data: 0.1362  max mem: 41808
Val:  [100/346]  eta: 0:03:54  loss: 0.2233 (0.3681)  acc1: 96.0938 (91.8085)  acc5: 100.0000 (99.1723)  time: 0.8704  data: 0.1348  max mem: 41808
Val:  [110/346]  eta: 0:03:43  loss: 0.1713 (0.3797)  acc1: 98.4375 (91.4626)  acc5: 100.0000 (99.1695)  time: 0.8644  data: 0.1369  max mem: 41808
Val:  [120/346]  eta: 0:03:32  loss: 0.2065 (0.3768)  acc1: 97.6562 (91.4321)  acc5: 100.0000 (99.2381)  time: 0.8779  data: 0.1460  max mem: 41808
Val:  [130/346]  eta: 0:03:21  loss: 0.1424 (0.3783)  acc1: 99.2188 (91.2691)  acc5: 100.0000 (99.2963)  time: 0.8764  data: 0.1468  max mem: 41808
Val:  [140/346]  eta: 0:03:12  loss: 0.1670 (0.3727)  acc1: 96.0938 (91.4062)  acc5: 100.0000 (99.3351)  time: 0.8985  data: 0.1493  max mem: 41808
Val:  [150/346]  eta: 0:03:02  loss: 0.2470 (0.3716)  acc1: 95.3125 (91.4839)  acc5: 100.0000 (99.3584)  time: 0.9138  data: 0.1473  max mem: 41808
Val:  [160/346]  eta: 0:02:53  loss: 0.2006 (0.3669)  acc1: 97.6562 (91.6149)  acc5: 100.0000 (99.3934)  time: 0.9008  data: 0.1484  max mem: 41808
Val:  [170/346]  eta: 0:02:43  loss: 0.2006 (0.3620)  acc1: 96.0938 (91.7124)  acc5: 100.0000 (99.4289)  time: 0.8903  data: 0.1561  max mem: 41808
Val:  [180/346]  eta: 0:02:33  loss: 0.2317 (0.3727)  acc1: 94.5312 (91.2465)  acc5: 100.0000 (99.4561)  time: 0.8879  data: 0.1428  max mem: 41808
Val:  [190/346]  eta: 0:02:24  loss: 0.2198 (0.3718)  acc1: 93.7500 (91.2754)  acc5: 100.0000 (99.4396)  time: 0.8904  data: 0.1382  max mem: 41808
Val:  [200/346]  eta: 0:02:14  loss: 0.3187 (0.3828)  acc1: 92.1875 (90.8232)  acc5: 100.0000 (99.4675)  time: 0.9115  data: 0.1424  max mem: 41808
Val:  [210/346]  eta: 0:02:05  loss: 0.1860 (0.3747)  acc1: 97.6562 (91.1286)  acc5: 100.0000 (99.4927)  time: 0.9130  data: 0.1451  max mem: 41808
Val:  [220/346]  eta: 0:01:56  loss: 0.1491 (0.3687)  acc1: 99.2188 (91.3462)  acc5: 100.0000 (99.4839)  time: 0.9001  data: 0.1474  max mem: 41808
Val:  [230/346]  eta: 0:01:46  loss: 0.1491 (0.3605)  acc1: 98.4375 (91.6261)  acc5: 100.0000 (99.4961)  time: 0.9185  data: 0.1542  max mem: 41808
Val:  [240/346]  eta: 0:01:37  loss: 0.1546 (0.3664)  acc1: 98.4375 (91.4938)  acc5: 100.0000 (99.4813)  time: 0.9210  data: 0.1565  max mem: 41808
Val:  [250/346]  eta: 0:01:28  loss: 0.2005 (0.3655)  acc1: 97.6562 (91.5557)  acc5: 100.0000 (99.4958)  time: 0.9138  data: 0.1544  max mem: 41808
Val:  [260/346]  eta: 0:01:19  loss: 0.1589 (0.3651)  acc1: 98.4375 (91.5799)  acc5: 100.0000 (99.4792)  time: 0.9120  data: 0.1515  max mem: 41808
Val:  [270/346]  eta: 0:01:09  loss: 0.1444 (0.3605)  acc1: 99.2188 (91.7176)  acc5: 100.0000 (99.4897)  time: 0.9047  data: 0.1482  max mem: 41808
Val:  [280/346]  eta: 0:01:00  loss: 0.1279 (0.3583)  acc1: 97.6562 (91.8122)  acc5: 100.0000 (99.5051)  time: 0.8875  data: 0.1460  max mem: 41808
Val:  [290/346]  eta: 0:00:51  loss: 0.1159 (0.3509)  acc1: 100.0000 (92.0425)  acc5: 100.0000 (99.5221)  time: 0.8781  data: 0.1455  max mem: 41808
Val:  [300/346]  eta: 0:00:42  loss: 0.1189 (0.3515)  acc1: 100.0000 (92.0577)  acc5: 100.0000 (99.4679)  time: 0.8800  data: 0.1469  max mem: 41808
Val:  [310/346]  eta: 0:00:32  loss: 0.1261 (0.3528)  acc1: 100.0000 (91.9865)  acc5: 100.0000 (99.4549)  time: 0.8759  data: 0.1446  max mem: 41808
Val:  [320/346]  eta: 0:00:23  loss: 0.1330 (0.3516)  acc1: 99.2188 (91.9733)  acc5: 100.0000 (99.4719)  time: 0.8907  data: 0.1346  max mem: 41808
Val:  [330/346]  eta: 0:00:14  loss: 0.3304 (0.3650)  acc1: 89.0625 (91.5597)  acc5: 100.0000 (99.4076)  time: 0.9007  data: 0.1373  max mem: 41808
Val:  [340/346]  eta: 0:00:05  loss: 0.4374 (0.3749)  acc1: 89.0625 (91.2436)  acc5: 100.0000 (99.3791)  time: 0.8876  data: 0.1560  max mem: 41808
Val:  [345/346]  eta: 0:00:00  loss: 0.2032 (0.3714)  acc1: 95.3125 (91.3531)  acc5: 100.0000 (99.3875)  time: 0.8968  data: 0.1683  max mem: 41808
Val: Total time: 0:05:16 (0.9140 s / it)
* Acc@1 91.382 Acc@5 99.447 loss 0.369
Accuracy of the network on the 88494 val videos: 91.4%
[2025-05-23 19:20:50,517] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-05-23 19:20:50,521] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2025-05-23 19:20:50,521] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /project/mmendoscope/Natural_Comparison/Cholec75/unified_base_st_settingE_videomae-st_Cholec80_0.0005_0.75_online_key_frame_frame16_Fixed_Stride_4/checkpoint-best/mp_rank_00_model_states.pt
[2025-05-23 19:20:50,521] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /project/mmendoscope/Natural_Comparison/Cholec75/unified_base_st_settingE_videomae-st_Cholec80_0.0005_0.75_online_key_frame_frame16_Fixed_Stride_4/checkpoint-best/mp_rank_00_model_states.pt...
[2025-05-23 19:20:53,595] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /project/mmendoscope/Natural_Comparison/Cholec75/unified_base_st_settingE_videomae-st_Cholec80_0.0005_0.75_online_key_frame_frame16_Fixed_Stride_4/checkpoint-best/mp_rank_00_model_states.pt.
[2025-05-23 19:20:53,595] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 91.38%   Max Epoch: 20
Epoch: [21]  [   0/1349]  eta: 2:04:20  lr: 0.000360  min_lr: 0.000009  loss: 0.9714 (0.9714)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 5.5305  data: 4.9170  max mem: 41808
Epoch: [21]  [  10/1349]  eta: 0:17:34  lr: 0.000360  min_lr: 0.000009  loss: 0.8566 (0.8464)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7876  data: 0.4471  max mem: 41808
Epoch: [21]  [  20/1349]  eta: 0:12:23  lr: 0.000360  min_lr: 0.000009  loss: 0.8345 (0.8222)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3110  data: 0.0001  max mem: 41808
Epoch: [21]  [  30/1349]  eta: 0:10:31  lr: 0.000360  min_lr: 0.000009  loss: 0.8452 (0.8341)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3084  data: 0.0001  max mem: 41808
[2025-05-23 19:21:09,355] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 19:21:09,356] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 19:21:09,356] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 19:21:09,356] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 19:21:09,659] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 28363
[2025-05-23 19:21:09,659] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 28363
[2025-05-23 19:21:09,659] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 19:21:09,659] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 19:21:09,659] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [21]  [  40/1349]  eta: 0:09:31  lr: 0.000359  min_lr: 0.000009  loss: 0.8790 (0.8331)  loss_scale: 131072.0000 (134268.8780)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
Epoch: [21]  [  50/1349]  eta: 0:08:54  lr: 0.000359  min_lr: 0.000009  loss: 0.8860 (0.8439)  loss_scale: 131072.0000 (133642.0392)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0002  max mem: 41808
Epoch: [21]  [  60/1349]  eta: 0:08:28  lr: 0.000359  min_lr: 0.000009  loss: 0.8741 (0.8396)  loss_scale: 131072.0000 (133220.7213)  weight_decay: 0.0500 (0.0500)  time: 0.3089  data: 0.0002  max mem: 41808
Epoch: [21]  [  70/1349]  eta: 0:08:09  lr: 0.000359  min_lr: 0.000009  loss: 0.8741 (0.8485)  loss_scale: 131072.0000 (132918.0845)  weight_decay: 0.0500 (0.0500)  time: 0.3087  data: 0.0002  max mem: 41808
Epoch: [21]  [  80/1349]  eta: 0:07:53  lr: 0.000359  min_lr: 0.000009  loss: 0.8705 (0.8432)  loss_scale: 131072.0000 (132690.1728)  weight_decay: 0.0500 (0.0500)  time: 0.3087  data: 0.0001  max mem: 41808
Epoch: [21]  [  90/1349]  eta: 0:07:40  lr: 0.000359  min_lr: 0.000009  loss: 0.8745 (0.8510)  loss_scale: 131072.0000 (132512.3516)  weight_decay: 0.0500 (0.0500)  time: 0.3082  data: 0.0001  max mem: 41808
Epoch: [21]  [ 100/1349]  eta: 0:07:30  lr: 0.000359  min_lr: 0.000009  loss: 0.9207 (0.8453)  loss_scale: 131072.0000 (132369.7426)  weight_decay: 0.0500 (0.0500)  time: 0.3079  data: 0.0001  max mem: 41808
Epoch: [21]  [ 110/1349]  eta: 0:07:20  lr: 0.000359  min_lr: 0.000009  loss: 0.7459 (0.8333)  loss_scale: 131072.0000 (132252.8288)  weight_decay: 0.0500 (0.0500)  time: 0.3086  data: 0.0002  max mem: 41808
Epoch: [21]  [ 120/1349]  eta: 0:07:12  lr: 0.000358  min_lr: 0.000009  loss: 0.6701 (0.8222)  loss_scale: 131072.0000 (132155.2397)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0002  max mem: 41808
Epoch: [21]  [ 130/1349]  eta: 0:07:04  lr: 0.000358  min_lr: 0.000009  loss: 0.7660 (0.8273)  loss_scale: 131072.0000 (132072.5496)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0002  max mem: 41808
Epoch: [21]  [ 140/1349]  eta: 0:06:57  lr: 0.000358  min_lr: 0.000009  loss: 0.9451 (0.8357)  loss_scale: 131072.0000 (132001.5887)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0002  max mem: 41808
Epoch: [21]  [ 150/1349]  eta: 0:06:51  lr: 0.000358  min_lr: 0.000009  loss: 0.9038 (0.8350)  loss_scale: 131072.0000 (131940.0265)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [21]  [ 160/1349]  eta: 0:06:45  lr: 0.000358  min_lr: 0.000009  loss: 0.7366 (0.8287)  loss_scale: 131072.0000 (131886.1118)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
[2025-05-23 19:21:49,391] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 19:21:49,391] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 19:21:49,394] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 19:21:49,394] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [21]  [ 170/1349]  eta: 0:06:39  lr: 0.000358  min_lr: 0.000009  loss: 0.7802 (0.8302)  loss_scale: 131072.0000 (137970.5263)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
[2025-05-23 19:21:53,366] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 28505
[2025-05-23 19:21:53,366] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 28505
[2025-05-23 19:21:53,366] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 19:21:53,366] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 19:21:53,366] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [21]  [ 180/1349]  eta: 0:06:33  lr: 0.000358  min_lr: 0.000008  loss: 0.8428 (0.8310)  loss_scale: 262144.0000 (141210.1657)  weight_decay: 0.0500 (0.0500)  time: 0.3062  data: 0.0001  max mem: 41808
Epoch: [21]  [ 190/1349]  eta: 0:06:28  lr: 0.000358  min_lr: 0.000008  loss: 0.8977 (0.8313)  loss_scale: 131072.0000 (140679.3717)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
[2025-05-23 19:21:59,839] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 28526
[2025-05-23 19:21:59,839] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 28526
[2025-05-23 19:21:59,839] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 19:21:59,839] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 19:21:59,839] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [21]  [ 200/1349]  eta: 0:06:23  lr: 0.000358  min_lr: 0.000008  loss: 0.8379 (0.8304)  loss_scale: 131072.0000 (138897.1940)  weight_decay: 0.0500 (0.0500)  time: 0.3085  data: 0.0001  max mem: 41808
Epoch: [21]  [ 210/1349]  eta: 0:06:19  lr: 0.000357  min_lr: 0.000008  loss: 0.8130 (0.8272)  loss_scale: 65536.0000 (135420.3602)  weight_decay: 0.0500 (0.0500)  time: 0.3084  data: 0.0001  max mem: 41808
Epoch: [21]  [ 220/1349]  eta: 0:06:14  lr: 0.000357  min_lr: 0.000008  loss: 0.7663 (0.8258)  loss_scale: 65536.0000 (132258.1719)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0002  max mem: 41808
Epoch: [21]  [ 230/1349]  eta: 0:06:09  lr: 0.000357  min_lr: 0.000008  loss: 0.8971 (0.8298)  loss_scale: 65536.0000 (129369.7662)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0002  max mem: 41808
Epoch: [21]  [ 240/1349]  eta: 0:06:05  lr: 0.000357  min_lr: 0.000008  loss: 0.8508 (0.8250)  loss_scale: 65536.0000 (126721.0622)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0001  max mem: 41808
Epoch: [21]  [ 250/1349]  eta: 0:06:01  lr: 0.000357  min_lr: 0.000008  loss: 0.8486 (0.8270)  loss_scale: 65536.0000 (124283.4104)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0002  max mem: 41808
Epoch: [21]  [ 260/1349]  eta: 0:05:57  lr: 0.000357  min_lr: 0.000008  loss: 0.9115 (0.8287)  loss_scale: 65536.0000 (122032.5517)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
Epoch: [21]  [ 270/1349]  eta: 0:05:52  lr: 0.000357  min_lr: 0.000008  loss: 0.8415 (0.8271)  loss_scale: 65536.0000 (119947.8081)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [21]  [ 280/1349]  eta: 0:05:48  lr: 0.000357  min_lr: 0.000008  loss: 0.8623 (0.8281)  loss_scale: 65536.0000 (118011.4448)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [21]  [ 290/1349]  eta: 0:05:44  lr: 0.000356  min_lr: 0.000008  loss: 0.7601 (0.8235)  loss_scale: 65536.0000 (116208.1649)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [21]  [ 300/1349]  eta: 0:05:41  lr: 0.000356  min_lr: 0.000008  loss: 0.7601 (0.8214)  loss_scale: 65536.0000 (114524.7043)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
Epoch: [21]  [ 310/1349]  eta: 0:05:37  lr: 0.000356  min_lr: 0.000008  loss: 0.8271 (0.8213)  loss_scale: 65536.0000 (112949.5048)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
Epoch: [21]  [ 320/1349]  eta: 0:05:33  lr: 0.000356  min_lr: 0.000008  loss: 0.8837 (0.8229)  loss_scale: 65536.0000 (111472.4486)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
[2025-05-23 19:22:39,528] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 19:22:39,528] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 19:22:39,528] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-05-23 19:22:39,528] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [21]  [ 330/1349]  eta: 0:05:29  lr: 0.000356  min_lr: 0.000008  loss: 0.9057 (0.8233)  loss_scale: 65536.0000 (111074.6103)  weight_decay: 0.0500 (0.0500)  time: 0.3091  data: 0.0001  max mem: 41808
Epoch: [21]  [ 340/1349]  eta: 0:05:26  lr: 0.000356  min_lr: 0.000008  loss: 0.8996 (0.8244)  loss_scale: 131072.0000 (111661.0440)  weight_decay: 0.0500 (0.0500)  time: 0.3088  data: 0.0001  max mem: 41808
Epoch: [21]  [ 350/1349]  eta: 0:05:22  lr: 0.000356  min_lr: 0.000008  loss: 0.8980 (0.8257)  loss_scale: 131072.0000 (112214.0627)  weight_decay: 0.0500 (0.0500)  time: 0.3064  data: 0.0001  max mem: 41808
Epoch: [21]  [ 360/1349]  eta: 0:05:18  lr: 0.000356  min_lr: 0.000008  loss: 0.8634 (0.8260)  loss_scale: 131072.0000 (112736.4432)  weight_decay: 0.0500 (0.0500)  time: 0.3064  data: 0.0001  max mem: 41808
Epoch: [21]  [ 370/1349]  eta: 0:05:14  lr: 0.000356  min_lr: 0.000008  loss: 0.8018 (0.8239)  loss_scale: 131072.0000 (113230.6631)  weight_decay: 0.0500 (0.0500)  time: 0.3059  data: 0.0001  max mem: 41808
Epoch: [21]  [ 380/1349]  eta: 0:05:11  lr: 0.000355  min_lr: 0.000008  loss: 0.6612 (0.8212)  loss_scale: 131072.0000 (113698.9396)  weight_decay: 0.0500 (0.0500)  time: 0.3059  data: 0.0001  max mem: 41808
Epoch: [21]  [ 390/1349]  eta: 0:05:07  lr: 0.000355  min_lr: 0.000008  loss: 0.8165 (0.8226)  loss_scale: 131072.0000 (114143.2634)  weight_decay: 0.0500 (0.0500)  time: 0.3064  data: 0.0001  max mem: 41808
Epoch: [21]  [ 400/1349]  eta: 0:05:04  lr: 0.000355  min_lr: 0.000008  loss: 0.8391 (0.8227)  loss_scale: 131072.0000 (114565.4264)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [21]  [ 410/1349]  eta: 0:05:00  lr: 0.000355  min_lr: 0.000008  loss: 0.8332 (0.8229)  loss_scale: 131072.0000 (114967.0462)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
Epoch: [21]  [ 420/1349]  eta: 0:04:57  lr: 0.000355  min_lr: 0.000008  loss: 0.9087 (0.8245)  loss_scale: 131072.0000 (115349.5867)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
Epoch: [21]  [ 430/1349]  eta: 0:04:53  lr: 0.000355  min_lr: 0.000008  loss: 0.8312 (0.8236)  loss_scale: 131072.0000 (115714.3759)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
Epoch: [21]  [ 440/1349]  eta: 0:04:50  lr: 0.000355  min_lr: 0.000008  loss: 0.7750 (0.8225)  loss_scale: 131072.0000 (116062.6213)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [21]  [ 450/1349]  eta: 0:04:46  lr: 0.000355  min_lr: 0.000008  loss: 0.7762 (0.8226)  loss_scale: 131072.0000 (116395.4235)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
[2025-05-23 19:23:18,810] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 19:23:18,810] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 19:23:18,810] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 19:23:18,810] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [21]  [ 460/1349]  eta: 0:04:43  lr: 0.000355  min_lr: 0.000008  loss: 0.8394 (0.8224)  loss_scale: 131072.0000 (118704.0347)  weight_decay: 0.0500 (0.0500)  time: 0.3079  data: 0.0001  max mem: 41808
Epoch: [21]  [ 470/1349]  eta: 0:04:40  lr: 0.000354  min_lr: 0.000008  loss: 0.8883 (0.8228)  loss_scale: 262144.0000 (121749.4692)  weight_decay: 0.0500 (0.0500)  time: 0.3082  data: 0.0001  max mem: 41808
[2025-05-23 19:23:24,969] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 28803
[2025-05-23 19:23:24,969] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 28803
[2025-05-23 19:23:24,969] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 19:23:24,969] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 19:23:24,969] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [21]  [ 480/1349]  eta: 0:04:36  lr: 0.000354  min_lr: 0.000008  loss: 0.9034 (0.8240)  loss_scale: 262144.0000 (122760.7817)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0001  max mem: 41808
Epoch: [21]  [ 490/1349]  eta: 0:04:33  lr: 0.000354  min_lr: 0.000008  loss: 0.8992 (0.8251)  loss_scale: 131072.0000 (122930.0530)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0002  max mem: 41808
Epoch: [21]  [ 500/1349]  eta: 0:04:29  lr: 0.000354  min_lr: 0.000008  loss: 0.8417 (0.8251)  loss_scale: 131072.0000 (123092.5669)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
Epoch: [21]  [ 510/1349]  eta: 0:04:26  lr: 0.000354  min_lr: 0.000008  loss: 0.8149 (0.8249)  loss_scale: 131072.0000 (123248.7202)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0001  max mem: 41808
Epoch: [21]  [ 520/1349]  eta: 0:04:23  lr: 0.000354  min_lr: 0.000008  loss: 0.8573 (0.8255)  loss_scale: 131072.0000 (123398.8791)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0001  max mem: 41808
Epoch: [21]  [ 530/1349]  eta: 0:04:19  lr: 0.000354  min_lr: 0.000008  loss: 0.8635 (0.8264)  loss_scale: 131072.0000 (123543.3823)  weight_decay: 0.0500 (0.0500)  time: 0.3064  data: 0.0002  max mem: 41808
Epoch: [21]  [ 540/1349]  eta: 0:04:16  lr: 0.000354  min_lr: 0.000008  loss: 0.8395 (0.8264)  loss_scale: 131072.0000 (123682.5434)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0002  max mem: 41808
Epoch: [21]  [ 550/1349]  eta: 0:04:13  lr: 0.000353  min_lr: 0.000008  loss: 0.8160 (0.8260)  loss_scale: 131072.0000 (123816.6534)  weight_decay: 0.0500 (0.0500)  time: 0.3066  data: 0.0001  max mem: 41808
Epoch: [21]  [ 560/1349]  eta: 0:04:09  lr: 0.000353  min_lr: 0.000008  loss: 0.8473 (0.8272)  loss_scale: 131072.0000 (123945.9822)  weight_decay: 0.0500 (0.0500)  time: 0.3064  data: 0.0001  max mem: 41808
Epoch: [21]  [ 570/1349]  eta: 0:04:06  lr: 0.000353  min_lr: 0.000008  loss: 0.8131 (0.8254)  loss_scale: 131072.0000 (124070.7811)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0001  max mem: 41808
Epoch: [21]  [ 580/1349]  eta: 0:04:03  lr: 0.000353  min_lr: 0.000008  loss: 0.7683 (0.8244)  loss_scale: 131072.0000 (124191.2840)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
Epoch: [21]  [ 590/1349]  eta: 0:04:00  lr: 0.000353  min_lr: 0.000008  loss: 0.7787 (0.8237)  loss_scale: 131072.0000 (124307.7090)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0001  max mem: 41808
Epoch: [21]  [ 600/1349]  eta: 0:03:56  lr: 0.000353  min_lr: 0.000008  loss: 0.8185 (0.8245)  loss_scale: 131072.0000 (124420.2596)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
[2025-05-23 19:24:04,540] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 19:24:04,540] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 19:24:04,540] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 19:24:04,540] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 19:24:06,377] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 28938
[2025-05-23 19:24:06,377] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 28938
[2025-05-23 19:24:06,377] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 19:24:06,377] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 19:24:06,377] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [21]  [ 610/1349]  eta: 0:03:53  lr: 0.000353  min_lr: 0.000008  loss: 0.9278 (0.8250)  loss_scale: 131072.0000 (125816.2488)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0001  max mem: 41808
Epoch: [21]  [ 620/1349]  eta: 0:03:50  lr: 0.000353  min_lr: 0.000008  loss: 0.9278 (0.8255)  loss_scale: 131072.0000 (125900.8824)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0002  max mem: 41808
Epoch: [21]  [ 630/1349]  eta: 0:03:46  lr: 0.000353  min_lr: 0.000008  loss: 0.8908 (0.8265)  loss_scale: 131072.0000 (125982.8336)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [21]  [ 640/1349]  eta: 0:03:43  lr: 0.000352  min_lr: 0.000008  loss: 0.8878 (0.8272)  loss_scale: 131072.0000 (126062.2278)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
[2025-05-23 19:24:18,362] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 28977
[2025-05-23 19:24:18,362] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 28977
[2025-05-23 19:24:18,363] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 19:24:18,363] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 19:24:18,363] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [21]  [ 650/1349]  eta: 0:03:40  lr: 0.000352  min_lr: 0.000008  loss: 0.8671 (0.8264)  loss_scale: 131072.0000 (125837.1736)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [21]  [ 660/1349]  eta: 0:03:37  lr: 0.000352  min_lr: 0.000008  loss: 0.8597 (0.8271)  loss_scale: 65536.0000 (124924.9017)  weight_decay: 0.0500 (0.0500)  time: 0.3090  data: 0.0001  max mem: 41808
[2025-05-23 19:24:25,171] [INFO] [logging.py:96:log_dist] [Rank 0] step=29000, skipped=177, lr=[8.363361608064399e-06, 8.363361608064399e-06, 1.1151148810752533e-05, 1.1151148810752533e-05, 1.4868198414336709e-05, 1.4868198414336709e-05, 1.9824264552448946e-05, 1.9824264552448946e-05, 2.6432352736598594e-05, 2.6432352736598594e-05, 3.5243136982131456e-05, 3.5243136982131456e-05, 4.6990849309508615e-05, 4.6990849309508615e-05, 6.265446574601149e-05, 6.265446574601149e-05, 8.353928766134864e-05, 8.353928766134864e-05, 0.00011138571688179819, 0.00011138571688179819, 0.00014851428917573092, 0.00014851428917573092, 0.0001980190522343079, 0.0001980190522343079, 0.00026402540297907717, 0.00026402540297907717, 0.0003520338706387696, 0.0003520338706387696], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-05-23 19:24:25,171] [INFO] [timer.py:260:stop] epoch=0/micro_step=29000/global_step=29000, RunningAvgSamplesPerSec=208.37090819496575, CurrSamplesPerSec=212.88029761239173, MemAllocated=1.37GB, MaxMemAllocated=40.83GB
Epoch: [21]  [ 670/1349]  eta: 0:03:34  lr: 0.000352  min_lr: 0.000008  loss: 0.8367 (0.8258)  loss_scale: 65536.0000 (124039.8212)  weight_decay: 0.0500 (0.0500)  time: 0.3095  data: 0.0001  max mem: 41808
Epoch: [21]  [ 680/1349]  eta: 0:03:30  lr: 0.000352  min_lr: 0.000008  loss: 0.8209 (0.8260)  loss_scale: 65536.0000 (123180.7342)  weight_decay: 0.0500 (0.0500)  time: 0.3078  data: 0.0001  max mem: 41808
Epoch: [21]  [ 690/1349]  eta: 0:03:27  lr: 0.000352  min_lr: 0.000008  loss: 0.8500 (0.8264)  loss_scale: 65536.0000 (122346.5123)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
Epoch: [21]  [ 700/1349]  eta: 0:03:24  lr: 0.000352  min_lr: 0.000008  loss: 0.8470 (0.8260)  loss_scale: 65536.0000 (121536.0913)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [21]  [ 710/1349]  eta: 0:03:21  lr: 0.000352  min_lr: 0.000008  loss: 0.8130 (0.8260)  loss_scale: 65536.0000 (120748.4669)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [21]  [ 720/1349]  eta: 0:03:17  lr: 0.000351  min_lr: 0.000008  loss: 0.8358 (0.8264)  loss_scale: 65536.0000 (119982.6907)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [21]  [ 730/1349]  eta: 0:03:14  lr: 0.000351  min_lr: 0.000008  loss: 0.8358 (0.8257)  loss_scale: 65536.0000 (119237.8659)  weight_decay: 0.0500 (0.0500)  time: 0.3080  data: 0.0001  max mem: 41808
Epoch: [21]  [ 740/1349]  eta: 0:03:11  lr: 0.000351  min_lr: 0.000008  loss: 0.8302 (0.8265)  loss_scale: 65536.0000 (118513.1444)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
Epoch: [21]  [ 750/1349]  eta: 0:03:08  lr: 0.000351  min_lr: 0.000008  loss: 0.8530 (0.8272)  loss_scale: 65536.0000 (117807.7230)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [21]  [ 760/1349]  eta: 0:03:05  lr: 0.000351  min_lr: 0.000008  loss: 0.8855 (0.8284)  loss_scale: 65536.0000 (117120.8410)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
Epoch: [21]  [ 770/1349]  eta: 0:03:01  lr: 0.000351  min_lr: 0.000008  loss: 0.8461 (0.8283)  loss_scale: 65536.0000 (116451.7769)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
[2025-05-23 19:24:58,058] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 19:24:58,058] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 19:24:58,059] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-05-23 19:24:58,059] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [21]  [ 780/1349]  eta: 0:02:58  lr: 0.000351  min_lr: 0.000008  loss: 0.8305 (0.8284)  loss_scale: 65536.0000 (116135.4981)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0001  max mem: 41808
Epoch: [21]  [ 790/1349]  eta: 0:02:55  lr: 0.000351  min_lr: 0.000008  loss: 0.8377 (0.8289)  loss_scale: 131072.0000 (116324.3287)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
Epoch: [21]  [ 800/1349]  eta: 0:02:52  lr: 0.000350  min_lr: 0.000008  loss: 0.8013 (0.8274)  loss_scale: 131072.0000 (116508.4444)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [21]  [ 810/1349]  eta: 0:02:49  lr: 0.000350  min_lr: 0.000008  loss: 0.7478 (0.8273)  loss_scale: 131072.0000 (116688.0197)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [21]  [ 820/1349]  eta: 0:02:45  lr: 0.000350  min_lr: 0.000008  loss: 0.8390 (0.8272)  loss_scale: 131072.0000 (116863.2205)  weight_decay: 0.0500 (0.0500)  time: 0.3078  data: 0.0001  max mem: 41808
Epoch: [21]  [ 830/1349]  eta: 0:02:42  lr: 0.000350  min_lr: 0.000008  loss: 0.8743 (0.8285)  loss_scale: 131072.0000 (117034.2046)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [21]  [ 840/1349]  eta: 0:02:39  lr: 0.000350  min_lr: 0.000008  loss: 0.8890 (0.8286)  loss_scale: 131072.0000 (117201.1225)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
Epoch: [21]  [ 850/1349]  eta: 0:02:36  lr: 0.000350  min_lr: 0.000008  loss: 0.8331 (0.8285)  loss_scale: 131072.0000 (117364.1175)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
Epoch: [21]  [ 860/1349]  eta: 0:02:33  lr: 0.000350  min_lr: 0.000008  loss: 0.8296 (0.8288)  loss_scale: 131072.0000 (117523.3264)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [21]  [ 870/1349]  eta: 0:02:30  lr: 0.000350  min_lr: 0.000008  loss: 0.8454 (0.8291)  loss_scale: 131072.0000 (117678.8794)  weight_decay: 0.0500 (0.0500)  time: 0.3079  data: 0.0001  max mem: 41808
Epoch: [21]  [ 880/1349]  eta: 0:02:26  lr: 0.000350  min_lr: 0.000008  loss: 0.8345 (0.8290)  loss_scale: 131072.0000 (117830.9012)  weight_decay: 0.0500 (0.0500)  time: 0.3079  data: 0.0001  max mem: 41808
Epoch: [21]  [ 890/1349]  eta: 0:02:23  lr: 0.000349  min_lr: 0.000008  loss: 0.7960 (0.8283)  loss_scale: 131072.0000 (117979.5107)  weight_decay: 0.0500 (0.0500)  time: 0.3089  data: 0.0001  max mem: 41808
Epoch: [21]  [ 900/1349]  eta: 0:02:20  lr: 0.000349  min_lr: 0.000008  loss: 0.7920 (0.8280)  loss_scale: 131072.0000 (118124.8213)  weight_decay: 0.0500 (0.0500)  time: 0.3088  data: 0.0001  max mem: 41808
[2025-05-23 19:25:37,457] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 19:25:37,457] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 19:25:37,457] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 19:25:37,457] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [21]  [ 910/1349]  eta: 0:02:17  lr: 0.000349  min_lr: 0.000008  loss: 0.7674 (0.8265)  loss_scale: 131072.0000 (119130.2042)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
[2025-05-23 19:25:40,530] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 29244
[2025-05-23 19:25:40,530] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 29244
[2025-05-23 19:25:40,530] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 19:25:40,530] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 19:25:40,530] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [21]  [ 920/1349]  eta: 0:02:14  lr: 0.000349  min_lr: 0.000008  loss: 0.8528 (0.8278)  loss_scale: 131072.0000 (119829.1249)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [21]  [ 930/1349]  eta: 0:02:11  lr: 0.000349  min_lr: 0.000008  loss: 0.8609 (0.8277)  loss_scale: 131072.0000 (119949.8861)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0002  max mem: 41808
Epoch: [21]  [ 940/1349]  eta: 0:02:08  lr: 0.000349  min_lr: 0.000008  loss: 0.8446 (0.8275)  loss_scale: 131072.0000 (120068.0808)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0002  max mem: 41808
Epoch: [21]  [ 950/1349]  eta: 0:02:04  lr: 0.000349  min_lr: 0.000008  loss: 0.8612 (0.8279)  loss_scale: 131072.0000 (120183.7897)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [21]  [ 960/1349]  eta: 0:02:01  lr: 0.000349  min_lr: 0.000008  loss: 0.8034 (0.8271)  loss_scale: 131072.0000 (120297.0905)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
Epoch: [21]  [ 970/1349]  eta: 0:01:58  lr: 0.000348  min_lr: 0.000008  loss: 0.7901 (0.8274)  loss_scale: 131072.0000 (120408.0577)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [21]  [ 980/1349]  eta: 0:01:55  lr: 0.000348  min_lr: 0.000008  loss: 0.8081 (0.8267)  loss_scale: 131072.0000 (120516.7625)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [21]  [ 990/1349]  eta: 0:01:52  lr: 0.000348  min_lr: 0.000008  loss: 0.8081 (0.8265)  loss_scale: 131072.0000 (120623.2735)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0001  max mem: 41808
Epoch: [21]  [1000/1349]  eta: 0:01:49  lr: 0.000348  min_lr: 0.000008  loss: 0.8887 (0.8271)  loss_scale: 131072.0000 (120727.6563)  weight_decay: 0.0500 (0.0500)  time: 0.3087  data: 0.0004  max mem: 41808
Epoch: [21]  [1010/1349]  eta: 0:01:45  lr: 0.000348  min_lr: 0.000008  loss: 0.8550 (0.8270)  loss_scale: 131072.0000 (120829.9743)  weight_decay: 0.0500 (0.0500)  time: 0.3095  data: 0.0004  max mem: 41808
Epoch: [21]  [1020/1349]  eta: 0:01:42  lr: 0.000348  min_lr: 0.000008  loss: 0.8462 (0.8267)  loss_scale: 131072.0000 (120930.2880)  weight_decay: 0.0500 (0.0500)  time: 0.3081  data: 0.0002  max mem: 41808
[2025-05-23 19:26:14,400] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 29354
[2025-05-23 19:26:14,400] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 19:26:14,400] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 29354
[2025-05-23 19:26:14,400] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 19:26:14,400] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [21]  [1030/1349]  eta: 0:01:39  lr: 0.000348  min_lr: 0.000008  loss: 0.8462 (0.8270)  loss_scale: 131072.0000 (120647.2629)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0002  max mem: 41808
Epoch: [21]  [1040/1349]  eta: 0:01:36  lr: 0.000348  min_lr: 0.000008  loss: 0.8876 (0.8270)  loss_scale: 65536.0000 (120117.8559)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0002  max mem: 41808
Epoch: [21]  [1050/1349]  eta: 0:01:33  lr: 0.000348  min_lr: 0.000008  loss: 0.8235 (0.8268)  loss_scale: 65536.0000 (119598.5233)  weight_decay: 0.0500 (0.0500)  time: 0.3080  data: 0.0001  max mem: 41808
Epoch: [21]  [1060/1349]  eta: 0:01:30  lr: 0.000347  min_lr: 0.000008  loss: 0.8363 (0.8274)  loss_scale: 65536.0000 (119088.9802)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0001  max mem: 41808
Epoch: [21]  [1070/1349]  eta: 0:01:27  lr: 0.000347  min_lr: 0.000008  loss: 0.8897 (0.8276)  loss_scale: 65536.0000 (118588.9524)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
Epoch: [21]  [1080/1349]  eta: 0:01:24  lr: 0.000347  min_lr: 0.000008  loss: 0.8471 (0.8278)  loss_scale: 65536.0000 (118098.1758)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
Epoch: [21]  [1090/1349]  eta: 0:01:20  lr: 0.000347  min_lr: 0.000008  loss: 0.8413 (0.8283)  loss_scale: 65536.0000 (117616.3960)  weight_decay: 0.0500 (0.0500)  time: 0.3081  data: 0.0001  max mem: 41808
Epoch: [21]  [1100/1349]  eta: 0:01:17  lr: 0.000347  min_lr: 0.000008  loss: 0.8413 (0.8281)  loss_scale: 65536.0000 (117143.3678)  weight_decay: 0.0500 (0.0500)  time: 0.3085  data: 0.0001  max mem: 41808
Epoch: [21]  [1110/1349]  eta: 0:01:14  lr: 0.000347  min_lr: 0.000008  loss: 0.8715 (0.8285)  loss_scale: 65536.0000 (116678.8551)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [21]  [1120/1349]  eta: 0:01:11  lr: 0.000347  min_lr: 0.000008  loss: 0.7904 (0.8270)  loss_scale: 65536.0000 (116222.6298)  weight_decay: 0.0500 (0.0500)  time: 0.3080  data: 0.0001  max mem: 41808
Epoch: [21]  [1130/1349]  eta: 0:01:08  lr: 0.000347  min_lr: 0.000008  loss: 0.6146 (0.8260)  loss_scale: 65536.0000 (115774.4721)  weight_decay: 0.0500 (0.0500)  time: 0.3080  data: 0.0001  max mem: 41808
Epoch: [21]  [1140/1349]  eta: 0:01:05  lr: 0.000346  min_lr: 0.000008  loss: 0.6305 (0.8249)  loss_scale: 65536.0000 (115334.1700)  weight_decay: 0.0500 (0.0500)  time: 0.3079  data: 0.0001  max mem: 41808
Epoch: [21]  [1150/1349]  eta: 0:01:02  lr: 0.000346  min_lr: 0.000008  loss: 0.7623 (0.8252)  loss_scale: 65536.0000 (114901.5187)  weight_decay: 0.0500 (0.0500)  time: 0.3083  data: 0.0001  max mem: 41808
[2025-05-23 19:26:54,119] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 19:26:54,120] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-05-23 19:26:54,120] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 19:26:54,120] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [21]  [1160/1349]  eta: 0:00:58  lr: 0.000346  min_lr: 0.000008  loss: 0.8524 (0.8251)  loss_scale: 65536.0000 (114871.4556)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
Epoch: [21]  [1170/1349]  eta: 0:00:55  lr: 0.000346  min_lr: 0.000008  loss: 0.8102 (0.8248)  loss_scale: 131072.0000 (115009.8036)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0001  max mem: 41808
Epoch: [21]  [1180/1349]  eta: 0:00:52  lr: 0.000346  min_lr: 0.000008  loss: 0.7974 (0.8250)  loss_scale: 131072.0000 (115145.8086)  weight_decay: 0.0500 (0.0500)  time: 0.3078  data: 0.0001  max mem: 41808
Epoch: [21]  [1190/1349]  eta: 0:00:49  lr: 0.000346  min_lr: 0.000008  loss: 0.7972 (0.8248)  loss_scale: 131072.0000 (115279.5298)  weight_decay: 0.0500 (0.0500)  time: 0.3096  data: 0.0001  max mem: 41808
Epoch: [21]  [1200/1349]  eta: 0:00:46  lr: 0.000346  min_lr: 0.000008  loss: 0.7887 (0.8249)  loss_scale: 131072.0000 (115411.0241)  weight_decay: 0.0500 (0.0500)  time: 0.3093  data: 0.0001  max mem: 41808
Epoch: [21]  [1210/1349]  eta: 0:00:43  lr: 0.000346  min_lr: 0.000008  loss: 0.8049 (0.8242)  loss_scale: 131072.0000 (115540.3468)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
[2025-05-23 19:27:13,540] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 29546
[2025-05-23 19:27:13,540] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 19:27:13,540] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
[2025-05-23 19:27:13,540] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 29546
[2025-05-23 19:27:13,540] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
Epoch: [21]  [1220/1349]  eta: 0:00:40  lr: 0.000346  min_lr: 0.000008  loss: 0.7593 (0.8237)  loss_scale: 131072.0000 (115452.8550)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0001  max mem: 41808
Epoch: [21]  [1230/1349]  eta: 0:00:37  lr: 0.000345  min_lr: 0.000008  loss: 0.7593 (0.8228)  loss_scale: 65536.0000 (115047.3566)  weight_decay: 0.0500 (0.0500)  time: 0.3087  data: 0.0001  max mem: 41808
Epoch: [21]  [1240/1349]  eta: 0:00:33  lr: 0.000345  min_lr: 0.000008  loss: 0.7377 (0.8224)  loss_scale: 65536.0000 (114648.3932)  weight_decay: 0.0500 (0.0500)  time: 0.3085  data: 0.0001  max mem: 41808
Epoch: [21]  [1250/1349]  eta: 0:00:30  lr: 0.000345  min_lr: 0.000008  loss: 0.7888 (0.8220)  loss_scale: 65536.0000 (114255.8082)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [21]  [1260/1349]  eta: 0:00:27  lr: 0.000345  min_lr: 0.000008  loss: 0.8226 (0.8215)  loss_scale: 65536.0000 (113869.4496)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
Epoch: [21]  [1270/1349]  eta: 0:00:24  lr: 0.000345  min_lr: 0.000008  loss: 0.8297 (0.8217)  loss_scale: 65536.0000 (113489.1707)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [21]  [1280/1349]  eta: 0:00:21  lr: 0.000345  min_lr: 0.000008  loss: 0.8751 (0.8215)  loss_scale: 65536.0000 (113114.8290)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [21]  [1290/1349]  eta: 0:00:18  lr: 0.000345  min_lr: 0.000008  loss: 0.8634 (0.8219)  loss_scale: 65536.0000 (112746.2866)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [21]  [1300/1349]  eta: 0:00:15  lr: 0.000345  min_lr: 0.000008  loss: 0.8482 (0.8219)  loss_scale: 65536.0000 (112383.4097)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0001  max mem: 41808
Epoch: [21]  [1310/1349]  eta: 0:00:12  lr: 0.000344  min_lr: 0.000008  loss: 0.8756 (0.8224)  loss_scale: 65536.0000 (112026.0686)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
Epoch: [21]  [1320/1349]  eta: 0:00:09  lr: 0.000344  min_lr: 0.000008  loss: 0.8739 (0.8219)  loss_scale: 65536.0000 (111674.1378)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [21]  [1330/1349]  eta: 0:00:05  lr: 0.000344  min_lr: 0.000008  loss: 0.8739 (0.8223)  loss_scale: 65536.0000 (111327.4951)  weight_decay: 0.0500 (0.0500)  time: 0.3063  data: 0.0001  max mem: 41808
Epoch: [21]  [1340/1349]  eta: 0:00:02  lr: 0.000344  min_lr: 0.000008  loss: 0.8473 (0.8218)  loss_scale: 65536.0000 (110986.0224)  weight_decay: 0.0500 (0.0500)  time: 0.3039  data: 0.0001  max mem: 41808
[2025-05-23 19:27:53,118] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 19:27:53,118] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-05-23 19:27:53,118] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 19:27:53,118] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [21]  [1348/1349]  eta: 0:00:00  lr: 0.000344  min_lr: 0.000008  loss: 0.7632 (0.8214)  loss_scale: 65536.0000 (110862.2328)  weight_decay: 0.0500 (0.0500)  time: 0.3018  data: 0.0001  max mem: 41808
Epoch: [21] Total time: 0:07:00 (0.3117 s / it)
Averaged stats: lr: 0.000344  min_lr: 0.000008  loss: 0.7632 (0.8224)  loss_scale: 65536.0000 (110862.2328)  weight_decay: 0.0500 (0.0500)  total_time: 420.4264 (420.4208)
Val:  [  0/346]  eta: 0:45:08  loss: 3.2895 (3.2895)  acc1: 0.0000 (0.0000)  acc5: 82.0312 (82.0312)  time: 7.8286  data: 6.8286  max mem: 41808
Val:  [ 10/346]  eta: 0:11:03  loss: 0.1376 (0.5836)  acc1: 100.0000 (84.8722)  acc5: 100.0000 (98.2244)  time: 1.9746  data: 1.2077  max mem: 41808
Val:  [ 20/346]  eta: 0:08:04  loss: 0.1250 (0.4206)  acc1: 100.0000 (89.2113)  acc5: 100.0000 (98.9211)  time: 1.1701  data: 0.4091  max mem: 41808
Val:  [ 30/346]  eta: 0:06:41  loss: 0.1116 (0.3575)  acc1: 100.0000 (91.2802)  acc5: 100.0000 (99.2692)  time: 0.8811  data: 0.0937  max mem: 41808
Val:  [ 40/346]  eta: 0:05:50  loss: 0.1146 (0.3964)  acc1: 100.0000 (90.1486)  acc5: 100.0000 (99.1997)  time: 0.7841  data: 0.0075  max mem: 41808
Val:  [ 50/346]  eta: 0:05:17  loss: 0.1227 (0.3529)  acc1: 99.2188 (91.6360)  acc5: 100.0000 (99.3413)  time: 0.7702  data: 0.0003  max mem: 41808
Val:  [ 60/346]  eta: 0:04:54  loss: 0.1705 (0.3415)  acc1: 97.6562 (91.8801)  acc5: 100.0000 (99.4493)  time: 0.7928  data: 0.0003  max mem: 41808
Val:  [ 70/346]  eta: 0:04:35  loss: 0.2499 (0.3644)  acc1: 92.1875 (91.3182)  acc5: 100.0000 (99.5268)  time: 0.8002  data: 0.0002  max mem: 41808
Val:  [ 80/346]  eta: 0:04:17  loss: 0.1709 (0.3651)  acc1: 96.8750 (91.3966)  acc5: 100.0000 (99.4792)  time: 0.7815  data: 0.0204  max mem: 41808
Val:  [ 90/346]  eta: 0:04:04  loss: 0.2397 (0.3692)  acc1: 93.7500 (91.2946)  acc5: 100.0000 (99.5364)  time: 0.8129  data: 0.0948  max mem: 41808
Val:  [100/346]  eta: 0:03:53  loss: 0.1391 (0.3466)  acc1: 98.4375 (92.0792)  acc5: 100.0000 (99.5823)  time: 0.8796  data: 0.1501  max mem: 41808
Val:  [110/346]  eta: 0:03:43  loss: 0.1391 (0.3665)  acc1: 98.4375 (91.5611)  acc5: 100.0000 (99.5777)  time: 0.8985  data: 0.1487  max mem: 41808
Val:  [120/346]  eta: 0:03:33  loss: 0.2129 (0.3590)  acc1: 96.0938 (91.7355)  acc5: 100.0000 (99.6126)  time: 0.9123  data: 0.1450  max mem: 41808
Val:  [130/346]  eta: 0:03:22  loss: 0.1172 (0.3637)  acc1: 100.0000 (91.3943)  acc5: 100.0000 (99.6124)  time: 0.8978  data: 0.1421  max mem: 41808
Val:  [140/346]  eta: 0:03:12  loss: 0.2196 (0.3622)  acc1: 96.0938 (91.4617)  acc5: 100.0000 (99.6398)  time: 0.8712  data: 0.1456  max mem: 41808
Val:  [150/346]  eta: 0:03:02  loss: 0.2776 (0.3622)  acc1: 92.9688 (91.5356)  acc5: 100.0000 (99.6327)  time: 0.8672  data: 0.1475  max mem: 41808
Val:  [160/346]  eta: 0:02:52  loss: 0.1620 (0.3564)  acc1: 96.0938 (91.6974)  acc5: 100.0000 (99.6312)  time: 0.8782  data: 0.1449  max mem: 41808
Val:  [170/346]  eta: 0:02:42  loss: 0.1519 (0.3542)  acc1: 97.6562 (91.6895)  acc5: 100.0000 (99.6528)  time: 0.8945  data: 0.1529  max mem: 41808
Val:  [180/346]  eta: 0:02:33  loss: 0.2326 (0.3645)  acc1: 94.5312 (91.1602)  acc5: 100.0000 (99.6720)  time: 0.8886  data: 0.1541  max mem: 41808
Val:  [190/346]  eta: 0:02:23  loss: 0.2326 (0.3621)  acc1: 92.9688 (91.2140)  acc5: 100.0000 (99.6728)  time: 0.8840  data: 0.1455  max mem: 41808
Val:  [200/346]  eta: 0:02:14  loss: 0.3027 (0.3784)  acc1: 89.8438 (90.6328)  acc5: 100.0000 (99.6891)  time: 0.8897  data: 0.1465  max mem: 41808
Val:  [210/346]  eta: 0:02:05  loss: 0.1441 (0.3687)  acc1: 96.0938 (91.0027)  acc5: 100.0000 (99.7038)  time: 0.9078  data: 0.1538  max mem: 41808
Val:  [220/346]  eta: 0:01:55  loss: 0.1426 (0.3638)  acc1: 100.0000 (91.1906)  acc5: 100.0000 (99.6960)  time: 0.8895  data: 0.1562  max mem: 41808
Val:  [230/346]  eta: 0:01:46  loss: 0.1329 (0.3550)  acc1: 99.2188 (91.5077)  acc5: 100.0000 (99.7024)  time: 0.8856  data: 0.1566  max mem: 41808
Val:  [240/346]  eta: 0:01:37  loss: 0.1329 (0.3580)  acc1: 99.2188 (91.4354)  acc5: 100.0000 (99.7050)  time: 0.9032  data: 0.1538  max mem: 41808
Val:  [250/346]  eta: 0:01:27  loss: 0.1566 (0.3563)  acc1: 97.6562 (91.4872)  acc5: 100.0000 (99.7136)  time: 0.9019  data: 0.1531  max mem: 41808
Val:  [260/346]  eta: 0:01:18  loss: 0.1470 (0.3568)  acc1: 99.2188 (91.4601)  acc5: 100.0000 (99.7097)  time: 0.9104  data: 0.1533  max mem: 41808
Val:  [270/346]  eta: 0:01:09  loss: 0.1301 (0.3534)  acc1: 99.2188 (91.5908)  acc5: 100.0000 (99.7060)  time: 0.9162  data: 0.1586  max mem: 41808
Val:  [280/346]  eta: 0:01:00  loss: 0.1245 (0.3519)  acc1: 99.2188 (91.6648)  acc5: 100.0000 (99.7081)  time: 0.9033  data: 0.1615  max mem: 41808
Val:  [290/346]  eta: 0:00:51  loss: 0.1107 (0.3444)  acc1: 100.0000 (91.8895)  acc5: 100.0000 (99.7181)  time: 0.8904  data: 0.1600  max mem: 41808
Val:  [300/346]  eta: 0:00:42  loss: 0.1129 (0.3457)  acc1: 100.0000 (91.8838)  acc5: 100.0000 (99.6626)  time: 0.8999  data: 0.1563  max mem: 41808
Val:  [310/346]  eta: 0:00:32  loss: 0.1228 (0.3507)  acc1: 100.0000 (91.7429)  acc5: 100.0000 (99.6659)  time: 0.8711  data: 0.1389  max mem: 41808
Val:  [320/346]  eta: 0:00:23  loss: 0.1333 (0.3480)  acc1: 99.2188 (91.8176)  acc5: 100.0000 (99.6714)  time: 0.8718  data: 0.1363  max mem: 41808
Val:  [330/346]  eta: 0:00:14  loss: 0.3167 (0.3629)  acc1: 88.2812 (91.4015)  acc5: 100.0000 (99.5209)  time: 0.8915  data: 0.1531  max mem: 41808
Val:  [340/346]  eta: 0:00:05  loss: 0.4694 (0.3724)  acc1: 87.5000 (91.0901)  acc5: 100.0000 (99.5280)  time: 0.8998  data: 0.1662  max mem: 41808
Val:  [345/346]  eta: 0:00:00  loss: 0.1521 (0.3691)  acc1: 96.8750 (91.1971)  acc5: 100.0000 (99.5344)  time: 0.9032  data: 0.1800  max mem: 41808
Val: Total time: 0:05:15 (0.9113 s / it)
* Acc@1 91.284 Acc@5 99.569 loss 0.366
Accuracy of the network on the 88494 val videos: 91.3%
Max accuracy: 91.38%   Max Epoch: 20
Epoch: [22]  [   0/1349]  eta: 1:52:43  lr: 0.000344  min_lr: 0.000008  loss: 0.9208 (0.9208)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 5.0136  data: 3.7863  max mem: 41808
Epoch: [22]  [  10/1349]  eta: 0:16:43  lr: 0.000344  min_lr: 0.000008  loss: 0.7651 (0.7274)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7494  data: 0.3444  max mem: 41808
Epoch: [22]  [  20/1349]  eta: 0:11:58  lr: 0.000344  min_lr: 0.000008  loss: 0.7651 (0.7521)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3171  data: 0.0002  max mem: 41808
Epoch: [22]  [  30/1349]  eta: 0:10:14  lr: 0.000344  min_lr: 0.000008  loss: 0.8228 (0.8005)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3097  data: 0.0001  max mem: 41808
Epoch: [22]  [  40/1349]  eta: 0:09:19  lr: 0.000343  min_lr: 0.000008  loss: 0.8619 (0.8034)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3086  data: 0.0001  max mem: 41808
Epoch: [22]  [  50/1349]  eta: 0:08:44  lr: 0.000343  min_lr: 0.000008  loss: 0.9091 (0.8196)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3084  data: 0.0001  max mem: 41808
Epoch: [22]  [  60/1349]  eta: 0:08:21  lr: 0.000343  min_lr: 0.000008  loss: 0.7813 (0.8104)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3107  data: 0.0002  max mem: 41808
Epoch: [22]  [  70/1349]  eta: 0:08:02  lr: 0.000343  min_lr: 0.000008  loss: 0.7813 (0.8141)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3102  data: 0.0002  max mem: 41808
Epoch: [22]  [  80/1349]  eta: 0:07:48  lr: 0.000343  min_lr: 0.000008  loss: 0.8532 (0.8097)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [22]  [  90/1349]  eta: 0:07:36  lr: 0.000343  min_lr: 0.000008  loss: 0.8748 (0.8148)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
[2025-05-23 19:33:45,068] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 29777
[2025-05-23 19:33:45,068] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 29777
[2025-05-23 19:33:45,069] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 19:33:45,069] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 19:33:45,069] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [22]  [ 100/1349]  eta: 0:07:25  lr: 0.000343  min_lr: 0.000008  loss: 0.8993 (0.8235)  loss_scale: 131072.0000 (129774.2574)  weight_decay: 0.0500 (0.0500)  time: 0.3063  data: 0.0001  max mem: 41808
Epoch: [22]  [ 110/1349]  eta: 0:07:16  lr: 0.000343  min_lr: 0.000008  loss: 0.9198 (0.8254)  loss_scale: 65536.0000 (123987.0270)  weight_decay: 0.0500 (0.0500)  time: 0.3057  data: 0.0001  max mem: 41808
Epoch: [22]  [ 120/1349]  eta: 0:07:07  lr: 0.000343  min_lr: 0.000008  loss: 0.8915 (0.8291)  loss_scale: 65536.0000 (119156.3636)  weight_decay: 0.0500 (0.0500)  time: 0.3060  data: 0.0001  max mem: 41808
Epoch: [22]  [ 130/1349]  eta: 0:07:00  lr: 0.000342  min_lr: 0.000008  loss: 0.8083 (0.8191)  loss_scale: 65536.0000 (115063.2061)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
Epoch: [22]  [ 140/1349]  eta: 0:06:54  lr: 0.000342  min_lr: 0.000008  loss: 0.7614 (0.8206)  loss_scale: 65536.0000 (111550.6383)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0001  max mem: 41808
Epoch: [22]  [ 150/1349]  eta: 0:06:47  lr: 0.000342  min_lr: 0.000008  loss: 0.8411 (0.8167)  loss_scale: 65536.0000 (108503.3113)  weight_decay: 0.0500 (0.0500)  time: 0.3079  data: 0.0001  max mem: 41808
Epoch: [22]  [ 160/1349]  eta: 0:06:42  lr: 0.000342  min_lr: 0.000008  loss: 0.7938 (0.8151)  loss_scale: 65536.0000 (105834.5342)  weight_decay: 0.0500 (0.0500)  time: 0.3078  data: 0.0001  max mem: 41808
Epoch: [22]  [ 170/1349]  eta: 0:06:36  lr: 0.000342  min_lr: 0.000008  loss: 0.8430 (0.8176)  loss_scale: 65536.0000 (103477.8947)  weight_decay: 0.0500 (0.0500)  time: 0.3085  data: 0.0001  max mem: 41808
Epoch: [22]  [ 180/1349]  eta: 0:06:31  lr: 0.000342  min_lr: 0.000008  loss: 0.8543 (0.8178)  loss_scale: 65536.0000 (101381.6575)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
Epoch: [22]  [ 190/1349]  eta: 0:06:26  lr: 0.000342  min_lr: 0.000008  loss: 0.8782 (0.8169)  loss_scale: 65536.0000 (99504.9215)  weight_decay: 0.0500 (0.0500)  time: 0.3063  data: 0.0001  max mem: 41808
Epoch: [22]  [ 200/1349]  eta: 0:06:21  lr: 0.000342  min_lr: 0.000008  loss: 0.8579 (0.8184)  loss_scale: 65536.0000 (97814.9254)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0002  max mem: 41808
Epoch: [22]  [ 210/1349]  eta: 0:06:16  lr: 0.000341  min_lr: 0.000008  loss: 0.8665 (0.8231)  loss_scale: 65536.0000 (96285.1185)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [22]  [ 220/1349]  eta: 0:06:12  lr: 0.000341  min_lr: 0.000008  loss: 0.8968 (0.8254)  loss_scale: 65536.0000 (94893.7557)  weight_decay: 0.0500 (0.0500)  time: 0.3060  data: 0.0001  max mem: 41808
[2025-05-23 19:34:24,685] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 19:34:24,685] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-05-23 19:34:24,685] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 19:34:24,685] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [22]  [ 230/1349]  eta: 0:06:07  lr: 0.000341  min_lr: 0.000008  loss: 0.8419 (0.8228)  loss_scale: 65536.0000 (94473.9740)  weight_decay: 0.0500 (0.0500)  time: 0.3062  data: 0.0001  max mem: 41808
Epoch: [22]  [ 240/1349]  eta: 0:06:03  lr: 0.000341  min_lr: 0.000008  loss: 0.8059 (0.8227)  loss_scale: 131072.0000 (95992.5643)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0001  max mem: 41808
Epoch: [22]  [ 250/1349]  eta: 0:05:59  lr: 0.000341  min_lr: 0.000008  loss: 0.8493 (0.8225)  loss_scale: 131072.0000 (97390.1514)  weight_decay: 0.0500 (0.0500)  time: 0.3064  data: 0.0001  max mem: 41808
Epoch: [22]  [ 260/1349]  eta: 0:05:55  lr: 0.000341  min_lr: 0.000008  loss: 0.8737 (0.8238)  loss_scale: 131072.0000 (98680.6437)  weight_decay: 0.0500 (0.0500)  time: 0.3083  data: 0.0002  max mem: 41808
Epoch: [22]  [ 270/1349]  eta: 0:05:51  lr: 0.000341  min_lr: 0.000008  loss: 0.8626 (0.8240)  loss_scale: 131072.0000 (99875.8967)  weight_decay: 0.0500 (0.0500)  time: 0.3090  data: 0.0002  max mem: 41808
Epoch: [22]  [ 280/1349]  eta: 0:05:47  lr: 0.000341  min_lr: 0.000008  loss: 0.8815 (0.8236)  loss_scale: 131072.0000 (100986.0783)  weight_decay: 0.0500 (0.0500)  time: 0.3081  data: 0.0001  max mem: 41808
Epoch: [22]  [ 290/1349]  eta: 0:05:43  lr: 0.000340  min_lr: 0.000008  loss: 0.8815 (0.8244)  loss_scale: 131072.0000 (102019.9588)  weight_decay: 0.0500 (0.0500)  time: 0.3078  data: 0.0001  max mem: 41808
Epoch: [22]  [ 300/1349]  eta: 0:05:39  lr: 0.000340  min_lr: 0.000008  loss: 0.8481 (0.8240)  loss_scale: 131072.0000 (102985.1429)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [22]  [ 310/1349]  eta: 0:05:35  lr: 0.000340  min_lr: 0.000008  loss: 0.8001 (0.8216)  loss_scale: 131072.0000 (103888.2572)  weight_decay: 0.0500 (0.0500)  time: 0.3081  data: 0.0001  max mem: 41808
Epoch: [22]  [ 320/1349]  eta: 0:05:32  lr: 0.000340  min_lr: 0.000008  loss: 0.8169 (0.8227)  loss_scale: 131072.0000 (104735.1028)  weight_decay: 0.0500 (0.0500)  time: 0.3079  data: 0.0001  max mem: 41808
[2025-05-23 19:34:53,308] [INFO] [logging.py:96:log_dist] [Rank 0] step=30000, skipped=181, lr=[8.08005115246343e-06, 8.08005115246343e-06, 1.0773401536617907e-05, 1.0773401536617907e-05, 1.4364535382157209e-05, 1.4364535382157209e-05, 1.9152713842876278e-05, 1.9152713842876278e-05, 2.5536951790501706e-05, 2.5536951790501706e-05, 3.4049269054002275e-05, 3.4049269054002275e-05, 4.539902540533637e-05, 4.539902540533637e-05, 6.053203387378182e-05, 6.053203387378182e-05, 8.070937849837576e-05, 8.070937849837576e-05, 0.00010761250466450101, 0.00010761250466450101, 0.00014348333955266801, 0.00014348333955266801, 0.00019131111940355735, 0.00019131111940355735, 0.0002550814925380765, 0.0002550814925380765, 0.0003401086567174353, 0.0003401086567174353], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-05-23 19:34:53,308] [INFO] [timer.py:260:stop] epoch=0/micro_step=30000/global_step=30000, RunningAvgSamplesPerSec=208.50439096817698, CurrSamplesPerSec=214.17779515371052, MemAllocated=1.37GB, MaxMemAllocated=40.83GB
Epoch: [22]  [ 330/1349]  eta: 0:05:28  lr: 0.000340  min_lr: 0.000008  loss: 0.8348 (0.8206)  loss_scale: 131072.0000 (105530.7795)  weight_decay: 0.0500 (0.0500)  time: 0.3066  data: 0.0001  max mem: 41808
Epoch: [22]  [ 340/1349]  eta: 0:05:24  lr: 0.000340  min_lr: 0.000008  loss: 0.8016 (0.8190)  loss_scale: 131072.0000 (106279.7889)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
[2025-05-23 19:35:01,303] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 30025
[2025-05-23 19:35:01,303] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 30025
[2025-05-23 19:35:01,303] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 19:35:01,303] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 19:35:01,303] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [22]  [ 350/1349]  eta: 0:05:21  lr: 0.000340  min_lr: 0.000008  loss: 0.8159 (0.8200)  loss_scale: 131072.0000 (106239.2707)  weight_decay: 0.0500 (0.0500)  time: 0.3078  data: 0.0001  max mem: 41808
Epoch: [22]  [ 360/1349]  eta: 0:05:17  lr: 0.000340  min_lr: 0.000008  loss: 0.8275 (0.8196)  loss_scale: 65536.0000 (105111.7562)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0002  max mem: 41808
Epoch: [22]  [ 370/1349]  eta: 0:05:13  lr: 0.000340  min_lr: 0.000008  loss: 0.8055 (0.8182)  loss_scale: 65536.0000 (104045.0243)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [22]  [ 380/1349]  eta: 0:05:10  lr: 0.000339  min_lr: 0.000008  loss: 0.7698 (0.8185)  loss_scale: 65536.0000 (103034.2887)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0002  max mem: 41808
Epoch: [22]  [ 390/1349]  eta: 0:05:06  lr: 0.000339  min_lr: 0.000008  loss: 0.8343 (0.8194)  loss_scale: 65536.0000 (102075.2532)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0002  max mem: 41808
Epoch: [22]  [ 400/1349]  eta: 0:05:03  lr: 0.000339  min_lr: 0.000008  loss: 0.8343 (0.8199)  loss_scale: 65536.0000 (101164.0499)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0001  max mem: 41808
Epoch: [22]  [ 410/1349]  eta: 0:04:59  lr: 0.000339  min_lr: 0.000008  loss: 0.7950 (0.8198)  loss_scale: 65536.0000 (100297.1873)  weight_decay: 0.0500 (0.0500)  time: 0.3078  data: 0.0001  max mem: 41808
Epoch: [22]  [ 420/1349]  eta: 0:04:56  lr: 0.000339  min_lr: 0.000008  loss: 0.8760 (0.8212)  loss_scale: 65536.0000 (99471.5059)  weight_decay: 0.0500 (0.0500)  time: 0.3078  data: 0.0001  max mem: 41808
Epoch: [22]  [ 430/1349]  eta: 0:04:53  lr: 0.000339  min_lr: 0.000008  loss: 0.8159 (0.8196)  loss_scale: 65536.0000 (98684.1392)  weight_decay: 0.0500 (0.0500)  time: 0.3079  data: 0.0001  max mem: 41808
Epoch: [22]  [ 440/1349]  eta: 0:04:49  lr: 0.000339  min_lr: 0.000008  loss: 0.8589 (0.8207)  loss_scale: 65536.0000 (97932.4807)  weight_decay: 0.0500 (0.0500)  time: 0.3078  data: 0.0002  max mem: 41808
Epoch: [22]  [ 450/1349]  eta: 0:04:46  lr: 0.000339  min_lr: 0.000008  loss: 0.9143 (0.8210)  loss_scale: 65536.0000 (97214.1552)  weight_decay: 0.0500 (0.0500)  time: 0.3081  data: 0.0002  max mem: 41808
Epoch: [22]  [ 460/1349]  eta: 0:04:42  lr: 0.000338  min_lr: 0.000008  loss: 0.8697 (0.8191)  loss_scale: 65536.0000 (96526.9935)  weight_decay: 0.0500 (0.0500)  time: 0.3093  data: 0.0001  max mem: 41808
Epoch: [22]  [ 470/1349]  eta: 0:04:39  lr: 0.000338  min_lr: 0.000008  loss: 0.7407 (0.8188)  loss_scale: 65536.0000 (95869.0106)  weight_decay: 0.0500 (0.0500)  time: 0.3094  data: 0.0001  max mem: 41808
[2025-05-23 19:35:41,059] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 19:35:41,060] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-05-23 19:35:41,060] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 19:35:41,060] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [22]  [ 480/1349]  eta: 0:04:36  lr: 0.000338  min_lr: 0.000008  loss: 0.7317 (0.8166)  loss_scale: 65536.0000 (95919.6341)  weight_decay: 0.0500 (0.0500)  time: 0.3085  data: 0.0001  max mem: 41808
Epoch: [22]  [ 490/1349]  eta: 0:04:32  lr: 0.000338  min_lr: 0.000008  loss: 0.7010 (0.8142)  loss_scale: 131072.0000 (96635.5682)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0001  max mem: 41808
Epoch: [22]  [ 500/1349]  eta: 0:04:29  lr: 0.000338  min_lr: 0.000008  loss: 0.7010 (0.8131)  loss_scale: 131072.0000 (97322.9222)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [22]  [ 510/1349]  eta: 0:04:26  lr: 0.000338  min_lr: 0.000008  loss: 0.6980 (0.8120)  loss_scale: 131072.0000 (97983.3738)  weight_decay: 0.0500 (0.0500)  time: 0.3080  data: 0.0001  max mem: 41808
Epoch: [22]  [ 520/1349]  eta: 0:04:22  lr: 0.000338  min_lr: 0.000008  loss: 0.7423 (0.8118)  loss_scale: 131072.0000 (98618.4722)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
Epoch: [22]  [ 530/1349]  eta: 0:04:19  lr: 0.000338  min_lr: 0.000008  loss: 0.8469 (0.8116)  loss_scale: 131072.0000 (99229.6497)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [22]  [ 540/1349]  eta: 0:04:16  lr: 0.000337  min_lr: 0.000008  loss: 0.8325 (0.8118)  loss_scale: 131072.0000 (99818.2329)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [22]  [ 550/1349]  eta: 0:04:12  lr: 0.000337  min_lr: 0.000008  loss: 0.7959 (0.8107)  loss_scale: 131072.0000 (100385.4519)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
Epoch: [22]  [ 560/1349]  eta: 0:04:09  lr: 0.000337  min_lr: 0.000008  loss: 0.8336 (0.8116)  loss_scale: 131072.0000 (100932.4492)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
Epoch: [22]  [ 570/1349]  eta: 0:04:06  lr: 0.000337  min_lr: 0.000008  loss: 0.8571 (0.8114)  loss_scale: 131072.0000 (101460.2872)  weight_decay: 0.0500 (0.0500)  time: 0.3082  data: 0.0001  max mem: 41808
Epoch: [22]  [ 580/1349]  eta: 0:04:03  lr: 0.000337  min_lr: 0.000008  loss: 0.7839 (0.8110)  loss_scale: 131072.0000 (101969.9552)  weight_decay: 0.0500 (0.0500)  time: 0.3083  data: 0.0001  max mem: 41808
Epoch: [22]  [ 590/1349]  eta: 0:03:59  lr: 0.000337  min_lr: 0.000008  loss: 0.8329 (0.8121)  loss_scale: 131072.0000 (102462.3756)  weight_decay: 0.0500 (0.0500)  time: 0.3085  data: 0.0002  max mem: 41808
Epoch: [22]  [ 600/1349]  eta: 0:03:56  lr: 0.000337  min_lr: 0.000008  loss: 0.8792 (0.8138)  loss_scale: 131072.0000 (102938.4093)  weight_decay: 0.0500 (0.0500)  time: 0.3090  data: 0.0002  max mem: 41808
[2025-05-23 19:36:20,471] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 19:36:20,471] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 19:36:20,471] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 19:36:20,471] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 19:36:21,083] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 30284
[2025-05-23 19:36:21,084] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 19:36:21,084] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
[2025-05-23 19:36:21,084] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 30284
[2025-05-23 19:36:21,084] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
Epoch: [22]  [ 610/1349]  eta: 0:03:53  lr: 0.000337  min_lr: 0.000008  loss: 0.8970 (0.8150)  loss_scale: 131072.0000 (103827.9018)  weight_decay: 0.0500 (0.0500)  time: 0.3090  data: 0.0002  max mem: 41808
Epoch: [22]  [ 620/1349]  eta: 0:03:50  lr: 0.000336  min_lr: 0.000008  loss: 0.7835 (0.8144)  loss_scale: 131072.0000 (104266.6151)  weight_decay: 0.0500 (0.0500)  time: 0.3095  data: 0.0003  max mem: 41808
Epoch: [22]  [ 630/1349]  eta: 0:03:46  lr: 0.000336  min_lr: 0.000008  loss: 0.7835 (0.8136)  loss_scale: 131072.0000 (104691.4231)  weight_decay: 0.0500 (0.0500)  time: 0.3096  data: 0.0002  max mem: 41808
Epoch: [22]  [ 640/1349]  eta: 0:03:43  lr: 0.000336  min_lr: 0.000008  loss: 0.8037 (0.8145)  loss_scale: 131072.0000 (105102.9766)  weight_decay: 0.0500 (0.0500)  time: 0.3085  data: 0.0001  max mem: 41808
Epoch: [22]  [ 650/1349]  eta: 0:03:40  lr: 0.000336  min_lr: 0.000008  loss: 0.8704 (0.8154)  loss_scale: 131072.0000 (105501.8863)  weight_decay: 0.0500 (0.0500)  time: 0.3089  data: 0.0002  max mem: 41808
Epoch: [22]  [ 660/1349]  eta: 0:03:37  lr: 0.000336  min_lr: 0.000008  loss: 0.8704 (0.8154)  loss_scale: 131072.0000 (105888.7262)  weight_decay: 0.0500 (0.0500)  time: 0.3084  data: 0.0002  max mem: 41808
Epoch: [22]  [ 670/1349]  eta: 0:03:33  lr: 0.000336  min_lr: 0.000008  loss: 0.8242 (0.8147)  loss_scale: 131072.0000 (106264.0358)  weight_decay: 0.0500 (0.0500)  time: 0.3090  data: 0.0001  max mem: 41808
Epoch: [22]  [ 680/1349]  eta: 0:03:30  lr: 0.000336  min_lr: 0.000008  loss: 0.7747 (0.8148)  loss_scale: 131072.0000 (106628.3231)  weight_decay: 0.0500 (0.0500)  time: 0.3096  data: 0.0001  max mem: 41808
Epoch: [22]  [ 690/1349]  eta: 0:03:27  lr: 0.000336  min_lr: 0.000008  loss: 0.7604 (0.8145)  loss_scale: 131072.0000 (106982.0666)  weight_decay: 0.0500 (0.0500)  time: 0.3082  data: 0.0001  max mem: 41808
Epoch: [22]  [ 700/1349]  eta: 0:03:24  lr: 0.000336  min_lr: 0.000008  loss: 0.7635 (0.8141)  loss_scale: 131072.0000 (107325.7175)  weight_decay: 0.0500 (0.0500)  time: 0.3082  data: 0.0002  max mem: 41808
Epoch: [22]  [ 710/1349]  eta: 0:03:21  lr: 0.000335  min_lr: 0.000008  loss: 0.8402 (0.8149)  loss_scale: 131072.0000 (107659.7018)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0002  max mem: 41808
Epoch: [22]  [ 720/1349]  eta: 0:03:17  lr: 0.000335  min_lr: 0.000008  loss: 0.8269 (0.8147)  loss_scale: 131072.0000 (107984.4216)  weight_decay: 0.0500 (0.0500)  time: 0.3079  data: 0.0001  max mem: 41808
Epoch: [22]  [ 730/1349]  eta: 0:03:14  lr: 0.000335  min_lr: 0.000008  loss: 0.8825 (0.8160)  loss_scale: 131072.0000 (108300.2572)  weight_decay: 0.0500 (0.0500)  time: 0.3078  data: 0.0001  max mem: 41808
[2025-05-23 19:37:00,903] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 19:37:00,903] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 19:37:00,903] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 19:37:00,903] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [22]  [ 740/1349]  eta: 0:03:11  lr: 0.000335  min_lr: 0.000008  loss: 0.8688 (0.8158)  loss_scale: 131072.0000 (109668.8799)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
[2025-05-23 19:37:02,752] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 30419
[2025-05-23 19:37:02,752] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 19:37:02,752] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
[2025-05-23 19:37:02,752] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 30419
[2025-05-23 19:37:02,752] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
Epoch: [22]  [ 750/1349]  eta: 0:03:08  lr: 0.000335  min_lr: 0.000008  loss: 0.6822 (0.8143)  loss_scale: 131072.0000 (109953.8748)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [22]  [ 760/1349]  eta: 0:03:05  lr: 0.000335  min_lr: 0.000008  loss: 0.8243 (0.8147)  loss_scale: 131072.0000 (110231.3798)  weight_decay: 0.0500 (0.0500)  time: 0.3079  data: 0.0001  max mem: 41808
Epoch: [22]  [ 770/1349]  eta: 0:03:01  lr: 0.000335  min_lr: 0.000008  loss: 0.8364 (0.8142)  loss_scale: 131072.0000 (110501.6861)  weight_decay: 0.0500 (0.0500)  time: 0.3084  data: 0.0001  max mem: 41808
Epoch: [22]  [ 780/1349]  eta: 0:02:58  lr: 0.000335  min_lr: 0.000008  loss: 0.8364 (0.8138)  loss_scale: 131072.0000 (110765.0704)  weight_decay: 0.0500 (0.0500)  time: 0.3083  data: 0.0001  max mem: 41808
Epoch: [22]  [ 790/1349]  eta: 0:02:55  lr: 0.000334  min_lr: 0.000008  loss: 0.8696 (0.8154)  loss_scale: 131072.0000 (111021.7952)  weight_decay: 0.0500 (0.0500)  time: 0.3080  data: 0.0001  max mem: 41808
Epoch: [22]  [ 800/1349]  eta: 0:02:52  lr: 0.000334  min_lr: 0.000008  loss: 0.8453 (0.8156)  loss_scale: 131072.0000 (111272.1099)  weight_decay: 0.0500 (0.0500)  time: 0.3082  data: 0.0001  max mem: 41808
Epoch: [22]  [ 810/1349]  eta: 0:02:49  lr: 0.000334  min_lr: 0.000008  loss: 0.7998 (0.8153)  loss_scale: 131072.0000 (111516.2515)  weight_decay: 0.0500 (0.0500)  time: 0.3083  data: 0.0001  max mem: 41808
Epoch: [22]  [ 820/1349]  eta: 0:02:46  lr: 0.000334  min_lr: 0.000008  loss: 0.7825 (0.8147)  loss_scale: 131072.0000 (111754.4458)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [22]  [ 830/1349]  eta: 0:02:42  lr: 0.000334  min_lr: 0.000008  loss: 0.7349 (0.8141)  loss_scale: 131072.0000 (111986.9073)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
Epoch: [22]  [ 840/1349]  eta: 0:02:39  lr: 0.000334  min_lr: 0.000008  loss: 0.7687 (0.8140)  loss_scale: 131072.0000 (112213.8407)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [22]  [ 850/1349]  eta: 0:02:36  lr: 0.000334  min_lr: 0.000008  loss: 0.8225 (0.8143)  loss_scale: 131072.0000 (112435.4407)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
Epoch: [22]  [ 860/1349]  eta: 0:02:33  lr: 0.000334  min_lr: 0.000008  loss: 0.8170 (0.8144)  loss_scale: 131072.0000 (112651.8931)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
[2025-05-23 19:37:42,431] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 19:37:42,431] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 19:37:42,431] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 19:37:42,431] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [22]  [ 870/1349]  eta: 0:02:30  lr: 0.000333  min_lr: 0.000008  loss: 0.8170 (0.8142)  loss_scale: 131072.0000 (113013.8599)  weight_decay: 0.0500 (0.0500)  time: 0.3060  data: 0.0001  max mem: 41808
[2025-05-23 19:37:44,274] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 30554
[2025-05-23 19:37:44,274] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 30554
[2025-05-23 19:37:44,274] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 19:37:44,274] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 19:37:44,274] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [22]  [ 880/1349]  eta: 0:02:26  lr: 0.000333  min_lr: 0.000008  loss: 0.8493 (0.8152)  loss_scale: 131072.0000 (113962.7151)  weight_decay: 0.0500 (0.0500)  time: 0.3064  data: 0.0001  max mem: 41808
Epoch: [22]  [ 890/1349]  eta: 0:02:23  lr: 0.000333  min_lr: 0.000008  loss: 0.8617 (0.8146)  loss_scale: 131072.0000 (114154.7385)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
Epoch: [22]  [ 900/1349]  eta: 0:02:20  lr: 0.000333  min_lr: 0.000008  loss: 0.8196 (0.8144)  loss_scale: 131072.0000 (114342.4994)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [22]  [ 910/1349]  eta: 0:02:17  lr: 0.000333  min_lr: 0.000008  loss: 0.8271 (0.8141)  loss_scale: 131072.0000 (114526.1383)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [22]  [ 920/1349]  eta: 0:02:14  lr: 0.000333  min_lr: 0.000008  loss: 0.8284 (0.8141)  loss_scale: 131072.0000 (114705.7894)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
Epoch: [22]  [ 930/1349]  eta: 0:02:11  lr: 0.000333  min_lr: 0.000008  loss: 0.8375 (0.8140)  loss_scale: 131072.0000 (114881.5811)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [22]  [ 940/1349]  eta: 0:02:07  lr: 0.000333  min_lr: 0.000008  loss: 0.8545 (0.8145)  loss_scale: 131072.0000 (115053.6366)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [22]  [ 950/1349]  eta: 0:02:04  lr: 0.000332  min_lr: 0.000008  loss: 0.8448 (0.8150)  loss_scale: 131072.0000 (115222.0736)  weight_decay: 0.0500 (0.0500)  time: 0.3081  data: 0.0001  max mem: 41808
Epoch: [22]  [ 960/1349]  eta: 0:02:01  lr: 0.000332  min_lr: 0.000008  loss: 0.9081 (0.8160)  loss_scale: 131072.0000 (115387.0052)  weight_decay: 0.0500 (0.0500)  time: 0.3082  data: 0.0001  max mem: 41808
Epoch: [22]  [ 970/1349]  eta: 0:01:58  lr: 0.000332  min_lr: 0.000008  loss: 0.9102 (0.8164)  loss_scale: 131072.0000 (115548.5396)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
Epoch: [22]  [ 980/1349]  eta: 0:01:55  lr: 0.000332  min_lr: 0.000008  loss: 0.8318 (0.8156)  loss_scale: 131072.0000 (115706.7808)  weight_decay: 0.0500 (0.0500)  time: 0.3094  data: 0.0001  max mem: 41808
Epoch: [22]  [ 990/1349]  eta: 0:01:52  lr: 0.000332  min_lr: 0.000008  loss: 0.8240 (0.8152)  loss_scale: 131072.0000 (115861.8285)  weight_decay: 0.0500 (0.0500)  time: 0.3093  data: 0.0001  max mem: 41808
Epoch: [22]  [1000/1349]  eta: 0:01:49  lr: 0.000332  min_lr: 0.000008  loss: 0.7883 (0.8143)  loss_scale: 131072.0000 (116013.7782)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
[2025-05-23 19:38:23,977] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 19:38:23,977] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 19:38:23,977] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 19:38:23,977] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [22]  [1010/1349]  eta: 0:01:45  lr: 0.000332  min_lr: 0.000008  loss: 0.7801 (0.8140)  loss_scale: 131072.0000 (116940.5974)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [22]  [1020/1349]  eta: 0:01:42  lr: 0.000332  min_lr: 0.000008  loss: 0.8840 (0.8143)  loss_scale: 262144.0000 (118362.7659)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
Epoch: [22]  [1030/1349]  eta: 0:01:39  lr: 0.000332  min_lr: 0.000008  loss: 0.8705 (0.8149)  loss_scale: 262144.0000 (119757.3463)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
[2025-05-23 19:38:34,116] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 30716
[2025-05-23 19:38:34,116] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 19:38:34,116] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 30716
[2025-05-23 19:38:34,116] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 19:38:34,116] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [22]  [1040/1349]  eta: 0:01:36  lr: 0.000331  min_lr: 0.000008  loss: 0.8328 (0.8144)  loss_scale: 262144.0000 (120747.4044)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
Epoch: [22]  [1050/1349]  eta: 0:01:33  lr: 0.000331  min_lr: 0.000008  loss: 0.8181 (0.8148)  loss_scale: 131072.0000 (120845.6403)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
Epoch: [22]  [1060/1349]  eta: 0:01:30  lr: 0.000331  min_lr: 0.000008  loss: 0.8181 (0.8145)  loss_scale: 131072.0000 (120942.0245)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [22]  [1070/1349]  eta: 0:01:27  lr: 0.000331  min_lr: 0.000008  loss: 0.7540 (0.8141)  loss_scale: 131072.0000 (121036.6088)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
Epoch: [22]  [1080/1349]  eta: 0:01:23  lr: 0.000331  min_lr: 0.000008  loss: 0.7326 (0.8133)  loss_scale: 131072.0000 (121129.4431)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
Epoch: [22]  [1090/1349]  eta: 0:01:20  lr: 0.000331  min_lr: 0.000008  loss: 0.8208 (0.8137)  loss_scale: 131072.0000 (121220.5756)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [22]  [1100/1349]  eta: 0:01:17  lr: 0.000331  min_lr: 0.000008  loss: 0.8840 (0.8148)  loss_scale: 131072.0000 (121310.0527)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
Epoch: [22]  [1110/1349]  eta: 0:01:14  lr: 0.000331  min_lr: 0.000008  loss: 0.8926 (0.8152)  loss_scale: 131072.0000 (121397.9190)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
Epoch: [22]  [1120/1349]  eta: 0:01:11  lr: 0.000330  min_lr: 0.000008  loss: 0.8574 (0.8153)  loss_scale: 131072.0000 (121484.2177)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [22]  [1130/1349]  eta: 0:01:08  lr: 0.000330  min_lr: 0.000008  loss: 0.8054 (0.8149)  loss_scale: 131072.0000 (121568.9903)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [22]  [1140/1349]  eta: 0:01:05  lr: 0.000330  min_lr: 0.000008  loss: 0.8319 (0.8154)  loss_scale: 131072.0000 (121652.2770)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
Epoch: [22]  [1150/1349]  eta: 0:01:02  lr: 0.000330  min_lr: 0.000008  loss: 0.8768 (0.8154)  loss_scale: 131072.0000 (121734.1164)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
Epoch: [22]  [1160/1349]  eta: 0:00:58  lr: 0.000330  min_lr: 0.000008  loss: 0.8678 (0.8160)  loss_scale: 131072.0000 (121814.5461)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
[2025-05-23 19:39:13,735] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 19:39:13,735] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 19:39:13,735] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 19:39:13,735] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [22]  [1170/1349]  eta: 0:00:55  lr: 0.000330  min_lr: 0.000008  loss: 0.8469 (0.8154)  loss_scale: 131072.0000 (122341.3288)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0001  max mem: 41808
Epoch: [22]  [1180/1349]  eta: 0:00:52  lr: 0.000330  min_lr: 0.000008  loss: 0.8087 (0.8154)  loss_scale: 262144.0000 (123525.0940)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0001  max mem: 41808
[2025-05-23 19:39:18,346] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 30860
[2025-05-23 19:39:18,346] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 30860
[2025-05-23 19:39:18,346] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 19:39:18,346] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 19:39:18,346] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [22]  [1190/1349]  eta: 0:00:49  lr: 0.000330  min_lr: 0.000008  loss: 0.8244 (0.8162)  loss_scale: 262144.0000 (123698.5122)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0001  max mem: 41808
Epoch: [22]  [1200/1349]  eta: 0:00:46  lr: 0.000329  min_lr: 0.000008  loss: 0.8720 (0.8164)  loss_scale: 131072.0000 (123759.9067)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0001  max mem: 41808
Epoch: [22]  [1210/1349]  eta: 0:00:43  lr: 0.000329  min_lr: 0.000008  loss: 0.8454 (0.8162)  loss_scale: 131072.0000 (123820.2874)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
Epoch: [22]  [1220/1349]  eta: 0:00:40  lr: 0.000329  min_lr: 0.000008  loss: 0.8268 (0.8163)  loss_scale: 131072.0000 (123879.6790)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [22]  [1230/1349]  eta: 0:00:37  lr: 0.000329  min_lr: 0.000008  loss: 0.8058 (0.8159)  loss_scale: 131072.0000 (123938.1056)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
Epoch: [22]  [1240/1349]  eta: 0:00:33  lr: 0.000329  min_lr: 0.000008  loss: 0.8184 (0.8163)  loss_scale: 131072.0000 (123995.5907)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
Epoch: [22]  [1250/1349]  eta: 0:00:30  lr: 0.000329  min_lr: 0.000008  loss: 0.8719 (0.8166)  loss_scale: 131072.0000 (124052.1567)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [22]  [1260/1349]  eta: 0:00:27  lr: 0.000329  min_lr: 0.000008  loss: 0.8389 (0.8165)  loss_scale: 131072.0000 (124107.8255)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [22]  [1270/1349]  eta: 0:00:24  lr: 0.000329  min_lr: 0.000008  loss: 0.8033 (0.8164)  loss_scale: 131072.0000 (124162.6184)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [22]  [1280/1349]  eta: 0:00:21  lr: 0.000328  min_lr: 0.000008  loss: 0.8109 (0.8163)  loss_scale: 131072.0000 (124216.5558)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [22]  [1290/1349]  eta: 0:00:18  lr: 0.000328  min_lr: 0.000008  loss: 0.8448 (0.8170)  loss_scale: 131072.0000 (124269.6576)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [22]  [1300/1349]  eta: 0:00:15  lr: 0.000328  min_lr: 0.000008  loss: 0.8481 (0.8170)  loss_scale: 131072.0000 (124321.9431)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [22]  [1310/1349]  eta: 0:00:12  lr: 0.000328  min_lr: 0.000008  loss: 0.7507 (0.8159)  loss_scale: 131072.0000 (124373.4310)  weight_decay: 0.0500 (0.0500)  time: 0.3086  data: 0.0004  max mem: 41808
[2025-05-23 19:39:58,006] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 19:39:58,006] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 19:39:58,006] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 19:39:58,006] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 19:39:58,924] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 30992
[2025-05-23 19:39:58,924] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 30992
[2025-05-23 19:39:58,924] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 19:39:58,924] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 19:39:58,924] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [22]  [1320/1349]  eta: 0:00:09  lr: 0.000328  min_lr: 0.000008  loss: 0.7364 (0.8154)  loss_scale: 131072.0000 (124721.8047)  weight_decay: 0.0500 (0.0500)  time: 0.3081  data: 0.0005  max mem: 41808
[2025-05-23 19:40:01,068] [INFO] [logging.py:96:log_dist] [Rank 0] step=31000, skipped=188, lr=[7.791040363185471e-06, 7.791040363185471e-06, 1.0388053817580627e-05, 1.0388053817580627e-05, 1.3850738423440837e-05, 1.3850738423440837e-05, 1.846765123125445e-05, 1.846765123125445e-05, 2.4623534975005933e-05, 2.4623534975005933e-05, 3.283137996667458e-05, 3.283137996667458e-05, 4.377517328889944e-05, 4.377517328889944e-05, 5.836689771853258e-05, 5.836689771853258e-05, 7.782253029137677e-05, 7.782253029137677e-05, 0.0001037633737218357, 0.0001037633737218357, 0.0001383511649624476, 0.0001383511649624476, 0.00018446821994993012, 0.00018446821994993012, 0.0002459576265999068, 0.0002459576265999068, 0.0003279435021332091, 0.0003279435021332091], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-05-23 19:40:01,069] [INFO] [timer.py:260:stop] epoch=0/micro_step=31000/global_step=31000, RunningAvgSamplesPerSec=208.65062890003128, CurrSamplesPerSec=214.47589462347034, MemAllocated=1.37GB, MaxMemAllocated=40.83GB
Epoch: [22]  [1330/1349]  eta: 0:00:05  lr: 0.000328  min_lr: 0.000008  loss: 0.8132 (0.8158)  loss_scale: 131072.0000 (124769.5147)  weight_decay: 0.0500 (0.0500)  time: 0.3045  data: 0.0001  max mem: 41808
Epoch: [22]  [1340/1349]  eta: 0:00:02  lr: 0.000328  min_lr: 0.000008  loss: 0.8699 (0.8160)  loss_scale: 131072.0000 (124816.5130)  weight_decay: 0.0500 (0.0500)  time: 0.3026  data: 0.0001  max mem: 41808
Epoch: [22]  [1348/1349]  eta: 0:00:00  lr: 0.000328  min_lr: 0.000008  loss: 0.8016 (0.8151)  loss_scale: 131072.0000 (124853.6101)  weight_decay: 0.0500 (0.0500)  time: 0.3022  data: 0.0001  max mem: 41808
Epoch: [22] Total time: 0:07:00 (0.3115 s / it)
Averaged stats: lr: 0.000328  min_lr: 0.000008  loss: 0.8016 (0.8162)  loss_scale: 131072.0000 (124853.6101)  weight_decay: 0.0500 (0.0500)  total_time: 420.1889 (420.1835)
Val:  [  0/346]  eta: 1:01:26  loss: 2.7982 (2.7982)  acc1: 6.2500 (6.2500)  acc5: 88.2812 (88.2812)  time: 10.6545  data: 9.5763  max mem: 41808
Val:  [ 10/346]  eta: 0:11:26  loss: 0.1550 (0.5291)  acc1: 98.4375 (85.5824)  acc5: 100.0000 (98.8636)  time: 2.0434  data: 1.2592  max mem: 41808
Val:  [ 20/346]  eta: 0:08:21  loss: 0.1250 (0.4328)  acc1: 100.0000 (88.7649)  acc5: 100.0000 (99.3304)  time: 1.0818  data: 0.2561  max mem: 41808
Val:  [ 30/346]  eta: 0:07:14  loss: 0.1060 (0.3670)  acc1: 100.0000 (91.0534)  acc5: 100.0000 (99.5464)  time: 1.0053  data: 0.0426  max mem: 41808
Val:  [ 40/346]  eta: 0:06:42  loss: 0.1623 (0.4372)  acc1: 97.6562 (89.0434)  acc5: 100.0000 (98.9139)  time: 1.0805  data: 0.0003  max mem: 41808
Val:  [ 50/346]  eta: 0:06:29  loss: 0.1367 (0.3821)  acc1: 98.4375 (90.9161)  acc5: 100.0000 (99.1268)  time: 1.2250  data: 0.0443  max mem: 41808
Val:  [ 60/346]  eta: 0:06:23  loss: 0.1407 (0.3734)  acc1: 98.4375 (90.9708)  acc5: 100.0000 (99.2188)  time: 1.3950  data: 0.1307  max mem: 41808
Val:  [ 70/346]  eta: 0:06:15  loss: 0.2274 (0.3936)  acc1: 96.0938 (90.2619)  acc5: 100.0000 (99.3178)  time: 1.4801  data: 0.1706  max mem: 41808
Val:  [ 80/346]  eta: 0:06:04  loss: 0.2065 (0.3966)  acc1: 95.3125 (90.2006)  acc5: 100.0000 (98.9776)  time: 1.4620  data: 0.1717  max mem: 41808
Val:  [ 90/346]  eta: 0:05:54  loss: 0.2490 (0.3904)  acc1: 92.1875 (90.3159)  acc5: 100.0000 (99.0814)  time: 1.4637  data: 0.1708  max mem: 41808
Val:  [100/346]  eta: 0:05:41  loss: 0.2242 (0.3662)  acc1: 94.5312 (91.1587)  acc5: 100.0000 (99.1723)  time: 1.4577  data: 0.1657  max mem: 41808
Val:  [110/346]  eta: 0:05:28  loss: 0.1335 (0.3900)  acc1: 98.4375 (90.4209)  acc5: 100.0000 (99.1836)  time: 1.4388  data: 0.1642  max mem: 41808
Val:  [120/346]  eta: 0:05:17  loss: 0.2241 (0.3844)  acc1: 95.3125 (90.5863)  acc5: 100.0000 (99.2510)  time: 1.4826  data: 0.1681  max mem: 41808
Val:  [130/346]  eta: 0:05:03  loss: 0.1219 (0.3750)  acc1: 99.2188 (90.8099)  acc5: 100.0000 (99.3022)  time: 1.4723  data: 0.1496  max mem: 41808
Val:  [140/346]  eta: 0:04:50  loss: 0.2795 (0.3801)  acc1: 93.7500 (90.6416)  acc5: 100.0000 (99.2797)  time: 1.4567  data: 0.1454  max mem: 41808
Val:  [150/346]  eta: 0:04:37  loss: 0.3182 (0.3824)  acc1: 92.9688 (90.6302)  acc5: 100.0000 (99.3067)  time: 1.4808  data: 0.1609  max mem: 41808
Val:  [160/346]  eta: 0:04:24  loss: 0.2165 (0.3744)  acc1: 96.0938 (90.8725)  acc5: 100.0000 (99.3449)  time: 1.4851  data: 0.1808  max mem: 41808
Val:  [170/346]  eta: 0:04:10  loss: 0.1794 (0.3702)  acc1: 96.0938 (90.9859)  acc5: 100.0000 (99.3787)  time: 1.4718  data: 0.1886  max mem: 41808
Val:  [180/346]  eta: 0:03:56  loss: 0.1484 (0.3824)  acc1: 96.0938 (90.5300)  acc5: 100.0000 (99.4087)  time: 1.4701  data: 0.1859  max mem: 41808
Val:  [190/346]  eta: 0:03:42  loss: 0.3276 (0.3804)  acc1: 88.2812 (90.6127)  acc5: 100.0000 (99.4233)  time: 1.4672  data: 0.1977  max mem: 41808
Val:  [200/346]  eta: 0:03:28  loss: 0.3042 (0.3879)  acc1: 92.1875 (90.3490)  acc5: 100.0000 (99.4403)  time: 1.4508  data: 0.1867  max mem: 41808
Val:  [210/346]  eta: 0:03:14  loss: 0.2042 (0.3830)  acc1: 98.4375 (90.5509)  acc5: 100.0000 (99.4668)  time: 1.4555  data: 0.1886  max mem: 41808
Val:  [220/346]  eta: 0:03:00  loss: 0.1899 (0.3814)  acc1: 99.2188 (90.6674)  acc5: 100.0000 (99.3884)  time: 1.4632  data: 0.1975  max mem: 41808
Val:  [230/346]  eta: 0:02:46  loss: 0.1559 (0.3741)  acc1: 98.4375 (90.9328)  acc5: 100.0000 (99.4115)  time: 1.4954  data: 0.2081  max mem: 41808
Val:  [240/346]  eta: 0:02:32  loss: 0.1901 (0.3847)  acc1: 95.3125 (90.6639)  acc5: 100.0000 (99.4262)  time: 1.5002  data: 0.2068  max mem: 41808
Val:  [250/346]  eta: 0:02:18  loss: 0.2180 (0.3808)  acc1: 95.3125 (90.7744)  acc5: 100.0000 (99.4304)  time: 1.4957  data: 0.1843  max mem: 41808
Val:  [260/346]  eta: 0:02:03  loss: 0.1699 (0.3774)  acc1: 97.6562 (90.8734)  acc5: 100.0000 (99.4492)  time: 1.4465  data: 0.1681  max mem: 41808
Val:  [270/346]  eta: 0:01:49  loss: 0.1347 (0.3757)  acc1: 97.6562 (90.9536)  acc5: 100.0000 (99.4407)  time: 1.4222  data: 0.1717  max mem: 41808
Val:  [280/346]  eta: 0:01:34  loss: 0.1175 (0.3741)  acc1: 100.0000 (91.0476)  acc5: 100.0000 (99.3939)  time: 1.4522  data: 0.1867  max mem: 41808
Val:  [290/346]  eta: 0:01:20  loss: 0.1091 (0.3652)  acc1: 100.0000 (91.3445)  acc5: 100.0000 (99.4147)  time: 1.4516  data: 0.1883  max mem: 41808
Val:  [300/346]  eta: 0:01:06  loss: 0.1104 (0.3661)  acc1: 100.0000 (91.3517)  acc5: 100.0000 (99.3875)  time: 1.4589  data: 0.1916  max mem: 41808
Val:  [310/346]  eta: 0:00:51  loss: 0.1551 (0.3672)  acc1: 98.4375 (91.3108)  acc5: 100.0000 (99.3996)  time: 1.4652  data: 0.1908  max mem: 41808
Val:  [320/346]  eta: 0:00:37  loss: 0.1551 (0.3732)  acc1: 97.6562 (91.0923)  acc5: 100.0000 (99.4110)  time: 1.4807  data: 0.1881  max mem: 41808
Val:  [330/346]  eta: 0:00:23  loss: 0.4473 (0.3887)  acc1: 85.9375 (90.6368)  acc5: 100.0000 (99.4147)  time: 1.4992  data: 0.1805  max mem: 41808
Val:  [340/346]  eta: 0:00:08  loss: 0.5183 (0.3961)  acc1: 85.9375 (90.4303)  acc5: 100.0000 (99.4181)  time: 1.3902  data: 0.1685  max mem: 41808
Val:  [345/346]  eta: 0:00:01  loss: 0.2096 (0.3928)  acc1: 92.9688 (90.5327)  acc5: 100.0000 (99.4259)  time: 1.2296  data: 0.1702  max mem: 41808
Val: Total time: 0:08:15 (1.4322 s / it)
* Acc@1 90.567 Acc@5 99.444 loss 0.391
Accuracy of the network on the 88494 val videos: 90.6%
Max accuracy: 91.38%   Max Epoch: 20
Epoch: [23]  [   0/1349]  eta: 1:40:53  lr: 0.000328  min_lr: 0.000008  loss: 0.8009 (0.8009)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 4.4876  data: 3.3654  max mem: 41808
Epoch: [23]  [  10/1349]  eta: 0:15:54  lr: 0.000327  min_lr: 0.000008  loss: 0.7595 (0.7357)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7131  data: 0.3222  max mem: 41808
Epoch: [23]  [  20/1349]  eta: 0:11:33  lr: 0.000327  min_lr: 0.000008  loss: 0.7766 (0.7740)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3238  data: 0.0090  max mem: 41808
Epoch: [23]  [  30/1349]  eta: 0:09:57  lr: 0.000327  min_lr: 0.000008  loss: 0.8079 (0.7843)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3097  data: 0.0002  max mem: 41808
Epoch: [23]  [  40/1349]  eta: 0:09:06  lr: 0.000327  min_lr: 0.000008  loss: 0.8255 (0.8031)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
Epoch: [23]  [  50/1349]  eta: 0:08:34  lr: 0.000327  min_lr: 0.000008  loss: 0.8453 (0.8067)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
Epoch: [23]  [  60/1349]  eta: 0:08:11  lr: 0.000327  min_lr: 0.000008  loss: 0.8502 (0.8134)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0002  max mem: 41808
Epoch: [23]  [  70/1349]  eta: 0:07:54  lr: 0.000327  min_lr: 0.000008  loss: 0.8458 (0.8119)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3080  data: 0.0002  max mem: 41808
Epoch: [23]  [  80/1349]  eta: 0:07:41  lr: 0.000327  min_lr: 0.000008  loss: 0.8341 (0.8188)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3088  data: 0.0001  max mem: 41808
Epoch: [23]  [  90/1349]  eta: 0:07:29  lr: 0.000326  min_lr: 0.000008  loss: 0.8521 (0.8208)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3083  data: 0.0001  max mem: 41808
[2025-05-23 19:48:58,828] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 19:48:58,828] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 19:48:58,828] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 19:48:58,828] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 19:49:00,365] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 31126
[2025-05-23 19:49:00,365] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 19:49:00,365] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 31126
[2025-05-23 19:49:00,365] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 19:49:00,365] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [23]  [ 100/1349]  eta: 0:07:20  lr: 0.000326  min_lr: 0.000008  loss: 0.7766 (0.8178)  loss_scale: 131072.0000 (137560.7129)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0002  max mem: 41808
Epoch: [23]  [ 110/1349]  eta: 0:07:11  lr: 0.000326  min_lr: 0.000008  loss: 0.7738 (0.8164)  loss_scale: 131072.0000 (136976.1441)  weight_decay: 0.0500 (0.0500)  time: 0.3084  data: 0.0002  max mem: 41808
Epoch: [23]  [ 120/1349]  eta: 0:07:04  lr: 0.000326  min_lr: 0.000008  loss: 0.8170 (0.8183)  loss_scale: 131072.0000 (136488.1983)  weight_decay: 0.0500 (0.0500)  time: 0.3096  data: 0.0002  max mem: 41808
Epoch: [23]  [ 130/1349]  eta: 0:06:57  lr: 0.000326  min_lr: 0.000008  loss: 0.8382 (0.8180)  loss_scale: 131072.0000 (136074.7481)  weight_decay: 0.0500 (0.0500)  time: 0.3082  data: 0.0002  max mem: 41808
Epoch: [23]  [ 140/1349]  eta: 0:06:50  lr: 0.000326  min_lr: 0.000008  loss: 0.8382 (0.8169)  loss_scale: 131072.0000 (135719.9433)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0002  max mem: 41808
Epoch: [23]  [ 150/1349]  eta: 0:06:44  lr: 0.000326  min_lr: 0.000008  loss: 0.8574 (0.8175)  loss_scale: 131072.0000 (135412.1325)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [23]  [ 160/1349]  eta: 0:06:39  lr: 0.000326  min_lr: 0.000008  loss: 0.8406 (0.8175)  loss_scale: 131072.0000 (135142.5590)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
Epoch: [23]  [ 170/1349]  eta: 0:06:33  lr: 0.000326  min_lr: 0.000008  loss: 0.8430 (0.8236)  loss_scale: 131072.0000 (134904.5146)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
[2025-05-23 19:49:24,066] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 31203
[2025-05-23 19:49:24,066] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 31203
[2025-05-23 19:49:24,066] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 19:49:24,066] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 19:49:24,066] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [23]  [ 180/1349]  eta: 0:06:28  lr: 0.000325  min_lr: 0.000008  loss: 0.8839 (0.8249)  loss_scale: 131072.0000 (132882.3867)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
Epoch: [23]  [ 190/1349]  eta: 0:06:23  lr: 0.000325  min_lr: 0.000008  loss: 0.8126 (0.8249)  loss_scale: 65536.0000 (129356.3979)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
Epoch: [23]  [ 200/1349]  eta: 0:06:19  lr: 0.000325  min_lr: 0.000008  loss: 0.8126 (0.8240)  loss_scale: 65536.0000 (126181.2537)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [23]  [ 210/1349]  eta: 0:06:14  lr: 0.000325  min_lr: 0.000008  loss: 0.8343 (0.8234)  loss_scale: 65536.0000 (123307.0711)  weight_decay: 0.0500 (0.0500)  time: 0.3066  data: 0.0001  max mem: 41808
Epoch: [23]  [ 220/1349]  eta: 0:06:10  lr: 0.000325  min_lr: 0.000008  loss: 0.8623 (0.8255)  loss_scale: 65536.0000 (120692.9955)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
Epoch: [23]  [ 230/1349]  eta: 0:06:06  lr: 0.000325  min_lr: 0.000008  loss: 0.8242 (0.8248)  loss_scale: 65536.0000 (118305.2468)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
Epoch: [23]  [ 240/1349]  eta: 0:06:01  lr: 0.000325  min_lr: 0.000008  loss: 0.7898 (0.8233)  loss_scale: 65536.0000 (116115.6515)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
Epoch: [23]  [ 250/1349]  eta: 0:05:57  lr: 0.000325  min_lr: 0.000008  loss: 0.7604 (0.8195)  loss_scale: 65536.0000 (114100.5259)  weight_decay: 0.0500 (0.0500)  time: 0.3063  data: 0.0001  max mem: 41808
Epoch: [23]  [ 260/1349]  eta: 0:05:53  lr: 0.000324  min_lr: 0.000008  loss: 0.7594 (0.8158)  loss_scale: 65536.0000 (112239.8161)  weight_decay: 0.0500 (0.0500)  time: 0.3058  data: 0.0001  max mem: 41808
Epoch: [23]  [ 270/1349]  eta: 0:05:49  lr: 0.000324  min_lr: 0.000008  loss: 0.8033 (0.8163)  loss_scale: 65536.0000 (110516.4280)  weight_decay: 0.0500 (0.0500)  time: 0.3059  data: 0.0001  max mem: 41808
Epoch: [23]  [ 280/1349]  eta: 0:05:45  lr: 0.000324  min_lr: 0.000008  loss: 0.8036 (0.8157)  loss_scale: 65536.0000 (108915.7011)  weight_decay: 0.0500 (0.0500)  time: 0.3064  data: 0.0001  max mem: 41808
Epoch: [23]  [ 290/1349]  eta: 0:05:41  lr: 0.000324  min_lr: 0.000008  loss: 0.8267 (0.8163)  loss_scale: 65536.0000 (107424.9897)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
Epoch: [23]  [ 300/1349]  eta: 0:05:38  lr: 0.000324  min_lr: 0.000008  loss: 0.8663 (0.8170)  loss_scale: 65536.0000 (106033.3289)  weight_decay: 0.0500 (0.0500)  time: 0.3089  data: 0.0002  max mem: 41808
[2025-05-23 19:50:03,700] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 19:50:03,700] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-05-23 19:50:03,700] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 19:50:03,700] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [23]  [ 310/1349]  eta: 0:05:34  lr: 0.000324  min_lr: 0.000008  loss: 0.8391 (0.8163)  loss_scale: 65536.0000 (105995.5241)  weight_decay: 0.0500 (0.0500)  time: 0.3096  data: 0.0002  max mem: 41808
Epoch: [23]  [ 320/1349]  eta: 0:05:30  lr: 0.000324  min_lr: 0.000008  loss: 0.7987 (0.8175)  loss_scale: 131072.0000 (106776.7227)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0002  max mem: 41808
Epoch: [23]  [ 330/1349]  eta: 0:05:27  lr: 0.000324  min_lr: 0.000008  loss: 0.7987 (0.8174)  loss_scale: 131072.0000 (107510.7190)  weight_decay: 0.0500 (0.0500)  time: 0.3083  data: 0.0001  max mem: 41808
Epoch: [23]  [ 340/1349]  eta: 0:05:23  lr: 0.000323  min_lr: 0.000008  loss: 0.8048 (0.8188)  loss_scale: 131072.0000 (108201.6657)  weight_decay: 0.0500 (0.0500)  time: 0.3087  data: 0.0001  max mem: 41808
Epoch: [23]  [ 350/1349]  eta: 0:05:20  lr: 0.000323  min_lr: 0.000008  loss: 0.8508 (0.8179)  loss_scale: 131072.0000 (108853.2422)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [23]  [ 360/1349]  eta: 0:05:16  lr: 0.000323  min_lr: 0.000008  loss: 0.8539 (0.8200)  loss_scale: 131072.0000 (109468.7202)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0001  max mem: 41808
Epoch: [23]  [ 370/1349]  eta: 0:05:12  lr: 0.000323  min_lr: 0.000008  loss: 0.8919 (0.8203)  loss_scale: 131072.0000 (110051.0189)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0001  max mem: 41808
Epoch: [23]  [ 380/1349]  eta: 0:05:09  lr: 0.000323  min_lr: 0.000008  loss: 0.7914 (0.8192)  loss_scale: 131072.0000 (110602.7507)  weight_decay: 0.0500 (0.0500)  time: 0.3064  data: 0.0001  max mem: 41808
Epoch: [23]  [ 390/1349]  eta: 0:05:05  lr: 0.000323  min_lr: 0.000008  loss: 0.7914 (0.8189)  loss_scale: 131072.0000 (111126.2609)  weight_decay: 0.0500 (0.0500)  time: 0.3062  data: 0.0001  max mem: 41808
Epoch: [23]  [ 400/1349]  eta: 0:05:02  lr: 0.000323  min_lr: 0.000008  loss: 0.7613 (0.8167)  loss_scale: 131072.0000 (111623.6608)  weight_decay: 0.0500 (0.0500)  time: 0.3061  data: 0.0001  max mem: 41808
Epoch: [23]  [ 410/1349]  eta: 0:04:58  lr: 0.000323  min_lr: 0.000008  loss: 0.7626 (0.8167)  loss_scale: 131072.0000 (112096.8564)  weight_decay: 0.0500 (0.0500)  time: 0.3060  data: 0.0001  max mem: 41808
Epoch: [23]  [ 420/1349]  eta: 0:04:55  lr: 0.000322  min_lr: 0.000008  loss: 0.8209 (0.8156)  loss_scale: 131072.0000 (112547.5724)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [23]  [ 430/1349]  eta: 0:04:52  lr: 0.000322  min_lr: 0.000008  loss: 0.7870 (0.8155)  loss_scale: 131072.0000 (112977.3735)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
[2025-05-23 19:50:43,008] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 19:50:43,008] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 19:50:43,008] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 19:50:43,008] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [23]  [ 440/1349]  eta: 0:04:48  lr: 0.000322  min_lr: 0.000008  loss: 0.8532 (0.8151)  loss_scale: 131072.0000 (115765.4059)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0001  max mem: 41808
[2025-05-23 19:50:47,302] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 31474
[2025-05-23 19:50:47,302] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 19:50:47,302] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
[2025-05-23 19:50:47,302] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 31474
[2025-05-23 19:50:47,302] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
Epoch: [23]  [ 450/1349]  eta: 0:04:45  lr: 0.000322  min_lr: 0.000008  loss: 0.8054 (0.8145)  loss_scale: 262144.0000 (117848.5499)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
Epoch: [23]  [ 460/1349]  eta: 0:04:41  lr: 0.000322  min_lr: 0.000008  loss: 0.8129 (0.8159)  loss_scale: 131072.0000 (118135.3926)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
[2025-05-23 19:50:52,226] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 31490
[2025-05-23 19:50:52,226] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 31490
[2025-05-23 19:50:52,226] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 19:50:52,226] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 19:50:52,226] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [23]  [ 470/1349]  eta: 0:04:38  lr: 0.000322  min_lr: 0.000008  loss: 0.8548 (0.8163)  loss_scale: 131072.0000 (117296.9172)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [23]  [ 480/1349]  eta: 0:04:35  lr: 0.000322  min_lr: 0.000008  loss: 0.8264 (0.8160)  loss_scale: 65536.0000 (116220.8067)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
Epoch: [23]  [ 490/1349]  eta: 0:04:31  lr: 0.000322  min_lr: 0.000008  loss: 0.7995 (0.8169)  loss_scale: 65536.0000 (115188.5295)  weight_decay: 0.0500 (0.0500)  time: 0.3079  data: 0.0001  max mem: 41808
Epoch: [23]  [ 500/1349]  eta: 0:04:28  lr: 0.000321  min_lr: 0.000008  loss: 0.8191 (0.8171)  loss_scale: 65536.0000 (114197.4611)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
Epoch: [23]  [ 510/1349]  eta: 0:04:25  lr: 0.000321  min_lr: 0.000008  loss: 0.8466 (0.8179)  loss_scale: 65536.0000 (113245.1820)  weight_decay: 0.0500 (0.0500)  time: 0.3078  data: 0.0001  max mem: 41808
Epoch: [23]  [ 520/1349]  eta: 0:04:22  lr: 0.000321  min_lr: 0.000008  loss: 0.8607 (0.8185)  loss_scale: 65536.0000 (112329.4587)  weight_decay: 0.0500 (0.0500)  time: 0.3098  data: 0.0002  max mem: 41808
Epoch: [23]  [ 530/1349]  eta: 0:04:18  lr: 0.000321  min_lr: 0.000008  loss: 0.8444 (0.8177)  loss_scale: 65536.0000 (111448.2260)  weight_decay: 0.0500 (0.0500)  time: 0.3102  data: 0.0001  max mem: 41808
Epoch: [23]  [ 540/1349]  eta: 0:04:15  lr: 0.000321  min_lr: 0.000008  loss: 0.8014 (0.8172)  loss_scale: 65536.0000 (110599.5712)  weight_decay: 0.0500 (0.0500)  time: 0.3080  data: 0.0001  max mem: 41808
Epoch: [23]  [ 550/1349]  eta: 0:04:12  lr: 0.000321  min_lr: 0.000008  loss: 0.8256 (0.8167)  loss_scale: 65536.0000 (109781.7205)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
Epoch: [23]  [ 560/1349]  eta: 0:04:08  lr: 0.000321  min_lr: 0.000008  loss: 0.8257 (0.8168)  loss_scale: 65536.0000 (108993.0267)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
Epoch: [23]  [ 570/1349]  eta: 0:04:05  lr: 0.000321  min_lr: 0.000008  loss: 0.8047 (0.8157)  loss_scale: 65536.0000 (108231.9580)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [23]  [ 580/1349]  eta: 0:04:02  lr: 0.000320  min_lr: 0.000008  loss: 0.8047 (0.8159)  loss_scale: 65536.0000 (107497.0878)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0002  max mem: 41808
Epoch: [23]  [ 590/1349]  eta: 0:03:59  lr: 0.000320  min_lr: 0.000008  loss: 0.8474 (0.8162)  loss_scale: 65536.0000 (106787.0863)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0002  max mem: 41808
[2025-05-23 19:51:31,949] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 19:51:31,949] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 19:51:31,949] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-05-23 19:51:31,949] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [23]  [ 600/1349]  eta: 0:03:55  lr: 0.000320  min_lr: 0.000008  loss: 0.8313 (0.8165)  loss_scale: 65536.0000 (107082.1165)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
Epoch: [23]  [ 610/1349]  eta: 0:03:52  lr: 0.000320  min_lr: 0.000008  loss: 0.8313 (0.8159)  loss_scale: 131072.0000 (107474.7496)  weight_decay: 0.0500 (0.0500)  time: 0.3066  data: 0.0001  max mem: 41808
Epoch: [23]  [ 620/1349]  eta: 0:03:49  lr: 0.000320  min_lr: 0.000008  loss: 0.8498 (0.8159)  loss_scale: 131072.0000 (107854.7375)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [23]  [ 630/1349]  eta: 0:03:46  lr: 0.000320  min_lr: 0.000008  loss: 0.8208 (0.8157)  loss_scale: 131072.0000 (108222.6815)  weight_decay: 0.0500 (0.0500)  time: 0.3090  data: 0.0001  max mem: 41808
Epoch: [23]  [ 640/1349]  eta: 0:03:42  lr: 0.000320  min_lr: 0.000008  loss: 0.7978 (0.8158)  loss_scale: 131072.0000 (108579.1451)  weight_decay: 0.0500 (0.0500)  time: 0.3089  data: 0.0001  max mem: 41808
Epoch: [23]  [ 650/1349]  eta: 0:03:39  lr: 0.000320  min_lr: 0.000008  loss: 0.8094 (0.8159)  loss_scale: 131072.0000 (108924.6575)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
Epoch: [23]  [ 660/1349]  eta: 0:03:36  lr: 0.000319  min_lr: 0.000008  loss: 0.8094 (0.8149)  loss_scale: 131072.0000 (109259.7156)  weight_decay: 0.0500 (0.0500)  time: 0.3059  data: 0.0001  max mem: 41808
Epoch: [23]  [ 670/1349]  eta: 0:03:33  lr: 0.000319  min_lr: 0.000008  loss: 0.8229 (0.8150)  loss_scale: 131072.0000 (109584.7869)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
Epoch: [23]  [ 680/1349]  eta: 0:03:30  lr: 0.000319  min_lr: 0.000008  loss: 0.8229 (0.8147)  loss_scale: 131072.0000 (109900.3113)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
Epoch: [23]  [ 690/1349]  eta: 0:03:26  lr: 0.000319  min_lr: 0.000008  loss: 0.7601 (0.8139)  loss_scale: 131072.0000 (110206.7033)  weight_decay: 0.0500 (0.0500)  time: 0.3091  data: 0.0001  max mem: 41808
Epoch: [23]  [ 700/1349]  eta: 0:03:23  lr: 0.000319  min_lr: 0.000008  loss: 0.8184 (0.8139)  loss_scale: 131072.0000 (110504.3538)  weight_decay: 0.0500 (0.0500)  time: 0.3115  data: 0.0001  max mem: 41808
Epoch: [23]  [ 710/1349]  eta: 0:03:20  lr: 0.000319  min_lr: 0.000008  loss: 0.7980 (0.8135)  loss_scale: 131072.0000 (110793.6315)  weight_decay: 0.0500 (0.0500)  time: 0.3096  data: 0.0001  max mem: 41808
[2025-05-23 19:52:11,374] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 19:52:11,375] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 19:52:11,375] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 19:52:11,375] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [23]  [ 720/1349]  eta: 0:03:17  lr: 0.000319  min_lr: 0.000008  loss: 0.8303 (0.8138)  loss_scale: 131072.0000 (111256.6768)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
[2025-05-23 19:52:12,602] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 31751
[2025-05-23 19:52:12,602] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 31751
[2025-05-23 19:52:12,602] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 19:52:12,602] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 19:52:12,602] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [23]  [ 730/1349]  eta: 0:03:14  lr: 0.000319  min_lr: 0.000008  loss: 0.8309 (0.8136)  loss_scale: 131072.0000 (112065.6635)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0002  max mem: 41808
Epoch: [23]  [ 740/1349]  eta: 0:03:10  lr: 0.000318  min_lr: 0.000008  loss: 0.7942 (0.8129)  loss_scale: 131072.0000 (112322.1592)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0001  max mem: 41808
Epoch: [23]  [ 750/1349]  eta: 0:03:07  lr: 0.000318  min_lr: 0.000008  loss: 0.8160 (0.8138)  loss_scale: 131072.0000 (112571.8242)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
[2025-05-23 19:52:20,883] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 31778
[2025-05-23 19:52:20,883] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 31778
[2025-05-23 19:52:20,884] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 19:52:20,884] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 19:52:20,884] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [23]  [ 760/1349]  eta: 0:03:04  lr: 0.000318  min_lr: 0.000008  loss: 0.8942 (0.8148)  loss_scale: 65536.0000 (111953.7451)  weight_decay: 0.0500 (0.0500)  time: 0.3064  data: 0.0001  max mem: 41808
Epoch: [23]  [ 770/1349]  eta: 0:03:01  lr: 0.000318  min_lr: 0.000008  loss: 0.8501 (0.8145)  loss_scale: 65536.0000 (111351.6991)  weight_decay: 0.0500 (0.0500)  time: 0.3062  data: 0.0001  max mem: 41808
Epoch: [23]  [ 780/1349]  eta: 0:02:58  lr: 0.000318  min_lr: 0.000008  loss: 0.7552 (0.8139)  loss_scale: 65536.0000 (110765.0704)  weight_decay: 0.0500 (0.0500)  time: 0.3064  data: 0.0001  max mem: 41808
Epoch: [23]  [ 790/1349]  eta: 0:02:55  lr: 0.000318  min_lr: 0.000008  loss: 0.7722 (0.8129)  loss_scale: 65536.0000 (110193.2743)  weight_decay: 0.0500 (0.0500)  time: 0.3062  data: 0.0001  max mem: 41808
Epoch: [23]  [ 800/1349]  eta: 0:02:51  lr: 0.000318  min_lr: 0.000008  loss: 0.7722 (0.8124)  loss_scale: 65536.0000 (109635.7553)  weight_decay: 0.0500 (0.0500)  time: 0.3064  data: 0.0001  max mem: 41808
Epoch: [23]  [ 810/1349]  eta: 0:02:48  lr: 0.000318  min_lr: 0.000008  loss: 0.7874 (0.8129)  loss_scale: 65536.0000 (109091.9852)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
Epoch: [23]  [ 820/1349]  eta: 0:02:45  lr: 0.000317  min_lr: 0.000008  loss: 0.8661 (0.8133)  loss_scale: 65536.0000 (108561.4616)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
Epoch: [23]  [ 830/1349]  eta: 0:02:42  lr: 0.000317  min_lr: 0.000008  loss: 0.8700 (0.8134)  loss_scale: 65536.0000 (108043.7064)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [23]  [ 840/1349]  eta: 0:02:39  lr: 0.000317  min_lr: 0.000008  loss: 0.8213 (0.8129)  loss_scale: 65536.0000 (107538.2640)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0002  max mem: 41808
Epoch: [23]  [ 850/1349]  eta: 0:02:36  lr: 0.000317  min_lr: 0.000008  loss: 0.8213 (0.8132)  loss_scale: 65536.0000 (107044.7004)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [23]  [ 860/1349]  eta: 0:02:32  lr: 0.000317  min_lr: 0.000008  loss: 0.8288 (0.8132)  loss_scale: 65536.0000 (106562.6016)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
Epoch: [23]  [ 870/1349]  eta: 0:02:29  lr: 0.000317  min_lr: 0.000008  loss: 0.8392 (0.8137)  loss_scale: 65536.0000 (106091.5729)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
[2025-05-23 19:53:00,483] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 19:53:00,483] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-05-23 19:53:00,483] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 19:53:00,483] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [23]  [ 880/1349]  eta: 0:02:26  lr: 0.000317  min_lr: 0.000008  loss: 0.7974 (0.8136)  loss_scale: 65536.0000 (105705.6254)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [23]  [ 890/1349]  eta: 0:02:23  lr: 0.000317  min_lr: 0.000008  loss: 0.7875 (0.8131)  loss_scale: 131072.0000 (105990.3210)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0002  max mem: 41808
Epoch: [23]  [ 900/1349]  eta: 0:02:20  lr: 0.000316  min_lr: 0.000008  loss: 0.8365 (0.8136)  loss_scale: 131072.0000 (106268.6970)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [23]  [ 910/1349]  eta: 0:02:17  lr: 0.000316  min_lr: 0.000008  loss: 0.8857 (0.8145)  loss_scale: 131072.0000 (106540.9616)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0001  max mem: 41808
Epoch: [23]  [ 920/1349]  eta: 0:02:13  lr: 0.000316  min_lr: 0.000008  loss: 0.8828 (0.8149)  loss_scale: 131072.0000 (106807.3138)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
Epoch: [23]  [ 930/1349]  eta: 0:02:10  lr: 0.000316  min_lr: 0.000008  loss: 0.8413 (0.8148)  loss_scale: 131072.0000 (107067.9441)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
Epoch: [23]  [ 940/1349]  eta: 0:02:07  lr: 0.000316  min_lr: 0.000008  loss: 0.8162 (0.8141)  loss_scale: 131072.0000 (107323.0351)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
Epoch: [23]  [ 950/1349]  eta: 0:02:04  lr: 0.000316  min_lr: 0.000008  loss: 0.7170 (0.8135)  loss_scale: 131072.0000 (107572.7613)  weight_decay: 0.0500 (0.0500)  time: 0.3078  data: 0.0001  max mem: 41808
Epoch: [23]  [ 960/1349]  eta: 0:02:01  lr: 0.000316  min_lr: 0.000008  loss: 0.8087 (0.8144)  loss_scale: 131072.0000 (107817.2903)  weight_decay: 0.0500 (0.0500)  time: 0.3096  data: 0.0001  max mem: 41808
Epoch: [23]  [ 970/1349]  eta: 0:01:58  lr: 0.000316  min_lr: 0.000007  loss: 0.8788 (0.8145)  loss_scale: 131072.0000 (108056.7827)  weight_decay: 0.0500 (0.0500)  time: 0.3098  data: 0.0001  max mem: 41808
[2025-05-23 19:53:28,818] [INFO] [logging.py:96:log_dist] [Rank 0] step=32000, skipped=194, lr=[7.497103110109869e-06, 7.497103110109869e-06, 9.996137480146491e-06, 9.996137480146491e-06, 1.3328183306861989e-05, 1.3328183306861989e-05, 1.7770911075815984e-05, 1.7770911075815984e-05, 2.369454810108798e-05, 2.369454810108798e-05, 3.1592730801450644e-05, 3.1592730801450644e-05, 4.2123641068600856e-05, 4.2123641068600856e-05, 5.616485475813447e-05, 5.616485475813447e-05, 7.488647301084596e-05, 7.488647301084596e-05, 9.984863068112795e-05, 9.984863068112795e-05, 0.00013313150757483726, 0.00013313150757483726, 0.00017750867676644968, 0.00017750867676644968, 0.0002366782356885996, 0.0002366782356885996, 0.0003155709809181328, 0.0003155709809181328], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-05-23 19:53:28,818] [INFO] [timer.py:260:stop] epoch=0/micro_step=32000/global_step=32000, RunningAvgSamplesPerSec=208.77543577520032, CurrSamplesPerSec=214.12158808735072, MemAllocated=1.37GB, MaxMemAllocated=40.83GB
Epoch: [23]  [ 980/1349]  eta: 0:01:55  lr: 0.000315  min_lr: 0.000007  loss: 0.8273 (0.8142)  loss_scale: 131072.0000 (108291.3925)  weight_decay: 0.0500 (0.0500)  time: 0.3082  data: 0.0001  max mem: 41808
Epoch: [23]  [ 990/1349]  eta: 0:01:51  lr: 0.000315  min_lr: 0.000007  loss: 0.8662 (0.8151)  loss_scale: 131072.0000 (108521.2674)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [23]  [1000/1349]  eta: 0:01:48  lr: 0.000315  min_lr: 0.000007  loss: 0.9016 (0.8152)  loss_scale: 131072.0000 (108746.5495)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
[2025-05-23 19:53:39,862] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 19:53:39,862] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 19:53:39,862] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 19:53:39,862] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [23]  [1010/1349]  eta: 0:01:45  lr: 0.000315  min_lr: 0.000007  loss: 0.8710 (0.8153)  loss_scale: 131072.0000 (109356.3126)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
[2025-05-23 19:53:40,777] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 32038
[2025-05-23 19:53:40,777] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 32038
[2025-05-23 19:53:40,777] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 19:53:40,777] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 19:53:40,777] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [23]  [1020/1349]  eta: 0:01:42  lr: 0.000315  min_lr: 0.000007  loss: 0.8569 (0.8153)  loss_scale: 131072.0000 (109569.0029)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
Epoch: [23]  [1030/1349]  eta: 0:01:39  lr: 0.000315  min_lr: 0.000007  loss: 0.7956 (0.8155)  loss_scale: 131072.0000 (109777.5674)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
Epoch: [23]  [1040/1349]  eta: 0:01:36  lr: 0.000315  min_lr: 0.000007  loss: 0.7905 (0.8148)  loss_scale: 131072.0000 (109982.1249)  weight_decay: 0.0500 (0.0500)  time: 0.3066  data: 0.0001  max mem: 41808
Epoch: [23]  [1050/1349]  eta: 0:01:33  lr: 0.000315  min_lr: 0.000007  loss: 0.7492 (0.8140)  loss_scale: 131072.0000 (110182.7897)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
[2025-05-23 19:53:55,200] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 32085
[2025-05-23 19:53:55,200] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 32085
[2025-05-23 19:53:55,200] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 19:53:55,200] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 19:53:55,200] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [23]  [1060/1349]  eta: 0:01:30  lr: 0.000314  min_lr: 0.000007  loss: 0.8120 (0.8147)  loss_scale: 131072.0000 (110194.3676)  weight_decay: 0.0500 (0.0500)  time: 0.3064  data: 0.0001  max mem: 41808
Epoch: [23]  [1070/1349]  eta: 0:01:26  lr: 0.000314  min_lr: 0.000007  loss: 0.8700 (0.8150)  loss_scale: 65536.0000 (109777.3894)  weight_decay: 0.0500 (0.0500)  time: 0.3066  data: 0.0001  max mem: 41808
Epoch: [23]  [1080/1349]  eta: 0:01:23  lr: 0.000314  min_lr: 0.000007  loss: 0.7985 (0.8143)  loss_scale: 65536.0000 (109368.1258)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [23]  [1090/1349]  eta: 0:01:20  lr: 0.000314  min_lr: 0.000007  loss: 0.7251 (0.8135)  loss_scale: 65536.0000 (108966.3648)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
Epoch: [23]  [1100/1349]  eta: 0:01:17  lr: 0.000314  min_lr: 0.000007  loss: 0.8045 (0.8138)  loss_scale: 65536.0000 (108571.9019)  weight_decay: 0.0500 (0.0500)  time: 0.3063  data: 0.0001  max mem: 41808
Epoch: [23]  [1110/1349]  eta: 0:01:14  lr: 0.000314  min_lr: 0.000007  loss: 0.8141 (0.8131)  loss_scale: 65536.0000 (108184.5401)  weight_decay: 0.0500 (0.0500)  time: 0.3087  data: 0.0001  max mem: 41808
Epoch: [23]  [1120/1349]  eta: 0:01:11  lr: 0.000314  min_lr: 0.000007  loss: 0.8111 (0.8133)  loss_scale: 65536.0000 (107804.0892)  weight_decay: 0.0500 (0.0500)  time: 0.3095  data: 0.0001  max mem: 41808
Epoch: [23]  [1130/1349]  eta: 0:01:08  lr: 0.000314  min_lr: 0.000007  loss: 0.8464 (0.8138)  loss_scale: 65536.0000 (107430.3660)  weight_decay: 0.0500 (0.0500)  time: 0.3080  data: 0.0001  max mem: 41808
Epoch: [23]  [1140/1349]  eta: 0:01:05  lr: 0.000313  min_lr: 0.000007  loss: 0.8736 (0.8143)  loss_scale: 65536.0000 (107063.1937)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
Epoch: [23]  [1150/1349]  eta: 0:01:01  lr: 0.000313  min_lr: 0.000007  loss: 0.8709 (0.8144)  loss_scale: 65536.0000 (106702.4014)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [23]  [1160/1349]  eta: 0:00:58  lr: 0.000313  min_lr: 0.000007  loss: 0.8250 (0.8143)  loss_scale: 65536.0000 (106347.8243)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0002  max mem: 41808
Epoch: [23]  [1170/1349]  eta: 0:00:55  lr: 0.000313  min_lr: 0.000007  loss: 0.7850 (0.8137)  loss_scale: 65536.0000 (105999.3032)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
Epoch: [23]  [1180/1349]  eta: 0:00:52  lr: 0.000313  min_lr: 0.000007  loss: 0.7211 (0.8131)  loss_scale: 65536.0000 (105656.6842)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
[2025-05-23 19:54:34,868] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 19:54:34,869] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-05-23 19:54:34,868] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 19:54:34,869] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [23]  [1190/1349]  eta: 0:00:49  lr: 0.000313  min_lr: 0.000007  loss: 0.7211 (0.8122)  loss_scale: 65536.0000 (105539.9228)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
Epoch: [23]  [1200/1349]  eta: 0:00:46  lr: 0.000313  min_lr: 0.000007  loss: 0.7527 (0.8118)  loss_scale: 131072.0000 (105752.5129)  weight_decay: 0.0500 (0.0500)  time: 0.3064  data: 0.0001  max mem: 41808
Epoch: [23]  [1210/1349]  eta: 0:00:43  lr: 0.000313  min_lr: 0.000007  loss: 0.8273 (0.8117)  loss_scale: 131072.0000 (105961.5921)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
[2025-05-23 19:54:42,228] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 32238
[2025-05-23 19:54:42,228] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 32238
[2025-05-23 19:54:42,228] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 19:54:42,228] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 19:54:42,229] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
[2025-05-23 19:54:42,528] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 32239
[2025-05-23 19:54:42,528] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 32239
[2025-05-23 19:54:42,529] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-05-23 19:54:42,529] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2025-05-23 19:54:42,529] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [23]  [1220/1349]  eta: 0:00:40  lr: 0.000312  min_lr: 0.000007  loss: 0.8395 (0.8121)  loss_scale: 65536.0000 (105388.9730)  weight_decay: 0.0500 (0.0500)  time: 0.3062  data: 0.0002  max mem: 41808
Epoch: [23]  [1230/1349]  eta: 0:00:37  lr: 0.000312  min_lr: 0.000007  loss: 0.7900 (0.8119)  loss_scale: 32768.0000 (104799.0382)  weight_decay: 0.0500 (0.0500)  time: 0.3060  data: 0.0002  max mem: 41808
Epoch: [23]  [1240/1349]  eta: 0:00:33  lr: 0.000312  min_lr: 0.000007  loss: 0.8306 (0.8123)  loss_scale: 32768.0000 (104218.6108)  weight_decay: 0.0500 (0.0500)  time: 0.3064  data: 0.0001  max mem: 41808
Epoch: [23]  [1250/1349]  eta: 0:00:30  lr: 0.000312  min_lr: 0.000007  loss: 0.8838 (0.8128)  loss_scale: 32768.0000 (103647.4628)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
Epoch: [23]  [1260/1349]  eta: 0:00:27  lr: 0.000312  min_lr: 0.000007  loss: 0.8734 (0.8127)  loss_scale: 32768.0000 (103085.3735)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [23]  [1270/1349]  eta: 0:00:24  lr: 0.000312  min_lr: 0.000007  loss: 0.8143 (0.8125)  loss_scale: 32768.0000 (102532.1290)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [23]  [1280/1349]  eta: 0:00:21  lr: 0.000312  min_lr: 0.000007  loss: 0.8143 (0.8124)  loss_scale: 32768.0000 (101987.5222)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [23]  [1290/1349]  eta: 0:00:18  lr: 0.000312  min_lr: 0.000007  loss: 0.8197 (0.8123)  loss_scale: 32768.0000 (101451.3524)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0002  max mem: 41808
Epoch: [23]  [1300/1349]  eta: 0:00:15  lr: 0.000311  min_lr: 0.000007  loss: 0.8552 (0.8124)  loss_scale: 32768.0000 (100923.4251)  weight_decay: 0.0500 (0.0500)  time: 0.3090  data: 0.0001  max mem: 41808
Epoch: [23]  [1310/1349]  eta: 0:00:12  lr: 0.000311  min_lr: 0.000007  loss: 0.8066 (0.8123)  loss_scale: 32768.0000 (100403.5515)  weight_decay: 0.0500 (0.0500)  time: 0.3085  data: 0.0001  max mem: 41808
Epoch: [23]  [1320/1349]  eta: 0:00:09  lr: 0.000311  min_lr: 0.000007  loss: 0.8209 (0.8125)  loss_scale: 32768.0000 (99891.5488)  weight_decay: 0.0500 (0.0500)  time: 0.3066  data: 0.0001  max mem: 41808
Epoch: [23]  [1330/1349]  eta: 0:00:05  lr: 0.000311  min_lr: 0.000007  loss: 0.8403 (0.8124)  loss_scale: 32768.0000 (99387.2397)  weight_decay: 0.0500 (0.0500)  time: 0.3048  data: 0.0001  max mem: 41808
Epoch: [23]  [1340/1349]  eta: 0:00:02  lr: 0.000311  min_lr: 0.000007  loss: 0.8608 (0.8129)  loss_scale: 32768.0000 (98890.4519)  weight_decay: 0.0500 (0.0500)  time: 0.3026  data: 0.0001  max mem: 41808
[2025-05-23 19:55:22,070] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 19:55:22,070] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2025-05-23 19:55:22,070] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 19:55:22,070] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [23]  [1348/1349]  eta: 0:00:00  lr: 0.000311  min_lr: 0.000007  loss: 0.9096 (0.8129)  loss_scale: 32768.0000 (98692.6494)  weight_decay: 0.0500 (0.0500)  time: 0.3021  data: 0.0001  max mem: 41808
Epoch: [23] Total time: 0:06:59 (0.3109 s / it)
Averaged stats: lr: 0.000311  min_lr: 0.000007  loss: 0.9096 (0.8157)  loss_scale: 32768.0000 (98692.6494)  weight_decay: 0.0500 (0.0500)  total_time: 419.4621 (419.4524)
Val:  [  0/346]  eta: 1:29:40  loss: 3.6561 (3.6561)  acc1: 0.0000 (0.0000)  acc5: 61.7188 (61.7188)  time: 15.5517  data: 14.7908  max mem: 41808
Val:  [ 10/346]  eta: 0:11:50  loss: 0.1167 (0.6734)  acc1: 100.0000 (83.9489)  acc5: 100.0000 (96.3068)  time: 2.1157  data: 1.3449  max mem: 41808
Val:  [ 20/346]  eta: 0:08:13  loss: 0.1162 (0.4728)  acc1: 100.0000 (88.8393)  acc5: 100.0000 (97.9911)  time: 0.8116  data: 0.0460  max mem: 41808
Val:  [ 30/346]  eta: 0:06:43  loss: 0.0978 (0.4025)  acc1: 100.0000 (90.6502)  acc5: 100.0000 (98.5887)  time: 0.8136  data: 0.0460  max mem: 41808
Val:  [ 40/346]  eta: 0:05:51  loss: 0.1012 (0.4343)  acc1: 100.0000 (89.8438)  acc5: 100.0000 (98.0373)  time: 0.7671  data: 0.0003  max mem: 41808
Val:  [ 50/346]  eta: 0:05:17  loss: 0.1059 (0.3865)  acc1: 99.2188 (91.2990)  acc5: 100.0000 (98.4069)  time: 0.7589  data: 0.0003  max mem: 41808
Val:  [ 60/346]  eta: 0:04:55  loss: 0.1173 (0.3646)  acc1: 98.4375 (91.8161)  acc5: 100.0000 (98.6680)  time: 0.7907  data: 0.0262  max mem: 41808
Val:  [ 70/346]  eta: 0:04:38  loss: 0.2166 (0.3836)  acc1: 95.3125 (91.1972)  acc5: 100.0000 (98.8116)  time: 0.8467  data: 0.0955  max mem: 41808
Val:  [ 80/346]  eta: 0:04:23  loss: 0.2173 (0.3809)  acc1: 95.3125 (91.4062)  acc5: 100.0000 (98.6883)  time: 0.8700  data: 0.1398  max mem: 41808
Val:  [ 90/346]  eta: 0:04:10  loss: 0.2391 (0.3808)  acc1: 94.5312 (91.3118)  acc5: 100.0000 (98.8152)  time: 0.8691  data: 0.1423  max mem: 41808
Val:  [100/346]  eta: 0:03:57  loss: 0.1789 (0.3616)  acc1: 95.3125 (91.8394)  acc5: 100.0000 (98.9325)  time: 0.8680  data: 0.1485  max mem: 41808
Val:  [110/346]  eta: 0:03:46  loss: 0.1598 (0.3775)  acc1: 97.6562 (91.2444)  acc5: 100.0000 (99.0217)  time: 0.8753  data: 0.1507  max mem: 41808
Val:  [120/346]  eta: 0:03:36  loss: 0.1750 (0.3781)  acc1: 96.8750 (91.1867)  acc5: 100.0000 (99.0961)  time: 0.9072  data: 0.1506  max mem: 41808
Val:  [130/346]  eta: 0:03:25  loss: 0.1124 (0.3754)  acc1: 99.2188 (91.1200)  acc5: 100.0000 (99.1531)  time: 0.9068  data: 0.1528  max mem: 41808
Val:  [140/346]  eta: 0:03:15  loss: 0.1884 (0.3733)  acc1: 96.0938 (91.1957)  acc5: 100.0000 (99.2132)  time: 0.8884  data: 0.1444  max mem: 41808
Val:  [150/346]  eta: 0:03:04  loss: 0.2858 (0.3748)  acc1: 92.9688 (91.2303)  acc5: 100.0000 (99.2032)  time: 0.8881  data: 0.1418  max mem: 41808
Val:  [160/346]  eta: 0:02:54  loss: 0.2449 (0.3690)  acc1: 92.1875 (91.3432)  acc5: 100.0000 (99.2527)  time: 0.8823  data: 0.1471  max mem: 41808
Val:  [170/346]  eta: 0:02:44  loss: 0.2237 (0.3653)  acc1: 95.3125 (91.4108)  acc5: 100.0000 (99.2964)  time: 0.8886  data: 0.1394  max mem: 41808
Val:  [180/346]  eta: 0:02:35  loss: 0.2517 (0.3736)  acc1: 92.1875 (91.0005)  acc5: 100.0000 (99.3353)  time: 0.9016  data: 0.1372  max mem: 41808
Val:  [190/346]  eta: 0:02:25  loss: 0.2558 (0.3724)  acc1: 92.1875 (91.0422)  acc5: 100.0000 (99.3210)  time: 0.9020  data: 0.1491  max mem: 41808
Val:  [200/346]  eta: 0:02:15  loss: 0.3771 (0.3949)  acc1: 87.5000 (90.3529)  acc5: 100.0000 (99.3354)  time: 0.8916  data: 0.1563  max mem: 41808
Val:  [210/346]  eta: 0:02:06  loss: 0.1491 (0.3845)  acc1: 96.8750 (90.7028)  acc5: 100.0000 (99.3669)  time: 0.8961  data: 0.1604  max mem: 41808
Val:  [220/346]  eta: 0:01:56  loss: 0.1247 (0.3767)  acc1: 100.0000 (90.9644)  acc5: 100.0000 (99.3708)  time: 0.9018  data: 0.1604  max mem: 41808
Val:  [230/346]  eta: 0:01:47  loss: 0.1114 (0.3675)  acc1: 100.0000 (91.2676)  acc5: 100.0000 (99.3946)  time: 0.8963  data: 0.1496  max mem: 41808
Val:  [240/346]  eta: 0:01:38  loss: 0.1718 (0.3729)  acc1: 96.8750 (91.1534)  acc5: 100.0000 (99.4035)  time: 0.9067  data: 0.1556  max mem: 41808
Val:  [250/346]  eta: 0:01:28  loss: 0.2103 (0.3729)  acc1: 96.8750 (91.1977)  acc5: 100.0000 (99.4242)  time: 0.9132  data: 0.1595  max mem: 41808
Val:  [260/346]  eta: 0:01:19  loss: 0.1252 (0.3705)  acc1: 99.2188 (91.2805)  acc5: 100.0000 (99.4403)  time: 0.8877  data: 0.1489  max mem: 41808
Val:  [270/346]  eta: 0:01:10  loss: 0.1440 (0.3672)  acc1: 97.6562 (91.3947)  acc5: 100.0000 (99.4580)  time: 0.8889  data: 0.1455  max mem: 41808
Val:  [280/346]  eta: 0:01:00  loss: 0.1565 (0.3664)  acc1: 97.6562 (91.4507)  acc5: 100.0000 (99.4217)  time: 0.9157  data: 0.1555  max mem: 41808
Val:  [290/346]  eta: 0:00:51  loss: 0.0988 (0.3574)  acc1: 100.0000 (91.7338)  acc5: 100.0000 (99.4416)  time: 0.8962  data: 0.1610  max mem: 41808
Val:  [300/346]  eta: 0:00:42  loss: 0.0999 (0.3572)  acc1: 100.0000 (91.7696)  acc5: 100.0000 (99.4108)  time: 0.8845  data: 0.1529  max mem: 41808
Val:  [310/346]  eta: 0:00:33  loss: 0.1240 (0.3628)  acc1: 99.2188 (91.6122)  acc5: 100.0000 (99.4298)  time: 0.8920  data: 0.1523  max mem: 41808
Val:  [320/346]  eta: 0:00:23  loss: 0.1240 (0.3629)  acc1: 99.2188 (91.5985)  acc5: 100.0000 (99.4475)  time: 0.8923  data: 0.1541  max mem: 41808
Val:  [330/346]  eta: 0:00:14  loss: 0.3916 (0.3752)  acc1: 86.7188 (91.2103)  acc5: 100.0000 (99.3887)  time: 0.8932  data: 0.1587  max mem: 41808
Val:  [340/346]  eta: 0:00:05  loss: 0.5281 (0.3834)  acc1: 86.7188 (90.9801)  acc5: 100.0000 (99.4043)  time: 0.9090  data: 0.1652  max mem: 41808
Val:  [345/346]  eta: 0:00:00  loss: 0.1489 (0.3797)  acc1: 95.4023 (91.0932)  acc5: 100.0000 (99.4124)  time: 0.9185  data: 0.1795  max mem: 41808
Val: Total time: 0:05:18 (0.9191 s / it)
* Acc@1 91.097 Acc@5 99.429 loss 0.378
Accuracy of the network on the 88494 val videos: 91.1%
Max accuracy: 91.38%   Max Epoch: 20
Epoch: [24]  [   0/1349]  eta: 1:54:16  lr: 0.000311  min_lr: 0.000007  loss: 0.8625 (0.8625)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 5.0829  data: 3.5888  max mem: 41808
Epoch: [24]  [  10/1349]  eta: 0:16:42  lr: 0.000311  min_lr: 0.000007  loss: 0.8029 (0.7780)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7491  data: 0.3264  max mem: 41808
Epoch: [24]  [  20/1349]  eta: 0:11:56  lr: 0.000311  min_lr: 0.000007  loss: 0.8029 (0.7763)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3123  data: 0.0001  max mem: 41808
Epoch: [24]  [  30/1349]  eta: 0:10:12  lr: 0.000310  min_lr: 0.000007  loss: 0.8100 (0.7641)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
Epoch: [24]  [  40/1349]  eta: 0:09:16  lr: 0.000310  min_lr: 0.000007  loss: 0.7688 (0.7662)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3059  data: 0.0001  max mem: 41808
Epoch: [24]  [  50/1349]  eta: 0:08:42  lr: 0.000310  min_lr: 0.000007  loss: 0.7805 (0.7708)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3058  data: 0.0001  max mem: 41808
Epoch: [24]  [  60/1349]  eta: 0:08:18  lr: 0.000310  min_lr: 0.000007  loss: 0.7976 (0.7686)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3063  data: 0.0001  max mem: 41808
Epoch: [24]  [  70/1349]  eta: 0:07:59  lr: 0.000310  min_lr: 0.000007  loss: 0.8244 (0.7896)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3064  data: 0.0001  max mem: 41808
Epoch: [24]  [  80/1349]  eta: 0:07:45  lr: 0.000310  min_lr: 0.000007  loss: 0.8513 (0.7978)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3066  data: 0.0001  max mem: 41808
Epoch: [24]  [  90/1349]  eta: 0:07:33  lr: 0.000310  min_lr: 0.000007  loss: 0.8562 (0.8020)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3078  data: 0.0001  max mem: 41808
Epoch: [24]  [ 100/1349]  eta: 0:07:23  lr: 0.000310  min_lr: 0.000007  loss: 0.8718 (0.8037)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3078  data: 0.0001  max mem: 41808
Epoch: [24]  [ 110/1349]  eta: 0:07:14  lr: 0.000309  min_lr: 0.000007  loss: 0.8853 (0.8031)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
[2025-05-23 20:01:24,598] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 20:01:24,598] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 20:01:24,598] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-05-23 20:01:24,598] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [24]  [ 120/1349]  eta: 0:07:06  lr: 0.000309  min_lr: 0.000007  loss: 0.8290 (0.8041)  loss_scale: 65536.0000 (66077.6198)  weight_decay: 0.0500 (0.0500)  time: 0.3078  data: 0.0001  max mem: 41808
Epoch: [24]  [ 130/1349]  eta: 0:06:59  lr: 0.000309  min_lr: 0.000007  loss: 0.7906 (0.8033)  loss_scale: 131072.0000 (71039.0229)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
Epoch: [24]  [ 140/1349]  eta: 0:06:52  lr: 0.000309  min_lr: 0.000007  loss: 0.8045 (0.8030)  loss_scale: 131072.0000 (75296.6809)  weight_decay: 0.0500 (0.0500)  time: 0.3066  data: 0.0001  max mem: 41808
Epoch: [24]  [ 150/1349]  eta: 0:06:46  lr: 0.000309  min_lr: 0.000007  loss: 0.7893 (0.7991)  loss_scale: 131072.0000 (78990.4106)  weight_decay: 0.0500 (0.0500)  time: 0.3064  data: 0.0002  max mem: 41808
Epoch: [24]  [ 160/1349]  eta: 0:06:40  lr: 0.000309  min_lr: 0.000007  loss: 0.7818 (0.7998)  loss_scale: 131072.0000 (82225.2919)  weight_decay: 0.0500 (0.0500)  time: 0.3063  data: 0.0001  max mem: 41808
Epoch: [24]  [ 170/1349]  eta: 0:06:35  lr: 0.000309  min_lr: 0.000007  loss: 0.7959 (0.8003)  loss_scale: 131072.0000 (85081.8246)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
Epoch: [24]  [ 180/1349]  eta: 0:06:30  lr: 0.000309  min_lr: 0.000007  loss: 0.8293 (0.8003)  loss_scale: 131072.0000 (87622.7182)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0001  max mem: 41808
Epoch: [24]  [ 190/1349]  eta: 0:06:25  lr: 0.000308  min_lr: 0.000007  loss: 0.8293 (0.7998)  loss_scale: 131072.0000 (89897.5497)  weight_decay: 0.0500 (0.0500)  time: 0.3060  data: 0.0001  max mem: 41808
Epoch: [24]  [ 200/1349]  eta: 0:06:20  lr: 0.000308  min_lr: 0.000007  loss: 0.7118 (0.7959)  loss_scale: 131072.0000 (91946.0299)  weight_decay: 0.0500 (0.0500)  time: 0.3059  data: 0.0001  max mem: 41808
Epoch: [24]  [ 210/1349]  eta: 0:06:15  lr: 0.000308  min_lr: 0.000007  loss: 0.7093 (0.7972)  loss_scale: 131072.0000 (93800.3412)  weight_decay: 0.0500 (0.0500)  time: 0.3064  data: 0.0001  max mem: 41808
[2025-05-23 20:01:54,014] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 32592
[2025-05-23 20:01:54,014] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 20:01:54,014] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 32592
[2025-05-23 20:01:54,014] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 20:01:54,014] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [24]  [ 220/1349]  eta: 0:06:11  lr: 0.000308  min_lr: 0.000007  loss: 0.8625 (0.7966)  loss_scale: 131072.0000 (94004.1267)  weight_decay: 0.0500 (0.0500)  time: 0.3059  data: 0.0001  max mem: 41808
Epoch: [24]  [ 230/1349]  eta: 0:06:06  lr: 0.000308  min_lr: 0.000007  loss: 0.8530 (0.7990)  loss_scale: 65536.0000 (92771.7403)  weight_decay: 0.0500 (0.0500)  time: 0.3054  data: 0.0002  max mem: 41808
Epoch: [24]  [ 240/1349]  eta: 0:06:02  lr: 0.000308  min_lr: 0.000007  loss: 0.8530 (0.8029)  loss_scale: 65536.0000 (91641.6266)  weight_decay: 0.0500 (0.0500)  time: 0.3061  data: 0.0001  max mem: 41808
Epoch: [24]  [ 250/1349]  eta: 0:05:58  lr: 0.000308  min_lr: 0.000007  loss: 0.9199 (0.8072)  loss_scale: 65536.0000 (90601.5618)  weight_decay: 0.0500 (0.0500)  time: 0.3059  data: 0.0001  max mem: 41808
Epoch: [24]  [ 260/1349]  eta: 0:05:54  lr: 0.000308  min_lr: 0.000007  loss: 0.9199 (0.8084)  loss_scale: 65536.0000 (89641.1954)  weight_decay: 0.0500 (0.0500)  time: 0.3057  data: 0.0001  max mem: 41808
Epoch: [24]  [ 270/1349]  eta: 0:05:50  lr: 0.000307  min_lr: 0.000007  loss: 0.8942 (0.8100)  loss_scale: 65536.0000 (88751.7048)  weight_decay: 0.0500 (0.0500)  time: 0.3061  data: 0.0001  max mem: 41808
Epoch: [24]  [ 280/1349]  eta: 0:05:46  lr: 0.000307  min_lr: 0.000007  loss: 0.8287 (0.8109)  loss_scale: 65536.0000 (87925.5231)  weight_decay: 0.0500 (0.0500)  time: 0.3085  data: 0.0001  max mem: 41808
Epoch: [24]  [ 290/1349]  eta: 0:05:42  lr: 0.000307  min_lr: 0.000007  loss: 0.8276 (0.8096)  loss_scale: 65536.0000 (87156.1237)  weight_decay: 0.0500 (0.0500)  time: 0.3086  data: 0.0001  max mem: 41808
Epoch: [24]  [ 300/1349]  eta: 0:05:38  lr: 0.000307  min_lr: 0.000007  loss: 0.7095 (0.8078)  loss_scale: 65536.0000 (86437.8472)  weight_decay: 0.0500 (0.0500)  time: 0.3063  data: 0.0001  max mem: 41808
Epoch: [24]  [ 310/1349]  eta: 0:05:34  lr: 0.000307  min_lr: 0.000007  loss: 0.6951 (0.8060)  loss_scale: 65536.0000 (85765.7621)  weight_decay: 0.0500 (0.0500)  time: 0.3063  data: 0.0001  max mem: 41808
Epoch: [24]  [ 320/1349]  eta: 0:05:31  lr: 0.000307  min_lr: 0.000007  loss: 0.7749 (0.8049)  loss_scale: 65536.0000 (85135.5514)  weight_decay: 0.0500 (0.0500)  time: 0.3062  data: 0.0001  max mem: 41808
Epoch: [24]  [ 330/1349]  eta: 0:05:27  lr: 0.000307  min_lr: 0.000007  loss: 0.8365 (0.8058)  loss_scale: 65536.0000 (84543.4199)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0001  max mem: 41808
Epoch: [24]  [ 340/1349]  eta: 0:05:23  lr: 0.000307  min_lr: 0.000007  loss: 0.7687 (0.8030)  loss_scale: 65536.0000 (83986.0176)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
[2025-05-23 20:02:33,582] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 20:02:33,582] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-05-23 20:02:33,582] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 20:02:33,582] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [24]  [ 350/1349]  eta: 0:05:20  lr: 0.000306  min_lr: 0.000007  loss: 0.7561 (0.8015)  loss_scale: 65536.0000 (84580.6496)  weight_decay: 0.0500 (0.0500)  time: 0.3081  data: 0.0001  max mem: 41808
Epoch: [24]  [ 360/1349]  eta: 0:05:16  lr: 0.000306  min_lr: 0.000007  loss: 0.8043 (0.8029)  loss_scale: 131072.0000 (85868.4986)  weight_decay: 0.0500 (0.0500)  time: 0.3086  data: 0.0001  max mem: 41808
Epoch: [24]  [ 370/1349]  eta: 0:05:13  lr: 0.000306  min_lr: 0.000007  loss: 0.7977 (0.8014)  loss_scale: 131072.0000 (87086.9218)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
Epoch: [24]  [ 380/1349]  eta: 0:05:09  lr: 0.000306  min_lr: 0.000007  loss: 0.7622 (0.8008)  loss_scale: 131072.0000 (88241.3858)  weight_decay: 0.0500 (0.0500)  time: 0.3064  data: 0.0001  max mem: 41808
Epoch: [24]  [ 390/1349]  eta: 0:05:06  lr: 0.000306  min_lr: 0.000007  loss: 0.7904 (0.8007)  loss_scale: 131072.0000 (89336.7980)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
Epoch: [24]  [ 400/1349]  eta: 0:05:02  lr: 0.000306  min_lr: 0.000007  loss: 0.7916 (0.8013)  loss_scale: 131072.0000 (90377.5761)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0001  max mem: 41808
Epoch: [24]  [ 410/1349]  eta: 0:04:59  lr: 0.000306  min_lr: 0.000007  loss: 0.7916 (0.8004)  loss_scale: 131072.0000 (91367.7080)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [24]  [ 420/1349]  eta: 0:04:55  lr: 0.000306  min_lr: 0.000007  loss: 0.8216 (0.8026)  loss_scale: 131072.0000 (92310.8029)  weight_decay: 0.0500 (0.0500)  time: 0.3060  data: 0.0001  max mem: 41808
Epoch: [24]  [ 430/1349]  eta: 0:04:52  lr: 0.000305  min_lr: 0.000007  loss: 0.8009 (0.8006)  loss_scale: 131072.0000 (93210.1346)  weight_decay: 0.0500 (0.0500)  time: 0.3059  data: 0.0001  max mem: 41808
Epoch: [24]  [ 440/1349]  eta: 0:04:48  lr: 0.000305  min_lr: 0.000007  loss: 0.7599 (0.7996)  loss_scale: 131072.0000 (94068.6803)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
Epoch: [24]  [ 450/1349]  eta: 0:04:45  lr: 0.000305  min_lr: 0.000007  loss: 0.8162 (0.8002)  loss_scale: 131072.0000 (94889.1530)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [24]  [ 460/1349]  eta: 0:04:42  lr: 0.000305  min_lr: 0.000007  loss: 0.8180 (0.8005)  loss_scale: 131072.0000 (95674.0304)  weight_decay: 0.0500 (0.0500)  time: 0.3064  data: 0.0001  max mem: 41808
Epoch: [24]  [ 470/1349]  eta: 0:04:38  lr: 0.000305  min_lr: 0.000007  loss: 0.8083 (0.8003)  loss_scale: 131072.0000 (96425.5796)  weight_decay: 0.0500 (0.0500)  time: 0.3063  data: 0.0001  max mem: 41808
[2025-05-23 20:03:12,874] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 20:03:12,874] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 20:03:12,874] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 20:03:12,875] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [24]  [ 480/1349]  eta: 0:04:35  lr: 0.000305  min_lr: 0.000007  loss: 0.8214 (0.8013)  loss_scale: 131072.0000 (99325.8711)  weight_decay: 0.0500 (0.0500)  time: 0.3063  data: 0.0001  max mem: 41808
Epoch: [24]  [ 490/1349]  eta: 0:04:31  lr: 0.000305  min_lr: 0.000007  loss: 0.8634 (0.8014)  loss_scale: 262144.0000 (102641.9226)  weight_decay: 0.0500 (0.0500)  time: 0.3066  data: 0.0001  max mem: 41808
Epoch: [24]  [ 500/1349]  eta: 0:04:28  lr: 0.000305  min_lr: 0.000007  loss: 0.8573 (0.8021)  loss_scale: 262144.0000 (105825.5968)  weight_decay: 0.0500 (0.0500)  time: 0.3064  data: 0.0001  max mem: 41808
Epoch: [24]  [ 510/1349]  eta: 0:04:25  lr: 0.000304  min_lr: 0.000007  loss: 0.8020 (0.8019)  loss_scale: 262144.0000 (108884.6654)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
[2025-05-23 20:03:25,155] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 32889
[2025-05-23 20:03:25,155] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 32889
[2025-05-23 20:03:25,155] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 20:03:25,155] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 20:03:25,155] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [24]  [ 520/1349]  eta: 0:04:22  lr: 0.000304  min_lr: 0.000007  loss: 0.8020 (0.8024)  loss_scale: 262144.0000 (109813.6814)  weight_decay: 0.0500 (0.0500)  time: 0.3087  data: 0.0001  max mem: 41808
Epoch: [24]  [ 530/1349]  eta: 0:04:18  lr: 0.000304  min_lr: 0.000007  loss: 0.7756 (0.8012)  loss_scale: 131072.0000 (110214.0264)  weight_decay: 0.0500 (0.0500)  time: 0.3084  data: 0.0001  max mem: 41808
Epoch: [24]  [ 540/1349]  eta: 0:04:15  lr: 0.000304  min_lr: 0.000007  loss: 0.8122 (0.8014)  loss_scale: 131072.0000 (110599.5712)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [24]  [ 550/1349]  eta: 0:04:12  lr: 0.000304  min_lr: 0.000007  loss: 0.8190 (0.8014)  loss_scale: 131072.0000 (110971.1216)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
Epoch: [24]  [ 560/1349]  eta: 0:04:08  lr: 0.000304  min_lr: 0.000007  loss: 0.8146 (0.8014)  loss_scale: 131072.0000 (111329.4260)  weight_decay: 0.0500 (0.0500)  time: 0.3062  data: 0.0001  max mem: 41808
Epoch: [24]  [ 570/1349]  eta: 0:04:05  lr: 0.000304  min_lr: 0.000007  loss: 0.8146 (0.8023)  loss_scale: 131072.0000 (111675.1804)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
[2025-05-23 20:03:42,986] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 32947
[2025-05-23 20:03:42,986] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 20:03:42,986] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 32947
[2025-05-23 20:03:42,986] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 20:03:42,986] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [24]  [ 580/1349]  eta: 0:04:02  lr: 0.000304  min_lr: 0.000007  loss: 0.8313 (0.8028)  loss_scale: 65536.0000 (110881.0465)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
Epoch: [24]  [ 590/1349]  eta: 0:03:59  lr: 0.000303  min_lr: 0.000007  loss: 0.8099 (0.8027)  loss_scale: 65536.0000 (110113.7868)  weight_decay: 0.0500 (0.0500)  time: 0.3066  data: 0.0001  max mem: 41808
Epoch: [24]  [ 600/1349]  eta: 0:03:55  lr: 0.000303  min_lr: 0.000007  loss: 0.7583 (0.8024)  loss_scale: 65536.0000 (109372.0599)  weight_decay: 0.0500 (0.0500)  time: 0.3063  data: 0.0001  max mem: 41808
Epoch: [24]  [ 610/1349]  eta: 0:03:52  lr: 0.000303  min_lr: 0.000007  loss: 0.7973 (0.8025)  loss_scale: 65536.0000 (108654.6121)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0001  max mem: 41808
Epoch: [24]  [ 620/1349]  eta: 0:03:49  lr: 0.000303  min_lr: 0.000007  loss: 0.8879 (0.8042)  loss_scale: 65536.0000 (107960.2705)  weight_decay: 0.0500 (0.0500)  time: 0.3079  data: 0.0001  max mem: 41808
[2025-05-23 20:03:58,963] [INFO] [logging.py:96:log_dist] [Rank 0] step=33000, skipped=201, lr=[7.199026454464132e-06, 7.199026454464132e-06, 9.598701939285509e-06, 9.598701939285509e-06, 1.279826925238068e-05, 1.279826925238068e-05, 1.706435900317424e-05, 1.706435900317424e-05, 2.2752478670898984e-05, 2.2752478670898984e-05, 3.0336638227865315e-05, 3.0336638227865315e-05, 4.0448850970487086e-05, 4.0448850970487086e-05, 5.393180129398278e-05, 5.393180129398278e-05, 7.190906839197704e-05, 7.190906839197704e-05, 9.587875785596939e-05, 9.587875785596939e-05, 0.00012783834380795917, 0.00012783834380795917, 0.00017045112507727892, 0.00017045112507727892, 0.00022726816676970523, 0.00022726816676970523, 0.00030302422235960696, 0.00030302422235960696], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-05-23 20:03:58,964] [INFO] [timer.py:260:stop] epoch=0/micro_step=33000/global_step=33000, RunningAvgSamplesPerSec=208.89578113649895, CurrSamplesPerSec=214.16053695815725, MemAllocated=1.37GB, MaxMemAllocated=40.83GB
Epoch: [24]  [ 630/1349]  eta: 0:03:46  lr: 0.000303  min_lr: 0.000007  loss: 0.8753 (0.8053)  loss_scale: 65536.0000 (107287.9366)  weight_decay: 0.0500 (0.0500)  time: 0.3064  data: 0.0001  max mem: 41808
Epoch: [24]  [ 640/1349]  eta: 0:03:42  lr: 0.000303  min_lr: 0.000007  loss: 0.8430 (0.8045)  loss_scale: 65536.0000 (106636.5803)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0001  max mem: 41808
Epoch: [24]  [ 650/1349]  eta: 0:03:39  lr: 0.000303  min_lr: 0.000007  loss: 0.8037 (0.8046)  loss_scale: 65536.0000 (106005.2350)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0001  max mem: 41808
Epoch: [24]  [ 660/1349]  eta: 0:03:36  lr: 0.000303  min_lr: 0.000007  loss: 0.8644 (0.8049)  loss_scale: 65536.0000 (105392.9924)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
Epoch: [24]  [ 670/1349]  eta: 0:03:33  lr: 0.000302  min_lr: 0.000007  loss: 0.9257 (0.8056)  loss_scale: 65536.0000 (104798.9985)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
Epoch: [24]  [ 680/1349]  eta: 0:03:30  lr: 0.000302  min_lr: 0.000007  loss: 0.8380 (0.8057)  loss_scale: 65536.0000 (104222.4493)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0002  max mem: 41808
Epoch: [24]  [ 690/1349]  eta: 0:03:26  lr: 0.000302  min_lr: 0.000007  loss: 0.8360 (0.8056)  loss_scale: 65536.0000 (103662.5876)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0001  max mem: 41808
[2025-05-23 20:04:22,584] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 20:04:22,584] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 20:04:22,584] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-05-23 20:04:22,584] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [24]  [ 700/1349]  eta: 0:03:23  lr: 0.000302  min_lr: 0.000007  loss: 0.8001 (0.8059)  loss_scale: 65536.0000 (103212.1883)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
Epoch: [24]  [ 710/1349]  eta: 0:03:20  lr: 0.000302  min_lr: 0.000007  loss: 0.8563 (0.8073)  loss_scale: 131072.0000 (103604.0281)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
Epoch: [24]  [ 720/1349]  eta: 0:03:17  lr: 0.000302  min_lr: 0.000007  loss: 0.8196 (0.8066)  loss_scale: 131072.0000 (103984.9986)  weight_decay: 0.0500 (0.0500)  time: 0.3066  data: 0.0001  max mem: 41808
Epoch: [24]  [ 730/1349]  eta: 0:03:14  lr: 0.000302  min_lr: 0.000007  loss: 0.7762 (0.8065)  loss_scale: 131072.0000 (104355.5458)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
Epoch: [24]  [ 740/1349]  eta: 0:03:10  lr: 0.000302  min_lr: 0.000007  loss: 0.8448 (0.8068)  loss_scale: 131072.0000 (104716.0918)  weight_decay: 0.0500 (0.0500)  time: 0.3066  data: 0.0001  max mem: 41808
Epoch: [24]  [ 750/1349]  eta: 0:03:07  lr: 0.000301  min_lr: 0.000007  loss: 0.8740 (0.8077)  loss_scale: 131072.0000 (105067.0360)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
Epoch: [24]  [ 760/1349]  eta: 0:03:04  lr: 0.000301  min_lr: 0.000007  loss: 0.8961 (0.8079)  loss_scale: 131072.0000 (105408.7569)  weight_decay: 0.0500 (0.0500)  time: 0.3064  data: 0.0001  max mem: 41808
Epoch: [24]  [ 770/1349]  eta: 0:03:01  lr: 0.000301  min_lr: 0.000007  loss: 0.8512 (0.8087)  loss_scale: 131072.0000 (105741.6135)  weight_decay: 0.0500 (0.0500)  time: 0.3059  data: 0.0001  max mem: 41808
Epoch: [24]  [ 780/1349]  eta: 0:02:58  lr: 0.000301  min_lr: 0.000007  loss: 0.7776 (0.8072)  loss_scale: 131072.0000 (106065.9462)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
Epoch: [24]  [ 790/1349]  eta: 0:02:54  lr: 0.000301  min_lr: 0.000007  loss: 0.7687 (0.8079)  loss_scale: 131072.0000 (106382.0784)  weight_decay: 0.0500 (0.0500)  time: 0.3084  data: 0.0002  max mem: 41808
Epoch: [24]  [ 800/1349]  eta: 0:02:51  lr: 0.000301  min_lr: 0.000007  loss: 0.8278 (0.8075)  loss_scale: 131072.0000 (106690.3171)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
Epoch: [24]  [ 810/1349]  eta: 0:02:48  lr: 0.000301  min_lr: 0.000007  loss: 0.8278 (0.8085)  loss_scale: 131072.0000 (106990.9544)  weight_decay: 0.0500 (0.0500)  time: 0.3060  data: 0.0001  max mem: 41808
Epoch: [24]  [ 820/1349]  eta: 0:02:45  lr: 0.000301  min_lr: 0.000007  loss: 0.8341 (0.8082)  loss_scale: 131072.0000 (107284.2680)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
[2025-05-23 20:05:01,873] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 20:05:01,873] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 20:05:01,873] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 20:05:01,873] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [24]  [ 830/1349]  eta: 0:02:42  lr: 0.000300  min_lr: 0.000007  loss: 0.8031 (0.8077)  loss_scale: 131072.0000 (108043.7064)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
Epoch: [24]  [ 840/1349]  eta: 0:02:39  lr: 0.000300  min_lr: 0.000007  loss: 0.7893 (0.8078)  loss_scale: 262144.0000 (109876.0523)  weight_decay: 0.0500 (0.0500)  time: 0.3061  data: 0.0001  max mem: 41808
Epoch: [24]  [ 850/1349]  eta: 0:02:35  lr: 0.000300  min_lr: 0.000007  loss: 0.8045 (0.8079)  loss_scale: 262144.0000 (111665.3349)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
[2025-05-23 20:05:09,548] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 33229
[2025-05-23 20:05:09,548] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 33229
[2025-05-23 20:05:09,548] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 20:05:09,548] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 20:05:09,548] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
[2025-05-23 20:05:10,162] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 33231
[2025-05-23 20:05:10,162] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 20:05:10,162] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 33231
[2025-05-23 20:05:10,162] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 20:05:10,162] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [24]  [ 860/1349]  eta: 0:02:32  lr: 0.000300  min_lr: 0.000007  loss: 0.7759 (0.8079)  loss_scale: 262144.0000 (111738.4994)  weight_decay: 0.0500 (0.0500)  time: 0.3081  data: 0.0002  max mem: 41808
Epoch: [24]  [ 870/1349]  eta: 0:02:29  lr: 0.000300  min_lr: 0.000007  loss: 0.7736 (0.8073)  loss_scale: 65536.0000 (111208.0459)  weight_decay: 0.0500 (0.0500)  time: 0.3088  data: 0.0002  max mem: 41808
Epoch: [24]  [ 880/1349]  eta: 0:02:26  lr: 0.000300  min_lr: 0.000007  loss: 0.7757 (0.8067)  loss_scale: 65536.0000 (110689.6345)  weight_decay: 0.0500 (0.0500)  time: 0.3087  data: 0.0001  max mem: 41808
Epoch: [24]  [ 890/1349]  eta: 0:02:23  lr: 0.000300  min_lr: 0.000007  loss: 0.7637 (0.8062)  loss_scale: 65536.0000 (110182.8597)  weight_decay: 0.0500 (0.0500)  time: 0.3084  data: 0.0001  max mem: 41808
Epoch: [24]  [ 900/1349]  eta: 0:02:20  lr: 0.000300  min_lr: 0.000007  loss: 0.7637 (0.8063)  loss_scale: 65536.0000 (109687.3341)  weight_decay: 0.0500 (0.0500)  time: 0.3084  data: 0.0001  max mem: 41808
Epoch: [24]  [ 910/1349]  eta: 0:02:17  lr: 0.000299  min_lr: 0.000007  loss: 0.7974 (0.8062)  loss_scale: 65536.0000 (109202.6872)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0001  max mem: 41808
Epoch: [24]  [ 920/1349]  eta: 0:02:13  lr: 0.000299  min_lr: 0.000007  loss: 0.7855 (0.8052)  loss_scale: 65536.0000 (108728.5646)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [24]  [ 930/1349]  eta: 0:02:10  lr: 0.000299  min_lr: 0.000007  loss: 0.7634 (0.8047)  loss_scale: 65536.0000 (108264.6273)  weight_decay: 0.0500 (0.0500)  time: 0.3066  data: 0.0001  max mem: 41808
Epoch: [24]  [ 940/1349]  eta: 0:02:07  lr: 0.000299  min_lr: 0.000007  loss: 0.7617 (0.8037)  loss_scale: 65536.0000 (107810.5505)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [24]  [ 950/1349]  eta: 0:02:04  lr: 0.000299  min_lr: 0.000007  loss: 0.8122 (0.8052)  loss_scale: 65536.0000 (107366.0231)  weight_decay: 0.0500 (0.0500)  time: 0.3078  data: 0.0001  max mem: 41808
Epoch: [24]  [ 960/1349]  eta: 0:02:01  lr: 0.000299  min_lr: 0.000007  loss: 0.8400 (0.8050)  loss_scale: 65536.0000 (106930.7471)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [24]  [ 970/1349]  eta: 0:01:58  lr: 0.000299  min_lr: 0.000007  loss: 0.8093 (0.8051)  loss_scale: 65536.0000 (106504.4367)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
Epoch: [24]  [ 980/1349]  eta: 0:01:55  lr: 0.000299  min_lr: 0.000007  loss: 0.8351 (0.8045)  loss_scale: 65536.0000 (106086.8175)  weight_decay: 0.0500 (0.0500)  time: 0.3086  data: 0.0001  max mem: 41808
[2025-05-23 20:05:49,896] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 20:05:49,896] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 20:05:49,896] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-05-23 20:05:49,896] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [24]  [ 990/1349]  eta: 0:01:51  lr: 0.000298  min_lr: 0.000007  loss: 0.8387 (0.8049)  loss_scale: 65536.0000 (106140.5449)  weight_decay: 0.0500 (0.0500)  time: 0.3089  data: 0.0001  max mem: 41808
Epoch: [24]  [1000/1349]  eta: 0:01:48  lr: 0.000298  min_lr: 0.000007  loss: 0.8434 (0.8046)  loss_scale: 131072.0000 (106389.6104)  weight_decay: 0.0500 (0.0500)  time: 0.3084  data: 0.0001  max mem: 41808
Epoch: [24]  [1010/1349]  eta: 0:01:45  lr: 0.000298  min_lr: 0.000007  loss: 0.8102 (0.8043)  loss_scale: 131072.0000 (106633.7488)  weight_decay: 0.0500 (0.0500)  time: 0.3087  data: 0.0001  max mem: 41808
Epoch: [24]  [1020/1349]  eta: 0:01:42  lr: 0.000298  min_lr: 0.000007  loss: 0.8045 (0.8045)  loss_scale: 131072.0000 (106873.1048)  weight_decay: 0.0500 (0.0500)  time: 0.3091  data: 0.0001  max mem: 41808
Epoch: [24]  [1030/1349]  eta: 0:01:39  lr: 0.000298  min_lr: 0.000007  loss: 0.8039 (0.8043)  loss_scale: 131072.0000 (107107.8177)  weight_decay: 0.0500 (0.0500)  time: 0.3096  data: 0.0001  max mem: 41808
Epoch: [24]  [1040/1349]  eta: 0:01:36  lr: 0.000298  min_lr: 0.000007  loss: 0.7764 (0.8036)  loss_scale: 131072.0000 (107338.0211)  weight_decay: 0.0500 (0.0500)  time: 0.3091  data: 0.0001  max mem: 41808
Epoch: [24]  [1050/1349]  eta: 0:01:33  lr: 0.000298  min_lr: 0.000007  loss: 0.7620 (0.8036)  loss_scale: 131072.0000 (107563.8440)  weight_decay: 0.0500 (0.0500)  time: 0.3085  data: 0.0001  max mem: 41808
Epoch: [24]  [1060/1349]  eta: 0:01:30  lr: 0.000297  min_lr: 0.000007  loss: 0.8026 (0.8039)  loss_scale: 131072.0000 (107785.4100)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
Epoch: [24]  [1070/1349]  eta: 0:01:26  lr: 0.000297  min_lr: 0.000007  loss: 0.8792 (0.8046)  loss_scale: 131072.0000 (108002.8385)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0001  max mem: 41808
Epoch: [24]  [1080/1349]  eta: 0:01:23  lr: 0.000297  min_lr: 0.000007  loss: 0.8149 (0.8044)  loss_scale: 131072.0000 (108216.2442)  weight_decay: 0.0500 (0.0500)  time: 0.3083  data: 0.0002  max mem: 41808
Epoch: [24]  [1090/1349]  eta: 0:01:20  lr: 0.000297  min_lr: 0.000007  loss: 0.8149 (0.8047)  loss_scale: 131072.0000 (108425.7379)  weight_decay: 0.0500 (0.0500)  time: 0.3083  data: 0.0001  max mem: 41808
Epoch: [24]  [1100/1349]  eta: 0:01:17  lr: 0.000297  min_lr: 0.000007  loss: 0.8481 (0.8043)  loss_scale: 131072.0000 (108631.4260)  weight_decay: 0.0500 (0.0500)  time: 0.3081  data: 0.0001  max mem: 41808
Epoch: [24]  [1110/1349]  eta: 0:01:14  lr: 0.000297  min_lr: 0.000007  loss: 0.7821 (0.8040)  loss_scale: 131072.0000 (108833.4113)  weight_decay: 0.0500 (0.0500)  time: 0.3080  data: 0.0001  max mem: 41808
[2025-05-23 20:06:29,380] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 20:06:29,380] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 20:06:29,380] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 20:06:29,380] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 20:06:31,519] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 33495
[2025-05-23 20:06:31,519] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 20:06:31,519] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
[2025-05-23 20:06:31,519] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 33495
[2025-05-23 20:06:31,519] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
Epoch: [24]  [1120/1349]  eta: 0:01:11  lr: 0.000297  min_lr: 0.000007  loss: 0.8099 (0.8042)  loss_scale: 131072.0000 (109850.2623)  weight_decay: 0.0500 (0.0500)  time: 0.3064  data: 0.0001  max mem: 41808
Epoch: [24]  [1130/1349]  eta: 0:01:08  lr: 0.000297  min_lr: 0.000007  loss: 0.8265 (0.8038)  loss_scale: 131072.0000 (110037.8992)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [24]  [1140/1349]  eta: 0:01:05  lr: 0.000296  min_lr: 0.000007  loss: 0.7310 (0.8032)  loss_scale: 131072.0000 (110222.2472)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0001  max mem: 41808
Epoch: [24]  [1150/1349]  eta: 0:01:01  lr: 0.000296  min_lr: 0.000007  loss: 0.7735 (0.8026)  loss_scale: 131072.0000 (110403.3918)  weight_decay: 0.0500 (0.0500)  time: 0.3060  data: 0.0001  max mem: 41808
Epoch: [24]  [1160/1349]  eta: 0:00:58  lr: 0.000296  min_lr: 0.000007  loss: 0.8389 (0.8026)  loss_scale: 131072.0000 (110581.4160)  weight_decay: 0.0500 (0.0500)  time: 0.3057  data: 0.0001  max mem: 41808
Epoch: [24]  [1170/1349]  eta: 0:00:55  lr: 0.000296  min_lr: 0.000007  loss: 0.8613 (0.8030)  loss_scale: 131072.0000 (110756.3997)  weight_decay: 0.0500 (0.0500)  time: 0.3059  data: 0.0001  max mem: 41808
Epoch: [24]  [1180/1349]  eta: 0:00:52  lr: 0.000296  min_lr: 0.000007  loss: 0.8788 (0.8034)  loss_scale: 131072.0000 (110928.4200)  weight_decay: 0.0500 (0.0500)  time: 0.3061  data: 0.0001  max mem: 41808
Epoch: [24]  [1190/1349]  eta: 0:00:49  lr: 0.000296  min_lr: 0.000007  loss: 0.8674 (0.8038)  loss_scale: 131072.0000 (111097.5516)  weight_decay: 0.0500 (0.0500)  time: 0.3061  data: 0.0001  max mem: 41808
Epoch: [24]  [1200/1349]  eta: 0:00:46  lr: 0.000296  min_lr: 0.000007  loss: 0.8261 (0.8036)  loss_scale: 131072.0000 (111263.8668)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [24]  [1210/1349]  eta: 0:00:43  lr: 0.000296  min_lr: 0.000007  loss: 0.8147 (0.8041)  loss_scale: 131072.0000 (111427.4352)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [24]  [1220/1349]  eta: 0:00:40  lr: 0.000295  min_lr: 0.000007  loss: 0.8055 (0.8034)  loss_scale: 131072.0000 (111588.3243)  weight_decay: 0.0500 (0.0500)  time: 0.3062  data: 0.0001  max mem: 41808
Epoch: [24]  [1230/1349]  eta: 0:00:37  lr: 0.000295  min_lr: 0.000007  loss: 0.8502 (0.8037)  loss_scale: 131072.0000 (111746.5995)  weight_decay: 0.0500 (0.0500)  time: 0.3062  data: 0.0001  max mem: 41808
Epoch: [24]  [1240/1349]  eta: 0:00:33  lr: 0.000295  min_lr: 0.000007  loss: 0.8719 (0.8036)  loss_scale: 131072.0000 (111902.3239)  weight_decay: 0.0500 (0.0500)  time: 0.3081  data: 0.0001  max mem: 41808
[2025-05-23 20:07:11,123] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 20:07:11,124] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 20:07:11,124] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 20:07:11,124] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [24]  [1250/1349]  eta: 0:00:30  lr: 0.000295  min_lr: 0.000007  loss: 0.9160 (0.8044)  loss_scale: 131072.0000 (112369.8801)  weight_decay: 0.0500 (0.0500)  time: 0.3091  data: 0.0001  max mem: 41808
[2025-05-23 20:07:12,965] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 33630
[2025-05-23 20:07:12,966] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 20:07:12,966] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 33630
[2025-05-23 20:07:12,966] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 20:07:12,966] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [24]  [1260/1349]  eta: 0:00:27  lr: 0.000295  min_lr: 0.000007  loss: 0.9008 (0.8050)  loss_scale: 131072.0000 (112830.0206)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0001  max mem: 41808
Epoch: [24]  [1270/1349]  eta: 0:00:24  lr: 0.000295  min_lr: 0.000007  loss: 0.8977 (0.8052)  loss_scale: 131072.0000 (112973.5452)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
Epoch: [24]  [1280/1349]  eta: 0:00:21  lr: 0.000295  min_lr: 0.000007  loss: 0.8090 (0.8048)  loss_scale: 131072.0000 (113114.8290)  weight_decay: 0.0500 (0.0500)  time: 0.3091  data: 0.0001  max mem: 41808
Epoch: [24]  [1290/1349]  eta: 0:00:18  lr: 0.000295  min_lr: 0.000007  loss: 0.7886 (0.8049)  loss_scale: 131072.0000 (113253.9241)  weight_decay: 0.0500 (0.0500)  time: 0.3087  data: 0.0001  max mem: 41808
Epoch: [24]  [1300/1349]  eta: 0:00:15  lr: 0.000294  min_lr: 0.000007  loss: 0.7950 (0.8047)  loss_scale: 131072.0000 (113390.8809)  weight_decay: 0.0500 (0.0500)  time: 0.3084  data: 0.0001  max mem: 41808
Epoch: [24]  [1310/1349]  eta: 0:00:12  lr: 0.000294  min_lr: 0.000007  loss: 0.6982 (0.8038)  loss_scale: 131072.0000 (113525.7483)  weight_decay: 0.0500 (0.0500)  time: 0.3085  data: 0.0001  max mem: 41808
Epoch: [24]  [1320/1349]  eta: 0:00:09  lr: 0.000294  min_lr: 0.000007  loss: 0.7042 (0.8034)  loss_scale: 131072.0000 (113658.5738)  weight_decay: 0.0500 (0.0500)  time: 0.3082  data: 0.0002  max mem: 41808
Epoch: [24]  [1330/1349]  eta: 0:00:05  lr: 0.000294  min_lr: 0.000007  loss: 0.7675 (0.8035)  loss_scale: 131072.0000 (113789.4035)  weight_decay: 0.0500 (0.0500)  time: 0.3058  data: 0.0001  max mem: 41808
Epoch: [24]  [1340/1349]  eta: 0:00:02  lr: 0.000294  min_lr: 0.000007  loss: 0.7690 (0.8035)  loss_scale: 131072.0000 (113918.2819)  weight_decay: 0.0500 (0.0500)  time: 0.3026  data: 0.0001  max mem: 41808
Epoch: [24]  [1348/1349]  eta: 0:00:00  lr: 0.000294  min_lr: 0.000007  loss: 0.8247 (0.8039)  loss_scale: 131072.0000 (114020.0089)  weight_decay: 0.0500 (0.0500)  time: 0.3021  data: 0.0001  max mem: 41808
Epoch: [24] Total time: 0:06:59 (0.3110 s / it)
Averaged stats: lr: 0.000294  min_lr: 0.000007  loss: 0.8247 (0.8061)  loss_scale: 131072.0000 (114020.0089)  weight_decay: 0.0500 (0.0500)  total_time: 419.5318 (419.5283)
Val:  [  0/346]  eta: 1:27:56  loss: 3.3457 (3.3457)  acc1: 0.0000 (0.0000)  acc5: 55.4688 (55.4688)  time: 15.2504  data: 14.4481  max mem: 41808
Val:  [ 10/346]  eta: 0:12:07  loss: 0.1402 (0.6055)  acc1: 100.0000 (84.8011)  acc5: 100.0000 (95.5256)  time: 2.1640  data: 1.3742  max mem: 41808
Val:  [ 20/346]  eta: 0:08:34  loss: 0.1337 (0.4733)  acc1: 100.0000 (88.3185)  acc5: 100.0000 (97.1726)  time: 0.8931  data: 0.1312  max mem: 41808
Val:  [ 30/346]  eta: 0:06:55  loss: 0.1214 (0.4020)  acc1: 100.0000 (90.4990)  acc5: 100.0000 (98.0847)  time: 0.8481  data: 0.0980  max mem: 41808
Val:  [ 40/346]  eta: 0:05:59  loss: 0.1215 (0.4375)  acc1: 100.0000 (89.5579)  acc5: 100.0000 (97.7325)  time: 0.7533  data: 0.0003  max mem: 41808
Val:  [ 50/346]  eta: 0:05:23  loss: 0.1228 (0.3892)  acc1: 99.2188 (91.0999)  acc5: 100.0000 (98.1311)  time: 0.7504  data: 0.0003  max mem: 41808
Val:  [ 60/346]  eta: 0:04:59  loss: 0.1716 (0.3720)  acc1: 97.6562 (91.4703)  acc5: 100.0000 (98.4375)  time: 0.7892  data: 0.0300  max mem: 41808
Val:  [ 70/346]  eta: 0:04:42  loss: 0.2430 (0.4002)  acc1: 93.7500 (90.6580)  acc5: 100.0000 (97.8433)  time: 0.8488  data: 0.1090  max mem: 41808
Val:  [ 80/346]  eta: 0:04:28  loss: 0.1945 (0.3948)  acc1: 96.0938 (90.9819)  acc5: 100.0000 (97.8877)  time: 0.8827  data: 0.1489  max mem: 41808
Val:  [ 90/346]  eta: 0:04:15  loss: 0.2622 (0.4007)  acc1: 92.1875 (90.8053)  acc5: 100.0000 (97.9481)  time: 0.9068  data: 0.1367  max mem: 41808
Val:  [100/346]  eta: 0:04:03  loss: 0.2259 (0.3806)  acc1: 95.3125 (91.4681)  acc5: 100.0000 (98.1513)  time: 0.9239  data: 0.1480  max mem: 41808
Val:  [110/346]  eta: 0:03:51  loss: 0.1812 (0.3910)  acc1: 96.0938 (91.0825)  acc5: 100.0000 (98.3178)  time: 0.9099  data: 0.1537  max mem: 41808
Val:  [120/346]  eta: 0:03:40  loss: 0.2129 (0.3892)  acc1: 96.0938 (91.0511)  acc5: 100.0000 (98.4440)  time: 0.8901  data: 0.1523  max mem: 41808
Val:  [130/346]  eta: 0:03:29  loss: 0.1227 (0.3992)  acc1: 98.4375 (90.6250)  acc5: 100.0000 (98.5448)  time: 0.8857  data: 0.1438  max mem: 41808
Val:  [140/346]  eta: 0:03:18  loss: 0.2475 (0.4013)  acc1: 95.3125 (90.5474)  acc5: 100.0000 (98.6425)  time: 0.8954  data: 0.1373  max mem: 41808
Val:  [150/346]  eta: 0:03:08  loss: 0.2970 (0.4051)  acc1: 92.1875 (90.5319)  acc5: 100.0000 (98.4944)  time: 0.9050  data: 0.1500  max mem: 41808
Val:  [160/346]  eta: 0:02:58  loss: 0.2416 (0.3986)  acc1: 92.1875 (90.6687)  acc5: 100.0000 (98.5637)  time: 0.9155  data: 0.1539  max mem: 41808
Val:  [170/346]  eta: 0:02:47  loss: 0.1816 (0.3902)  acc1: 93.7500 (90.9083)  acc5: 100.0000 (98.6477)  time: 0.9114  data: 0.1560  max mem: 41808
Val:  [180/346]  eta: 0:02:38  loss: 0.2519 (0.3989)  acc1: 92.9688 (90.4912)  acc5: 100.0000 (98.7224)  time: 0.9151  data: 0.1613  max mem: 41808
Val:  [190/346]  eta: 0:02:27  loss: 0.3339 (0.3979)  acc1: 91.4062 (90.5350)  acc5: 100.0000 (98.7156)  time: 0.8953  data: 0.1574  max mem: 41808
Val:  [200/346]  eta: 0:02:17  loss: 0.4444 (0.4154)  acc1: 87.5000 (89.9759)  acc5: 100.0000 (98.6668)  time: 0.8725  data: 0.1520  max mem: 41808
Val:  [210/346]  eta: 0:02:08  loss: 0.1477 (0.4062)  acc1: 100.0000 (90.2844)  acc5: 100.0000 (98.7300)  time: 0.8803  data: 0.1508  max mem: 41808
Val:  [220/346]  eta: 0:01:58  loss: 0.1284 (0.4011)  acc1: 100.0000 (90.4801)  acc5: 100.0000 (98.7557)  time: 0.9069  data: 0.1576  max mem: 41808
Val:  [230/346]  eta: 0:01:48  loss: 0.1370 (0.3923)  acc1: 98.4375 (90.7670)  acc5: 100.0000 (98.8095)  time: 0.9035  data: 0.1503  max mem: 41808
Val:  [240/346]  eta: 0:01:39  loss: 0.2018 (0.3973)  acc1: 96.0938 (90.6671)  acc5: 100.0000 (98.8589)  time: 0.8779  data: 0.1388  max mem: 41808
Val:  [250/346]  eta: 0:01:29  loss: 0.2285 (0.3976)  acc1: 95.3125 (90.7277)  acc5: 100.0000 (98.8795)  time: 0.9015  data: 0.1487  max mem: 41808
Val:  [260/346]  eta: 0:01:20  loss: 0.1455 (0.3959)  acc1: 98.4375 (90.7537)  acc5: 100.0000 (98.8985)  time: 0.9050  data: 0.1497  max mem: 41808
Val:  [270/346]  eta: 0:01:10  loss: 0.1455 (0.3913)  acc1: 98.4375 (90.9104)  acc5: 100.0000 (98.8786)  time: 0.8958  data: 0.1451  max mem: 41808
Val:  [280/346]  eta: 0:01:01  loss: 0.1322 (0.3895)  acc1: 99.2188 (91.0059)  acc5: 100.0000 (98.8545)  time: 0.9011  data: 0.1433  max mem: 41808
Val:  [290/346]  eta: 0:00:52  loss: 0.1175 (0.3802)  acc1: 100.0000 (91.2854)  acc5: 100.0000 (98.8939)  time: 0.8965  data: 0.1472  max mem: 41808
Val:  [300/346]  eta: 0:00:42  loss: 0.1190 (0.3810)  acc1: 100.0000 (91.2869)  acc5: 100.0000 (98.9255)  time: 0.8953  data: 0.1550  max mem: 41808
Val:  [310/346]  eta: 0:00:33  loss: 0.1313 (0.3849)  acc1: 98.4375 (91.1550)  acc5: 100.0000 (98.8696)  time: 0.8877  data: 0.1503  max mem: 41808
Val:  [320/346]  eta: 0:00:24  loss: 0.1313 (0.3853)  acc1: 98.4375 (91.1361)  acc5: 100.0000 (98.9048)  time: 0.8864  data: 0.1483  max mem: 41808
Val:  [330/346]  eta: 0:00:14  loss: 0.4199 (0.4017)  acc1: 87.5000 (90.6651)  acc5: 100.0000 (98.7821)  time: 0.9073  data: 0.1563  max mem: 41808
Val:  [340/346]  eta: 0:00:05  loss: 0.5090 (0.4095)  acc1: 85.9375 (90.4234)  acc5: 100.0000 (98.8018)  time: 0.9024  data: 0.1615  max mem: 41808
Val:  [345/346]  eta: 0:00:00  loss: 0.1662 (0.4056)  acc1: 94.2529 (90.5327)  acc5: 100.0000 (98.8180)  time: 0.8923  data: 0.1716  max mem: 41808
Val: Total time: 0:05:20 (0.9252 s / it)
* Acc@1 90.550 Acc@5 98.843 loss 0.403
Accuracy of the network on the 88494 val videos: 90.5%
Max accuracy: 91.38%   Max Epoch: 20
Epoch: [25]  [   0/1349]  eta: 1:54:35  lr: 0.000294  min_lr: 0.000007  loss: 0.8207 (0.8207)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 5.0964  data: 4.3622  max mem: 41808
Epoch: [25]  [  10/1349]  eta: 0:16:45  lr: 0.000294  min_lr: 0.000007  loss: 0.8657 (0.8374)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7506  data: 0.3967  max mem: 41808
[2025-05-23 20:13:13,580] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 33745
[2025-05-23 20:13:13,581] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 20:13:13,580] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 33745
[2025-05-23 20:13:13,581] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 20:13:13,581] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [25]  [  20/1349]  eta: 0:11:56  lr: 0.000294  min_lr: 0.000007  loss: 0.8657 (0.8307)  loss_scale: 131072.0000 (127951.2381)  weight_decay: 0.0500 (0.0500)  time: 0.3116  data: 0.0001  max mem: 41808
Epoch: [25]  [  30/1349]  eta: 0:10:12  lr: 0.000293  min_lr: 0.000007  loss: 0.8362 (0.8266)  loss_scale: 65536.0000 (107817.2903)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [25]  [  40/1349]  eta: 0:09:17  lr: 0.000293  min_lr: 0.000007  loss: 0.8362 (0.8198)  loss_scale: 65536.0000 (97504.7805)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
Epoch: [25]  [  50/1349]  eta: 0:08:43  lr: 0.000293  min_lr: 0.000007  loss: 0.7545 (0.8153)  loss_scale: 65536.0000 (91236.3922)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
Epoch: [25]  [  60/1349]  eta: 0:08:18  lr: 0.000293  min_lr: 0.000007  loss: 0.8130 (0.8173)  loss_scale: 65536.0000 (87023.2131)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0001  max mem: 41808
Epoch: [25]  [  70/1349]  eta: 0:08:00  lr: 0.000293  min_lr: 0.000007  loss: 0.8301 (0.8172)  loss_scale: 65536.0000 (83996.8451)  weight_decay: 0.0500 (0.0500)  time: 0.3062  data: 0.0002  max mem: 41808
Epoch: [25]  [  80/1349]  eta: 0:07:45  lr: 0.000293  min_lr: 0.000007  loss: 0.7865 (0.8171)  loss_scale: 65536.0000 (81717.7284)  weight_decay: 0.0500 (0.0500)  time: 0.3063  data: 0.0002  max mem: 41808
Epoch: [25]  [  90/1349]  eta: 0:07:33  lr: 0.000293  min_lr: 0.000007  loss: 0.6690 (0.8039)  loss_scale: 65536.0000 (79939.5165)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0001  max mem: 41808
Epoch: [25]  [ 100/1349]  eta: 0:07:23  lr: 0.000293  min_lr: 0.000007  loss: 0.6128 (0.7925)  loss_scale: 65536.0000 (78513.4257)  weight_decay: 0.0500 (0.0500)  time: 0.3064  data: 0.0001  max mem: 41808
Epoch: [25]  [ 110/1349]  eta: 0:07:14  lr: 0.000292  min_lr: 0.000007  loss: 0.7125 (0.7955)  loss_scale: 65536.0000 (77344.2883)  weight_decay: 0.0500 (0.0500)  time: 0.3061  data: 0.0001  max mem: 41808
Epoch: [25]  [ 120/1349]  eta: 0:07:06  lr: 0.000292  min_lr: 0.000007  loss: 0.8355 (0.8002)  loss_scale: 65536.0000 (76368.3967)  weight_decay: 0.0500 (0.0500)  time: 0.3060  data: 0.0001  max mem: 41808
Epoch: [25]  [ 130/1349]  eta: 0:06:58  lr: 0.000292  min_lr: 0.000007  loss: 0.8011 (0.7984)  loss_scale: 65536.0000 (75541.4962)  weight_decay: 0.0500 (0.0500)  time: 0.3058  data: 0.0001  max mem: 41808
Epoch: [25]  [ 140/1349]  eta: 0:06:52  lr: 0.000292  min_lr: 0.000007  loss: 0.8011 (0.8018)  loss_scale: 65536.0000 (74831.8865)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
[2025-05-23 20:13:53,145] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 20:13:53,146] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-05-23 20:13:53,146] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 20:13:53,146] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [25]  [ 150/1349]  eta: 0:06:46  lr: 0.000292  min_lr: 0.000007  loss: 0.8559 (0.8040)  loss_scale: 65536.0000 (75084.2914)  weight_decay: 0.0500 (0.0500)  time: 0.3082  data: 0.0001  max mem: 41808
Epoch: [25]  [ 160/1349]  eta: 0:06:40  lr: 0.000292  min_lr: 0.000007  loss: 0.8712 (0.8027)  loss_scale: 131072.0000 (78561.7888)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [25]  [ 170/1349]  eta: 0:06:35  lr: 0.000292  min_lr: 0.000007  loss: 0.8573 (0.8060)  loss_scale: 131072.0000 (81632.5614)  weight_decay: 0.0500 (0.0500)  time: 0.3066  data: 0.0001  max mem: 41808
Epoch: [25]  [ 180/1349]  eta: 0:06:30  lr: 0.000292  min_lr: 0.000007  loss: 0.7860 (0.8003)  loss_scale: 131072.0000 (84364.0221)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
Epoch: [25]  [ 190/1349]  eta: 0:06:25  lr: 0.000291  min_lr: 0.000007  loss: 0.7554 (0.8003)  loss_scale: 131072.0000 (86809.4660)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
Epoch: [25]  [ 200/1349]  eta: 0:06:20  lr: 0.000291  min_lr: 0.000007  loss: 0.7854 (0.7969)  loss_scale: 131072.0000 (89011.5821)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
Epoch: [25]  [ 210/1349]  eta: 0:06:15  lr: 0.000291  min_lr: 0.000007  loss: 0.7425 (0.7949)  loss_scale: 131072.0000 (91004.9668)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
Epoch: [25]  [ 220/1349]  eta: 0:06:11  lr: 0.000291  min_lr: 0.000007  loss: 0.7815 (0.7952)  loss_scale: 131072.0000 (92817.9548)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
Epoch: [25]  [ 230/1349]  eta: 0:06:06  lr: 0.000291  min_lr: 0.000007  loss: 0.7965 (0.7941)  loss_scale: 131072.0000 (94473.9740)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
Epoch: [25]  [ 240/1349]  eta: 0:06:02  lr: 0.000291  min_lr: 0.000007  loss: 0.8254 (0.7960)  loss_scale: 131072.0000 (95992.5643)  weight_decay: 0.0500 (0.0500)  time: 0.3066  data: 0.0001  max mem: 41808
Epoch: [25]  [ 250/1349]  eta: 0:05:58  lr: 0.000291  min_lr: 0.000007  loss: 0.8535 (0.7956)  loss_scale: 131072.0000 (97390.1514)  weight_decay: 0.0500 (0.0500)  time: 0.3066  data: 0.0001  max mem: 41808
Epoch: [25]  [ 260/1349]  eta: 0:05:54  lr: 0.000291  min_lr: 0.000007  loss: 0.8280 (0.7963)  loss_scale: 131072.0000 (98680.6437)  weight_decay: 0.0500 (0.0500)  time: 0.3091  data: 0.0002  max mem: 41808
Epoch: [25]  [ 270/1349]  eta: 0:05:50  lr: 0.000290  min_lr: 0.000007  loss: 0.8280 (0.7978)  loss_scale: 131072.0000 (99875.8967)  weight_decay: 0.0500 (0.0500)  time: 0.3090  data: 0.0002  max mem: 41808
[2025-05-23 20:14:31,554] [INFO] [logging.py:96:log_dist] [Rank 0] step=34000, skipped=206, lr=[6.897608541349059e-06, 6.897608541349059e-06, 9.196811388465413e-06, 9.196811388465413e-06, 1.226241518462055e-05, 1.226241518462055e-05, 1.63498869128274e-05, 1.63498869128274e-05, 2.17998492171032e-05, 2.17998492171032e-05, 2.9066465622804266e-05, 2.9066465622804266e-05, 3.8755287497072357e-05, 3.8755287497072357e-05, 5.167371666276314e-05, 5.167371666276314e-05, 6.889828888368419e-05, 6.889828888368419e-05, 9.186438517824558e-05, 9.186438517824558e-05, 0.00012248584690432743, 0.00012248584690432743, 0.00016331446253910325, 0.00016331446253910325, 0.00021775261671880435, 0.00021775261671880435, 0.00029033682229173913, 0.00029033682229173913], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-05-23 20:14:31,555] [INFO] [timer.py:260:stop] epoch=0/micro_step=34000/global_step=34000, RunningAvgSamplesPerSec=209.0172963443317, CurrSamplesPerSec=213.2088265055686, MemAllocated=1.37GB, MaxMemAllocated=40.83GB
[2025-05-23 20:14:32,472] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 20:14:32,473] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 20:14:32,473] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 20:14:32,473] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 20:14:32,776] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 34003
[2025-05-23 20:14:32,776] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 34003
[2025-05-23 20:14:32,776] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 20:14:32,777] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 20:14:32,777] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [25]  [ 280/1349]  eta: 0:05:46  lr: 0.000290  min_lr: 0.000007  loss: 0.8227 (0.7988)  loss_scale: 131072.0000 (101452.5267)  weight_decay: 0.0500 (0.0500)  time: 0.3066  data: 0.0001  max mem: 41808
Epoch: [25]  [ 290/1349]  eta: 0:05:42  lr: 0.000290  min_lr: 0.000007  loss: 0.7753 (0.7971)  loss_scale: 131072.0000 (102470.3780)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
Epoch: [25]  [ 300/1349]  eta: 0:05:38  lr: 0.000290  min_lr: 0.000007  loss: 0.7695 (0.7970)  loss_scale: 131072.0000 (103420.5980)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
Epoch: [25]  [ 310/1349]  eta: 0:05:35  lr: 0.000290  min_lr: 0.000007  loss: 0.7665 (0.7966)  loss_scale: 131072.0000 (104309.7106)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
Epoch: [25]  [ 320/1349]  eta: 0:05:31  lr: 0.000290  min_lr: 0.000007  loss: 0.7556 (0.7947)  loss_scale: 131072.0000 (105143.4268)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
Epoch: [25]  [ 330/1349]  eta: 0:05:27  lr: 0.000290  min_lr: 0.000007  loss: 0.7545 (0.7953)  loss_scale: 131072.0000 (105926.7674)  weight_decay: 0.0500 (0.0500)  time: 0.3066  data: 0.0001  max mem: 41808
Epoch: [25]  [ 340/1349]  eta: 0:05:24  lr: 0.000289  min_lr: 0.000007  loss: 0.8464 (0.7945)  loss_scale: 131072.0000 (106664.1642)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0002  max mem: 41808
Epoch: [25]  [ 350/1349]  eta: 0:05:20  lr: 0.000289  min_lr: 0.000007  loss: 0.7308 (0.7928)  loss_scale: 131072.0000 (107359.5442)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0002  max mem: 41808
Epoch: [25]  [ 360/1349]  eta: 0:05:16  lr: 0.000289  min_lr: 0.000007  loss: 0.7009 (0.7912)  loss_scale: 131072.0000 (108016.3989)  weight_decay: 0.0500 (0.0500)  time: 0.3057  data: 0.0001  max mem: 41808
Epoch: [25]  [ 370/1349]  eta: 0:05:13  lr: 0.000289  min_lr: 0.000007  loss: 0.7773 (0.7925)  loss_scale: 131072.0000 (108637.8437)  weight_decay: 0.0500 (0.0500)  time: 0.3066  data: 0.0001  max mem: 41808
Epoch: [25]  [ 380/1349]  eta: 0:05:09  lr: 0.000289  min_lr: 0.000007  loss: 0.8332 (0.7945)  loss_scale: 131072.0000 (109226.6667)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [25]  [ 390/1349]  eta: 0:05:06  lr: 0.000289  min_lr: 0.000007  loss: 0.8935 (0.7964)  loss_scale: 131072.0000 (109785.3708)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
Epoch: [25]  [ 400/1349]  eta: 0:05:02  lr: 0.000289  min_lr: 0.000007  loss: 0.7995 (0.7953)  loss_scale: 131072.0000 (110316.2095)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
[2025-05-23 20:15:12,367] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 20:15:12,367] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 20:15:12,367] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 20:15:12,368] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 20:15:12,672] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 34133
[2025-05-23 20:15:12,672] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 34133
[2025-05-23 20:15:12,672] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 20:15:12,672] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 20:15:12,672] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [25]  [ 410/1349]  eta: 0:04:59  lr: 0.000289  min_lr: 0.000007  loss: 0.7995 (0.7961)  loss_scale: 131072.0000 (111140.1265)  weight_decay: 0.0500 (0.0500)  time: 0.3066  data: 0.0001  max mem: 41808
Epoch: [25]  [ 420/1349]  eta: 0:04:55  lr: 0.000288  min_lr: 0.000007  loss: 0.7677 (0.7952)  loss_scale: 131072.0000 (111613.5677)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
[2025-05-23 20:15:16,655] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 34146
[2025-05-23 20:15:16,655] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 34146
[2025-05-23 20:15:16,655] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 20:15:16,655] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 20:15:16,655] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [25]  [ 430/1349]  eta: 0:04:52  lr: 0.000288  min_lr: 0.000007  loss: 0.8350 (0.7958)  loss_scale: 65536.0000 (110544.4826)  weight_decay: 0.0500 (0.0500)  time: 0.3064  data: 0.0001  max mem: 41808
Epoch: [25]  [ 440/1349]  eta: 0:04:48  lr: 0.000288  min_lr: 0.000007  loss: 0.8350 (0.7958)  loss_scale: 65536.0000 (109523.8821)  weight_decay: 0.0500 (0.0500)  time: 0.3062  data: 0.0001  max mem: 41808
Epoch: [25]  [ 450/1349]  eta: 0:04:45  lr: 0.000288  min_lr: 0.000007  loss: 0.8345 (0.7963)  loss_scale: 65536.0000 (108548.5410)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
Epoch: [25]  [ 460/1349]  eta: 0:04:42  lr: 0.000288  min_lr: 0.000007  loss: 0.8355 (0.7962)  loss_scale: 65536.0000 (107615.5141)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
Epoch: [25]  [ 470/1349]  eta: 0:04:38  lr: 0.000288  min_lr: 0.000007  loss: 0.7943 (0.7948)  loss_scale: 65536.0000 (106722.1062)  weight_decay: 0.0500 (0.0500)  time: 0.3066  data: 0.0001  max mem: 41808
Epoch: [25]  [ 480/1349]  eta: 0:04:35  lr: 0.000288  min_lr: 0.000007  loss: 0.7913 (0.7942)  loss_scale: 65536.0000 (105865.8462)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
Epoch: [25]  [ 490/1349]  eta: 0:04:32  lr: 0.000288  min_lr: 0.000007  loss: 0.8569 (0.7955)  loss_scale: 65536.0000 (105044.4644)  weight_decay: 0.0500 (0.0500)  time: 0.3066  data: 0.0001  max mem: 41808
Epoch: [25]  [ 500/1349]  eta: 0:04:28  lr: 0.000287  min_lr: 0.000007  loss: 0.8569 (0.7939)  loss_scale: 65536.0000 (104255.8723)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
Epoch: [25]  [ 510/1349]  eta: 0:04:25  lr: 0.000287  min_lr: 0.000007  loss: 0.8207 (0.7949)  loss_scale: 65536.0000 (103498.1448)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
Epoch: [25]  [ 520/1349]  eta: 0:04:22  lr: 0.000287  min_lr: 0.000007  loss: 0.8744 (0.7951)  loss_scale: 65536.0000 (102769.5048)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
Epoch: [25]  [ 530/1349]  eta: 0:04:18  lr: 0.000287  min_lr: 0.000007  loss: 0.8499 (0.7960)  loss_scale: 65536.0000 (102068.3089)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0001  max mem: 41808
Epoch: [25]  [ 540/1349]  eta: 0:04:15  lr: 0.000287  min_lr: 0.000007  loss: 0.8211 (0.7955)  loss_scale: 65536.0000 (101393.0351)  weight_decay: 0.0500 (0.0500)  time: 0.3064  data: 0.0001  max mem: 41808
[2025-05-23 20:15:56,226] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 20:15:56,226] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-05-23 20:15:56,226] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 20:15:56,226] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [25]  [ 550/1349]  eta: 0:04:12  lr: 0.000287  min_lr: 0.000007  loss: 0.7725 (0.7954)  loss_scale: 65536.0000 (100861.2123)  weight_decay: 0.0500 (0.0500)  time: 0.3066  data: 0.0001  max mem: 41808
Epoch: [25]  [ 560/1349]  eta: 0:04:08  lr: 0.000287  min_lr: 0.000007  loss: 0.7808 (0.7957)  loss_scale: 131072.0000 (101399.7291)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [25]  [ 570/1349]  eta: 0:04:05  lr: 0.000287  min_lr: 0.000007  loss: 0.8061 (0.7966)  loss_scale: 131072.0000 (101919.3835)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
Epoch: [25]  [ 580/1349]  eta: 0:04:02  lr: 0.000286  min_lr: 0.000007  loss: 0.7866 (0.7963)  loss_scale: 131072.0000 (102421.1497)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
[2025-05-23 20:16:06,360] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 34308
[2025-05-23 20:16:06,360] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 34308
[2025-05-23 20:16:06,360] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 20:16:06,360] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 20:16:06,360] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [25]  [ 590/1349]  eta: 0:03:59  lr: 0.000286  min_lr: 0.000007  loss: 0.7866 (0.7968)  loss_scale: 131072.0000 (102018.8156)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0001  max mem: 41808
Epoch: [25]  [ 600/1349]  eta: 0:03:55  lr: 0.000286  min_lr: 0.000007  loss: 0.7898 (0.7960)  loss_scale: 65536.0000 (101411.7804)  weight_decay: 0.0500 (0.0500)  time: 0.3080  data: 0.0002  max mem: 41808
Epoch: [25]  [ 610/1349]  eta: 0:03:52  lr: 0.000286  min_lr: 0.000007  loss: 0.7718 (0.7953)  loss_scale: 65536.0000 (100824.6154)  weight_decay: 0.0500 (0.0500)  time: 0.3084  data: 0.0001  max mem: 41808
Epoch: [25]  [ 620/1349]  eta: 0:03:49  lr: 0.000286  min_lr: 0.000007  loss: 0.7759 (0.7955)  loss_scale: 65536.0000 (100256.3607)  weight_decay: 0.0500 (0.0500)  time: 0.3066  data: 0.0001  max mem: 41808
Epoch: [25]  [ 630/1349]  eta: 0:03:46  lr: 0.000286  min_lr: 0.000007  loss: 0.8357 (0.7964)  loss_scale: 65536.0000 (99706.1173)  weight_decay: 0.0500 (0.0500)  time: 0.3066  data: 0.0002  max mem: 41808
Epoch: [25]  [ 640/1349]  eta: 0:03:42  lr: 0.000286  min_lr: 0.000007  loss: 0.8468 (0.7960)  loss_scale: 65536.0000 (99173.0421)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
Epoch: [25]  [ 650/1349]  eta: 0:03:39  lr: 0.000286  min_lr: 0.000007  loss: 0.7769 (0.7961)  loss_scale: 65536.0000 (98656.3441)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [25]  [ 660/1349]  eta: 0:03:36  lr: 0.000285  min_lr: 0.000007  loss: 0.8489 (0.7975)  loss_scale: 65536.0000 (98155.2799)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [25]  [ 670/1349]  eta: 0:03:33  lr: 0.000285  min_lr: 0.000007  loss: 0.8489 (0.7977)  loss_scale: 65536.0000 (97669.1505)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [25]  [ 680/1349]  eta: 0:03:30  lr: 0.000285  min_lr: 0.000007  loss: 0.8348 (0.7983)  loss_scale: 65536.0000 (97197.2981)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
Epoch: [25]  [ 690/1349]  eta: 0:03:26  lr: 0.000285  min_lr: 0.000007  loss: 0.8348 (0.7984)  loss_scale: 65536.0000 (96739.1027)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [25]  [ 700/1349]  eta: 0:03:23  lr: 0.000285  min_lr: 0.000007  loss: 0.8551 (0.7989)  loss_scale: 65536.0000 (96293.9800)  weight_decay: 0.0500 (0.0500)  time: 0.3083  data: 0.0001  max mem: 41808
Epoch: [25]  [ 710/1349]  eta: 0:03:20  lr: 0.000285  min_lr: 0.000007  loss: 0.8327 (0.7976)  loss_scale: 65536.0000 (95861.3783)  weight_decay: 0.0500 (0.0500)  time: 0.3085  data: 0.0001  max mem: 41808
[2025-05-23 20:16:46,035] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 20:16:46,035] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-05-23 20:16:46,035] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 20:16:46,035] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [25]  [ 720/1349]  eta: 0:03:17  lr: 0.000285  min_lr: 0.000007  loss: 0.6986 (0.7969)  loss_scale: 65536.0000 (96258.8405)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [25]  [ 730/1349]  eta: 0:03:14  lr: 0.000285  min_lr: 0.000007  loss: 0.7429 (0.7969)  loss_scale: 131072.0000 (96735.0807)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
Epoch: [25]  [ 740/1349]  eta: 0:03:10  lr: 0.000284  min_lr: 0.000007  loss: 0.7913 (0.7984)  loss_scale: 131072.0000 (97198.4669)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0001  max mem: 41808
Epoch: [25]  [ 750/1349]  eta: 0:03:07  lr: 0.000284  min_lr: 0.000007  loss: 0.8314 (0.7983)  loss_scale: 131072.0000 (97649.5126)  weight_decay: 0.0500 (0.0500)  time: 0.3063  data: 0.0001  max mem: 41808
Epoch: [25]  [ 760/1349]  eta: 0:03:04  lr: 0.000284  min_lr: 0.000007  loss: 0.8039 (0.7980)  loss_scale: 131072.0000 (98088.7043)  weight_decay: 0.0500 (0.0500)  time: 0.3064  data: 0.0001  max mem: 41808
Epoch: [25]  [ 770/1349]  eta: 0:03:01  lr: 0.000284  min_lr: 0.000007  loss: 0.8163 (0.7980)  loss_scale: 131072.0000 (98516.5032)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
Epoch: [25]  [ 780/1349]  eta: 0:02:58  lr: 0.000284  min_lr: 0.000007  loss: 0.8040 (0.7978)  loss_scale: 131072.0000 (98933.3470)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0002  max mem: 41808
Epoch: [25]  [ 790/1349]  eta: 0:02:55  lr: 0.000284  min_lr: 0.000007  loss: 0.8442 (0.7987)  loss_scale: 131072.0000 (99339.6511)  weight_decay: 0.0500 (0.0500)  time: 0.3080  data: 0.0001  max mem: 41808
Epoch: [25]  [ 800/1349]  eta: 0:02:51  lr: 0.000284  min_lr: 0.000007  loss: 0.8753 (0.7992)  loss_scale: 131072.0000 (99735.8102)  weight_decay: 0.0500 (0.0500)  time: 0.3083  data: 0.0001  max mem: 41808
Epoch: [25]  [ 810/1349]  eta: 0:02:48  lr: 0.000283  min_lr: 0.000007  loss: 0.7347 (0.7987)  loss_scale: 131072.0000 (100122.1998)  weight_decay: 0.0500 (0.0500)  time: 0.3080  data: 0.0001  max mem: 41808
Epoch: [25]  [ 820/1349]  eta: 0:02:45  lr: 0.000283  min_lr: 0.000007  loss: 0.7129 (0.7984)  loss_scale: 131072.0000 (100499.1766)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0002  max mem: 41808
Epoch: [25]  [ 830/1349]  eta: 0:02:42  lr: 0.000283  min_lr: 0.000007  loss: 0.7453 (0.7986)  loss_scale: 131072.0000 (100867.0806)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0002  max mem: 41808
[2025-05-23 20:17:22,907] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 34557
[2025-05-23 20:17:22,907] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 34557
[2025-05-23 20:17:22,907] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 20:17:22,907] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 20:17:22,908] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [25]  [ 840/1349]  eta: 0:02:39  lr: 0.000283  min_lr: 0.000007  loss: 0.8340 (0.7988)  loss_scale: 131072.0000 (100524.8989)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
Epoch: [25]  [ 850/1349]  eta: 0:02:36  lr: 0.000283  min_lr: 0.000007  loss: 0.8340 (0.7986)  loss_scale: 65536.0000 (100113.7485)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
Epoch: [25]  [ 860/1349]  eta: 0:02:32  lr: 0.000283  min_lr: 0.000007  loss: 0.7862 (0.7987)  loss_scale: 65536.0000 (99712.1487)  weight_decay: 0.0500 (0.0500)  time: 0.3092  data: 0.0001  max mem: 41808
Epoch: [25]  [ 870/1349]  eta: 0:02:29  lr: 0.000283  min_lr: 0.000007  loss: 0.8150 (0.7983)  loss_scale: 65536.0000 (99319.7704)  weight_decay: 0.0500 (0.0500)  time: 0.3096  data: 0.0001  max mem: 41808
Epoch: [25]  [ 880/1349]  eta: 0:02:26  lr: 0.000283  min_lr: 0.000007  loss: 0.7829 (0.7984)  loss_scale: 65536.0000 (98936.2997)  weight_decay: 0.0500 (0.0500)  time: 0.3083  data: 0.0001  max mem: 41808
Epoch: [25]  [ 890/1349]  eta: 0:02:23  lr: 0.000282  min_lr: 0.000007  loss: 0.7829 (0.7982)  loss_scale: 65536.0000 (98561.4366)  weight_decay: 0.0500 (0.0500)  time: 0.3079  data: 0.0002  max mem: 41808
Epoch: [25]  [ 900/1349]  eta: 0:02:20  lr: 0.000282  min_lr: 0.000007  loss: 0.8184 (0.7982)  loss_scale: 65536.0000 (98194.8946)  weight_decay: 0.0500 (0.0500)  time: 0.3078  data: 0.0002  max mem: 41808
Epoch: [25]  [ 910/1349]  eta: 0:02:17  lr: 0.000282  min_lr: 0.000007  loss: 0.8286 (0.7985)  loss_scale: 65536.0000 (97836.3996)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
Epoch: [25]  [ 920/1349]  eta: 0:02:13  lr: 0.000282  min_lr: 0.000007  loss: 0.7988 (0.7977)  loss_scale: 65536.0000 (97485.6895)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0001  max mem: 41808
Epoch: [25]  [ 930/1349]  eta: 0:02:10  lr: 0.000282  min_lr: 0.000007  loss: 0.8138 (0.7987)  loss_scale: 65536.0000 (97142.5134)  weight_decay: 0.0500 (0.0500)  time: 0.3093  data: 0.0004  max mem: 41808
Epoch: [25]  [ 940/1349]  eta: 0:02:07  lr: 0.000282  min_lr: 0.000007  loss: 0.8476 (0.7986)  loss_scale: 65536.0000 (96806.6312)  weight_decay: 0.0500 (0.0500)  time: 0.3093  data: 0.0004  max mem: 41808
Epoch: [25]  [ 950/1349]  eta: 0:02:04  lr: 0.000282  min_lr: 0.000007  loss: 0.8390 (0.7991)  loss_scale: 65536.0000 (96477.8128)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [25]  [ 960/1349]  eta: 0:02:01  lr: 0.000282  min_lr: 0.000007  loss: 0.8390 (0.7988)  loss_scale: 65536.0000 (96155.8377)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
[2025-05-23 20:18:02,664] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 20:18:02,664] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-05-23 20:18:02,664] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 20:18:02,664] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [25]  [ 970/1349]  eta: 0:01:58  lr: 0.000281  min_lr: 0.000007  loss: 0.7932 (0.7991)  loss_scale: 65536.0000 (96515.4274)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [25]  [ 980/1349]  eta: 0:01:55  lr: 0.000281  min_lr: 0.000007  loss: 0.7778 (0.7986)  loss_scale: 131072.0000 (96867.6860)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
Epoch: [25]  [ 990/1349]  eta: 0:01:52  lr: 0.000281  min_lr: 0.000007  loss: 0.7778 (0.7986)  loss_scale: 131072.0000 (97212.8355)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0001  max mem: 41808
Epoch: [25]  [1000/1349]  eta: 0:01:48  lr: 0.000281  min_lr: 0.000007  loss: 0.8135 (0.7991)  loss_scale: 131072.0000 (97551.0889)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
Epoch: [25]  [1010/1349]  eta: 0:01:45  lr: 0.000281  min_lr: 0.000007  loss: 0.8074 (0.7994)  loss_scale: 131072.0000 (97882.6508)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0002  max mem: 41808
Epoch: [25]  [1020/1349]  eta: 0:01:42  lr: 0.000281  min_lr: 0.000007  loss: 0.8037 (0.7993)  loss_scale: 131072.0000 (98207.7179)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
Epoch: [25]  [1030/1349]  eta: 0:01:39  lr: 0.000281  min_lr: 0.000007  loss: 0.8021 (0.7995)  loss_scale: 131072.0000 (98526.4791)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0001  max mem: 41808
Epoch: [25]  [1040/1349]  eta: 0:01:36  lr: 0.000281  min_lr: 0.000007  loss: 0.8325 (0.7997)  loss_scale: 131072.0000 (98839.1162)  weight_decay: 0.0500 (0.0500)  time: 0.3066  data: 0.0001  max mem: 41808
Epoch: [25]  [1050/1349]  eta: 0:01:33  lr: 0.000280  min_lr: 0.000007  loss: 0.8300 (0.7999)  loss_scale: 131072.0000 (99145.8040)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
Epoch: [25]  [1060/1349]  eta: 0:01:30  lr: 0.000280  min_lr: 0.000007  loss: 0.7934 (0.7997)  loss_scale: 131072.0000 (99446.7107)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [25]  [1070/1349]  eta: 0:01:26  lr: 0.000280  min_lr: 0.000007  loss: 0.7909 (0.7992)  loss_scale: 131072.0000 (99741.9981)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
Epoch: [25]  [1080/1349]  eta: 0:01:23  lr: 0.000280  min_lr: 0.000007  loss: 0.7910 (0.7992)  loss_scale: 131072.0000 (100031.8224)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
[2025-05-23 20:18:41,957] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 20:18:41,957] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 20:18:41,957] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 20:18:41,957] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [25]  [1090/1349]  eta: 0:01:20  lr: 0.000280  min_lr: 0.000007  loss: 0.8173 (0.7995)  loss_scale: 131072.0000 (100556.6123)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
Epoch: [25]  [1100/1349]  eta: 0:01:17  lr: 0.000280  min_lr: 0.000007  loss: 0.7308 (0.7987)  loss_scale: 262144.0000 (102024.2543)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
[2025-05-23 20:18:46,250] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 34828
[2025-05-23 20:18:46,250] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 34828
[2025-05-23 20:18:46,251] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 20:18:46,251] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 20:18:46,251] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [25]  [1110/1349]  eta: 0:01:14  lr: 0.000280  min_lr: 0.000007  loss: 0.7234 (0.7988)  loss_scale: 262144.0000 (102521.6634)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0001  max mem: 41808
Epoch: [25]  [1120/1349]  eta: 0:01:11  lr: 0.000280  min_lr: 0.000007  loss: 0.8108 (0.7988)  loss_scale: 131072.0000 (102776.3497)  weight_decay: 0.0500 (0.0500)  time: 0.3066  data: 0.0001  max mem: 41808
Epoch: [25]  [1130/1349]  eta: 0:01:08  lr: 0.000279  min_lr: 0.000007  loss: 0.8278 (0.7992)  loss_scale: 131072.0000 (103026.5323)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [25]  [1140/1349]  eta: 0:01:05  lr: 0.000279  min_lr: 0.000007  loss: 0.8278 (0.7994)  loss_scale: 131072.0000 (103272.3295)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0001  max mem: 41808
Epoch: [25]  [1150/1349]  eta: 0:01:01  lr: 0.000279  min_lr: 0.000007  loss: 0.8126 (0.7996)  loss_scale: 131072.0000 (103513.8558)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [25]  [1160/1349]  eta: 0:00:58  lr: 0.000279  min_lr: 0.000007  loss: 0.7803 (0.7991)  loss_scale: 131072.0000 (103751.2214)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0001  max mem: 41808
Epoch: [25]  [1170/1349]  eta: 0:00:55  lr: 0.000279  min_lr: 0.000007  loss: 0.7350 (0.7986)  loss_scale: 131072.0000 (103984.5329)  weight_decay: 0.0500 (0.0500)  time: 0.3063  data: 0.0001  max mem: 41808
Epoch: [25]  [1180/1349]  eta: 0:00:52  lr: 0.000279  min_lr: 0.000007  loss: 0.7390 (0.7984)  loss_scale: 131072.0000 (104213.8933)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [25]  [1190/1349]  eta: 0:00:49  lr: 0.000279  min_lr: 0.000007  loss: 0.8065 (0.7983)  loss_scale: 131072.0000 (104439.4022)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [25]  [1200/1349]  eta: 0:00:46  lr: 0.000278  min_lr: 0.000007  loss: 0.8174 (0.7980)  loss_scale: 131072.0000 (104661.1557)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0001  max mem: 41808
Epoch: [25]  [1210/1349]  eta: 0:00:43  lr: 0.000278  min_lr: 0.000007  loss: 0.7938 (0.7977)  loss_scale: 131072.0000 (104879.2469)  weight_decay: 0.0500 (0.0500)  time: 0.3063  data: 0.0001  max mem: 41808
[2025-05-23 20:19:20,927] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 34941
[2025-05-23 20:19:20,927] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 34941
[2025-05-23 20:19:20,927] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 20:19:20,927] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 20:19:20,927] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [25]  [1220/1349]  eta: 0:00:40  lr: 0.000278  min_lr: 0.000007  loss: 0.7734 (0.7976)  loss_scale: 131072.0000 (104825.3956)  weight_decay: 0.0500 (0.0500)  time: 0.3056  data: 0.0001  max mem: 41808
Epoch: [25]  [1230/1349]  eta: 0:00:37  lr: 0.000278  min_lr: 0.000007  loss: 0.8097 (0.7978)  loss_scale: 65536.0000 (104506.2291)  weight_decay: 0.0500 (0.0500)  time: 0.3054  data: 0.0001  max mem: 41808
Epoch: [25]  [1240/1349]  eta: 0:00:33  lr: 0.000278  min_lr: 0.000007  loss: 0.8021 (0.7975)  loss_scale: 65536.0000 (104192.2063)  weight_decay: 0.0500 (0.0500)  time: 0.3058  data: 0.0001  max mem: 41808
Epoch: [25]  [1250/1349]  eta: 0:00:30  lr: 0.000278  min_lr: 0.000007  loss: 0.7439 (0.7970)  loss_scale: 65536.0000 (103883.2038)  weight_decay: 0.0500 (0.0500)  time: 0.3059  data: 0.0001  max mem: 41808
Epoch: [25]  [1260/1349]  eta: 0:00:27  lr: 0.000278  min_lr: 0.000007  loss: 0.8077 (0.7973)  loss_scale: 65536.0000 (103579.1023)  weight_decay: 0.0500 (0.0500)  time: 0.3081  data: 0.0001  max mem: 41808
Epoch: [25]  [1270/1349]  eta: 0:00:24  lr: 0.000278  min_lr: 0.000007  loss: 0.8012 (0.7968)  loss_scale: 65536.0000 (103279.7860)  weight_decay: 0.0500 (0.0500)  time: 0.3080  data: 0.0001  max mem: 41808
[2025-05-23 20:19:38,716] [INFO] [logging.py:96:log_dist] [Rank 0] step=35000, skipped=213, lr=[6.593656462585125e-06, 6.593656462585125e-06, 8.7915419501135e-06, 8.7915419501135e-06, 1.1722055933484666e-05, 1.1722055933484666e-05, 1.562940791131289e-05, 1.562940791131289e-05, 2.0839210548417187e-05, 2.0839210548417187e-05, 2.778561406455625e-05, 2.778561406455625e-05, 3.704748541940833e-05, 3.704748541940833e-05, 4.939664722587778e-05, 4.939664722587778e-05, 6.586219630117036e-05, 6.586219630117036e-05, 8.781626173489382e-05, 8.781626173489382e-05, 0.00011708834897985843, 0.00011708834897985843, 0.00015611779863981124, 0.00015611779863981124, 0.00020815706485308166, 0.00020815706485308166, 0.0002775427531374422, 0.0002775427531374422], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-05-23 20:19:38,717] [INFO] [timer.py:260:stop] epoch=0/micro_step=35000/global_step=35000, RunningAvgSamplesPerSec=209.14421293296147, CurrSamplesPerSec=215.1306088658328, MemAllocated=1.37GB, MaxMemAllocated=40.83GB
Epoch: [25]  [1280/1349]  eta: 0:00:21  lr: 0.000277  min_lr: 0.000007  loss: 0.7017 (0.7966)  loss_scale: 65536.0000 (102985.1429)  weight_decay: 0.0500 (0.0500)  time: 0.3057  data: 0.0001  max mem: 41808
Epoch: [25]  [1290/1349]  eta: 0:00:18  lr: 0.000277  min_lr: 0.000007  loss: 0.7381 (0.7963)  loss_scale: 65536.0000 (102695.0643)  weight_decay: 0.0500 (0.0500)  time: 0.3057  data: 0.0001  max mem: 41808
Epoch: [25]  [1300/1349]  eta: 0:00:15  lr: 0.000277  min_lr: 0.000007  loss: 0.8520 (0.7969)  loss_scale: 65536.0000 (102409.4450)  weight_decay: 0.0500 (0.0500)  time: 0.3057  data: 0.0001  max mem: 41808
Epoch: [25]  [1310/1349]  eta: 0:00:12  lr: 0.000277  min_lr: 0.000007  loss: 0.8238 (0.7963)  loss_scale: 65536.0000 (102128.1831)  weight_decay: 0.0500 (0.0500)  time: 0.3056  data: 0.0001  max mem: 41808
Epoch: [25]  [1320/1349]  eta: 0:00:09  lr: 0.000277  min_lr: 0.000007  loss: 0.8238 (0.7964)  loss_scale: 65536.0000 (101851.1794)  weight_decay: 0.0500 (0.0500)  time: 0.3054  data: 0.0001  max mem: 41808
Epoch: [25]  [1330/1349]  eta: 0:00:05  lr: 0.000277  min_lr: 0.000007  loss: 0.8516 (0.7964)  loss_scale: 65536.0000 (101578.3381)  weight_decay: 0.0500 (0.0500)  time: 0.3035  data: 0.0001  max mem: 41808
Epoch: [25]  [1340/1349]  eta: 0:00:02  lr: 0.000277  min_lr: 0.000007  loss: 0.8438 (0.7961)  loss_scale: 65536.0000 (101309.5660)  weight_decay: 0.0500 (0.0500)  time: 0.3012  data: 0.0001  max mem: 41808
[2025-05-23 20:20:00,304] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 20:20:00,305] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-05-23 20:20:00,305] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 20:20:00,305] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [25]  [1348/1349]  eta: 0:00:00  lr: 0.000277  min_lr: 0.000007  loss: 0.7593 (0.7962)  loss_scale: 65536.0000 (101291.7420)  weight_decay: 0.0500 (0.0500)  time: 0.3008  data: 0.0001  max mem: 41808
Epoch: [25] Total time: 0:06:59 (0.3108 s / it)
Averaged stats: lr: 0.000277  min_lr: 0.000007  loss: 0.7593 (0.8016)  loss_scale: 65536.0000 (101291.7420)  weight_decay: 0.0500 (0.0500)  total_time: 419.2468 (419.2416)
Val:  [  0/346]  eta: 0:59:38  loss: 2.8696 (2.8696)  acc1: 3.9062 (3.9062)  acc5: 91.4062 (91.4062)  time: 10.3422  data: 9.0987  max mem: 41808
Val:  [ 10/346]  eta: 0:12:05  loss: 0.1278 (0.5554)  acc1: 100.0000 (85.4403)  acc5: 100.0000 (98.9347)  time: 2.1584  data: 1.3555  max mem: 41808
Val:  [ 20/346]  eta: 0:08:29  loss: 0.1243 (0.4379)  acc1: 100.0000 (89.2485)  acc5: 100.0000 (99.1071)  time: 1.1233  data: 0.3523  max mem: 41808
Val:  [ 30/346]  eta: 0:06:48  loss: 0.1057 (0.3707)  acc1: 100.0000 (91.4062)  acc5: 100.0000 (99.3952)  time: 0.8155  data: 0.0619  max mem: 41808
Val:  [ 40/346]  eta: 0:05:54  loss: 0.1093 (0.4086)  acc1: 100.0000 (90.3773)  acc5: 100.0000 (99.1997)  time: 0.7341  data: 0.0003  max mem: 41808
Val:  [ 50/346]  eta: 0:05:18  loss: 0.1236 (0.3632)  acc1: 99.2188 (91.8352)  acc5: 100.0000 (99.3413)  time: 0.7378  data: 0.0003  max mem: 41808
Val:  [ 60/346]  eta: 0:04:54  loss: 0.1366 (0.3497)  acc1: 99.2188 (92.1235)  acc5: 100.0000 (99.4493)  time: 0.7685  data: 0.0292  max mem: 41808
Val:  [ 70/346]  eta: 0:04:39  loss: 0.1958 (0.3702)  acc1: 96.0938 (91.4503)  acc5: 100.0000 (99.4828)  time: 0.8554  data: 0.1025  max mem: 41808
Val:  [ 80/346]  eta: 0:04:26  loss: 0.1909 (0.3751)  acc1: 93.7500 (91.3966)  acc5: 100.0000 (99.3152)  time: 0.9097  data: 0.1500  max mem: 41808
Val:  [ 90/346]  eta: 0:04:12  loss: 0.3015 (0.3771)  acc1: 92.9688 (91.3032)  acc5: 100.0000 (99.3561)  time: 0.8993  data: 0.1460  max mem: 41808
Val:  [100/346]  eta: 0:04:00  loss: 0.1491 (0.3540)  acc1: 98.4375 (92.0947)  acc5: 100.0000 (99.4199)  time: 0.8880  data: 0.1369  max mem: 41808
Val:  [110/346]  eta: 0:03:48  loss: 0.1465 (0.3669)  acc1: 99.2188 (91.7370)  acc5: 100.0000 (99.3666)  time: 0.8816  data: 0.1428  max mem: 41808
Val:  [120/346]  eta: 0:03:37  loss: 0.1699 (0.3672)  acc1: 96.8750 (91.6322)  acc5: 100.0000 (99.4189)  time: 0.8920  data: 0.1502  max mem: 41808
Val:  [130/346]  eta: 0:03:27  loss: 0.1102 (0.3665)  acc1: 100.0000 (91.5017)  acc5: 100.0000 (99.4334)  time: 0.9099  data: 0.1450  max mem: 41808
Val:  [140/346]  eta: 0:03:16  loss: 0.2374 (0.3737)  acc1: 96.0938 (91.2456)  acc5: 100.0000 (99.4736)  time: 0.9031  data: 0.1455  max mem: 41808
Val:  [150/346]  eta: 0:03:05  loss: 0.2855 (0.3723)  acc1: 93.7500 (91.3390)  acc5: 100.0000 (99.4723)  time: 0.8737  data: 0.1418  max mem: 41808
Val:  [160/346]  eta: 0:02:56  loss: 0.1908 (0.3635)  acc1: 96.8750 (91.6198)  acc5: 100.0000 (99.4905)  time: 0.8838  data: 0.1446  max mem: 41808
Val:  [170/346]  eta: 0:02:46  loss: 0.1783 (0.3606)  acc1: 96.8750 (91.6164)  acc5: 100.0000 (99.5203)  time: 0.9152  data: 0.1553  max mem: 41808
Val:  [180/346]  eta: 0:02:36  loss: 0.1865 (0.3720)  acc1: 92.9688 (91.1343)  acc5: 100.0000 (99.4691)  time: 0.9101  data: 0.1512  max mem: 41808
Val:  [190/346]  eta: 0:02:26  loss: 0.3036 (0.3705)  acc1: 92.9688 (91.2099)  acc5: 100.0000 (99.4683)  time: 0.9173  data: 0.1448  max mem: 41808
Val:  [200/346]  eta: 0:02:17  loss: 0.3062 (0.3775)  acc1: 92.1875 (90.8971)  acc5: 100.0000 (99.4869)  time: 0.9198  data: 0.1498  max mem: 41808
Val:  [210/346]  eta: 0:02:07  loss: 0.1681 (0.3691)  acc1: 97.6562 (91.2026)  acc5: 100.0000 (99.5113)  time: 0.9180  data: 0.1607  max mem: 41808
Val:  [220/346]  eta: 0:01:58  loss: 0.1393 (0.3643)  acc1: 100.0000 (91.3709)  acc5: 100.0000 (99.5122)  time: 0.9199  data: 0.1572  max mem: 41808
Val:  [230/346]  eta: 0:01:48  loss: 0.1292 (0.3562)  acc1: 100.0000 (91.6633)  acc5: 100.0000 (99.5333)  time: 0.9051  data: 0.1521  max mem: 41808
Val:  [240/346]  eta: 0:01:39  loss: 0.1463 (0.3617)  acc1: 97.6562 (91.5683)  acc5: 100.0000 (99.5397)  time: 0.8974  data: 0.1601  max mem: 41808
Val:  [250/346]  eta: 0:01:29  loss: 0.1831 (0.3608)  acc1: 97.6562 (91.6179)  acc5: 100.0000 (99.5425)  time: 0.8975  data: 0.1599  max mem: 41808
Val:  [260/346]  eta: 0:01:20  loss: 0.1541 (0.3593)  acc1: 98.4375 (91.6637)  acc5: 100.0000 (99.5510)  time: 0.9145  data: 0.1641  max mem: 41808
Val:  [270/346]  eta: 0:01:10  loss: 0.1372 (0.3549)  acc1: 98.4375 (91.8012)  acc5: 100.0000 (99.5589)  time: 0.9146  data: 0.1693  max mem: 41808
Val:  [280/346]  eta: 0:01:01  loss: 0.1237 (0.3541)  acc1: 99.2188 (91.8511)  acc5: 100.0000 (99.5691)  time: 0.9098  data: 0.1550  max mem: 41808
Val:  [290/346]  eta: 0:00:52  loss: 0.1069 (0.3462)  acc1: 100.0000 (92.1016)  acc5: 100.0000 (99.5839)  time: 0.9229  data: 0.1545  max mem: 41808
Val:  [300/346]  eta: 0:00:42  loss: 0.1067 (0.3461)  acc1: 100.0000 (92.1615)  acc5: 100.0000 (99.5743)  time: 0.9084  data: 0.1550  max mem: 41808
Val:  [310/346]  eta: 0:00:33  loss: 0.1341 (0.3489)  acc1: 99.2188 (92.0644)  acc5: 100.0000 (99.5453)  time: 0.8942  data: 0.1453  max mem: 41808
Val:  [320/346]  eta: 0:00:24  loss: 0.1341 (0.3502)  acc1: 99.2188 (92.0147)  acc5: 100.0000 (99.5595)  time: 0.8780  data: 0.1443  max mem: 41808
Val:  [330/346]  eta: 0:00:14  loss: 0.3088 (0.3643)  acc1: 87.5000 (91.6116)  acc5: 100.0000 (99.4147)  time: 0.8715  data: 0.1490  max mem: 41808
Val:  [340/346]  eta: 0:00:05  loss: 0.4652 (0.3733)  acc1: 87.5000 (91.3604)  acc5: 100.0000 (99.3860)  time: 0.9002  data: 0.1616  max mem: 41808
Val:  [345/346]  eta: 0:00:00  loss: 0.1809 (0.3700)  acc1: 95.4023 (91.4661)  acc5: 100.0000 (99.3943)  time: 0.9006  data: 0.1725  max mem: 41808
Val: Total time: 0:05:20 (0.9261 s / it)
* Acc@1 91.505 Acc@5 99.404 loss 0.368
Accuracy of the network on the 88494 val videos: 91.5%
[2025-05-23 20:25:21,932] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-05-23 20:25:21,936] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2025-05-23 20:25:21,936] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /project/mmendoscope/Natural_Comparison/Cholec75/unified_base_st_settingE_videomae-st_Cholec80_0.0005_0.75_online_key_frame_frame16_Fixed_Stride_4/checkpoint-best/mp_rank_00_model_states.pt
[2025-05-23 20:25:21,936] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /project/mmendoscope/Natural_Comparison/Cholec75/unified_base_st_settingE_videomae-st_Cholec80_0.0005_0.75_online_key_frame_frame16_Fixed_Stride_4/checkpoint-best/mp_rank_00_model_states.pt...
[2025-05-23 20:25:26,028] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /project/mmendoscope/Natural_Comparison/Cholec75/unified_base_st_settingE_videomae-st_Cholec80_0.0005_0.75_online_key_frame_frame16_Fixed_Stride_4/checkpoint-best/mp_rank_00_model_states.pt.
[2025-05-23 20:25:26,028] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 91.50%   Max Epoch: 25
Epoch: [26]  [   0/1349]  eta: 1:35:53  lr: 0.000277  min_lr: 0.000007  loss: 0.5195 (0.5195)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 4.2649  data: 3.9290  max mem: 41808
Epoch: [26]  [  10/1349]  eta: 0:16:07  lr: 0.000276  min_lr: 0.000007  loss: 0.7345 (0.7231)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7225  data: 0.3982  max mem: 41808
Epoch: [26]  [  20/1349]  eta: 0:11:40  lr: 0.000276  min_lr: 0.000007  loss: 0.7626 (0.7506)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3399  data: 0.0226  max mem: 41808
Epoch: [26]  [  30/1349]  eta: 0:10:03  lr: 0.000276  min_lr: 0.000007  loss: 0.8194 (0.7906)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3118  data: 0.0001  max mem: 41808
Epoch: [26]  [  40/1349]  eta: 0:09:11  lr: 0.000276  min_lr: 0.000007  loss: 0.8636 (0.7948)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3104  data: 0.0002  max mem: 41808
Epoch: [26]  [  50/1349]  eta: 0:08:38  lr: 0.000276  min_lr: 0.000007  loss: 0.8554 (0.8024)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3080  data: 0.0001  max mem: 41808
Epoch: [26]  [  60/1349]  eta: 0:08:14  lr: 0.000276  min_lr: 0.000007  loss: 0.8344 (0.8054)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [26]  [  70/1349]  eta: 0:07:57  lr: 0.000276  min_lr: 0.000007  loss: 0.8305 (0.8112)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0002  max mem: 41808
Epoch: [26]  [  80/1349]  eta: 0:07:43  lr: 0.000276  min_lr: 0.000007  loss: 0.8283 (0.8051)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3079  data: 0.0002  max mem: 41808
Epoch: [26]  [  90/1349]  eta: 0:07:31  lr: 0.000275  min_lr: 0.000007  loss: 0.8076 (0.8056)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0002  max mem: 41808
Epoch: [26]  [ 100/1349]  eta: 0:07:21  lr: 0.000275  min_lr: 0.000007  loss: 0.8076 (0.8058)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
Epoch: [26]  [ 110/1349]  eta: 0:07:13  lr: 0.000275  min_lr: 0.000007  loss: 0.8297 (0.8054)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0001  max mem: 41808
Epoch: [26]  [ 120/1349]  eta: 0:07:05  lr: 0.000275  min_lr: 0.000007  loss: 0.8182 (0.7991)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
[2025-05-23 20:26:09,144] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 20:26:09,145] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 20:26:09,145] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 20:26:09,145] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [26]  [ 130/1349]  eta: 0:06:58  lr: 0.000275  min_lr: 0.000007  loss: 0.7677 (0.7939)  loss_scale: 131072.0000 (138075.8473)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
[2025-05-23 20:26:12,517] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 35209
[2025-05-23 20:26:12,517] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 35209
[2025-05-23 20:26:12,517] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 20:26:12,517] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 20:26:12,517] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [26]  [ 140/1349]  eta: 0:06:51  lr: 0.000275  min_lr: 0.000007  loss: 0.7848 (0.7951)  loss_scale: 262144.0000 (141297.4752)  weight_decay: 0.0500 (0.0500)  time: 0.3063  data: 0.0001  max mem: 41808
Epoch: [26]  [ 150/1349]  eta: 0:06:45  lr: 0.000275  min_lr: 0.000007  loss: 0.8182 (0.7992)  loss_scale: 131072.0000 (140620.2914)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
Epoch: [26]  [ 160/1349]  eta: 0:06:39  lr: 0.000275  min_lr: 0.000007  loss: 0.8575 (0.8011)  loss_scale: 131072.0000 (140027.2298)  weight_decay: 0.0500 (0.0500)  time: 0.3079  data: 0.0001  max mem: 41808
Epoch: [26]  [ 170/1349]  eta: 0:06:34  lr: 0.000274  min_lr: 0.000007  loss: 0.7917 (0.7993)  loss_scale: 131072.0000 (139503.5322)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [26]  [ 180/1349]  eta: 0:06:29  lr: 0.000274  min_lr: 0.000007  loss: 0.7673 (0.7942)  loss_scale: 131072.0000 (139037.7017)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0001  max mem: 41808
Epoch: [26]  [ 190/1349]  eta: 0:06:24  lr: 0.000274  min_lr: 0.000007  loss: 0.8067 (0.7964)  loss_scale: 131072.0000 (138620.6492)  weight_decay: 0.0500 (0.0500)  time: 0.3062  data: 0.0001  max mem: 41808
Epoch: [26]  [ 200/1349]  eta: 0:06:19  lr: 0.000274  min_lr: 0.000007  loss: 0.8903 (0.7993)  loss_scale: 131072.0000 (138245.0945)  weight_decay: 0.0500 (0.0500)  time: 0.3063  data: 0.0001  max mem: 41808
Epoch: [26]  [ 210/1349]  eta: 0:06:15  lr: 0.000274  min_lr: 0.000007  loss: 0.8483 (0.7983)  loss_scale: 131072.0000 (137905.1374)  weight_decay: 0.0500 (0.0500)  time: 0.3062  data: 0.0001  max mem: 41808
Epoch: [26]  [ 220/1349]  eta: 0:06:10  lr: 0.000274  min_lr: 0.000007  loss: 0.7541 (0.7956)  loss_scale: 131072.0000 (137595.9457)  weight_decay: 0.0500 (0.0500)  time: 0.3062  data: 0.0001  max mem: 41808
Epoch: [26]  [ 230/1349]  eta: 0:06:06  lr: 0.000274  min_lr: 0.000007  loss: 0.8523 (0.7997)  loss_scale: 131072.0000 (137313.5238)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0001  max mem: 41808
Epoch: [26]  [ 240/1349]  eta: 0:06:01  lr: 0.000273  min_lr: 0.000006  loss: 0.8792 (0.8019)  loss_scale: 131072.0000 (137054.5394)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0001  max mem: 41808
Epoch: [26]  [ 250/1349]  eta: 0:05:58  lr: 0.000273  min_lr: 0.000006  loss: 0.8719 (0.8034)  loss_scale: 131072.0000 (136816.1912)  weight_decay: 0.0500 (0.0500)  time: 0.3094  data: 0.0001  max mem: 41808
Epoch: [26]  [ 260/1349]  eta: 0:05:54  lr: 0.000273  min_lr: 0.000006  loss: 0.8719 (0.8052)  loss_scale: 131072.0000 (136596.1073)  weight_decay: 0.0500 (0.0500)  time: 0.3095  data: 0.0001  max mem: 41808
[2025-05-23 20:26:52,144] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 20:26:52,144] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 20:26:52,144] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 20:26:52,144] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [26]  [ 270/1349]  eta: 0:05:50  lr: 0.000273  min_lr: 0.000006  loss: 0.8235 (0.8028)  loss_scale: 131072.0000 (139777.8893)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
[2025-05-23 20:26:56,129] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 35351
[2025-05-23 20:26:56,129] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 35351
[2025-05-23 20:26:56,129] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 20:26:56,129] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 20:26:56,129] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [26]  [ 280/1349]  eta: 0:05:46  lr: 0.000273  min_lr: 0.000006  loss: 0.7927 (0.8001)  loss_scale: 262144.0000 (142266.7616)  weight_decay: 0.0500 (0.0500)  time: 0.3063  data: 0.0001  max mem: 41808
Epoch: [26]  [ 290/1349]  eta: 0:05:42  lr: 0.000273  min_lr: 0.000006  loss: 0.7240 (0.7982)  loss_scale: 131072.0000 (141882.0619)  weight_decay: 0.0500 (0.0500)  time: 0.3057  data: 0.0001  max mem: 41808
Epoch: [26]  [ 300/1349]  eta: 0:05:38  lr: 0.000273  min_lr: 0.000006  loss: 0.7676 (0.7987)  loss_scale: 131072.0000 (141522.9236)  weight_decay: 0.0500 (0.0500)  time: 0.3059  data: 0.0001  max mem: 41808
Epoch: [26]  [ 310/1349]  eta: 0:05:34  lr: 0.000273  min_lr: 0.000006  loss: 0.8607 (0.8002)  loss_scale: 131072.0000 (141186.8810)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0002  max mem: 41808
Epoch: [26]  [ 320/1349]  eta: 0:05:30  lr: 0.000272  min_lr: 0.000006  loss: 0.8006 (0.7971)  loss_scale: 131072.0000 (140871.7757)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0002  max mem: 41808
Epoch: [26]  [ 330/1349]  eta: 0:05:27  lr: 0.000272  min_lr: 0.000006  loss: 0.7435 (0.7976)  loss_scale: 131072.0000 (140575.7100)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0002  max mem: 41808
Epoch: [26]  [ 340/1349]  eta: 0:05:23  lr: 0.000272  min_lr: 0.000006  loss: 0.7855 (0.7972)  loss_scale: 131072.0000 (140297.0088)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0002  max mem: 41808
Epoch: [26]  [ 350/1349]  eta: 0:05:20  lr: 0.000272  min_lr: 0.000006  loss: 0.8008 (0.7979)  loss_scale: 131072.0000 (140034.1880)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [26]  [ 360/1349]  eta: 0:05:16  lr: 0.000272  min_lr: 0.000006  loss: 0.8138 (0.7979)  loss_scale: 131072.0000 (139785.9280)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0002  max mem: 41808
Epoch: [26]  [ 370/1349]  eta: 0:05:12  lr: 0.000272  min_lr: 0.000006  loss: 0.8139 (0.7992)  loss_scale: 131072.0000 (139551.0512)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
Epoch: [26]  [ 380/1349]  eta: 0:05:09  lr: 0.000272  min_lr: 0.000006  loss: 0.8365 (0.7996)  loss_scale: 131072.0000 (139328.5039)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
Epoch: [26]  [ 390/1349]  eta: 0:05:05  lr: 0.000272  min_lr: 0.000006  loss: 0.7643 (0.7990)  loss_scale: 131072.0000 (139117.3402)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
Epoch: [26]  [ 400/1349]  eta: 0:05:02  lr: 0.000271  min_lr: 0.000006  loss: 0.8463 (0.8007)  loss_scale: 131072.0000 (138916.7082)  weight_decay: 0.0500 (0.0500)  time: 0.3064  data: 0.0001  max mem: 41808
[2025-05-23 20:27:35,702] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 20:27:35,702] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 20:27:35,702] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 20:27:35,702] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 20:27:36,007] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 35481
[2025-05-23 20:27:36,007] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 20:27:36,007] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 35481
[2025-05-23 20:27:36,007] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 20:27:36,007] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [26]  [ 410/1349]  eta: 0:04:58  lr: 0.000271  min_lr: 0.000006  loss: 0.8862 (0.8017)  loss_scale: 131072.0000 (139044.7494)  weight_decay: 0.0500 (0.0500)  time: 0.3057  data: 0.0001  max mem: 41808
Epoch: [26]  [ 420/1349]  eta: 0:04:55  lr: 0.000271  min_lr: 0.000006  loss: 0.8977 (0.8039)  loss_scale: 131072.0000 (138855.3729)  weight_decay: 0.0500 (0.0500)  time: 0.3053  data: 0.0001  max mem: 41808
Epoch: [26]  [ 430/1349]  eta: 0:04:52  lr: 0.000271  min_lr: 0.000006  loss: 0.8809 (0.8026)  loss_scale: 131072.0000 (138674.7842)  weight_decay: 0.0500 (0.0500)  time: 0.3058  data: 0.0001  max mem: 41808
Epoch: [26]  [ 440/1349]  eta: 0:04:48  lr: 0.000271  min_lr: 0.000006  loss: 0.7656 (0.8025)  loss_scale: 131072.0000 (138502.3855)  weight_decay: 0.0500 (0.0500)  time: 0.3064  data: 0.0001  max mem: 41808
Epoch: [26]  [ 450/1349]  eta: 0:04:45  lr: 0.000271  min_lr: 0.000006  loss: 0.7923 (0.8029)  loss_scale: 131072.0000 (138337.6319)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
Epoch: [26]  [ 460/1349]  eta: 0:04:41  lr: 0.000271  min_lr: 0.000006  loss: 0.8055 (0.8030)  loss_scale: 131072.0000 (138180.0260)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [26]  [ 470/1349]  eta: 0:04:38  lr: 0.000271  min_lr: 0.000006  loss: 0.8055 (0.8021)  loss_scale: 131072.0000 (138029.1125)  weight_decay: 0.0500 (0.0500)  time: 0.3063  data: 0.0001  max mem: 41808
Epoch: [26]  [ 480/1349]  eta: 0:04:35  lr: 0.000270  min_lr: 0.000006  loss: 0.7982 (0.8019)  loss_scale: 131072.0000 (137884.4740)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
Epoch: [26]  [ 490/1349]  eta: 0:04:31  lr: 0.000270  min_lr: 0.000006  loss: 0.8341 (0.8015)  loss_scale: 131072.0000 (137745.7271)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0001  max mem: 41808
Epoch: [26]  [ 500/1349]  eta: 0:04:28  lr: 0.000270  min_lr: 0.000006  loss: 0.8341 (0.8013)  loss_scale: 131072.0000 (137612.5190)  weight_decay: 0.0500 (0.0500)  time: 0.3058  data: 0.0001  max mem: 41808
Epoch: [26]  [ 510/1349]  eta: 0:04:25  lr: 0.000270  min_lr: 0.000006  loss: 0.8602 (0.8013)  loss_scale: 131072.0000 (137484.5245)  weight_decay: 0.0500 (0.0500)  time: 0.3059  data: 0.0001  max mem: 41808
Epoch: [26]  [ 520/1349]  eta: 0:04:21  lr: 0.000270  min_lr: 0.000006  loss: 0.8390 (0.8010)  loss_scale: 131072.0000 (137361.4434)  weight_decay: 0.0500 (0.0500)  time: 0.3057  data: 0.0001  max mem: 41808
Epoch: [26]  [ 530/1349]  eta: 0:04:18  lr: 0.000270  min_lr: 0.000006  loss: 0.8229 (0.8003)  loss_scale: 131072.0000 (137242.9981)  weight_decay: 0.0500 (0.0500)  time: 0.3057  data: 0.0001  max mem: 41808
[2025-05-23 20:28:15,523] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 20:28:15,523] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 20:28:15,523] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 20:28:15,524] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [26]  [ 540/1349]  eta: 0:04:15  lr: 0.000270  min_lr: 0.000006  loss: 0.8160 (0.8005)  loss_scale: 131072.0000 (138340.3179)  weight_decay: 0.0500 (0.0500)  time: 0.3057  data: 0.0001  max mem: 41808
Epoch: [26]  [ 550/1349]  eta: 0:04:11  lr: 0.000270  min_lr: 0.000006  loss: 0.8368 (0.8011)  loss_scale: 262144.0000 (140587.2087)  weight_decay: 0.0500 (0.0500)  time: 0.3059  data: 0.0001  max mem: 41808
[2025-05-23 20:28:20,418] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 35626
[2025-05-23 20:28:20,418] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 20:28:20,418] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 35626
[2025-05-23 20:28:20,418] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 20:28:20,418] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [26]  [ 560/1349]  eta: 0:04:08  lr: 0.000269  min_lr: 0.000006  loss: 0.8450 (0.8026)  loss_scale: 262144.0000 (140651.2371)  weight_decay: 0.0500 (0.0500)  time: 0.3055  data: 0.0001  max mem: 41808
Epoch: [26]  [ 570/1349]  eta: 0:04:05  lr: 0.000269  min_lr: 0.000006  loss: 0.8450 (0.8034)  loss_scale: 131072.0000 (140483.4746)  weight_decay: 0.0500 (0.0500)  time: 0.3054  data: 0.0001  max mem: 41808
[2025-05-23 20:28:28,052] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 35651
[2025-05-23 20:28:28,052] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 35651
[2025-05-23 20:28:28,052] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 20:28:28,052] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 20:28:28,052] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [26]  [ 580/1349]  eta: 0:04:02  lr: 0.000269  min_lr: 0.000006  loss: 0.8666 (0.8034)  loss_scale: 131072.0000 (139870.2926)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
Epoch: [26]  [ 590/1349]  eta: 0:03:58  lr: 0.000269  min_lr: 0.000006  loss: 0.8540 (0.8035)  loss_scale: 65536.0000 (138612.5212)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [26]  [ 600/1349]  eta: 0:03:55  lr: 0.000269  min_lr: 0.000006  loss: 0.8134 (0.8029)  loss_scale: 65536.0000 (137396.6057)  weight_decay: 0.0500 (0.0500)  time: 0.3064  data: 0.0001  max mem: 41808
Epoch: [26]  [ 610/1349]  eta: 0:03:52  lr: 0.000269  min_lr: 0.000006  loss: 0.7885 (0.8024)  loss_scale: 65536.0000 (136220.4910)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0001  max mem: 41808
Epoch: [26]  [ 620/1349]  eta: 0:03:49  lr: 0.000269  min_lr: 0.000006  loss: 0.8236 (0.8025)  loss_scale: 65536.0000 (135082.2544)  weight_decay: 0.0500 (0.0500)  time: 0.3061  data: 0.0001  max mem: 41808
Epoch: [26]  [ 630/1349]  eta: 0:03:45  lr: 0.000268  min_lr: 0.000006  loss: 0.8639 (0.8021)  loss_scale: 65536.0000 (133980.0951)  weight_decay: 0.0500 (0.0500)  time: 0.3062  data: 0.0001  max mem: 41808
Epoch: [26]  [ 640/1349]  eta: 0:03:42  lr: 0.000268  min_lr: 0.000006  loss: 0.8780 (0.8027)  loss_scale: 65536.0000 (132912.3245)  weight_decay: 0.0500 (0.0500)  time: 0.3061  data: 0.0001  max mem: 41808
Epoch: [26]  [ 650/1349]  eta: 0:03:39  lr: 0.000268  min_lr: 0.000006  loss: 0.8780 (0.8037)  loss_scale: 65536.0000 (131877.3579)  weight_decay: 0.0500 (0.0500)  time: 0.3061  data: 0.0001  max mem: 41808
Epoch: [26]  [ 660/1349]  eta: 0:03:36  lr: 0.000268  min_lr: 0.000006  loss: 0.8535 (0.8035)  loss_scale: 65536.0000 (130873.7065)  weight_decay: 0.0500 (0.0500)  time: 0.3060  data: 0.0001  max mem: 41808
Epoch: [26]  [ 670/1349]  eta: 0:03:32  lr: 0.000268  min_lr: 0.000006  loss: 0.8136 (0.8036)  loss_scale: 65536.0000 (129899.9702)  weight_decay: 0.0500 (0.0500)  time: 0.3056  data: 0.0001  max mem: 41808
Epoch: [26]  [ 680/1349]  eta: 0:03:29  lr: 0.000268  min_lr: 0.000006  loss: 0.7769 (0.8027)  loss_scale: 65536.0000 (128954.8311)  weight_decay: 0.0500 (0.0500)  time: 0.3057  data: 0.0001  max mem: 41808
Epoch: [26]  [ 690/1349]  eta: 0:03:26  lr: 0.000268  min_lr: 0.000006  loss: 0.7697 (0.8023)  loss_scale: 65536.0000 (128037.0478)  weight_decay: 0.0500 (0.0500)  time: 0.3059  data: 0.0001  max mem: 41808
Epoch: [26]  [ 700/1349]  eta: 0:03:23  lr: 0.000268  min_lr: 0.000006  loss: 0.8685 (0.8033)  loss_scale: 65536.0000 (127145.4494)  weight_decay: 0.0500 (0.0500)  time: 0.3060  data: 0.0001  max mem: 41808
[2025-05-23 20:29:07,573] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 20:29:07,573] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-05-23 20:29:07,573] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 20:29:07,573] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [26]  [ 710/1349]  eta: 0:03:20  lr: 0.000267  min_lr: 0.000006  loss: 0.8285 (0.8029)  loss_scale: 65536.0000 (126739.8031)  weight_decay: 0.0500 (0.0500)  time: 0.3057  data: 0.0001  max mem: 41808
Epoch: [26]  [ 720/1349]  eta: 0:03:16  lr: 0.000267  min_lr: 0.000006  loss: 0.7990 (0.8032)  loss_scale: 131072.0000 (126799.8890)  weight_decay: 0.0500 (0.0500)  time: 0.3054  data: 0.0001  max mem: 41808
Epoch: [26]  [ 730/1349]  eta: 0:03:13  lr: 0.000267  min_lr: 0.000006  loss: 0.8132 (0.8036)  loss_scale: 131072.0000 (126858.3311)  weight_decay: 0.0500 (0.0500)  time: 0.3056  data: 0.0001  max mem: 41808
Epoch: [26]  [ 740/1349]  eta: 0:03:10  lr: 0.000267  min_lr: 0.000006  loss: 0.7808 (0.8026)  loss_scale: 131072.0000 (126915.1957)  weight_decay: 0.0500 (0.0500)  time: 0.3057  data: 0.0001  max mem: 41808
Epoch: [26]  [ 750/1349]  eta: 0:03:07  lr: 0.000267  min_lr: 0.000006  loss: 0.7432 (0.8022)  loss_scale: 131072.0000 (126970.5459)  weight_decay: 0.0500 (0.0500)  time: 0.3055  data: 0.0001  max mem: 41808
Epoch: [26]  [ 760/1349]  eta: 0:03:04  lr: 0.000267  min_lr: 0.000006  loss: 0.8458 (0.8029)  loss_scale: 131072.0000 (127024.4415)  weight_decay: 0.0500 (0.0500)  time: 0.3055  data: 0.0001  max mem: 41808
Epoch: [26]  [ 770/1349]  eta: 0:03:00  lr: 0.000267  min_lr: 0.000006  loss: 0.8458 (0.8034)  loss_scale: 131072.0000 (127076.9390)  weight_decay: 0.0500 (0.0500)  time: 0.3056  data: 0.0001  max mem: 41808
Epoch: [26]  [ 780/1349]  eta: 0:02:57  lr: 0.000267  min_lr: 0.000006  loss: 0.8409 (0.8033)  loss_scale: 131072.0000 (127128.0922)  weight_decay: 0.0500 (0.0500)  time: 0.3055  data: 0.0001  max mem: 41808
Epoch: [26]  [ 790/1349]  eta: 0:02:54  lr: 0.000266  min_lr: 0.000006  loss: 0.8136 (0.8034)  loss_scale: 131072.0000 (127177.9520)  weight_decay: 0.0500 (0.0500)  time: 0.3082  data: 0.0001  max mem: 41808
Epoch: [26]  [ 800/1349]  eta: 0:02:51  lr: 0.000266  min_lr: 0.000006  loss: 0.7971 (0.8029)  loss_scale: 131072.0000 (127226.5668)  weight_decay: 0.0500 (0.0500)  time: 0.3083  data: 0.0001  max mem: 41808
Epoch: [26]  [ 810/1349]  eta: 0:02:48  lr: 0.000266  min_lr: 0.000006  loss: 0.7826 (0.8029)  loss_scale: 131072.0000 (127273.9827)  weight_decay: 0.0500 (0.0500)  time: 0.3062  data: 0.0001  max mem: 41808
Epoch: [26]  [ 820/1349]  eta: 0:02:45  lr: 0.000266  min_lr: 0.000006  loss: 0.7923 (0.8028)  loss_scale: 131072.0000 (127320.2436)  weight_decay: 0.0500 (0.0500)  time: 0.3066  data: 0.0001  max mem: 41808
Epoch: [26]  [ 830/1349]  eta: 0:02:42  lr: 0.000266  min_lr: 0.000006  loss: 0.8112 (0.8028)  loss_scale: 131072.0000 (127365.3911)  weight_decay: 0.0500 (0.0500)  time: 0.3066  data: 0.0002  max mem: 41808
[2025-05-23 20:29:46,788] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 20:29:46,788] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 20:29:46,788] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 20:29:46,788] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [26]  [ 840/1349]  eta: 0:02:38  lr: 0.000266  min_lr: 0.000006  loss: 0.8312 (0.8023)  loss_scale: 131072.0000 (128500.4328)  weight_decay: 0.0500 (0.0500)  time: 0.3064  data: 0.0001  max mem: 41808
[2025-05-23 20:29:50,151] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 35919
[2025-05-23 20:29:50,151] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 35919
[2025-05-23 20:29:50,151] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 20:29:50,151] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 20:29:50,151] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [26]  [ 850/1349]  eta: 0:02:35  lr: 0.000266  min_lr: 0.000006  loss: 0.8007 (0.8018)  loss_scale: 262144.0000 (129146.7356)  weight_decay: 0.0500 (0.0500)  time: 0.3057  data: 0.0001  max mem: 41808
Epoch: [26]  [ 860/1349]  eta: 0:02:32  lr: 0.000266  min_lr: 0.000006  loss: 0.7973 (0.8018)  loss_scale: 131072.0000 (129169.0964)  weight_decay: 0.0500 (0.0500)  time: 0.3055  data: 0.0001  max mem: 41808
Epoch: [26]  [ 870/1349]  eta: 0:02:29  lr: 0.000265  min_lr: 0.000006  loss: 0.7973 (0.8017)  loss_scale: 131072.0000 (129190.9437)  weight_decay: 0.0500 (0.0500)  time: 0.3062  data: 0.0001  max mem: 41808
Epoch: [26]  [ 880/1349]  eta: 0:02:26  lr: 0.000265  min_lr: 0.000006  loss: 0.8499 (0.8019)  loss_scale: 131072.0000 (129212.2951)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
Epoch: [26]  [ 890/1349]  eta: 0:02:23  lr: 0.000265  min_lr: 0.000006  loss: 0.8202 (0.8015)  loss_scale: 131072.0000 (129233.1672)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0002  max mem: 41808
Epoch: [26]  [ 900/1349]  eta: 0:02:19  lr: 0.000265  min_lr: 0.000006  loss: 0.8020 (0.8018)  loss_scale: 131072.0000 (129253.5760)  weight_decay: 0.0500 (0.0500)  time: 0.3061  data: 0.0002  max mem: 41808
Epoch: [26]  [ 910/1349]  eta: 0:02:16  lr: 0.000265  min_lr: 0.000006  loss: 0.8046 (0.8019)  loss_scale: 131072.0000 (129273.5368)  weight_decay: 0.0500 (0.0500)  time: 0.3060  data: 0.0001  max mem: 41808
Epoch: [26]  [ 920/1349]  eta: 0:02:13  lr: 0.000265  min_lr: 0.000006  loss: 0.8046 (0.8020)  loss_scale: 131072.0000 (129293.0641)  weight_decay: 0.0500 (0.0500)  time: 0.3126  data: 0.0001  max mem: 41808
[2025-05-23 20:30:14,819] [INFO] [logging.py:96:log_dist] [Rank 0] step=36000, skipped=219, lr=[6.287984095602677e-06, 6.287984095602677e-06, 8.383978794136903e-06, 8.383978794136903e-06, 1.1178638392182537e-05, 1.1178638392182537e-05, 1.4904851189576715e-05, 1.4904851189576715e-05, 1.987313491943562e-05, 1.987313491943562e-05, 2.649751322591416e-05, 2.649751322591416e-05, 3.5330017634552214e-05, 3.5330017634552214e-05, 4.7106690179402954e-05, 4.7106690179402954e-05, 6.280892023920394e-05, 6.280892023920394e-05, 8.374522698560524e-05, 8.374522698560524e-05, 0.00011166030264747366, 0.00011166030264747366, 0.0001488804035299649, 0.0001488804035299649, 0.00019850720470661985, 0.00019850720470661985, 0.0002646762729421598, 0.0002646762729421598], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-05-23 20:30:14,819] [INFO] [timer.py:260:stop] epoch=0/micro_step=36000/global_step=36000, RunningAvgSamplesPerSec=209.26909924377873, CurrSamplesPerSec=213.34031869660242, MemAllocated=1.37GB, MaxMemAllocated=40.83GB
Epoch: [26]  [ 930/1349]  eta: 0:02:10  lr: 0.000265  min_lr: 0.000006  loss: 0.8610 (0.8028)  loss_scale: 131072.0000 (129312.1719)  weight_decay: 0.0500 (0.0500)  time: 0.3127  data: 0.0001  max mem: 41808
Epoch: [26]  [ 940/1349]  eta: 0:02:07  lr: 0.000264  min_lr: 0.000006  loss: 0.8572 (0.8023)  loss_scale: 131072.0000 (129330.8735)  weight_decay: 0.0500 (0.0500)  time: 0.3059  data: 0.0001  max mem: 41808
Epoch: [26]  [ 950/1349]  eta: 0:02:04  lr: 0.000264  min_lr: 0.000006  loss: 0.7716 (0.8023)  loss_scale: 131072.0000 (129349.1819)  weight_decay: 0.0500 (0.0500)  time: 0.3057  data: 0.0001  max mem: 41808
Epoch: [26]  [ 960/1349]  eta: 0:02:01  lr: 0.000264  min_lr: 0.000006  loss: 0.7931 (0.8018)  loss_scale: 131072.0000 (129367.1093)  weight_decay: 0.0500 (0.0500)  time: 0.3054  data: 0.0001  max mem: 41808
Epoch: [26]  [ 970/1349]  eta: 0:01:58  lr: 0.000264  min_lr: 0.000006  loss: 0.8126 (0.8012)  loss_scale: 131072.0000 (129384.6674)  weight_decay: 0.0500 (0.0500)  time: 0.3061  data: 0.0001  max mem: 41808
[2025-05-23 20:30:29,818] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 20:30:29,818] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 20:30:29,818] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 20:30:29,818] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 20:30:30,122] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 36049
[2025-05-23 20:30:30,122] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 36049
[2025-05-23 20:30:30,122] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 20:30:30,122] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 20:30:30,122] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [26]  [ 980/1349]  eta: 0:01:54  lr: 0.000264  min_lr: 0.000006  loss: 0.8126 (0.8014)  loss_scale: 131072.0000 (129535.4781)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [26]  [ 990/1349]  eta: 0:01:51  lr: 0.000264  min_lr: 0.000006  loss: 0.7909 (0.8010)  loss_scale: 131072.0000 (129550.9828)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
Epoch: [26]  [1000/1349]  eta: 0:01:48  lr: 0.000264  min_lr: 0.000006  loss: 0.7908 (0.8007)  loss_scale: 131072.0000 (129566.1778)  weight_decay: 0.0500 (0.0500)  time: 0.3080  data: 0.0001  max mem: 41808
Epoch: [26]  [1010/1349]  eta: 0:01:45  lr: 0.000264  min_lr: 0.000006  loss: 0.7950 (0.8007)  loss_scale: 131072.0000 (129581.0722)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
Epoch: [26]  [1020/1349]  eta: 0:01:42  lr: 0.000263  min_lr: 0.000006  loss: 0.8030 (0.8007)  loss_scale: 131072.0000 (129595.6748)  weight_decay: 0.0500 (0.0500)  time: 0.3057  data: 0.0002  max mem: 41808
Epoch: [26]  [1030/1349]  eta: 0:01:39  lr: 0.000263  min_lr: 0.000006  loss: 0.8519 (0.8013)  loss_scale: 131072.0000 (129609.9942)  weight_decay: 0.0500 (0.0500)  time: 0.3066  data: 0.0001  max mem: 41808
Epoch: [26]  [1040/1349]  eta: 0:01:36  lr: 0.000263  min_lr: 0.000006  loss: 0.8285 (0.8009)  loss_scale: 131072.0000 (129624.0384)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [26]  [1050/1349]  eta: 0:01:33  lr: 0.000263  min_lr: 0.000006  loss: 0.8000 (0.8010)  loss_scale: 131072.0000 (129637.8154)  weight_decay: 0.0500 (0.0500)  time: 0.3064  data: 0.0001  max mem: 41808
Epoch: [26]  [1060/1349]  eta: 0:01:29  lr: 0.000263  min_lr: 0.000006  loss: 0.8161 (0.8003)  loss_scale: 131072.0000 (129651.3327)  weight_decay: 0.0500 (0.0500)  time: 0.3061  data: 0.0001  max mem: 41808
Epoch: [26]  [1070/1349]  eta: 0:01:26  lr: 0.000263  min_lr: 0.000006  loss: 0.7522 (0.8000)  loss_scale: 131072.0000 (129664.5976)  weight_decay: 0.0500 (0.0500)  time: 0.3061  data: 0.0001  max mem: 41808
Epoch: [26]  [1080/1349]  eta: 0:01:23  lr: 0.000263  min_lr: 0.000006  loss: 0.7935 (0.7997)  loss_scale: 131072.0000 (129677.6170)  weight_decay: 0.0500 (0.0500)  time: 0.3061  data: 0.0001  max mem: 41808
Epoch: [26]  [1090/1349]  eta: 0:01:20  lr: 0.000263  min_lr: 0.000006  loss: 0.8527 (0.8005)  loss_scale: 131072.0000 (129690.3978)  weight_decay: 0.0500 (0.0500)  time: 0.3059  data: 0.0001  max mem: 41808
Epoch: [26]  [1100/1349]  eta: 0:01:17  lr: 0.000262  min_lr: 0.000006  loss: 0.8761 (0.8006)  loss_scale: 131072.0000 (129702.9464)  weight_decay: 0.0500 (0.0500)  time: 0.3059  data: 0.0001  max mem: 41808
[2025-05-23 20:31:09,667] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 20:31:09,667] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 20:31:09,667] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 20:31:09,667] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [26]  [1110/1349]  eta: 0:01:14  lr: 0.000262  min_lr: 0.000006  loss: 0.8094 (0.8003)  loss_scale: 131072.0000 (130541.1053)  weight_decay: 0.0500 (0.0500)  time: 0.3058  data: 0.0001  max mem: 41808
Epoch: [26]  [1120/1349]  eta: 0:01:11  lr: 0.000262  min_lr: 0.000006  loss: 0.7705 (0.7999)  loss_scale: 262144.0000 (131715.0830)  weight_decay: 0.0500 (0.0500)  time: 0.3055  data: 0.0001  max mem: 41808
[2025-05-23 20:31:17,322] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 36203
[2025-05-23 20:31:17,322] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 36203
[2025-05-23 20:31:17,322] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 20:31:17,322] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 20:31:17,322] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [26]  [1130/1349]  eta: 0:01:08  lr: 0.000262  min_lr: 0.000006  loss: 0.7992 (0.8000)  loss_scale: 262144.0000 (132636.5199)  weight_decay: 0.0500 (0.0500)  time: 0.3061  data: 0.0001  max mem: 41808
Epoch: [26]  [1140/1349]  eta: 0:01:04  lr: 0.000262  min_lr: 0.000006  loss: 0.8331 (0.8002)  loss_scale: 131072.0000 (132622.8081)  weight_decay: 0.0500 (0.0500)  time: 0.3062  data: 0.0001  max mem: 41808
Epoch: [26]  [1150/1349]  eta: 0:01:01  lr: 0.000262  min_lr: 0.000006  loss: 0.8150 (0.8000)  loss_scale: 131072.0000 (132609.3345)  weight_decay: 0.0500 (0.0500)  time: 0.3060  data: 0.0001  max mem: 41808
Epoch: [26]  [1160/1349]  eta: 0:00:58  lr: 0.000262  min_lr: 0.000006  loss: 0.7852 (0.7998)  loss_scale: 131072.0000 (132596.0930)  weight_decay: 0.0500 (0.0500)  time: 0.3060  data: 0.0001  max mem: 41808
Epoch: [26]  [1170/1349]  eta: 0:00:55  lr: 0.000262  min_lr: 0.000006  loss: 0.7425 (0.7990)  loss_scale: 131072.0000 (132583.0777)  weight_decay: 0.0500 (0.0500)  time: 0.3061  data: 0.0001  max mem: 41808
Epoch: [26]  [1180/1349]  eta: 0:00:52  lr: 0.000261  min_lr: 0.000006  loss: 0.7755 (0.7990)  loss_scale: 131072.0000 (132570.2828)  weight_decay: 0.0500 (0.0500)  time: 0.3060  data: 0.0001  max mem: 41808
Epoch: [26]  [1190/1349]  eta: 0:00:49  lr: 0.000261  min_lr: 0.000006  loss: 0.7823 (0.7988)  loss_scale: 131072.0000 (132557.7028)  weight_decay: 0.0500 (0.0500)  time: 0.3057  data: 0.0001  max mem: 41808
Epoch: [26]  [1200/1349]  eta: 0:00:46  lr: 0.000261  min_lr: 0.000006  loss: 0.7785 (0.7987)  loss_scale: 131072.0000 (132545.3322)  weight_decay: 0.0500 (0.0500)  time: 0.3059  data: 0.0001  max mem: 41808
Epoch: [26]  [1210/1349]  eta: 0:00:43  lr: 0.000261  min_lr: 0.000006  loss: 0.7863 (0.7993)  loss_scale: 131072.0000 (132533.1660)  weight_decay: 0.0500 (0.0500)  time: 0.3058  data: 0.0001  max mem: 41808
Epoch: [26]  [1220/1349]  eta: 0:00:40  lr: 0.000261  min_lr: 0.000006  loss: 0.7920 (0.7993)  loss_scale: 131072.0000 (132521.1990)  weight_decay: 0.0500 (0.0500)  time: 0.3058  data: 0.0001  max mem: 41808
Epoch: [26]  [1230/1349]  eta: 0:00:36  lr: 0.000261  min_lr: 0.000006  loss: 0.8338 (0.7996)  loss_scale: 131072.0000 (132509.4265)  weight_decay: 0.0500 (0.0500)  time: 0.3058  data: 0.0001  max mem: 41808
Epoch: [26]  [1240/1349]  eta: 0:00:33  lr: 0.000261  min_lr: 0.000006  loss: 0.8988 (0.8002)  loss_scale: 131072.0000 (132497.8437)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0001  max mem: 41808
Epoch: [26]  [1250/1349]  eta: 0:00:30  lr: 0.000260  min_lr: 0.000006  loss: 0.8981 (0.8007)  loss_scale: 131072.0000 (132486.4460)  weight_decay: 0.0500 (0.0500)  time: 0.3086  data: 0.0001  max mem: 41808
[2025-05-23 20:31:55,923] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 36329
[2025-05-23 20:31:55,923] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 36329
[2025-05-23 20:31:55,923] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 20:31:55,923] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 20:31:55,923] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [26]  [1260/1349]  eta: 0:00:27  lr: 0.000260  min_lr: 0.000006  loss: 0.7796 (0.8004)  loss_scale: 131072.0000 (132163.4005)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
Epoch: [26]  [1270/1349]  eta: 0:00:24  lr: 0.000260  min_lr: 0.000006  loss: 0.7438 (0.7999)  loss_scale: 65536.0000 (131639.1880)  weight_decay: 0.0500 (0.0500)  time: 0.3055  data: 0.0001  max mem: 41808
Epoch: [26]  [1280/1349]  eta: 0:00:21  lr: 0.000260  min_lr: 0.000006  loss: 0.7907 (0.8001)  loss_scale: 65536.0000 (131123.1600)  weight_decay: 0.0500 (0.0500)  time: 0.3061  data: 0.0001  max mem: 41808
Epoch: [26]  [1290/1349]  eta: 0:00:18  lr: 0.000260  min_lr: 0.000006  loss: 0.7586 (0.7995)  loss_scale: 65536.0000 (130615.1263)  weight_decay: 0.0500 (0.0500)  time: 0.3058  data: 0.0001  max mem: 41808
Epoch: [26]  [1300/1349]  eta: 0:00:15  lr: 0.000260  min_lr: 0.000006  loss: 0.7429 (0.8000)  loss_scale: 65536.0000 (130114.9024)  weight_decay: 0.0500 (0.0500)  time: 0.3056  data: 0.0001  max mem: 41808
Epoch: [26]  [1310/1349]  eta: 0:00:12  lr: 0.000260  min_lr: 0.000006  loss: 0.8184 (0.8000)  loss_scale: 65536.0000 (129622.3097)  weight_decay: 0.0500 (0.0500)  time: 0.3054  data: 0.0001  max mem: 41808
Epoch: [26]  [1320/1349]  eta: 0:00:08  lr: 0.000260  min_lr: 0.000006  loss: 0.7738 (0.7994)  loss_scale: 65536.0000 (129137.1749)  weight_decay: 0.0500 (0.0500)  time: 0.3052  data: 0.0001  max mem: 41808
Epoch: [26]  [1330/1349]  eta: 0:00:05  lr: 0.000259  min_lr: 0.000006  loss: 0.6229 (0.7985)  loss_scale: 65536.0000 (128659.3298)  weight_decay: 0.0500 (0.0500)  time: 0.3038  data: 0.0001  max mem: 41808
Epoch: [26]  [1340/1349]  eta: 0:00:02  lr: 0.000259  min_lr: 0.000006  loss: 0.7715 (0.7985)  loss_scale: 65536.0000 (128188.6115)  weight_decay: 0.0500 (0.0500)  time: 0.3019  data: 0.0001  max mem: 41808
Epoch: [26]  [1348/1349]  eta: 0:00:00  lr: 0.000259  min_lr: 0.000006  loss: 0.8108 (0.7983)  loss_scale: 65536.0000 (127817.0615)  weight_decay: 0.0500 (0.0500)  time: 0.3014  data: 0.0001  max mem: 41808
Epoch: [26] Total time: 0:06:58 (0.3103 s / it)
Averaged stats: lr: 0.000259  min_lr: 0.000006  loss: 0.8108 (0.7981)  loss_scale: 65536.0000 (127817.0615)  weight_decay: 0.0500 (0.0500)  total_time: 418.5346 (418.5269)
Val:  [  0/346]  eta: 1:08:24  loss: 3.1331 (3.1331)  acc1: 4.6875 (4.6875)  acc5: 81.2500 (81.2500)  time: 11.8630  data: 10.8144  max mem: 41808
Val:  [ 10/346]  eta: 0:11:10  loss: 0.1442 (0.6218)  acc1: 99.2188 (84.3040)  acc5: 100.0000 (97.9403)  time: 1.9969  data: 1.1691  max mem: 41808
Val:  [ 20/346]  eta: 0:08:00  loss: 0.1360 (0.4571)  acc1: 99.2188 (88.8393)  acc5: 100.0000 (98.8467)  time: 0.9541  data: 0.1742  max mem: 41808
Val:  [ 30/346]  eta: 0:06:38  loss: 0.1075 (0.3817)  acc1: 100.0000 (91.0282)  acc5: 100.0000 (99.2188)  time: 0.8554  data: 0.0780  max mem: 41808
Val:  [ 40/346]  eta: 0:05:45  loss: 0.1080 (0.4198)  acc1: 100.0000 (90.0915)  acc5: 100.0000 (99.2950)  time: 0.7666  data: 0.0062  max mem: 41808
Val:  [ 50/346]  eta: 0:05:14  loss: 0.1241 (0.3748)  acc1: 99.2188 (91.5135)  acc5: 100.0000 (99.4179)  time: 0.7585  data: 0.0369  max mem: 41808
Val:  [ 60/346]  eta: 0:04:56  loss: 0.1241 (0.3685)  acc1: 98.4375 (91.3422)  acc5: 100.0000 (99.5133)  time: 0.8487  data: 0.1077  max mem: 41808
Val:  [ 70/346]  eta: 0:04:41  loss: 0.2073 (0.3876)  acc1: 94.5312 (90.7901)  acc5: 100.0000 (99.5268)  time: 0.9061  data: 0.1409  max mem: 41808
Val:  [ 80/346]  eta: 0:04:27  loss: 0.2052 (0.3878)  acc1: 94.5312 (90.8179)  acc5: 100.0000 (99.4888)  time: 0.9084  data: 0.1374  max mem: 41808
Val:  [ 90/346]  eta: 0:04:13  loss: 0.2465 (0.3949)  acc1: 93.7500 (90.6336)  acc5: 100.0000 (99.5106)  time: 0.8893  data: 0.1356  max mem: 41808
Val:  [100/346]  eta: 0:04:00  loss: 0.1758 (0.3736)  acc1: 97.6562 (91.3598)  acc5: 100.0000 (99.5591)  time: 0.8757  data: 0.1328  max mem: 41808
Val:  [110/346]  eta: 0:03:49  loss: 0.1730 (0.3858)  acc1: 98.4375 (90.9910)  acc5: 100.0000 (99.4510)  time: 0.8801  data: 0.1427  max mem: 41808
Val:  [120/346]  eta: 0:03:37  loss: 0.1991 (0.3864)  acc1: 96.0938 (90.9349)  acc5: 100.0000 (99.4964)  time: 0.8696  data: 0.1441  max mem: 41808
Val:  [130/346]  eta: 0:03:26  loss: 0.1146 (0.3991)  acc1: 99.2188 (90.4222)  acc5: 100.0000 (99.5289)  time: 0.8603  data: 0.1352  max mem: 41808
Val:  [140/346]  eta: 0:03:15  loss: 0.2094 (0.3922)  acc1: 96.0938 (90.5973)  acc5: 100.0000 (99.5623)  time: 0.8685  data: 0.1262  max mem: 41808
Val:  [150/346]  eta: 0:03:05  loss: 0.2814 (0.3904)  acc1: 95.3125 (90.7337)  acc5: 100.0000 (99.5706)  time: 0.8945  data: 0.1291  max mem: 41808
Val:  [160/346]  eta: 0:02:55  loss: 0.2857 (0.3893)  acc1: 92.1875 (90.6396)  acc5: 100.0000 (99.5924)  time: 0.9163  data: 0.1550  max mem: 41808
Val:  [170/346]  eta: 0:02:45  loss: 0.2979 (0.3923)  acc1: 89.8438 (90.4240)  acc5: 100.0000 (99.6162)  time: 0.8988  data: 0.1640  max mem: 41808
Val:  [180/346]  eta: 0:02:36  loss: 0.3054 (0.4035)  acc1: 89.8438 (89.7963)  acc5: 100.0000 (99.6374)  time: 0.9169  data: 0.1625  max mem: 41808
Val:  [190/346]  eta: 0:02:26  loss: 0.2856 (0.4030)  acc1: 93.7500 (89.8192)  acc5: 100.0000 (99.6441)  time: 0.9261  data: 0.1537  max mem: 41808
Val:  [200/346]  eta: 0:02:16  loss: 0.4272 (0.4269)  acc1: 87.5000 (88.9148)  acc5: 100.0000 (99.6463)  time: 0.8822  data: 0.1352  max mem: 41808
Val:  [210/346]  eta: 0:02:06  loss: 0.2256 (0.4159)  acc1: 92.9688 (89.3328)  acc5: 100.0000 (99.6631)  time: 0.8748  data: 0.1427  max mem: 41808
Val:  [220/346]  eta: 0:01:57  loss: 0.1428 (0.4081)  acc1: 97.6562 (89.6104)  acc5: 100.0000 (99.6571)  time: 0.9040  data: 0.1597  max mem: 41808
Val:  [230/346]  eta: 0:01:48  loss: 0.1680 (0.3988)  acc1: 96.8750 (89.9418)  acc5: 100.0000 (99.6652)  time: 0.9257  data: 0.1588  max mem: 41808
Val:  [240/346]  eta: 0:01:38  loss: 0.1372 (0.3994)  acc1: 100.0000 (89.9831)  acc5: 100.0000 (99.6758)  time: 0.9157  data: 0.1602  max mem: 41808
Val:  [250/346]  eta: 0:01:29  loss: 0.1574 (0.3985)  acc1: 98.4375 (90.0865)  acc5: 100.0000 (99.6763)  time: 0.9007  data: 0.1671  max mem: 41808
Val:  [260/346]  eta: 0:01:19  loss: 0.1553 (0.3944)  acc1: 98.4375 (90.2149)  acc5: 100.0000 (99.6767)  time: 0.9020  data: 0.1613  max mem: 41808
Val:  [270/346]  eta: 0:01:10  loss: 0.1525 (0.3891)  acc1: 98.4375 (90.3886)  acc5: 100.0000 (99.6771)  time: 0.9001  data: 0.1544  max mem: 41808
Val:  [280/346]  eta: 0:01:01  loss: 0.1364 (0.3870)  acc1: 98.4375 (90.4888)  acc5: 100.0000 (99.6775)  time: 0.9057  data: 0.1535  max mem: 41808
Val:  [290/346]  eta: 0:00:51  loss: 0.1096 (0.3796)  acc1: 100.0000 (90.7082)  acc5: 100.0000 (99.6886)  time: 0.9004  data: 0.1512  max mem: 41808
Val:  [300/346]  eta: 0:00:42  loss: 0.1100 (0.3785)  acc1: 100.0000 (90.8119)  acc5: 100.0000 (99.6859)  time: 0.8807  data: 0.1470  max mem: 41808
Val:  [310/346]  eta: 0:00:33  loss: 0.1229 (0.3814)  acc1: 99.2188 (90.7330)  acc5: 100.0000 (99.6584)  time: 0.8873  data: 0.1454  max mem: 41808
Val:  [320/346]  eta: 0:00:23  loss: 0.1352 (0.3783)  acc1: 98.4375 (90.8173)  acc5: 100.0000 (99.6690)  time: 0.8957  data: 0.1514  max mem: 41808
Val:  [330/346]  eta: 0:00:14  loss: 0.2638 (0.3886)  acc1: 91.4062 (90.5377)  acc5: 100.0000 (99.6601)  time: 0.9110  data: 0.1535  max mem: 41808
Val:  [340/346]  eta: 0:00:05  loss: 0.3763 (0.3974)  acc1: 90.6250 (90.2745)  acc5: 100.0000 (99.6586)  time: 0.9209  data: 0.1533  max mem: 41808
Val:  [345/346]  eta: 0:00:00  loss: 0.1546 (0.3938)  acc1: 95.4023 (90.3948)  acc5: 100.0000 (99.6633)  time: 0.9079  data: 0.1622  max mem: 41808
Val: Total time: 0:05:19 (0.9228 s / it)
* Acc@1 90.425 Acc@5 99.671 loss 0.392
Accuracy of the network on the 88494 val videos: 90.4%
Max accuracy: 91.50%   Max Epoch: 25
Epoch: [27]  [   0/1349]  eta: 1:57:25  lr: 0.000259  min_lr: 0.000006  loss: 0.6354 (0.6354)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 5.2224  data: 4.0299  max mem: 41808
Epoch: [27]  [  10/1349]  eta: 0:16:54  lr: 0.000259  min_lr: 0.000006  loss: 0.9163 (0.8548)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7576  data: 0.3664  max mem: 41808
Epoch: [27]  [  20/1349]  eta: 0:12:02  lr: 0.000259  min_lr: 0.000006  loss: 0.8674 (0.8119)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3098  data: 0.0001  max mem: 41808
Epoch: [27]  [  30/1349]  eta: 0:10:16  lr: 0.000259  min_lr: 0.000006  loss: 0.8228 (0.8032)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3083  data: 0.0001  max mem: 41808
[2025-05-23 20:37:59,902] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 20:37:59,903] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-05-23 20:37:59,902] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 20:37:59,903] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [27]  [  40/1349]  eta: 0:09:21  lr: 0.000259  min_lr: 0.000006  loss: 0.8060 (0.7931)  loss_scale: 65536.0000 (75126.6341)  weight_decay: 0.0500 (0.0500)  time: 0.3080  data: 0.0001  max mem: 41808
Epoch: [27]  [  50/1349]  eta: 0:08:46  lr: 0.000259  min_lr: 0.000006  loss: 0.7755 (0.7884)  loss_scale: 131072.0000 (86096.3137)  weight_decay: 0.0500 (0.0500)  time: 0.3085  data: 0.0001  max mem: 41808
Epoch: [27]  [  60/1349]  eta: 0:08:21  lr: 0.000258  min_lr: 0.000006  loss: 0.8079 (0.8028)  loss_scale: 131072.0000 (93469.3770)  weight_decay: 0.0500 (0.0500)  time: 0.3086  data: 0.0001  max mem: 41808
Epoch: [27]  [  70/1349]  eta: 0:08:03  lr: 0.000258  min_lr: 0.000006  loss: 0.7447 (0.7910)  loss_scale: 131072.0000 (98765.5211)  weight_decay: 0.0500 (0.0500)  time: 0.3090  data: 0.0001  max mem: 41808
Epoch: [27]  [  80/1349]  eta: 0:07:48  lr: 0.000258  min_lr: 0.000006  loss: 0.7064 (0.7836)  loss_scale: 131072.0000 (102753.9753)  weight_decay: 0.0500 (0.0500)  time: 0.3082  data: 0.0001  max mem: 41808
Epoch: [27]  [  90/1349]  eta: 0:07:36  lr: 0.000258  min_lr: 0.000006  loss: 0.7477 (0.7818)  loss_scale: 131072.0000 (105865.8462)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
Epoch: [27]  [ 100/1349]  eta: 0:07:25  lr: 0.000258  min_lr: 0.000006  loss: 0.8052 (0.7830)  loss_scale: 131072.0000 (108361.5050)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [27]  [ 110/1349]  eta: 0:07:16  lr: 0.000258  min_lr: 0.000006  loss: 0.8181 (0.7864)  loss_scale: 131072.0000 (110407.4955)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [27]  [ 120/1349]  eta: 0:07:08  lr: 0.000258  min_lr: 0.000006  loss: 0.8181 (0.7834)  loss_scale: 131072.0000 (112115.3058)  weight_decay: 0.0500 (0.0500)  time: 0.3082  data: 0.0001  max mem: 41808
Epoch: [27]  [ 130/1349]  eta: 0:07:01  lr: 0.000258  min_lr: 0.000006  loss: 0.8248 (0.7849)  loss_scale: 131072.0000 (113562.3817)  weight_decay: 0.0500 (0.0500)  time: 0.3094  data: 0.0001  max mem: 41808
Epoch: [27]  [ 140/1349]  eta: 0:06:55  lr: 0.000257  min_lr: 0.000006  loss: 0.8506 (0.7893)  loss_scale: 131072.0000 (114804.1986)  weight_decay: 0.0500 (0.0500)  time: 0.3088  data: 0.0001  max mem: 41808
Epoch: [27]  [ 150/1349]  eta: 0:06:48  lr: 0.000257  min_lr: 0.000006  loss: 0.8542 (0.7898)  loss_scale: 131072.0000 (115881.5364)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [27]  [ 160/1349]  eta: 0:06:42  lr: 0.000257  min_lr: 0.000006  loss: 0.8542 (0.7921)  loss_scale: 131072.0000 (116825.0435)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
[2025-05-23 20:38:39,353] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 20:38:39,353] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 20:38:39,353] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 20:38:39,353] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 20:38:39,655] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 36587
[2025-05-23 20:38:39,655] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 20:38:39,655] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 36587
[2025-05-23 20:38:39,655] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 20:38:39,655] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [27]  [ 170/1349]  eta: 0:06:37  lr: 0.000257  min_lr: 0.000006  loss: 0.9156 (0.7938)  loss_scale: 131072.0000 (118424.7018)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [27]  [ 180/1349]  eta: 0:06:31  lr: 0.000257  min_lr: 0.000006  loss: 0.9269 (0.7994)  loss_scale: 131072.0000 (119123.4475)  weight_decay: 0.0500 (0.0500)  time: 0.3066  data: 0.0001  max mem: 41808
Epoch: [27]  [ 190/1349]  eta: 0:06:26  lr: 0.000257  min_lr: 0.000006  loss: 0.8419 (0.7969)  loss_scale: 131072.0000 (119749.0262)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
Epoch: [27]  [ 200/1349]  eta: 0:06:22  lr: 0.000257  min_lr: 0.000006  loss: 0.8211 (0.7980)  loss_scale: 131072.0000 (120312.3582)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [27]  [ 210/1349]  eta: 0:06:17  lr: 0.000256  min_lr: 0.000006  loss: 0.8228 (0.7968)  loss_scale: 131072.0000 (120822.2938)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
Epoch: [27]  [ 220/1349]  eta: 0:06:12  lr: 0.000256  min_lr: 0.000006  loss: 0.8228 (0.7999)  loss_scale: 131072.0000 (121286.0814)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0002  max mem: 41808
Epoch: [27]  [ 230/1349]  eta: 0:06:08  lr: 0.000256  min_lr: 0.000006  loss: 0.8821 (0.8030)  loss_scale: 131072.0000 (121709.7143)  weight_decay: 0.0500 (0.0500)  time: 0.3097  data: 0.0001  max mem: 41808
Epoch: [27]  [ 240/1349]  eta: 0:06:04  lr: 0.000256  min_lr: 0.000006  loss: 0.8246 (0.8017)  loss_scale: 131072.0000 (122098.1909)  weight_decay: 0.0500 (0.0500)  time: 0.3089  data: 0.0001  max mem: 41808
Epoch: [27]  [ 250/1349]  eta: 0:05:59  lr: 0.000256  min_lr: 0.000006  loss: 0.7980 (0.8021)  loss_scale: 131072.0000 (122455.7131)  weight_decay: 0.0500 (0.0500)  time: 0.3057  data: 0.0002  max mem: 41808
Epoch: [27]  [ 260/1349]  eta: 0:05:55  lr: 0.000256  min_lr: 0.000006  loss: 0.8172 (0.8019)  loss_scale: 131072.0000 (122785.8391)  weight_decay: 0.0500 (0.0500)  time: 0.3063  data: 0.0001  max mem: 41808
Epoch: [27]  [ 270/1349]  eta: 0:05:51  lr: 0.000256  min_lr: 0.000006  loss: 0.8052 (0.8008)  loss_scale: 131072.0000 (123091.6015)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
Epoch: [27]  [ 280/1349]  eta: 0:05:47  lr: 0.000256  min_lr: 0.000006  loss: 0.7983 (0.8023)  loss_scale: 131072.0000 (123375.6014)  weight_decay: 0.0500 (0.0500)  time: 0.3063  data: 0.0001  max mem: 41808
Epoch: [27]  [ 290/1349]  eta: 0:05:43  lr: 0.000255  min_lr: 0.000006  loss: 0.8406 (0.8034)  loss_scale: 131072.0000 (123640.0825)  weight_decay: 0.0500 (0.0500)  time: 0.3061  data: 0.0001  max mem: 41808
[2025-05-23 20:39:19,283] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 20:39:19,284] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 20:39:19,284] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 20:39:19,284] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [27]  [ 300/1349]  eta: 0:05:39  lr: 0.000255  min_lr: 0.000006  loss: 0.8721 (0.8050)  loss_scale: 131072.0000 (127370.6312)  weight_decay: 0.0500 (0.0500)  time: 0.3064  data: 0.0002  max mem: 41808
[2025-05-23 20:39:22,953] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 36728
[2025-05-23 20:39:22,953] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 36728
[2025-05-23 20:39:22,953] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 20:39:22,953] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 20:39:22,953] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [27]  [ 310/1349]  eta: 0:05:36  lr: 0.000255  min_lr: 0.000006  loss: 0.8778 (0.8082)  loss_scale: 262144.0000 (129175.4598)  weight_decay: 0.0500 (0.0500)  time: 0.3059  data: 0.0002  max mem: 41808
Epoch: [27]  [ 320/1349]  eta: 0:05:32  lr: 0.000255  min_lr: 0.000006  loss: 0.8576 (0.8083)  loss_scale: 131072.0000 (129234.5421)  weight_decay: 0.0500 (0.0500)  time: 0.3055  data: 0.0002  max mem: 41808
Epoch: [27]  [ 330/1349]  eta: 0:05:28  lr: 0.000255  min_lr: 0.000006  loss: 0.8226 (0.8075)  loss_scale: 131072.0000 (129290.0544)  weight_decay: 0.0500 (0.0500)  time: 0.3056  data: 0.0001  max mem: 41808
Epoch: [27]  [ 340/1349]  eta: 0:05:24  lr: 0.000255  min_lr: 0.000006  loss: 0.8151 (0.8072)  loss_scale: 131072.0000 (129342.3109)  weight_decay: 0.0500 (0.0500)  time: 0.3059  data: 0.0002  max mem: 41808
Epoch: [27]  [ 350/1349]  eta: 0:05:21  lr: 0.000255  min_lr: 0.000006  loss: 0.8082 (0.8060)  loss_scale: 131072.0000 (129391.5897)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
Epoch: [27]  [ 360/1349]  eta: 0:05:17  lr: 0.000255  min_lr: 0.000006  loss: 0.8378 (0.8058)  loss_scale: 131072.0000 (129438.1385)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
Epoch: [27]  [ 370/1349]  eta: 0:05:13  lr: 0.000254  min_lr: 0.000006  loss: 0.8538 (0.8068)  loss_scale: 131072.0000 (129482.1779)  weight_decay: 0.0500 (0.0500)  time: 0.3060  data: 0.0001  max mem: 41808
Epoch: [27]  [ 380/1349]  eta: 0:05:10  lr: 0.000254  min_lr: 0.000006  loss: 0.8467 (0.8063)  loss_scale: 131072.0000 (129523.9055)  weight_decay: 0.0500 (0.0500)  time: 0.3061  data: 0.0001  max mem: 41808
Epoch: [27]  [ 390/1349]  eta: 0:05:06  lr: 0.000254  min_lr: 0.000006  loss: 0.7755 (0.8043)  loss_scale: 131072.0000 (129563.4987)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
Epoch: [27]  [ 400/1349]  eta: 0:05:03  lr: 0.000254  min_lr: 0.000006  loss: 0.7596 (0.8041)  loss_scale: 131072.0000 (129601.1172)  weight_decay: 0.0500 (0.0500)  time: 0.3084  data: 0.0001  max mem: 41808
Epoch: [27]  [ 410/1349]  eta: 0:04:59  lr: 0.000254  min_lr: 0.000006  loss: 0.7508 (0.8023)  loss_scale: 131072.0000 (129636.9051)  weight_decay: 0.0500 (0.0500)  time: 0.3085  data: 0.0001  max mem: 41808
Epoch: [27]  [ 420/1349]  eta: 0:04:56  lr: 0.000254  min_lr: 0.000006  loss: 0.8189 (0.8041)  loss_scale: 131072.0000 (129670.9929)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
Epoch: [27]  [ 430/1349]  eta: 0:04:52  lr: 0.000254  min_lr: 0.000006  loss: 0.8574 (0.8049)  loss_scale: 131072.0000 (129703.4988)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
[2025-05-23 20:40:02,534] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 20:40:02,534] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 20:40:02,534] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 20:40:02,535] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [27]  [ 440/1349]  eta: 0:04:49  lr: 0.000254  min_lr: 0.000006  loss: 0.8056 (0.8039)  loss_scale: 131072.0000 (131815.0385)  weight_decay: 0.0500 (0.0500)  time: 0.3078  data: 0.0001  max mem: 41808
[2025-05-23 20:40:05,608] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 36867
[2025-05-23 20:40:05,608] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 36867
[2025-05-23 20:40:05,608] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 20:40:05,608] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 20:40:05,608] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [27]  [ 450/1349]  eta: 0:04:46  lr: 0.000253  min_lr: 0.000006  loss: 0.7750 (0.8037)  loss_scale: 131072.0000 (132670.4390)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [27]  [ 460/1349]  eta: 0:04:42  lr: 0.000253  min_lr: 0.000006  loss: 0.8347 (0.8045)  loss_scale: 131072.0000 (132635.7657)  weight_decay: 0.0500 (0.0500)  time: 0.3066  data: 0.0001  max mem: 41808
Epoch: [27]  [ 470/1349]  eta: 0:04:39  lr: 0.000253  min_lr: 0.000006  loss: 0.8647 (0.8032)  loss_scale: 131072.0000 (132602.5648)  weight_decay: 0.0500 (0.0500)  time: 0.3100  data: 0.0001  max mem: 41808
Epoch: [27]  [ 480/1349]  eta: 0:04:36  lr: 0.000253  min_lr: 0.000006  loss: 0.8012 (0.8045)  loss_scale: 131072.0000 (132570.7443)  weight_decay: 0.0500 (0.0500)  time: 0.3115  data: 0.0002  max mem: 41808
Epoch: [27]  [ 490/1349]  eta: 0:04:32  lr: 0.000253  min_lr: 0.000006  loss: 0.8012 (0.8039)  loss_scale: 131072.0000 (132540.2200)  weight_decay: 0.0500 (0.0500)  time: 0.3089  data: 0.0002  max mem: 41808
Epoch: [27]  [ 500/1349]  eta: 0:04:29  lr: 0.000253  min_lr: 0.000006  loss: 0.7988 (0.8037)  loss_scale: 131072.0000 (132510.9142)  weight_decay: 0.0500 (0.0500)  time: 0.3081  data: 0.0002  max mem: 41808
Epoch: [27]  [ 510/1349]  eta: 0:04:26  lr: 0.000253  min_lr: 0.000006  loss: 0.7424 (0.8032)  loss_scale: 131072.0000 (132482.7554)  weight_decay: 0.0500 (0.0500)  time: 0.3095  data: 0.0002  max mem: 41808
Epoch: [27]  [ 520/1349]  eta: 0:04:22  lr: 0.000252  min_lr: 0.000006  loss: 0.8202 (0.8033)  loss_scale: 131072.0000 (132455.6775)  weight_decay: 0.0500 (0.0500)  time: 0.3087  data: 0.0001  max mem: 41808
[2025-05-23 20:40:29,399] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 36944
[2025-05-23 20:40:29,399] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 36944
[2025-05-23 20:40:29,399] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 20:40:29,399] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 20:40:29,399] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [27]  [ 530/1349]  eta: 0:04:19  lr: 0.000252  min_lr: 0.000006  loss: 0.8412 (0.8041)  loss_scale: 65536.0000 (131195.4200)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0001  max mem: 41808
Epoch: [27]  [ 540/1349]  eta: 0:04:16  lr: 0.000252  min_lr: 0.000006  loss: 0.8412 (0.8047)  loss_scale: 65536.0000 (129981.7523)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
Epoch: [27]  [ 550/1349]  eta: 0:04:12  lr: 0.000252  min_lr: 0.000006  loss: 0.8575 (0.8054)  loss_scale: 65536.0000 (128812.1379)  weight_decay: 0.0500 (0.0500)  time: 0.3080  data: 0.0001  max mem: 41808
Epoch: [27]  [ 560/1349]  eta: 0:04:09  lr: 0.000252  min_lr: 0.000006  loss: 0.8253 (0.8041)  loss_scale: 65536.0000 (127684.2210)  weight_decay: 0.0500 (0.0500)  time: 0.3079  data: 0.0001  max mem: 41808
Epoch: [27]  [ 570/1349]  eta: 0:04:06  lr: 0.000252  min_lr: 0.000006  loss: 0.7934 (0.8043)  loss_scale: 65536.0000 (126595.8109)  weight_decay: 0.0500 (0.0500)  time: 0.3086  data: 0.0001  max mem: 41808
[2025-05-23 20:40:46,342] [INFO] [logging.py:96:log_dist] [Rank 0] step=37000, skipped=226, lr=[5.98140992416264e-06, 5.98140992416264e-06, 7.975213232216854e-06, 7.975213232216854e-06, 1.0633617642955804e-05, 1.0633617642955804e-05, 1.4178156857274406e-05, 1.4178156857274406e-05, 1.890420914303254e-05, 1.890420914303254e-05, 2.5205612190710055e-05, 2.5205612190710055e-05, 3.360748292094674e-05, 3.360748292094674e-05, 4.480997722792899e-05, 4.480997722792899e-05, 5.974663630390532e-05, 5.974663630390532e-05, 7.966218173854042e-05, 7.966218173854042e-05, 0.00010621624231805389, 0.00010621624231805389, 0.00014162165642407187, 0.00014162165642407187, 0.00018882887523209583, 0.00018882887523209583, 0.0002517718336427944, 0.0002517718336427944], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-05-23 20:40:46,342] [INFO] [timer.py:260:stop] epoch=0/micro_step=37000/global_step=37000, RunningAvgSamplesPerSec=209.3693411678239, CurrSamplesPerSec=215.1418161533813, MemAllocated=1.37GB, MaxMemAllocated=40.83GB
Epoch: [27]  [ 580/1349]  eta: 0:04:03  lr: 0.000252  min_lr: 0.000006  loss: 0.7861 (0.8027)  loss_scale: 65536.0000 (125544.8675)  weight_decay: 0.0500 (0.0500)  time: 0.3079  data: 0.0001  max mem: 41808
Epoch: [27]  [ 590/1349]  eta: 0:03:59  lr: 0.000252  min_lr: 0.000006  loss: 0.7922 (0.8025)  loss_scale: 65536.0000 (124529.4890)  weight_decay: 0.0500 (0.0500)  time: 0.3066  data: 0.0001  max mem: 41808
Epoch: [27]  [ 600/1349]  eta: 0:03:56  lr: 0.000251  min_lr: 0.000006  loss: 0.8300 (0.8024)  loss_scale: 65536.0000 (123547.9002)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0001  max mem: 41808
Epoch: [27]  [ 610/1349]  eta: 0:03:53  lr: 0.000251  min_lr: 0.000006  loss: 0.7902 (0.8021)  loss_scale: 65536.0000 (122598.4419)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
Epoch: [27]  [ 620/1349]  eta: 0:03:49  lr: 0.000251  min_lr: 0.000006  loss: 0.7998 (0.8021)  loss_scale: 65536.0000 (121679.5620)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
Epoch: [27]  [ 630/1349]  eta: 0:03:46  lr: 0.000251  min_lr: 0.000006  loss: 0.8194 (0.8028)  loss_scale: 65536.0000 (120789.8067)  weight_decay: 0.0500 (0.0500)  time: 0.3062  data: 0.0002  max mem: 41808
Epoch: [27]  [ 640/1349]  eta: 0:03:43  lr: 0.000251  min_lr: 0.000006  loss: 0.8585 (0.8027)  loss_scale: 65536.0000 (119927.8128)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0001  max mem: 41808
[2025-05-23 20:41:09,029] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 20:41:09,029] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-05-23 20:41:09,029] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 20:41:09,029] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [27]  [ 650/1349]  eta: 0:03:40  lr: 0.000251  min_lr: 0.000006  loss: 0.8235 (0.8032)  loss_scale: 65536.0000 (119192.9708)  weight_decay: 0.0500 (0.0500)  time: 0.3062  data: 0.0001  max mem: 41808
Epoch: [27]  [ 660/1349]  eta: 0:03:36  lr: 0.000251  min_lr: 0.000006  loss: 0.8157 (0.8047)  loss_scale: 131072.0000 (119372.6838)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0001  max mem: 41808
Epoch: [27]  [ 670/1349]  eta: 0:03:33  lr: 0.000251  min_lr: 0.000006  loss: 0.8727 (0.8046)  loss_scale: 131072.0000 (119547.0402)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
Epoch: [27]  [ 680/1349]  eta: 0:03:30  lr: 0.000250  min_lr: 0.000006  loss: 0.7128 (0.8023)  loss_scale: 131072.0000 (119716.2761)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [27]  [ 690/1349]  eta: 0:03:27  lr: 0.000250  min_lr: 0.000006  loss: 0.7851 (0.8032)  loss_scale: 131072.0000 (119880.6136)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [27]  [ 700/1349]  eta: 0:03:24  lr: 0.000250  min_lr: 0.000006  loss: 0.8718 (0.8034)  loss_scale: 131072.0000 (120040.2625)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
[2025-05-23 20:41:26,222] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 37129
[2025-05-23 20:41:26,222] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 37129
[2025-05-23 20:41:26,222] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 20:41:26,222] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 20:41:26,222] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [27]  [ 710/1349]  eta: 0:03:20  lr: 0.000250  min_lr: 0.000006  loss: 0.7656 (0.8022)  loss_scale: 131072.0000 (119734.5485)  weight_decay: 0.0500 (0.0500)  time: 0.3060  data: 0.0001  max mem: 41808
Epoch: [27]  [ 720/1349]  eta: 0:03:17  lr: 0.000250  min_lr: 0.000006  loss: 0.6716 (0.8007)  loss_scale: 65536.0000 (118982.8350)  weight_decay: 0.0500 (0.0500)  time: 0.3061  data: 0.0001  max mem: 41808
Epoch: [27]  [ 730/1349]  eta: 0:03:14  lr: 0.000250  min_lr: 0.000006  loss: 0.7315 (0.8010)  loss_scale: 65536.0000 (118251.6881)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [27]  [ 740/1349]  eta: 0:03:11  lr: 0.000250  min_lr: 0.000006  loss: 0.8255 (0.8013)  loss_scale: 65536.0000 (117540.2753)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [27]  [ 750/1349]  eta: 0:03:08  lr: 0.000250  min_lr: 0.000006  loss: 0.8258 (0.8020)  loss_scale: 65536.0000 (116847.8083)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
Epoch: [27]  [ 760/1349]  eta: 0:03:04  lr: 0.000249  min_lr: 0.000006  loss: 0.8641 (0.8023)  loss_scale: 65536.0000 (116173.5401)  weight_decay: 0.0500 (0.0500)  time: 0.3063  data: 0.0001  max mem: 41808
Epoch: [27]  [ 770/1349]  eta: 0:03:01  lr: 0.000249  min_lr: 0.000006  loss: 0.8025 (0.8020)  loss_scale: 65536.0000 (115516.7626)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
Epoch: [27]  [ 780/1349]  eta: 0:02:58  lr: 0.000249  min_lr: 0.000006  loss: 0.7906 (0.8019)  loss_scale: 65536.0000 (114876.8041)  weight_decay: 0.0500 (0.0500)  time: 0.3081  data: 0.0002  max mem: 41808
Epoch: [27]  [ 790/1349]  eta: 0:02:55  lr: 0.000249  min_lr: 0.000006  loss: 0.7771 (0.8013)  loss_scale: 65536.0000 (114253.0265)  weight_decay: 0.0500 (0.0500)  time: 0.3083  data: 0.0002  max mem: 41808
Epoch: [27]  [ 800/1349]  eta: 0:02:52  lr: 0.000249  min_lr: 0.000006  loss: 0.7656 (0.8007)  loss_scale: 65536.0000 (113644.8240)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [27]  [ 810/1349]  eta: 0:02:48  lr: 0.000249  min_lr: 0.000006  loss: 0.8508 (0.8018)  loss_scale: 65536.0000 (113051.6202)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [27]  [ 820/1349]  eta: 0:02:45  lr: 0.000249  min_lr: 0.000006  loss: 0.7468 (0.8002)  loss_scale: 65536.0000 (112472.8672)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
Epoch: [27]  [ 830/1349]  eta: 0:02:42  lr: 0.000248  min_lr: 0.000006  loss: 0.7468 (0.8011)  loss_scale: 65536.0000 (111908.0433)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
[2025-05-23 20:42:05,851] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 20:42:05,851] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-05-23 20:42:05,851] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 20:42:05,851] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [27]  [ 840/1349]  eta: 0:02:39  lr: 0.000248  min_lr: 0.000006  loss: 0.8900 (0.8017)  loss_scale: 65536.0000 (111824.2093)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [27]  [ 850/1349]  eta: 0:02:36  lr: 0.000248  min_lr: 0.000006  loss: 0.8974 (0.8021)  loss_scale: 131072.0000 (112050.3878)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
Epoch: [27]  [ 860/1349]  eta: 0:02:33  lr: 0.000248  min_lr: 0.000006  loss: 0.8302 (0.8013)  loss_scale: 131072.0000 (112271.3124)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
Epoch: [27]  [ 870/1349]  eta: 0:02:29  lr: 0.000248  min_lr: 0.000006  loss: 0.7324 (0.8009)  loss_scale: 131072.0000 (112487.1642)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0002  max mem: 41808
Epoch: [27]  [ 880/1349]  eta: 0:02:26  lr: 0.000248  min_lr: 0.000006  loss: 0.7853 (0.8004)  loss_scale: 131072.0000 (112698.1158)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0002  max mem: 41808
[2025-05-23 20:42:22,718] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 37313
[2025-05-23 20:42:22,718] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 37313
[2025-05-23 20:42:22,718] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 20:42:22,718] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 20:42:22,718] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [27]  [ 890/1349]  eta: 0:02:23  lr: 0.000248  min_lr: 0.000006  loss: 0.8202 (0.8018)  loss_scale: 131072.0000 (112830.7789)  weight_decay: 0.0500 (0.0500)  time: 0.3062  data: 0.0001  max mem: 41808
Epoch: [27]  [ 900/1349]  eta: 0:02:20  lr: 0.000248  min_lr: 0.000006  loss: 0.8578 (0.8017)  loss_scale: 65536.0000 (112305.8646)  weight_decay: 0.0500 (0.0500)  time: 0.3080  data: 0.0001  max mem: 41808
Epoch: [27]  [ 910/1349]  eta: 0:02:17  lr: 0.000247  min_lr: 0.000006  loss: 0.8441 (0.8020)  loss_scale: 65536.0000 (111792.4742)  weight_decay: 0.0500 (0.0500)  time: 0.3084  data: 0.0001  max mem: 41808
Epoch: [27]  [ 920/1349]  eta: 0:02:14  lr: 0.000247  min_lr: 0.000006  loss: 0.7930 (0.8019)  loss_scale: 65536.0000 (111290.2324)  weight_decay: 0.0500 (0.0500)  time: 0.3060  data: 0.0001  max mem: 41808
Epoch: [27]  [ 930/1349]  eta: 0:02:10  lr: 0.000247  min_lr: 0.000006  loss: 0.8743 (0.8026)  loss_scale: 65536.0000 (110798.7798)  weight_decay: 0.0500 (0.0500)  time: 0.3059  data: 0.0001  max mem: 41808
Epoch: [27]  [ 940/1349]  eta: 0:02:07  lr: 0.000247  min_lr: 0.000006  loss: 0.8796 (0.8034)  loss_scale: 65536.0000 (110317.7726)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0001  max mem: 41808
Epoch: [27]  [ 950/1349]  eta: 0:02:04  lr: 0.000247  min_lr: 0.000006  loss: 0.8381 (0.8031)  loss_scale: 65536.0000 (109846.8812)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0001  max mem: 41808
Epoch: [27]  [ 960/1349]  eta: 0:02:01  lr: 0.000247  min_lr: 0.000006  loss: 0.8161 (0.8033)  loss_scale: 65536.0000 (109385.7898)  weight_decay: 0.0500 (0.0500)  time: 0.3066  data: 0.0001  max mem: 41808
Epoch: [27]  [ 970/1349]  eta: 0:01:58  lr: 0.000247  min_lr: 0.000006  loss: 0.8118 (0.8034)  loss_scale: 65536.0000 (108934.1957)  weight_decay: 0.0500 (0.0500)  time: 0.3064  data: 0.0001  max mem: 41808
Epoch: [27]  [ 980/1349]  eta: 0:01:55  lr: 0.000247  min_lr: 0.000006  loss: 0.8265 (0.8033)  loss_scale: 65536.0000 (108491.8084)  weight_decay: 0.0500 (0.0500)  time: 0.3061  data: 0.0001  max mem: 41808
Epoch: [27]  [ 990/1349]  eta: 0:01:52  lr: 0.000246  min_lr: 0.000006  loss: 0.8265 (0.8026)  loss_scale: 65536.0000 (108058.3491)  weight_decay: 0.0500 (0.0500)  time: 0.3063  data: 0.0002  max mem: 41808
Epoch: [27]  [1000/1349]  eta: 0:01:48  lr: 0.000246  min_lr: 0.000006  loss: 0.7466 (0.8022)  loss_scale: 65536.0000 (107633.5504)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0001  max mem: 41808
Epoch: [27]  [1010/1349]  eta: 0:01:45  lr: 0.000246  min_lr: 0.000006  loss: 0.7531 (0.8023)  loss_scale: 65536.0000 (107217.1553)  weight_decay: 0.0500 (0.0500)  time: 0.3064  data: 0.0001  max mem: 41808
[2025-05-23 20:43:02,287] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 20:43:02,287] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-05-23 20:43:02,287] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 20:43:02,287] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-05-23 20:43:02,590] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 37443
[2025-05-23 20:43:02,590] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 37443
[2025-05-23 20:43:02,590] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 20:43:02,590] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 20:43:02,590] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [27]  [1020/1349]  eta: 0:01:42  lr: 0.000246  min_lr: 0.000006  loss: 0.7787 (0.8019)  loss_scale: 65536.0000 (106873.1048)  weight_decay: 0.0500 (0.0500)  time: 0.3062  data: 0.0001  max mem: 41808
Epoch: [27]  [1030/1349]  eta: 0:01:39  lr: 0.000246  min_lr: 0.000006  loss: 0.7498 (0.8017)  loss_scale: 65536.0000 (106472.1629)  weight_decay: 0.0500 (0.0500)  time: 0.3064  data: 0.0001  max mem: 41808
Epoch: [27]  [1040/1349]  eta: 0:01:36  lr: 0.000246  min_lr: 0.000006  loss: 0.8675 (0.8027)  loss_scale: 65536.0000 (106078.9241)  weight_decay: 0.0500 (0.0500)  time: 0.3066  data: 0.0001  max mem: 41808
Epoch: [27]  [1050/1349]  eta: 0:01:33  lr: 0.000246  min_lr: 0.000006  loss: 0.8997 (0.8034)  loss_scale: 65536.0000 (105693.1684)  weight_decay: 0.0500 (0.0500)  time: 0.3062  data: 0.0001  max mem: 41808
Epoch: [27]  [1060/1349]  eta: 0:01:30  lr: 0.000246  min_lr: 0.000006  loss: 0.8285 (0.8029)  loss_scale: 65536.0000 (105314.6843)  weight_decay: 0.0500 (0.0500)  time: 0.3066  data: 0.0001  max mem: 41808
Epoch: [27]  [1070/1349]  eta: 0:01:26  lr: 0.000245  min_lr: 0.000006  loss: 0.8269 (0.8031)  loss_scale: 65536.0000 (104943.2680)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
Epoch: [27]  [1080/1349]  eta: 0:01:23  lr: 0.000245  min_lr: 0.000006  loss: 0.8269 (0.8036)  loss_scale: 65536.0000 (104578.7234)  weight_decay: 0.0500 (0.0500)  time: 0.3060  data: 0.0001  max mem: 41808
Epoch: [27]  [1090/1349]  eta: 0:01:20  lr: 0.000245  min_lr: 0.000006  loss: 0.8189 (0.8037)  loss_scale: 65536.0000 (104220.8616)  weight_decay: 0.0500 (0.0500)  time: 0.3060  data: 0.0001  max mem: 41808
Epoch: [27]  [1100/1349]  eta: 0:01:17  lr: 0.000245  min_lr: 0.000006  loss: 0.8390 (0.8039)  loss_scale: 65536.0000 (103869.5005)  weight_decay: 0.0500 (0.0500)  time: 0.3060  data: 0.0001  max mem: 41808
Epoch: [27]  [1110/1349]  eta: 0:01:14  lr: 0.000245  min_lr: 0.000006  loss: 0.8097 (0.8037)  loss_scale: 65536.0000 (103524.4644)  weight_decay: 0.0500 (0.0500)  time: 0.3060  data: 0.0002  max mem: 41808
Epoch: [27]  [1120/1349]  eta: 0:01:11  lr: 0.000245  min_lr: 0.000006  loss: 0.8097 (0.8042)  loss_scale: 65536.0000 (103185.5843)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0002  max mem: 41808
Epoch: [27]  [1130/1349]  eta: 0:01:08  lr: 0.000245  min_lr: 0.000006  loss: 0.8248 (0.8042)  loss_scale: 65536.0000 (102852.6967)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
Epoch: [27]  [1140/1349]  eta: 0:01:05  lr: 0.000244  min_lr: 0.000006  loss: 0.8392 (0.8049)  loss_scale: 65536.0000 (102525.6442)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
[2025-05-23 20:43:42,126] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 20:43:42,126] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-05-23 20:43:42,126] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 20:43:42,126] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [27]  [1150/1349]  eta: 0:01:01  lr: 0.000244  min_lr: 0.000006  loss: 0.8091 (0.8041)  loss_scale: 65536.0000 (102318.1512)  weight_decay: 0.0500 (0.0500)  time: 0.3064  data: 0.0001  max mem: 41808
Epoch: [27]  [1160/1349]  eta: 0:00:58  lr: 0.000244  min_lr: 0.000006  loss: 0.7490 (0.8041)  loss_scale: 131072.0000 (102565.8157)  weight_decay: 0.0500 (0.0500)  time: 0.3060  data: 0.0001  max mem: 41808
Epoch: [27]  [1170/1349]  eta: 0:00:55  lr: 0.000244  min_lr: 0.000006  loss: 0.7922 (0.8044)  loss_scale: 131072.0000 (102809.2502)  weight_decay: 0.0500 (0.0500)  time: 0.3061  data: 0.0001  max mem: 41808
Epoch: [27]  [1180/1349]  eta: 0:00:52  lr: 0.000244  min_lr: 0.000006  loss: 0.7884 (0.8040)  loss_scale: 131072.0000 (103048.5622)  weight_decay: 0.0500 (0.0500)  time: 0.3061  data: 0.0001  max mem: 41808
Epoch: [27]  [1190/1349]  eta: 0:00:49  lr: 0.000244  min_lr: 0.000006  loss: 0.8281 (0.8041)  loss_scale: 131072.0000 (103283.8556)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0001  max mem: 41808
Epoch: [27]  [1200/1349]  eta: 0:00:46  lr: 0.000244  min_lr: 0.000006  loss: 0.8281 (0.8037)  loss_scale: 131072.0000 (103515.2306)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
Epoch: [27]  [1210/1349]  eta: 0:00:43  lr: 0.000244  min_lr: 0.000006  loss: 0.7602 (0.8033)  loss_scale: 131072.0000 (103742.7845)  weight_decay: 0.0500 (0.0500)  time: 0.3082  data: 0.0002  max mem: 41808
Epoch: [27]  [1220/1349]  eta: 0:00:40  lr: 0.000243  min_lr: 0.000006  loss: 0.7602 (0.8028)  loss_scale: 131072.0000 (103966.6110)  weight_decay: 0.0500 (0.0500)  time: 0.3090  data: 0.0002  max mem: 41808
Epoch: [27]  [1230/1349]  eta: 0:00:37  lr: 0.000243  min_lr: 0.000006  loss: 0.7609 (0.8025)  loss_scale: 131072.0000 (104186.8010)  weight_decay: 0.0500 (0.0500)  time: 0.3097  data: 0.0002  max mem: 41808
[2025-05-23 20:44:10,161] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 37663
[2025-05-23 20:44:10,161] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 37663
[2025-05-23 20:44:10,161] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 20:44:10,161] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 20:44:10,161] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [27]  [1240/1349]  eta: 0:00:33  lr: 0.000243  min_lr: 0.000006  loss: 0.7914 (0.8030)  loss_scale: 131072.0000 (104350.6334)  weight_decay: 0.0500 (0.0500)  time: 0.3106  data: 0.0002  max mem: 41808
Epoch: [27]  [1250/1349]  eta: 0:00:30  lr: 0.000243  min_lr: 0.000006  loss: 0.8436 (0.8031)  loss_scale: 65536.0000 (104040.3645)  weight_decay: 0.0500 (0.0500)  time: 0.3083  data: 0.0002  max mem: 41808
Epoch: [27]  [1260/1349]  eta: 0:00:27  lr: 0.000243  min_lr: 0.000006  loss: 0.8436 (0.8033)  loss_scale: 65536.0000 (103735.0167)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
Epoch: [27]  [1270/1349]  eta: 0:00:24  lr: 0.000243  min_lr: 0.000006  loss: 0.8364 (0.8034)  loss_scale: 65536.0000 (103434.4736)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
Epoch: [27]  [1280/1349]  eta: 0:00:21  lr: 0.000243  min_lr: 0.000006  loss: 0.7839 (0.8029)  loss_scale: 65536.0000 (103138.6230)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [27]  [1290/1349]  eta: 0:00:18  lr: 0.000243  min_lr: 0.000006  loss: 0.7839 (0.8029)  loss_scale: 65536.0000 (102847.3555)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [27]  [1300/1349]  eta: 0:00:15  lr: 0.000242  min_lr: 0.000006  loss: 0.8152 (0.8029)  loss_scale: 65536.0000 (102560.5657)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [27]  [1310/1349]  eta: 0:00:12  lr: 0.000242  min_lr: 0.000006  loss: 0.7249 (0.8021)  loss_scale: 65536.0000 (102278.1510)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [27]  [1320/1349]  eta: 0:00:09  lr: 0.000242  min_lr: 0.000006  loss: 0.7259 (0.8021)  loss_scale: 65536.0000 (102000.0121)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0002  max mem: 41808
Epoch: [27]  [1330/1349]  eta: 0:00:05  lr: 0.000242  min_lr: 0.000006  loss: 0.7687 (0.8015)  loss_scale: 65536.0000 (101726.0526)  weight_decay: 0.0500 (0.0500)  time: 0.3050  data: 0.0002  max mem: 41808
Epoch: [27]  [1340/1349]  eta: 0:00:02  lr: 0.000242  min_lr: 0.000006  loss: 0.6830 (0.8006)  loss_scale: 65536.0000 (101456.1790)  weight_decay: 0.0500 (0.0500)  time: 0.3027  data: 0.0001  max mem: 41808
Epoch: [27]  [1348/1349]  eta: 0:00:00  lr: 0.000242  min_lr: 0.000006  loss: 0.7138 (0.8004)  loss_scale: 65536.0000 (101243.1609)  weight_decay: 0.0500 (0.0500)  time: 0.3022  data: 0.0001  max mem: 41808
Epoch: [27] Total time: 0:06:59 (0.3111 s / it)
Averaged stats: lr: 0.000242  min_lr: 0.000006  loss: 0.7138 (0.7982)  loss_scale: 65536.0000 (101243.1609)  weight_decay: 0.0500 (0.0500)  total_time: 419.6785 (419.6745)
Val:  [  0/346]  eta: 1:06:49  loss: 3.1895 (3.1895)  acc1: 4.6875 (4.6875)  acc5: 86.7188 (86.7188)  time: 11.5874  data: 10.5387  max mem: 41808
Val:  [ 10/346]  eta: 0:12:14  loss: 0.1318 (0.6011)  acc1: 100.0000 (85.1562)  acc5: 100.0000 (98.6506)  time: 2.1866  data: 1.3284  max mem: 41808
Val:  [ 20/346]  eta: 0:08:22  loss: 0.1245 (0.4693)  acc1: 100.0000 (88.5045)  acc5: 100.0000 (99.0327)  time: 1.0388  data: 0.2039  max mem: 41808
Val:  [ 30/346]  eta: 0:06:56  loss: 0.1028 (0.3906)  acc1: 100.0000 (90.7006)  acc5: 100.0000 (99.3196)  time: 0.8385  data: 0.0003  max mem: 41808
Val:  [ 40/346]  eta: 0:06:08  loss: 0.1025 (0.4243)  acc1: 100.0000 (90.1486)  acc5: 100.0000 (98.2851)  time: 0.8485  data: 0.0003  max mem: 41808
Val:  [ 50/346]  eta: 0:05:35  loss: 0.1122 (0.3739)  acc1: 99.2188 (91.7126)  acc5: 100.0000 (98.5907)  time: 0.8467  data: 0.0003  max mem: 41808
Val:  [ 60/346]  eta: 0:05:09  loss: 0.1526 (0.3644)  acc1: 98.4375 (91.6240)  acc5: 100.0000 (98.8217)  time: 0.8352  data: 0.0003  max mem: 41808
Val:  [ 70/346]  eta: 0:04:49  loss: 0.2194 (0.3866)  acc1: 95.3125 (90.9331)  acc5: 100.0000 (98.8116)  time: 0.8308  data: 0.0003  max mem: 41808
Val:  [ 80/346]  eta: 0:04:32  loss: 0.2354 (0.3877)  acc1: 92.1875 (90.9529)  acc5: 100.0000 (98.6304)  time: 0.8444  data: 0.0003  max mem: 41808
Val:  [ 90/346]  eta: 0:04:17  loss: 0.3267 (0.4020)  acc1: 91.4062 (90.4876)  acc5: 100.0000 (98.6779)  time: 0.8569  data: 0.0003  max mem: 41808
Val:  [100/346]  eta: 0:04:03  loss: 0.1960 (0.3797)  acc1: 96.0938 (91.2129)  acc5: 100.0000 (98.8088)  time: 0.8559  data: 0.0003  max mem: 41808
Val:  [110/346]  eta: 0:03:51  loss: 0.1791 (0.3851)  acc1: 96.8750 (91.0543)  acc5: 100.0000 (98.7683)  time: 0.8587  data: 0.0003  max mem: 41808
Val:  [120/346]  eta: 0:03:39  loss: 0.1895 (0.3889)  acc1: 96.0938 (90.9285)  acc5: 100.0000 (98.8572)  time: 0.8612  data: 0.0003  max mem: 41808
Val:  [130/346]  eta: 0:03:27  loss: 0.1036 (0.3974)  acc1: 99.2188 (90.5833)  acc5: 100.0000 (98.9206)  time: 0.8490  data: 0.0003  max mem: 41808
Val:  [140/346]  eta: 0:03:15  loss: 0.1963 (0.3905)  acc1: 96.8750 (90.7746)  acc5: 100.0000 (98.9805)  time: 0.8431  data: 0.0003  max mem: 41808
Val:  [150/346]  eta: 0:03:05  loss: 0.2138 (0.3898)  acc1: 95.3125 (90.8526)  acc5: 100.0000 (98.9549)  time: 0.8609  data: 0.0003  max mem: 41808
Val:  [160/346]  eta: 0:02:55  loss: 0.1605 (0.3832)  acc1: 97.6562 (91.0132)  acc5: 100.0000 (98.9955)  time: 0.8715  data: 0.0003  max mem: 41808
Val:  [170/346]  eta: 0:02:44  loss: 0.1478 (0.3777)  acc1: 97.6562 (91.0910)  acc5: 100.0000 (99.0543)  time: 0.8680  data: 0.0003  max mem: 41808
Val:  [180/346]  eta: 0:02:34  loss: 0.1719 (0.3894)  acc1: 92.1875 (90.5862)  acc5: 100.0000 (99.1065)  time: 0.8667  data: 0.0003  max mem: 41808
Val:  [190/346]  eta: 0:02:25  loss: 0.3310 (0.3901)  acc1: 89.8438 (90.5596)  acc5: 100.0000 (99.1083)  time: 0.8690  data: 0.0003  max mem: 41808
Val:  [200/346]  eta: 0:02:15  loss: 0.4526 (0.4083)  acc1: 85.9375 (89.9215)  acc5: 100.0000 (99.1488)  time: 0.8671  data: 0.0003  max mem: 41808
Val:  [210/346]  eta: 0:02:05  loss: 0.1579 (0.4013)  acc1: 97.6562 (90.1770)  acc5: 100.0000 (99.1891)  time: 0.8654  data: 0.0003  max mem: 41808
Val:  [220/346]  eta: 0:01:56  loss: 0.1293 (0.3963)  acc1: 97.6562 (90.3387)  acc5: 100.0000 (99.2011)  time: 0.8732  data: 0.0003  max mem: 41808
Val:  [230/346]  eta: 0:01:46  loss: 0.1719 (0.3878)  acc1: 96.0938 (90.6149)  acc5: 100.0000 (99.2221)  time: 0.8795  data: 0.0003  max mem: 41808
Val:  [240/346]  eta: 0:01:37  loss: 0.1252 (0.3908)  acc1: 100.0000 (90.6185)  acc5: 100.0000 (99.1993)  time: 0.8802  data: 0.0003  max mem: 41808
Val:  [250/346]  eta: 0:01:27  loss: 0.1546 (0.3915)  acc1: 98.4375 (90.6686)  acc5: 100.0000 (99.2001)  time: 0.8716  data: 0.0003  max mem: 41808
Val:  [260/346]  eta: 0:01:18  loss: 0.1407 (0.3893)  acc1: 98.4375 (90.7268)  acc5: 100.0000 (99.2038)  time: 0.8750  data: 0.0002  max mem: 41808
Val:  [270/346]  eta: 0:01:09  loss: 0.1407 (0.3846)  acc1: 97.6562 (90.8729)  acc5: 100.0000 (99.2274)  time: 0.8763  data: 0.0002  max mem: 41808
Val:  [280/346]  eta: 0:01:00  loss: 0.1266 (0.3823)  acc1: 97.6562 (90.9837)  acc5: 100.0000 (99.2076)  time: 0.8723  data: 0.0002  max mem: 41808
Val:  [290/346]  eta: 0:00:51  loss: 0.1014 (0.3734)  acc1: 100.0000 (91.2559)  acc5: 100.0000 (99.2349)  time: 0.8790  data: 0.0003  max mem: 41808
Val:  [300/346]  eta: 0:00:41  loss: 0.1024 (0.3730)  acc1: 100.0000 (91.3154)  acc5: 100.0000 (99.2395)  time: 0.8694  data: 0.0003  max mem: 41808
Val:  [310/346]  eta: 0:00:32  loss: 0.1092 (0.3742)  acc1: 99.2188 (91.2706)  acc5: 100.0000 (99.2238)  time: 0.8582  data: 0.0003  max mem: 41808
Val:  [320/346]  eta: 0:00:23  loss: 0.1140 (0.3725)  acc1: 99.2188 (91.3065)  acc5: 100.0000 (99.2480)  time: 0.8502  data: 0.0003  max mem: 41808
Val:  [330/346]  eta: 0:00:14  loss: 0.3596 (0.3860)  acc1: 89.8438 (90.9507)  acc5: 100.0000 (99.1149)  time: 0.8467  data: 0.0002  max mem: 41808
Val:  [340/346]  eta: 0:00:05  loss: 0.4579 (0.3935)  acc1: 88.2812 (90.7258)  acc5: 100.0000 (99.1294)  time: 0.8124  data: 0.0001  max mem: 41808
Val:  [345/346]  eta: 0:00:00  loss: 0.1358 (0.3896)  acc1: 95.4023 (90.8378)  acc5: 100.0000 (99.1412)  time: 0.7706  data: 0.0001  max mem: 41808
Val: Total time: 0:05:10 (0.8982 s / it)
* Acc@1 90.911 Acc@5 99.162 loss 0.387
Accuracy of the network on the 88494 val videos: 90.9%
Max accuracy: 91.50%   Max Epoch: 25
Epoch: [28]  [   0/1349]  eta: 1:33:06  lr: 0.000242  min_lr: 0.000006  loss: 0.8849 (0.8849)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 4.1414  data: 3.8024  max mem: 41808
Epoch: [28]  [  10/1349]  eta: 0:16:49  lr: 0.000242  min_lr: 0.000006  loss: 0.8696 (0.7847)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7540  data: 0.3458  max mem: 41808
[2025-05-23 20:50:05,730] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 20:50:05,730] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-05-23 20:50:05,730] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 20:50:05,730] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [28]  [  20/1349]  eta: 0:12:00  lr: 0.000242  min_lr: 0.000006  loss: 0.8696 (0.8120)  loss_scale: 65536.0000 (68656.7619)  weight_decay: 0.0500 (0.0500)  time: 0.3623  data: 0.0001  max mem: 41808
Epoch: [28]  [  30/1349]  eta: 0:10:15  lr: 0.000241  min_lr: 0.000006  loss: 0.8968 (0.8461)  loss_scale: 131072.0000 (88790.7097)  weight_decay: 0.0500 (0.0500)  time: 0.3089  data: 0.0001  max mem: 41808
Epoch: [28]  [  40/1349]  eta: 0:09:23  lr: 0.000241  min_lr: 0.000006  loss: 0.9194 (0.8534)  loss_scale: 131072.0000 (99103.2195)  weight_decay: 0.0500 (0.0500)  time: 0.3138  data: 0.0001  max mem: 41808
Epoch: [28]  [  50/1349]  eta: 0:08:48  lr: 0.000241  min_lr: 0.000006  loss: 0.8775 (0.8448)  loss_scale: 131072.0000 (105371.6078)  weight_decay: 0.0500 (0.0500)  time: 0.3134  data: 0.0001  max mem: 41808
Epoch: [28]  [  60/1349]  eta: 0:08:23  lr: 0.000241  min_lr: 0.000006  loss: 0.8397 (0.8399)  loss_scale: 131072.0000 (109584.7869)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [28]  [  70/1349]  eta: 0:08:04  lr: 0.000241  min_lr: 0.000006  loss: 0.7835 (0.8275)  loss_scale: 131072.0000 (112611.1549)  weight_decay: 0.0500 (0.0500)  time: 0.3085  data: 0.0002  max mem: 41808
Epoch: [28]  [  80/1349]  eta: 0:07:49  lr: 0.000241  min_lr: 0.000006  loss: 0.7932 (0.8242)  loss_scale: 131072.0000 (114890.2716)  weight_decay: 0.0500 (0.0500)  time: 0.3082  data: 0.0002  max mem: 41808
Epoch: [28]  [  90/1349]  eta: 0:07:37  lr: 0.000241  min_lr: 0.000006  loss: 0.7932 (0.8160)  loss_scale: 131072.0000 (116668.4835)  weight_decay: 0.0500 (0.0500)  time: 0.3063  data: 0.0002  max mem: 41808
Epoch: [28]  [ 100/1349]  eta: 0:07:26  lr: 0.000241  min_lr: 0.000006  loss: 0.8049 (0.8191)  loss_scale: 131072.0000 (118094.5743)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
Epoch: [28]  [ 110/1349]  eta: 0:07:17  lr: 0.000240  min_lr: 0.000006  loss: 0.8138 (0.8196)  loss_scale: 131072.0000 (119263.7117)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
Epoch: [28]  [ 120/1349]  eta: 0:07:08  lr: 0.000240  min_lr: 0.000006  loss: 0.8468 (0.8178)  loss_scale: 131072.0000 (120239.6033)  weight_decay: 0.0500 (0.0500)  time: 0.3061  data: 0.0002  max mem: 41808
Epoch: [28]  [ 130/1349]  eta: 0:07:01  lr: 0.000240  min_lr: 0.000006  loss: 0.8468 (0.8221)  loss_scale: 131072.0000 (121066.5038)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0002  max mem: 41808
Epoch: [28]  [ 140/1349]  eta: 0:06:54  lr: 0.000240  min_lr: 0.000006  loss: 0.8223 (0.8194)  loss_scale: 131072.0000 (121776.1135)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0002  max mem: 41808
[2025-05-23 20:50:45,193] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 20:50:45,193] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 20:50:45,193] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 20:50:45,193] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [28]  [ 150/1349]  eta: 0:06:48  lr: 0.000240  min_lr: 0.000006  loss: 0.8211 (0.8213)  loss_scale: 131072.0000 (124995.8146)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
Epoch: [28]  [ 160/1349]  eta: 0:06:42  lr: 0.000240  min_lr: 0.000006  loss: 0.8234 (0.8181)  loss_scale: 262144.0000 (133514.3354)  weight_decay: 0.0500 (0.0500)  time: 0.3062  data: 0.0001  max mem: 41808
Epoch: [28]  [ 170/1349]  eta: 0:06:36  lr: 0.000240  min_lr: 0.000006  loss: 0.8571 (0.8226)  loss_scale: 262144.0000 (141036.5380)  weight_decay: 0.0500 (0.0500)  time: 0.3056  data: 0.0001  max mem: 41808
Epoch: [28]  [ 180/1349]  eta: 0:06:31  lr: 0.000239  min_lr: 0.000006  loss: 0.8418 (0.8204)  loss_scale: 262144.0000 (147727.5580)  weight_decay: 0.0500 (0.0500)  time: 0.3061  data: 0.0001  max mem: 41808
[2025-05-23 20:50:57,442] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 37960
[2025-05-23 20:50:57,442] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 37960
[2025-05-23 20:50:57,442] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 20:50:57,442] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 20:50:57,442] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [28]  [ 190/1349]  eta: 0:06:26  lr: 0.000239  min_lr: 0.000006  loss: 0.8418 (0.8228)  loss_scale: 262144.0000 (151659.2251)  weight_decay: 0.0500 (0.0500)  time: 0.3064  data: 0.0001  max mem: 41808
Epoch: [28]  [ 200/1349]  eta: 0:06:21  lr: 0.000239  min_lr: 0.000006  loss: 0.8573 (0.8215)  loss_scale: 131072.0000 (150634.9851)  weight_decay: 0.0500 (0.0500)  time: 0.3066  data: 0.0001  max mem: 41808
Epoch: [28]  [ 210/1349]  eta: 0:06:16  lr: 0.000239  min_lr: 0.000006  loss: 0.8014 (0.8210)  loss_scale: 131072.0000 (149707.8294)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
Epoch: [28]  [ 220/1349]  eta: 0:06:12  lr: 0.000239  min_lr: 0.000006  loss: 0.7849 (0.8159)  loss_scale: 131072.0000 (148864.5792)  weight_decay: 0.0500 (0.0500)  time: 0.3086  data: 0.0001  max mem: 41808
[2025-05-23 20:51:09,445] [INFO] [logging.py:96:log_dist] [Rank 0] step=38000, skipped=231, lr=[5.674754846743078e-06, 5.674754846743078e-06, 7.566339795657437e-06, 7.566339795657437e-06, 1.0088453060876583e-05, 1.0088453060876583e-05, 1.3451270747835444e-05, 1.3451270747835444e-05, 1.7935027663780592e-05, 1.7935027663780592e-05, 2.391337021837412e-05, 2.391337021837412e-05, 3.188449362449883e-05, 3.188449362449883e-05, 4.251265816599844e-05, 4.251265816599844e-05, 5.6683544221331255e-05, 5.6683544221331255e-05, 7.557805896177501e-05, 7.557805896177501e-05, 0.00010077074528236668, 0.00010077074528236668, 0.00013436099370982223, 0.00013436099370982223, 0.00017914799161309632, 0.00017914799161309632, 0.00023886398881746174, 0.00023886398881746174], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-05-23 20:51:09,445] [INFO] [timer.py:260:stop] epoch=0/micro_step=38000/global_step=38000, RunningAvgSamplesPerSec=209.45952033877666, CurrSamplesPerSec=214.04697567012414, MemAllocated=1.37GB, MaxMemAllocated=40.83GB
Epoch: [28]  [ 230/1349]  eta: 0:06:08  lr: 0.000239  min_lr: 0.000006  loss: 0.7835 (0.8149)  loss_scale: 131072.0000 (148094.3377)  weight_decay: 0.0500 (0.0500)  time: 0.3084  data: 0.0001  max mem: 41808
Epoch: [28]  [ 240/1349]  eta: 0:06:03  lr: 0.000239  min_lr: 0.000006  loss: 0.7718 (0.8109)  loss_scale: 131072.0000 (147388.0166)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
[2025-05-23 20:51:14,067] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 38014
[2025-05-23 20:51:14,067] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 38014
[2025-05-23 20:51:14,067] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 20:51:14,067] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 20:51:14,068] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [28]  [ 250/1349]  eta: 0:05:59  lr: 0.000239  min_lr: 0.000006  loss: 0.8138 (0.8133)  loss_scale: 131072.0000 (144388.0797)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
Epoch: [28]  [ 260/1349]  eta: 0:05:55  lr: 0.000238  min_lr: 0.000006  loss: 0.8814 (0.8144)  loss_scale: 65536.0000 (141366.9272)  weight_decay: 0.0500 (0.0500)  time: 0.3059  data: 0.0001  max mem: 41808
Epoch: [28]  [ 270/1349]  eta: 0:05:51  lr: 0.000238  min_lr: 0.000006  loss: 0.8077 (0.8136)  loss_scale: 65536.0000 (138568.7380)  weight_decay: 0.0500 (0.0500)  time: 0.3062  data: 0.0001  max mem: 41808
Epoch: [28]  [ 280/1349]  eta: 0:05:47  lr: 0.000238  min_lr: 0.000006  loss: 0.7850 (0.8112)  loss_scale: 65536.0000 (135969.7082)  weight_decay: 0.0500 (0.0500)  time: 0.3059  data: 0.0001  max mem: 41808
Epoch: [28]  [ 290/1349]  eta: 0:05:43  lr: 0.000238  min_lr: 0.000006  loss: 0.7791 (0.8104)  loss_scale: 65536.0000 (133549.3058)  weight_decay: 0.0500 (0.0500)  time: 0.3056  data: 0.0001  max mem: 41808
Epoch: [28]  [ 300/1349]  eta: 0:05:39  lr: 0.000238  min_lr: 0.000006  loss: 0.8074 (0.8113)  loss_scale: 65536.0000 (131289.7276)  weight_decay: 0.0500 (0.0500)  time: 0.3057  data: 0.0001  max mem: 41808
Epoch: [28]  [ 310/1349]  eta: 0:05:35  lr: 0.000238  min_lr: 0.000006  loss: 0.8030 (0.8131)  loss_scale: 65536.0000 (129175.4598)  weight_decay: 0.0500 (0.0500)  time: 0.3059  data: 0.0001  max mem: 41808
Epoch: [28]  [ 320/1349]  eta: 0:05:32  lr: 0.000238  min_lr: 0.000006  loss: 0.8030 (0.8140)  loss_scale: 65536.0000 (127192.9221)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
Epoch: [28]  [ 330/1349]  eta: 0:05:28  lr: 0.000238  min_lr: 0.000006  loss: 0.7971 (0.8133)  loss_scale: 65536.0000 (125330.1752)  weight_decay: 0.0500 (0.0500)  time: 0.3083  data: 0.0001  max mem: 41808
Epoch: [28]  [ 340/1349]  eta: 0:05:24  lr: 0.000237  min_lr: 0.000006  loss: 0.7803 (0.8134)  loss_scale: 65536.0000 (123576.6804)  weight_decay: 0.0500 (0.0500)  time: 0.3080  data: 0.0001  max mem: 41808
Epoch: [28]  [ 350/1349]  eta: 0:05:21  lr: 0.000237  min_lr: 0.000006  loss: 0.8245 (0.8130)  loss_scale: 65536.0000 (121923.0997)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [28]  [ 360/1349]  eta: 0:05:17  lr: 0.000237  min_lr: 0.000006  loss: 0.8245 (0.8115)  loss_scale: 65536.0000 (120361.1302)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
Epoch: [28]  [ 370/1349]  eta: 0:05:13  lr: 0.000237  min_lr: 0.000006  loss: 0.6971 (0.8077)  loss_scale: 65536.0000 (118883.3639)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
[2025-05-23 20:51:53,637] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 20:51:53,637] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-05-23 20:51:53,638] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 20:51:53,638] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [28]  [ 380/1349]  eta: 0:05:10  lr: 0.000237  min_lr: 0.000006  loss: 0.7453 (0.8076)  loss_scale: 65536.0000 (119203.2756)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
Epoch: [28]  [ 390/1349]  eta: 0:05:06  lr: 0.000237  min_lr: 0.000006  loss: 0.7992 (0.8078)  loss_scale: 131072.0000 (119506.8235)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
Epoch: [28]  [ 400/1349]  eta: 0:05:03  lr: 0.000237  min_lr: 0.000006  loss: 0.8382 (0.8078)  loss_scale: 131072.0000 (119795.2319)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
Epoch: [28]  [ 410/1349]  eta: 0:04:59  lr: 0.000237  min_lr: 0.000006  loss: 0.7534 (0.8055)  loss_scale: 131072.0000 (120069.6058)  weight_decay: 0.0500 (0.0500)  time: 0.3057  data: 0.0001  max mem: 41808
Epoch: [28]  [ 420/1349]  eta: 0:04:56  lr: 0.000236  min_lr: 0.000006  loss: 0.7196 (0.8045)  loss_scale: 131072.0000 (120330.9454)  weight_decay: 0.0500 (0.0500)  time: 0.3056  data: 0.0001  max mem: 41808
Epoch: [28]  [ 430/1349]  eta: 0:04:52  lr: 0.000236  min_lr: 0.000006  loss: 0.8149 (0.8047)  loss_scale: 131072.0000 (120580.1578)  weight_decay: 0.0500 (0.0500)  time: 0.3062  data: 0.0001  max mem: 41808
Epoch: [28]  [ 440/1349]  eta: 0:04:49  lr: 0.000236  min_lr: 0.000006  loss: 0.7893 (0.8035)  loss_scale: 131072.0000 (120818.0680)  weight_decay: 0.0500 (0.0500)  time: 0.3064  data: 0.0001  max mem: 41808
Epoch: [28]  [ 450/1349]  eta: 0:04:45  lr: 0.000236  min_lr: 0.000006  loss: 0.7834 (0.8020)  loss_scale: 131072.0000 (121045.4279)  weight_decay: 0.0500 (0.0500)  time: 0.3061  data: 0.0001  max mem: 41808
Epoch: [28]  [ 460/1349]  eta: 0:04:42  lr: 0.000236  min_lr: 0.000006  loss: 0.8170 (0.8024)  loss_scale: 131072.0000 (121262.9241)  weight_decay: 0.0500 (0.0500)  time: 0.3060  data: 0.0001  max mem: 41808
Epoch: [28]  [ 470/1349]  eta: 0:04:39  lr: 0.000236  min_lr: 0.000006  loss: 0.8416 (0.8021)  loss_scale: 131072.0000 (121471.1847)  weight_decay: 0.0500 (0.0500)  time: 0.3059  data: 0.0001  max mem: 41808
Epoch: [28]  [ 480/1349]  eta: 0:04:35  lr: 0.000236  min_lr: 0.000006  loss: 0.8531 (0.8032)  loss_scale: 131072.0000 (121670.7859)  weight_decay: 0.0500 (0.0500)  time: 0.3057  data: 0.0001  max mem: 41808
Epoch: [28]  [ 490/1349]  eta: 0:04:32  lr: 0.000235  min_lr: 0.000006  loss: 0.8556 (0.8032)  loss_scale: 131072.0000 (121862.2566)  weight_decay: 0.0500 (0.0500)  time: 0.3059  data: 0.0001  max mem: 41808
[2025-05-23 20:52:32,837] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 20:52:32,837] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 20:52:32,837] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 20:52:32,837] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [28]  [ 500/1349]  eta: 0:04:29  lr: 0.000235  min_lr: 0.000006  loss: 0.8648 (0.8047)  loss_scale: 131072.0000 (122569.3253)  weight_decay: 0.0500 (0.0500)  time: 0.3057  data: 0.0001  max mem: 41808
[2025-05-23 20:52:34,056] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 38275
[2025-05-23 20:52:34,056] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 38275
[2025-05-23 20:52:34,056] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 20:52:34,056] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 20:52:34,056] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [28]  [ 510/1349]  eta: 0:04:25  lr: 0.000235  min_lr: 0.000006  loss: 0.8485 (0.8045)  loss_scale: 131072.0000 (123248.7202)  weight_decay: 0.0500 (0.0500)  time: 0.3052  data: 0.0001  max mem: 41808
Epoch: [28]  [ 520/1349]  eta: 0:04:22  lr: 0.000235  min_lr: 0.000006  loss: 0.8018 (0.8033)  loss_scale: 131072.0000 (123398.8791)  weight_decay: 0.0500 (0.0500)  time: 0.3054  data: 0.0001  max mem: 41808
Epoch: [28]  [ 530/1349]  eta: 0:04:18  lr: 0.000235  min_lr: 0.000006  loss: 0.7859 (0.8031)  loss_scale: 131072.0000 (123543.3823)  weight_decay: 0.0500 (0.0500)  time: 0.3056  data: 0.0001  max mem: 41808
Epoch: [28]  [ 540/1349]  eta: 0:04:15  lr: 0.000235  min_lr: 0.000006  loss: 0.7859 (0.8020)  loss_scale: 131072.0000 (123682.5434)  weight_decay: 0.0500 (0.0500)  time: 0.3056  data: 0.0001  max mem: 41808
Epoch: [28]  [ 550/1349]  eta: 0:04:12  lr: 0.000235  min_lr: 0.000006  loss: 0.8238 (0.8024)  loss_scale: 131072.0000 (123816.6534)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
Epoch: [28]  [ 560/1349]  eta: 0:04:09  lr: 0.000235  min_lr: 0.000006  loss: 0.8645 (0.8034)  loss_scale: 131072.0000 (123945.9822)  weight_decay: 0.0500 (0.0500)  time: 0.3082  data: 0.0001  max mem: 41808
Epoch: [28]  [ 570/1349]  eta: 0:04:05  lr: 0.000234  min_lr: 0.000006  loss: 0.7678 (0.8020)  loss_scale: 131072.0000 (124070.7811)  weight_decay: 0.0500 (0.0500)  time: 0.3062  data: 0.0001  max mem: 41808
Epoch: [28]  [ 580/1349]  eta: 0:04:02  lr: 0.000234  min_lr: 0.000006  loss: 0.7424 (0.8013)  loss_scale: 131072.0000 (124191.2840)  weight_decay: 0.0500 (0.0500)  time: 0.3064  data: 0.0001  max mem: 41808
Epoch: [28]  [ 590/1349]  eta: 0:03:59  lr: 0.000234  min_lr: 0.000006  loss: 0.7400 (0.8009)  loss_scale: 131072.0000 (124307.7090)  weight_decay: 0.0500 (0.0500)  time: 0.3063  data: 0.0001  max mem: 41808
Epoch: [28]  [ 600/1349]  eta: 0:03:56  lr: 0.000234  min_lr: 0.000006  loss: 0.7473 (0.8004)  loss_scale: 131072.0000 (124420.2596)  weight_decay: 0.0500 (0.0500)  time: 0.3058  data: 0.0001  max mem: 41808
Epoch: [28]  [ 610/1349]  eta: 0:03:52  lr: 0.000234  min_lr: 0.000006  loss: 0.8157 (0.8014)  loss_scale: 131072.0000 (124529.1260)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [28]  [ 620/1349]  eta: 0:03:49  lr: 0.000234  min_lr: 0.000006  loss: 0.8841 (0.8023)  loss_scale: 131072.0000 (124634.4863)  weight_decay: 0.0500 (0.0500)  time: 0.3083  data: 0.0001  max mem: 41808
Epoch: [28]  [ 630/1349]  eta: 0:03:46  lr: 0.000234  min_lr: 0.000006  loss: 0.8824 (0.8028)  loss_scale: 131072.0000 (124736.5071)  weight_decay: 0.0500 (0.0500)  time: 0.3082  data: 0.0001  max mem: 41808
[2025-05-23 20:53:13,652] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 20:53:13,652] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 20:53:13,652] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 20:53:13,652] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 20:53:15,186] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 38409
[2025-05-23 20:53:15,186] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 38409
[2025-05-23 20:53:15,187] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 20:53:15,187] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 20:53:15,187] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [28]  [ 640/1349]  eta: 0:03:43  lr: 0.000234  min_lr: 0.000006  loss: 0.8672 (0.8036)  loss_scale: 131072.0000 (125857.7473)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [28]  [ 650/1349]  eta: 0:03:39  lr: 0.000233  min_lr: 0.000006  loss: 0.8452 (0.8037)  loss_scale: 131072.0000 (125937.8433)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
Epoch: [28]  [ 660/1349]  eta: 0:03:36  lr: 0.000233  min_lr: 0.000006  loss: 0.8510 (0.8048)  loss_scale: 131072.0000 (126015.5159)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [28]  [ 670/1349]  eta: 0:03:33  lr: 0.000233  min_lr: 0.000006  loss: 0.8208 (0.8039)  loss_scale: 131072.0000 (126090.8733)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [28]  [ 680/1349]  eta: 0:03:30  lr: 0.000233  min_lr: 0.000006  loss: 0.7130 (0.8025)  loss_scale: 131072.0000 (126164.0176)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0002  max mem: 41808
Epoch: [28]  [ 690/1349]  eta: 0:03:26  lr: 0.000233  min_lr: 0.000006  loss: 0.7363 (0.8023)  loss_scale: 131072.0000 (126235.0449)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [28]  [ 700/1349]  eta: 0:03:23  lr: 0.000233  min_lr: 0.000006  loss: 0.7665 (0.8016)  loss_scale: 131072.0000 (126304.0456)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [28]  [ 710/1349]  eta: 0:03:20  lr: 0.000233  min_lr: 0.000006  loss: 0.8425 (0.8018)  loss_scale: 131072.0000 (126371.1055)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
Epoch: [28]  [ 720/1349]  eta: 0:03:17  lr: 0.000233  min_lr: 0.000006  loss: 0.8607 (0.8022)  loss_scale: 131072.0000 (126436.3051)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [28]  [ 730/1349]  eta: 0:03:14  lr: 0.000232  min_lr: 0.000006  loss: 0.7277 (0.8004)  loss_scale: 131072.0000 (126499.7209)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [28]  [ 740/1349]  eta: 0:03:11  lr: 0.000232  min_lr: 0.000006  loss: 0.6457 (0.7999)  loss_scale: 131072.0000 (126561.4251)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0001  max mem: 41808
Epoch: [28]  [ 750/1349]  eta: 0:03:07  lr: 0.000232  min_lr: 0.000006  loss: 0.7320 (0.7990)  loss_scale: 131072.0000 (126621.4860)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0001  max mem: 41808
Epoch: [28]  [ 760/1349]  eta: 0:03:04  lr: 0.000232  min_lr: 0.000006  loss: 0.7687 (0.7992)  loss_scale: 131072.0000 (126679.9685)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
[2025-05-23 20:53:54,833] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 20:53:54,833] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 20:53:54,833] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 20:53:54,833] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [28]  [ 770/1349]  eta: 0:03:01  lr: 0.000232  min_lr: 0.000006  loss: 0.8204 (0.7993)  loss_scale: 131072.0000 (127586.9468)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [28]  [ 780/1349]  eta: 0:02:58  lr: 0.000232  min_lr: 0.000006  loss: 0.8005 (0.7993)  loss_scale: 262144.0000 (129309.8284)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
[2025-05-23 20:54:01,593] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 38560
[2025-05-23 20:54:01,593] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 38560
[2025-05-23 20:54:01,593] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 20:54:01,593] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 20:54:01,593] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [28]  [ 790/1349]  eta: 0:02:55  lr: 0.000232  min_lr: 0.000006  loss: 0.8005 (0.7998)  loss_scale: 262144.0000 (130492.0354)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
Epoch: [28]  [ 800/1349]  eta: 0:02:51  lr: 0.000231  min_lr: 0.000005  loss: 0.7406 (0.7984)  loss_scale: 131072.0000 (130499.2759)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
[2025-05-23 20:54:06,810] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 38577
[2025-05-23 20:54:06,810] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 38577
[2025-05-23 20:54:06,810] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 20:54:06,810] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 20:54:06,810] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [28]  [ 810/1349]  eta: 0:02:48  lr: 0.000231  min_lr: 0.000005  loss: 0.6840 (0.7978)  loss_scale: 131072.0000 (130021.4846)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0002  max mem: 41808
Epoch: [28]  [ 820/1349]  eta: 0:02:45  lr: 0.000231  min_lr: 0.000005  loss: 0.7157 (0.7965)  loss_scale: 65536.0000 (129236.0341)  weight_decay: 0.0500 (0.0500)  time: 0.3061  data: 0.0001  max mem: 41808
Epoch: [28]  [ 830/1349]  eta: 0:02:42  lr: 0.000231  min_lr: 0.000005  loss: 0.7157 (0.7961)  loss_scale: 65536.0000 (128469.4874)  weight_decay: 0.0500 (0.0500)  time: 0.3066  data: 0.0001  max mem: 41808
Epoch: [28]  [ 840/1349]  eta: 0:02:39  lr: 0.000231  min_lr: 0.000005  loss: 0.7952 (0.7963)  loss_scale: 65536.0000 (127721.1700)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [28]  [ 850/1349]  eta: 0:02:36  lr: 0.000231  min_lr: 0.000005  loss: 0.8491 (0.7964)  loss_scale: 65536.0000 (126990.4395)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
Epoch: [28]  [ 860/1349]  eta: 0:02:32  lr: 0.000231  min_lr: 0.000005  loss: 0.8717 (0.7976)  loss_scale: 65536.0000 (126276.6829)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
Epoch: [28]  [ 870/1349]  eta: 0:02:29  lr: 0.000231  min_lr: 0.000005  loss: 0.8511 (0.7967)  loss_scale: 65536.0000 (125579.3157)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
Epoch: [28]  [ 880/1349]  eta: 0:02:26  lr: 0.000230  min_lr: 0.000005  loss: 0.7603 (0.7962)  loss_scale: 65536.0000 (124897.7798)  weight_decay: 0.0500 (0.0500)  time: 0.3091  data: 0.0001  max mem: 41808
Epoch: [28]  [ 890/1349]  eta: 0:02:23  lr: 0.000230  min_lr: 0.000005  loss: 0.7603 (0.7962)  loss_scale: 65536.0000 (124231.5421)  weight_decay: 0.0500 (0.0500)  time: 0.3097  data: 0.0001  max mem: 41808
Epoch: [28]  [ 900/1349]  eta: 0:02:20  lr: 0.000230  min_lr: 0.000005  loss: 0.8119 (0.7962)  loss_scale: 65536.0000 (123580.0932)  weight_decay: 0.0500 (0.0500)  time: 0.3087  data: 0.0002  max mem: 41808
Epoch: [28]  [ 910/1349]  eta: 0:02:17  lr: 0.000230  min_lr: 0.000005  loss: 0.8903 (0.7974)  loss_scale: 65536.0000 (122942.9462)  weight_decay: 0.0500 (0.0500)  time: 0.3087  data: 0.0002  max mem: 41808
Epoch: [28]  [ 920/1349]  eta: 0:02:14  lr: 0.000230  min_lr: 0.000005  loss: 0.8493 (0.7971)  loss_scale: 65536.0000 (122319.6352)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
Epoch: [28]  [ 930/1349]  eta: 0:02:10  lr: 0.000230  min_lr: 0.000005  loss: 0.8493 (0.7977)  loss_scale: 65536.0000 (121709.7143)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
[2025-05-23 20:54:46,496] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 20:54:46,496] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-05-23 20:54:46,497] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 20:54:46,497] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [28]  [ 940/1349]  eta: 0:02:07  lr: 0.000230  min_lr: 0.000005  loss: 0.9041 (0.7992)  loss_scale: 65536.0000 (121600.2721)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [28]  [ 950/1349]  eta: 0:02:04  lr: 0.000230  min_lr: 0.000005  loss: 0.8872 (0.7992)  loss_scale: 131072.0000 (121699.8696)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [28]  [ 960/1349]  eta: 0:02:01  lr: 0.000229  min_lr: 0.000005  loss: 0.8294 (0.7988)  loss_scale: 131072.0000 (121797.3944)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
Epoch: [28]  [ 970/1349]  eta: 0:01:58  lr: 0.000229  min_lr: 0.000005  loss: 0.8428 (0.7989)  loss_scale: 131072.0000 (121892.9104)  weight_decay: 0.0500 (0.0500)  time: 0.3066  data: 0.0001  max mem: 41808
Epoch: [28]  [ 980/1349]  eta: 0:01:55  lr: 0.000229  min_lr: 0.000005  loss: 0.8648 (0.7990)  loss_scale: 131072.0000 (121986.4791)  weight_decay: 0.0500 (0.0500)  time: 0.3063  data: 0.0001  max mem: 41808
Epoch: [28]  [ 990/1349]  eta: 0:01:52  lr: 0.000229  min_lr: 0.000005  loss: 0.8239 (0.7988)  loss_scale: 131072.0000 (122078.1594)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [28]  [1000/1349]  eta: 0:01:48  lr: 0.000229  min_lr: 0.000005  loss: 0.8521 (0.7991)  loss_scale: 131072.0000 (122168.0080)  weight_decay: 0.0500 (0.0500)  time: 0.3092  data: 0.0001  max mem: 41808
Epoch: [28]  [1010/1349]  eta: 0:01:45  lr: 0.000229  min_lr: 0.000005  loss: 0.7784 (0.7984)  loss_scale: 131072.0000 (122256.0791)  weight_decay: 0.0500 (0.0500)  time: 0.3083  data: 0.0001  max mem: 41808
Epoch: [28]  [1020/1349]  eta: 0:01:42  lr: 0.000229  min_lr: 0.000005  loss: 0.8807 (0.7997)  loss_scale: 131072.0000 (122342.4251)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0002  max mem: 41808
Epoch: [28]  [1030/1349]  eta: 0:01:39  lr: 0.000229  min_lr: 0.000005  loss: 0.8807 (0.7999)  loss_scale: 131072.0000 (122427.0960)  weight_decay: 0.0500 (0.0500)  time: 0.3080  data: 0.0001  max mem: 41808
Epoch: [28]  [1040/1349]  eta: 0:01:36  lr: 0.000228  min_lr: 0.000005  loss: 0.8161 (0.7992)  loss_scale: 131072.0000 (122510.1402)  weight_decay: 0.0500 (0.0500)  time: 0.3090  data: 0.0001  max mem: 41808
Epoch: [28]  [1050/1349]  eta: 0:01:33  lr: 0.000228  min_lr: 0.000005  loss: 0.7412 (0.7988)  loss_scale: 131072.0000 (122591.6042)  weight_decay: 0.0500 (0.0500)  time: 0.3093  data: 0.0001  max mem: 41808
Epoch: [28]  [1060/1349]  eta: 0:01:30  lr: 0.000228  min_lr: 0.000005  loss: 0.7662 (0.7988)  loss_scale: 131072.0000 (122671.5325)  weight_decay: 0.0500 (0.0500)  time: 0.3093  data: 0.0001  max mem: 41808
[2025-05-23 20:55:25,931] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 20:55:25,931] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 20:55:25,931] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 20:55:25,931] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 20:55:27,156] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 38838
[2025-05-23 20:55:27,156] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 20:55:27,156] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 38838
[2025-05-23 20:55:27,156] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 20:55:27,156] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [28]  [1070/1349]  eta: 0:01:26  lr: 0.000228  min_lr: 0.000005  loss: 0.8334 (0.7992)  loss_scale: 131072.0000 (123239.4995)  weight_decay: 0.0500 (0.0500)  time: 0.3088  data: 0.0001  max mem: 41808
[2025-05-23 20:55:30,554] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 38849
[2025-05-23 20:55:30,554] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 20:55:30,554] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 38849
[2025-05-23 20:55:30,554] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 20:55:30,554] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [28]  [1080/1349]  eta: 0:01:23  lr: 0.000228  min_lr: 0.000005  loss: 0.8646 (0.7999)  loss_scale: 131072.0000 (123069.4542)  weight_decay: 0.0500 (0.0500)  time: 0.3078  data: 0.0002  max mem: 41808
Epoch: [28]  [1090/1349]  eta: 0:01:20  lr: 0.000228  min_lr: 0.000005  loss: 0.9028 (0.7996)  loss_scale: 65536.0000 (122542.1082)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
Epoch: [28]  [1100/1349]  eta: 0:01:17  lr: 0.000228  min_lr: 0.000005  loss: 0.7114 (0.7985)  loss_scale: 65536.0000 (122024.3415)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [28]  [1110/1349]  eta: 0:01:14  lr: 0.000227  min_lr: 0.000005  loss: 0.7114 (0.7985)  loss_scale: 65536.0000 (121515.8956)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [28]  [1120/1349]  eta: 0:01:11  lr: 0.000227  min_lr: 0.000005  loss: 0.8195 (0.7990)  loss_scale: 65536.0000 (121016.5210)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [28]  [1130/1349]  eta: 0:01:08  lr: 0.000227  min_lr: 0.000005  loss: 0.8195 (0.7993)  loss_scale: 65536.0000 (120525.9770)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
Epoch: [28]  [1140/1349]  eta: 0:01:05  lr: 0.000227  min_lr: 0.000005  loss: 0.7831 (0.7992)  loss_scale: 65536.0000 (120044.0316)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [28]  [1150/1349]  eta: 0:01:01  lr: 0.000227  min_lr: 0.000005  loss: 0.7631 (0.7988)  loss_scale: 65536.0000 (119570.4605)  weight_decay: 0.0500 (0.0500)  time: 0.3079  data: 0.0001  max mem: 41808
Epoch: [28]  [1160/1349]  eta: 0:00:58  lr: 0.000227  min_lr: 0.000005  loss: 0.8020 (0.7986)  loss_scale: 65536.0000 (119105.0474)  weight_decay: 0.0500 (0.0500)  time: 0.3079  data: 0.0001  max mem: 41808
Epoch: [28]  [1170/1349]  eta: 0:00:55  lr: 0.000227  min_lr: 0.000005  loss: 0.8160 (0.7988)  loss_scale: 65536.0000 (118647.5833)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
Epoch: [28]  [1180/1349]  eta: 0:00:52  lr: 0.000227  min_lr: 0.000005  loss: 0.7916 (0.7984)  loss_scale: 65536.0000 (118197.8662)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [28]  [1190/1349]  eta: 0:00:49  lr: 0.000226  min_lr: 0.000005  loss: 0.8317 (0.7987)  loss_scale: 65536.0000 (117755.7011)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
Epoch: [28]  [1200/1349]  eta: 0:00:46  lr: 0.000226  min_lr: 0.000005  loss: 0.8249 (0.7985)  loss_scale: 65536.0000 (117320.8993)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
[2025-05-23 20:56:10,200] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 20:56:10,200] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-05-23 20:56:10,204] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 20:56:10,204] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [28]  [1210/1349]  eta: 0:00:43  lr: 0.000226  min_lr: 0.000005  loss: 0.7240 (0.7978)  loss_scale: 65536.0000 (117163.8646)  weight_decay: 0.0500 (0.0500)  time: 0.3084  data: 0.0001  max mem: 41808
Epoch: [28]  [1220/1349]  eta: 0:00:40  lr: 0.000226  min_lr: 0.000005  loss: 0.7293 (0.7974)  loss_scale: 131072.0000 (117277.7723)  weight_decay: 0.0500 (0.0500)  time: 0.3105  data: 0.0002  max mem: 41808
[2025-05-23 20:56:16,732] [INFO] [logging.py:96:log_dist] [Rank 0] step=39000, skipped=238, lr=[5.368839978459994e-06, 5.368839978459994e-06, 7.158453304613325e-06, 7.158453304613325e-06, 9.5446044061511e-06, 9.5446044061511e-06, 1.2726139208201466e-05, 1.2726139208201466e-05, 1.696818561093529e-05, 1.696818561093529e-05, 2.2624247481247052e-05, 2.2624247481247052e-05, 3.0165663308329402e-05, 3.0165663308329402e-05, 4.022088441110587e-05, 4.022088441110587e-05, 5.362784588147449e-05, 5.362784588147449e-05, 7.150379450863265e-05, 7.150379450863265e-05, 9.533839267817688e-05, 9.533839267817688e-05, 0.00012711785690423582, 0.00012711785690423582, 0.00016949047587231443, 0.00016949047587231443, 0.00022598730116308593, 0.00022598730116308593], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-05-23 20:56:16,732] [INFO] [timer.py:260:stop] epoch=0/micro_step=39000/global_step=39000, RunningAvgSamplesPerSec=209.560631603496, CurrSamplesPerSec=212.8338814694064, MemAllocated=1.37GB, MaxMemAllocated=40.83GB
Epoch: [28]  [1230/1349]  eta: 0:00:37  lr: 0.000226  min_lr: 0.000005  loss: 0.7328 (0.7969)  loss_scale: 131072.0000 (117389.8294)  weight_decay: 0.0500 (0.0500)  time: 0.3098  data: 0.0002  max mem: 41808
Epoch: [28]  [1240/1349]  eta: 0:00:33  lr: 0.000226  min_lr: 0.000005  loss: 0.8149 (0.7971)  loss_scale: 131072.0000 (117500.0806)  weight_decay: 0.0500 (0.0500)  time: 0.3085  data: 0.0002  max mem: 41808
Epoch: [28]  [1250/1349]  eta: 0:00:30  lr: 0.000226  min_lr: 0.000005  loss: 0.8244 (0.7971)  loss_scale: 131072.0000 (117608.5691)  weight_decay: 0.0500 (0.0500)  time: 0.3094  data: 0.0002  max mem: 41808
Epoch: [28]  [1260/1349]  eta: 0:00:27  lr: 0.000226  min_lr: 0.000005  loss: 0.8435 (0.7977)  loss_scale: 131072.0000 (117715.3370)  weight_decay: 0.0500 (0.0500)  time: 0.3088  data: 0.0001  max mem: 41808
Epoch: [28]  [1270/1349]  eta: 0:00:24  lr: 0.000225  min_lr: 0.000005  loss: 0.8808 (0.7983)  loss_scale: 131072.0000 (117820.4249)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [28]  [1280/1349]  eta: 0:00:21  lr: 0.000225  min_lr: 0.000005  loss: 0.8322 (0.7981)  loss_scale: 131072.0000 (117923.8720)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
Epoch: [28]  [1290/1349]  eta: 0:00:18  lr: 0.000225  min_lr: 0.000005  loss: 0.7611 (0.7977)  loss_scale: 131072.0000 (118025.7165)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
Epoch: [28]  [1300/1349]  eta: 0:00:15  lr: 0.000225  min_lr: 0.000005  loss: 0.7578 (0.7972)  loss_scale: 131072.0000 (118125.9954)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
Epoch: [28]  [1310/1349]  eta: 0:00:12  lr: 0.000225  min_lr: 0.000005  loss: 0.7327 (0.7970)  loss_scale: 131072.0000 (118224.7445)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [28]  [1320/1349]  eta: 0:00:09  lr: 0.000225  min_lr: 0.000005  loss: 0.8336 (0.7971)  loss_scale: 131072.0000 (118321.9985)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0002  max mem: 41808
Epoch: [28]  [1330/1349]  eta: 0:00:05  lr: 0.000225  min_lr: 0.000005  loss: 0.8449 (0.7970)  loss_scale: 131072.0000 (118417.7911)  weight_decay: 0.0500 (0.0500)  time: 0.3054  data: 0.0001  max mem: 41808
[2025-05-23 20:56:49,613] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 20:56:49,613] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 20:56:49,614] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 20:56:49,614] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 20:56:50,518] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 39109
[2025-05-23 20:56:50,518] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 39109
[2025-05-23 20:56:50,518] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 20:56:50,518] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 20:56:50,518] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [28]  [1340/1349]  eta: 0:00:02  lr: 0.000225  min_lr: 0.000005  loss: 0.7586 (0.7969)  loss_scale: 131072.0000 (118805.3811)  weight_decay: 0.0500 (0.0500)  time: 0.3026  data: 0.0001  max mem: 41808
Epoch: [28]  [1348/1349]  eta: 0:00:00  lr: 0.000224  min_lr: 0.000005  loss: 0.7910 (0.7970)  loss_scale: 131072.0000 (118878.1260)  weight_decay: 0.0500 (0.0500)  time: 0.3018  data: 0.0001  max mem: 41808
Epoch: [28] Total time: 0:06:59 (0.3112 s / it)
Averaged stats: lr: 0.000224  min_lr: 0.000005  loss: 0.7910 (0.7969)  loss_scale: 131072.0000 (118878.1260)  weight_decay: 0.0500 (0.0500)  total_time: 419.8256 (419.8192)
Val:  [  0/346]  eta: 0:55:37  loss: 2.9151 (2.9151)  acc1: 7.0312 (7.0312)  acc5: 82.0312 (82.0312)  time: 9.6450  data: 8.5662  max mem: 41808
Val:  [ 10/346]  eta: 0:10:56  loss: 0.1193 (0.5400)  acc1: 100.0000 (86.1506)  acc5: 100.0000 (97.7273)  time: 1.9533  data: 1.1152  max mem: 41808
Val:  [ 20/346]  eta: 0:07:46  loss: 0.1185 (0.4405)  acc1: 100.0000 (89.1369)  acc5: 100.0000 (98.7723)  time: 1.0196  data: 0.2193  max mem: 41808
Val:  [ 30/346]  eta: 0:06:24  loss: 0.0983 (0.3996)  acc1: 100.0000 (89.8185)  acc5: 100.0000 (99.1431)  time: 0.8122  data: 0.0534  max mem: 41808
Val:  [ 40/346]  eta: 0:05:37  loss: 0.1097 (0.4572)  acc1: 99.2188 (88.4718)  acc5: 100.0000 (98.3232)  time: 0.7566  data: 0.0193  max mem: 41808
Val:  [ 50/346]  eta: 0:05:11  loss: 0.1267 (0.3979)  acc1: 99.2188 (90.4259)  acc5: 100.0000 (98.6520)  time: 0.7987  data: 0.0392  max mem: 41808
Val:  [ 60/346]  eta: 0:04:51  loss: 0.1490 (0.3896)  acc1: 97.6562 (90.5353)  acc5: 100.0000 (98.8601)  time: 0.8510  data: 0.1066  max mem: 41808
Val:  [ 70/346]  eta: 0:04:36  loss: 0.2340 (0.4087)  acc1: 95.3125 (90.0638)  acc5: 100.0000 (98.9877)  time: 0.8667  data: 0.1355  max mem: 41808
Val:  [ 80/346]  eta: 0:04:23  loss: 0.4273 (0.4360)  acc1: 89.0625 (88.9371)  acc5: 100.0000 (98.8426)  time: 0.9012  data: 0.1443  max mem: 41808
Val:  [ 90/346]  eta: 0:04:11  loss: 0.3090 (0.4260)  acc1: 89.8438 (89.2600)  acc5: 100.0000 (98.9354)  time: 0.9163  data: 0.1474  max mem: 41808
Val:  [100/346]  eta: 0:03:59  loss: 0.1759 (0.3965)  acc1: 97.6562 (90.2692)  acc5: 100.0000 (99.0408)  time: 0.9032  data: 0.1407  max mem: 41808
Val:  [110/346]  eta: 0:03:47  loss: 0.1311 (0.4046)  acc1: 99.2188 (89.9704)  acc5: 100.0000 (99.0428)  time: 0.8852  data: 0.1457  max mem: 41808
Val:  [120/346]  eta: 0:03:36  loss: 0.1963 (0.4033)  acc1: 95.3125 (90.0310)  acc5: 100.0000 (99.1025)  time: 0.8755  data: 0.1608  max mem: 41808
Val:  [130/346]  eta: 0:03:25  loss: 0.1815 (0.3976)  acc1: 96.0938 (90.1658)  acc5: 100.0000 (99.1651)  time: 0.8791  data: 0.1610  max mem: 41808
Val:  [140/346]  eta: 0:03:15  loss: 0.2402 (0.4068)  acc1: 95.3125 (89.8327)  acc5: 100.0000 (99.2243)  time: 0.8846  data: 0.1499  max mem: 41808
Val:  [150/346]  eta: 0:03:04  loss: 0.3087 (0.4048)  acc1: 89.8438 (89.8800)  acc5: 100.0000 (99.2653)  time: 0.8777  data: 0.1504  max mem: 41808
Val:  [160/346]  eta: 0:02:54  loss: 0.2130 (0.3932)  acc1: 96.0938 (90.2611)  acc5: 100.0000 (99.3012)  time: 0.8649  data: 0.1521  max mem: 41808
Val:  [170/346]  eta: 0:02:44  loss: 0.1178 (0.3844)  acc1: 99.2188 (90.4879)  acc5: 100.0000 (99.3421)  time: 0.8845  data: 0.1534  max mem: 41808
Val:  [180/346]  eta: 0:02:35  loss: 0.2194 (0.4036)  acc1: 93.7500 (89.8179)  acc5: 100.0000 (99.3785)  time: 0.9209  data: 0.1641  max mem: 41808
Val:  [190/346]  eta: 0:02:25  loss: 0.2372 (0.4000)  acc1: 93.7500 (89.9705)  acc5: 100.0000 (99.4028)  time: 0.9265  data: 0.1634  max mem: 41808
Val:  [200/346]  eta: 0:02:16  loss: 0.2000 (0.4069)  acc1: 92.1875 (89.6688)  acc5: 100.0000 (99.4286)  time: 0.9139  data: 0.1524  max mem: 41808
Val:  [210/346]  eta: 0:02:06  loss: 0.1503 (0.4010)  acc1: 96.8750 (89.8549)  acc5: 100.0000 (99.4520)  time: 0.9027  data: 0.1473  max mem: 41808
Val:  [220/346]  eta: 0:01:57  loss: 0.1393 (0.3965)  acc1: 99.2188 (90.0240)  acc5: 100.0000 (99.4627)  time: 0.8957  data: 0.1571  max mem: 41808
Val:  [230/346]  eta: 0:01:47  loss: 0.1421 (0.3897)  acc1: 98.4375 (90.2428)  acc5: 100.0000 (99.4792)  time: 0.8964  data: 0.1583  max mem: 41808
Val:  [240/346]  eta: 0:01:38  loss: 0.1538 (0.3933)  acc1: 98.4375 (90.1939)  acc5: 100.0000 (99.4943)  time: 0.8894  data: 0.1470  max mem: 41808
Val:  [250/346]  eta: 0:01:28  loss: 0.1605 (0.3910)  acc1: 98.4375 (90.3106)  acc5: 100.0000 (99.4709)  time: 0.8978  data: 0.1552  max mem: 41808
Val:  [260/346]  eta: 0:01:19  loss: 0.1364 (0.3869)  acc1: 98.4375 (90.4514)  acc5: 100.0000 (99.4732)  time: 0.9089  data: 0.1587  max mem: 41808
Val:  [270/346]  eta: 0:01:10  loss: 0.1364 (0.3828)  acc1: 97.6562 (90.5933)  acc5: 100.0000 (99.4840)  time: 0.9172  data: 0.1617  max mem: 41808
Val:  [280/346]  eta: 0:01:00  loss: 0.1021 (0.3810)  acc1: 99.2188 (90.6834)  acc5: 100.0000 (99.4523)  time: 0.9159  data: 0.1626  max mem: 41808
Val:  [290/346]  eta: 0:00:51  loss: 0.0980 (0.3715)  acc1: 100.0000 (90.9928)  acc5: 100.0000 (99.4711)  time: 0.9020  data: 0.1502  max mem: 41808
Val:  [300/346]  eta: 0:00:42  loss: 0.0991 (0.3710)  acc1: 100.0000 (91.0610)  acc5: 100.0000 (99.4108)  time: 0.8980  data: 0.1335  max mem: 41808
Val:  [310/346]  eta: 0:00:33  loss: 0.1258 (0.3708)  acc1: 98.4375 (91.0671)  acc5: 100.0000 (99.3971)  time: 0.8968  data: 0.1387  max mem: 41808
Val:  [320/346]  eta: 0:00:23  loss: 0.1200 (0.3755)  acc1: 99.2188 (90.8927)  acc5: 100.0000 (99.4159)  time: 0.8862  data: 0.1494  max mem: 41808
Val:  [330/346]  eta: 0:00:14  loss: 0.5013 (0.3924)  acc1: 85.9375 (90.3677)  acc5: 100.0000 (99.3297)  time: 0.8859  data: 0.1436  max mem: 41808
Val:  [340/346]  eta: 0:00:05  loss: 0.5013 (0.3996)  acc1: 85.9375 (90.2034)  acc5: 100.0000 (99.3470)  time: 0.9139  data: 0.1588  max mem: 41808
Val:  [345/346]  eta: 0:00:00  loss: 0.2321 (0.3960)  acc1: 93.7500 (90.3202)  acc5: 100.0000 (99.3559)  time: 0.9134  data: 0.1749  max mem: 41808
Val: Total time: 0:05:18 (0.9208 s / it)
* Acc@1 90.407 Acc@5 99.362 loss 0.393
Accuracy of the network on the 88494 val videos: 90.4%
Max accuracy: 91.50%   Max Epoch: 25
Epoch: [29]  [   0/1349]  eta: 2:03:23  lr: 0.000224  min_lr: 0.000005  loss: 0.8630 (0.8630)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 5.4878  data: 3.2304  max mem: 41808
Epoch: [29]  [  10/1349]  eta: 0:17:28  lr: 0.000224  min_lr: 0.000005  loss: 0.8000 (0.8170)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7833  data: 0.2938  max mem: 41808
Epoch: [29]  [  20/1349]  eta: 0:12:19  lr: 0.000224  min_lr: 0.000005  loss: 0.7841 (0.7896)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3102  data: 0.0001  max mem: 41808
Epoch: [29]  [  30/1349]  eta: 0:10:28  lr: 0.000224  min_lr: 0.000005  loss: 0.7563 (0.7656)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3078  data: 0.0001  max mem: 41808
Epoch: [29]  [  40/1349]  eta: 0:09:29  lr: 0.000224  min_lr: 0.000005  loss: 0.7563 (0.7675)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3078  data: 0.0001  max mem: 41808
Epoch: [29]  [  50/1349]  eta: 0:08:52  lr: 0.000224  min_lr: 0.000005  loss: 0.7534 (0.7627)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [29]  [  60/1349]  eta: 0:08:26  lr: 0.000224  min_lr: 0.000005  loss: 0.7534 (0.7616)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [29]  [  70/1349]  eta: 0:08:07  lr: 0.000224  min_lr: 0.000005  loss: 0.7545 (0.7601)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0001  max mem: 41808
Epoch: [29]  [  80/1349]  eta: 0:07:52  lr: 0.000223  min_lr: 0.000005  loss: 0.7329 (0.7548)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3080  data: 0.0001  max mem: 41808
Epoch: [29]  [  90/1349]  eta: 0:07:39  lr: 0.000223  min_lr: 0.000005  loss: 0.7329 (0.7555)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [29]  [ 100/1349]  eta: 0:07:28  lr: 0.000223  min_lr: 0.000005  loss: 0.8308 (0.7666)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [29]  [ 110/1349]  eta: 0:07:19  lr: 0.000223  min_lr: 0.000005  loss: 0.8590 (0.7765)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
[2025-05-23 21:02:54,287] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 21:02:54,287] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 21:02:54,287] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 21:02:54,288] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [29]  [ 120/1349]  eta: 0:07:11  lr: 0.000223  min_lr: 0.000005  loss: 0.8052 (0.7778)  loss_scale: 131072.0000 (135404.9587)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0002  max mem: 41808
[2025-05-23 21:02:56,130] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 39244
[2025-05-23 21:02:56,130] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 21:02:56,130] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 39244
[2025-05-23 21:02:56,130] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 21:02:56,130] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [29]  [ 130/1349]  eta: 0:07:03  lr: 0.000223  min_lr: 0.000005  loss: 0.7897 (0.7795)  loss_scale: 131072.0000 (137075.2977)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0002  max mem: 41808
Epoch: [29]  [ 140/1349]  eta: 0:06:56  lr: 0.000223  min_lr: 0.000005  loss: 0.8291 (0.7829)  loss_scale: 131072.0000 (136649.5319)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [29]  [ 150/1349]  eta: 0:06:50  lr: 0.000222  min_lr: 0.000005  loss: 0.8291 (0.7824)  loss_scale: 131072.0000 (136280.1589)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0001  max mem: 41808
Epoch: [29]  [ 160/1349]  eta: 0:06:44  lr: 0.000222  min_lr: 0.000005  loss: 0.7952 (0.7821)  loss_scale: 131072.0000 (135956.6708)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0001  max mem: 41808
Epoch: [29]  [ 170/1349]  eta: 0:06:38  lr: 0.000222  min_lr: 0.000005  loss: 0.8074 (0.7868)  loss_scale: 131072.0000 (135671.0175)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0002  max mem: 41808
Epoch: [29]  [ 180/1349]  eta: 0:06:33  lr: 0.000222  min_lr: 0.000005  loss: 0.8384 (0.7877)  loss_scale: 131072.0000 (135416.9282)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
Epoch: [29]  [ 190/1349]  eta: 0:06:27  lr: 0.000222  min_lr: 0.000005  loss: 0.7208 (0.7843)  loss_scale: 131072.0000 (135189.4450)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
Epoch: [29]  [ 200/1349]  eta: 0:06:23  lr: 0.000222  min_lr: 0.000005  loss: 0.7852 (0.7874)  loss_scale: 131072.0000 (134984.5970)  weight_decay: 0.0500 (0.0500)  time: 0.3093  data: 0.0001  max mem: 41808
Epoch: [29]  [ 210/1349]  eta: 0:06:18  lr: 0.000222  min_lr: 0.000005  loss: 0.7852 (0.7846)  loss_scale: 131072.0000 (134799.1659)  weight_decay: 0.0500 (0.0500)  time: 0.3094  data: 0.0001  max mem: 41808
Epoch: [29]  [ 220/1349]  eta: 0:06:13  lr: 0.000222  min_lr: 0.000005  loss: 0.7908 (0.7852)  loss_scale: 131072.0000 (134630.5158)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [29]  [ 230/1349]  eta: 0:06:09  lr: 0.000221  min_lr: 0.000005  loss: 0.8121 (0.7831)  loss_scale: 131072.0000 (134476.4675)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [29]  [ 240/1349]  eta: 0:06:05  lr: 0.000221  min_lr: 0.000005  loss: 0.7671 (0.7824)  loss_scale: 131072.0000 (134335.2033)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0001  max mem: 41808
Epoch: [29]  [ 250/1349]  eta: 0:06:00  lr: 0.000221  min_lr: 0.000005  loss: 0.7589 (0.7794)  loss_scale: 131072.0000 (134205.1952)  weight_decay: 0.0500 (0.0500)  time: 0.3078  data: 0.0001  max mem: 41808
[2025-05-23 21:03:35,837] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 21:03:35,837] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 21:03:35,837] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 21:03:35,837] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [29]  [ 260/1349]  eta: 0:05:56  lr: 0.000221  min_lr: 0.000005  loss: 0.7918 (0.7813)  loss_scale: 131072.0000 (138604.8736)  weight_decay: 0.0500 (0.0500)  time: 0.3079  data: 0.0001  max mem: 41808
[2025-05-23 21:03:40,444] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 39388
[2025-05-23 21:03:40,445] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 39388
[2025-05-23 21:03:40,445] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 21:03:40,445] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 21:03:40,445] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [29]  [ 270/1349]  eta: 0:05:52  lr: 0.000221  min_lr: 0.000005  loss: 0.8457 (0.7842)  loss_scale: 262144.0000 (141228.8708)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0002  max mem: 41808
Epoch: [29]  [ 280/1349]  eta: 0:05:48  lr: 0.000221  min_lr: 0.000005  loss: 0.8457 (0.7829)  loss_scale: 131072.0000 (140867.4164)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0001  max mem: 41808
Epoch: [29]  [ 290/1349]  eta: 0:05:44  lr: 0.000221  min_lr: 0.000005  loss: 0.8159 (0.7835)  loss_scale: 131072.0000 (140530.8041)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
Epoch: [29]  [ 300/1349]  eta: 0:05:40  lr: 0.000221  min_lr: 0.000005  loss: 0.8159 (0.7835)  loss_scale: 131072.0000 (140216.5581)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
Epoch: [29]  [ 310/1349]  eta: 0:05:36  lr: 0.000220  min_lr: 0.000005  loss: 0.8128 (0.7827)  loss_scale: 131072.0000 (139922.5209)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0001  max mem: 41808
Epoch: [29]  [ 320/1349]  eta: 0:05:33  lr: 0.000220  min_lr: 0.000005  loss: 0.7941 (0.7822)  loss_scale: 131072.0000 (139646.8037)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
Epoch: [29]  [ 330/1349]  eta: 0:05:29  lr: 0.000220  min_lr: 0.000005  loss: 0.7037 (0.7798)  loss_scale: 131072.0000 (139387.7462)  weight_decay: 0.0500 (0.0500)  time: 0.3079  data: 0.0001  max mem: 41808
Epoch: [29]  [ 340/1349]  eta: 0:05:25  lr: 0.000220  min_lr: 0.000005  loss: 0.7037 (0.7800)  loss_scale: 131072.0000 (139143.8827)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
Epoch: [29]  [ 350/1349]  eta: 0:05:22  lr: 0.000220  min_lr: 0.000005  loss: 0.7697 (0.7776)  loss_scale: 131072.0000 (138913.9145)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [29]  [ 360/1349]  eta: 0:05:18  lr: 0.000220  min_lr: 0.000005  loss: 0.7556 (0.7781)  loss_scale: 131072.0000 (138696.6870)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [29]  [ 370/1349]  eta: 0:05:14  lr: 0.000220  min_lr: 0.000005  loss: 0.8077 (0.7782)  loss_scale: 131072.0000 (138491.1698)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
Epoch: [29]  [ 380/1349]  eta: 0:05:11  lr: 0.000220  min_lr: 0.000005  loss: 0.8116 (0.7790)  loss_scale: 131072.0000 (138296.4409)  weight_decay: 0.0500 (0.0500)  time: 0.3082  data: 0.0001  max mem: 41808
Epoch: [29]  [ 390/1349]  eta: 0:05:07  lr: 0.000219  min_lr: 0.000005  loss: 0.8142 (0.7799)  loss_scale: 131072.0000 (138111.6726)  weight_decay: 0.0500 (0.0500)  time: 0.3107  data: 0.0002  max mem: 41808
[2025-05-23 21:04:20,190] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 21:04:20,190] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 21:04:20,190] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 21:04:20,190] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [29]  [ 400/1349]  eta: 0:05:04  lr: 0.000219  min_lr: 0.000005  loss: 0.7798 (0.7782)  loss_scale: 131072.0000 (139570.4339)  weight_decay: 0.0500 (0.0500)  time: 0.3105  data: 0.0002  max mem: 41808
[2025-05-23 21:04:23,265] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 39527
[2025-05-23 21:04:23,265] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 39527
[2025-05-23 21:04:23,265] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 21:04:23,265] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 21:04:23,265] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [29]  [ 410/1349]  eta: 0:05:00  lr: 0.000219  min_lr: 0.000005  loss: 0.7655 (0.7789)  loss_scale: 131072.0000 (140958.2092)  weight_decay: 0.0500 (0.0500)  time: 0.3078  data: 0.0001  max mem: 41808
Epoch: [29]  [ 420/1349]  eta: 0:04:57  lr: 0.000219  min_lr: 0.000005  loss: 0.7974 (0.7793)  loss_scale: 131072.0000 (140723.3824)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [29]  [ 430/1349]  eta: 0:04:53  lr: 0.000219  min_lr: 0.000005  loss: 0.7957 (0.7802)  loss_scale: 131072.0000 (140499.4524)  weight_decay: 0.0500 (0.0500)  time: 0.3080  data: 0.0001  max mem: 41808
Epoch: [29]  [ 440/1349]  eta: 0:04:50  lr: 0.000219  min_lr: 0.000005  loss: 0.7690 (0.7795)  loss_scale: 131072.0000 (140285.6780)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0001  max mem: 41808
Epoch: [29]  [ 450/1349]  eta: 0:04:46  lr: 0.000219  min_lr: 0.000005  loss: 0.8599 (0.7820)  loss_scale: 131072.0000 (140081.3836)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
Epoch: [29]  [ 460/1349]  eta: 0:04:43  lr: 0.000219  min_lr: 0.000005  loss: 0.8328 (0.7807)  loss_scale: 131072.0000 (139885.9523)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
Epoch: [29]  [ 470/1349]  eta: 0:04:40  lr: 0.000218  min_lr: 0.000005  loss: 0.7814 (0.7817)  loss_scale: 131072.0000 (139698.8195)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0002  max mem: 41808
Epoch: [29]  [ 480/1349]  eta: 0:04:36  lr: 0.000218  min_lr: 0.000005  loss: 0.7814 (0.7803)  loss_scale: 131072.0000 (139519.4678)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
Epoch: [29]  [ 490/1349]  eta: 0:04:33  lr: 0.000218  min_lr: 0.000005  loss: 0.7944 (0.7826)  loss_scale: 131072.0000 (139347.4216)  weight_decay: 0.0500 (0.0500)  time: 0.3084  data: 0.0001  max mem: 41808
Epoch: [29]  [ 500/1349]  eta: 0:04:30  lr: 0.000218  min_lr: 0.000005  loss: 0.8271 (0.7822)  loss_scale: 131072.0000 (139182.2435)  weight_decay: 0.0500 (0.0500)  time: 0.3087  data: 0.0001  max mem: 41808
Epoch: [29]  [ 510/1349]  eta: 0:04:26  lr: 0.000218  min_lr: 0.000005  loss: 0.8090 (0.7834)  loss_scale: 131072.0000 (139023.5303)  weight_decay: 0.0500 (0.0500)  time: 0.3078  data: 0.0001  max mem: 41808
Epoch: [29]  [ 520/1349]  eta: 0:04:23  lr: 0.000218  min_lr: 0.000005  loss: 0.8504 (0.7842)  loss_scale: 131072.0000 (138870.9098)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
Epoch: [29]  [ 530/1349]  eta: 0:04:20  lr: 0.000218  min_lr: 0.000005  loss: 0.8255 (0.7845)  loss_scale: 131072.0000 (138724.0377)  weight_decay: 0.0500 (0.0500)  time: 0.3083  data: 0.0002  max mem: 41808
[2025-05-23 21:05:02,968] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 21:05:02,968] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 21:05:02,968] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 21:05:02,968] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [29]  [ 540/1349]  eta: 0:04:16  lr: 0.000217  min_lr: 0.000005  loss: 0.8114 (0.7850)  loss_scale: 131072.0000 (140036.2588)  weight_decay: 0.0500 (0.0500)  time: 0.3085  data: 0.0002  max mem: 41808
[2025-05-23 21:05:07,561] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 39671
[2025-05-23 21:05:07,561] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 39671
[2025-05-23 21:05:07,561] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 21:05:07,561] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 21:05:07,561] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [29]  [ 550/1349]  eta: 0:04:13  lr: 0.000217  min_lr: 0.000005  loss: 0.8114 (0.7838)  loss_scale: 262144.0000 (142014.4900)  weight_decay: 0.0500 (0.0500)  time: 0.3061  data: 0.0001  max mem: 41808
Epoch: [29]  [ 560/1349]  eta: 0:04:10  lr: 0.000217  min_lr: 0.000005  loss: 0.7419 (0.7837)  loss_scale: 131072.0000 (141819.4367)  weight_decay: 0.0500 (0.0500)  time: 0.3053  data: 0.0001  max mem: 41808
Epoch: [29]  [ 570/1349]  eta: 0:04:06  lr: 0.000217  min_lr: 0.000005  loss: 0.7419 (0.7833)  loss_scale: 131072.0000 (141631.2154)  weight_decay: 0.0500 (0.0500)  time: 0.3059  data: 0.0001  max mem: 41808
Epoch: [29]  [ 580/1349]  eta: 0:04:03  lr: 0.000217  min_lr: 0.000005  loss: 0.7786 (0.7841)  loss_scale: 131072.0000 (141449.4733)  weight_decay: 0.0500 (0.0500)  time: 0.3064  data: 0.0001  max mem: 41808
Epoch: [29]  [ 590/1349]  eta: 0:04:00  lr: 0.000217  min_lr: 0.000005  loss: 0.8233 (0.7847)  loss_scale: 131072.0000 (141273.8816)  weight_decay: 0.0500 (0.0500)  time: 0.3064  data: 0.0001  max mem: 41808
Epoch: [29]  [ 600/1349]  eta: 0:03:56  lr: 0.000217  min_lr: 0.000005  loss: 0.8413 (0.7844)  loss_scale: 131072.0000 (141104.1331)  weight_decay: 0.0500 (0.0500)  time: 0.3066  data: 0.0001  max mem: 41808
Epoch: [29]  [ 610/1349]  eta: 0:03:53  lr: 0.000217  min_lr: 0.000005  loss: 0.8100 (0.7850)  loss_scale: 131072.0000 (140939.9411)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
Epoch: [29]  [ 620/1349]  eta: 0:03:50  lr: 0.000216  min_lr: 0.000005  loss: 0.7609 (0.7839)  loss_scale: 131072.0000 (140781.0370)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
Epoch: [29]  [ 630/1349]  eta: 0:03:47  lr: 0.000216  min_lr: 0.000005  loss: 0.7380 (0.7842)  loss_scale: 131072.0000 (140627.1696)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [29]  [ 640/1349]  eta: 0:03:43  lr: 0.000216  min_lr: 0.000005  loss: 0.8314 (0.7851)  loss_scale: 131072.0000 (140478.1030)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
Epoch: [29]  [ 650/1349]  eta: 0:03:40  lr: 0.000216  min_lr: 0.000005  loss: 0.8476 (0.7869)  loss_scale: 131072.0000 (140333.6160)  weight_decay: 0.0500 (0.0500)  time: 0.3066  data: 0.0001  max mem: 41808
Epoch: [29]  [ 660/1349]  eta: 0:03:37  lr: 0.000216  min_lr: 0.000005  loss: 0.8682 (0.7870)  loss_scale: 131072.0000 (140193.5008)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
Epoch: [29]  [ 670/1349]  eta: 0:03:34  lr: 0.000216  min_lr: 0.000005  loss: 0.8749 (0.7885)  loss_scale: 131072.0000 (140057.5618)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
[2025-05-23 21:05:44,671] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 39792
[2025-05-23 21:05:44,671] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 39792
[2025-05-23 21:05:44,671] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 21:05:44,671] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 21:05:44,671] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [29]  [ 680/1349]  eta: 0:03:30  lr: 0.000216  min_lr: 0.000005  loss: 0.8459 (0.7890)  loss_scale: 65536.0000 (138963.2658)  weight_decay: 0.0500 (0.0500)  time: 0.3064  data: 0.0001  max mem: 41808
Epoch: [29]  [ 690/1349]  eta: 0:03:27  lr: 0.000216  min_lr: 0.000005  loss: 0.8793 (0.7907)  loss_scale: 65536.0000 (137900.6425)  weight_decay: 0.0500 (0.0500)  time: 0.3079  data: 0.0001  max mem: 41808
Epoch: [29]  [ 700/1349]  eta: 0:03:24  lr: 0.000215  min_lr: 0.000005  loss: 0.8705 (0.7906)  loss_scale: 65536.0000 (136868.3367)  weight_decay: 0.0500 (0.0500)  time: 0.3089  data: 0.0001  max mem: 41808
Epoch: [29]  [ 710/1349]  eta: 0:03:21  lr: 0.000215  min_lr: 0.000005  loss: 0.7681 (0.7902)  loss_scale: 65536.0000 (135865.0689)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
Epoch: [29]  [ 720/1349]  eta: 0:03:17  lr: 0.000215  min_lr: 0.000005  loss: 0.7846 (0.7911)  loss_scale: 65536.0000 (134889.6311)  weight_decay: 0.0500 (0.0500)  time: 0.3079  data: 0.0001  max mem: 41808
Epoch: [29]  [ 730/1349]  eta: 0:03:14  lr: 0.000215  min_lr: 0.000005  loss: 0.7984 (0.7909)  loss_scale: 65536.0000 (133940.8810)  weight_decay: 0.0500 (0.0500)  time: 0.3079  data: 0.0001  max mem: 41808
Epoch: [29]  [ 740/1349]  eta: 0:03:11  lr: 0.000215  min_lr: 0.000005  loss: 0.7553 (0.7903)  loss_scale: 65536.0000 (133017.7382)  weight_decay: 0.0500 (0.0500)  time: 0.3066  data: 0.0001  max mem: 41808
Epoch: [29]  [ 750/1349]  eta: 0:03:08  lr: 0.000215  min_lr: 0.000005  loss: 0.7402 (0.7905)  loss_scale: 65536.0000 (132119.1798)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0001  max mem: 41808
Epoch: [29]  [ 760/1349]  eta: 0:03:05  lr: 0.000215  min_lr: 0.000005  loss: 0.7402 (0.7900)  loss_scale: 65536.0000 (131244.2365)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
Epoch: [29]  [ 770/1349]  eta: 0:03:01  lr: 0.000215  min_lr: 0.000005  loss: 0.8077 (0.7906)  loss_scale: 65536.0000 (130391.9896)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
Epoch: [29]  [ 780/1349]  eta: 0:02:58  lr: 0.000214  min_lr: 0.000005  loss: 0.7887 (0.7892)  loss_scale: 65536.0000 (129561.5672)  weight_decay: 0.0500 (0.0500)  time: 0.3064  data: 0.0001  max mem: 41808
Epoch: [29]  [ 790/1349]  eta: 0:02:55  lr: 0.000214  min_lr: 0.000005  loss: 0.7676 (0.7898)  loss_scale: 65536.0000 (128752.1416)  weight_decay: 0.0500 (0.0500)  time: 0.3066  data: 0.0001  max mem: 41808
[2025-05-23 21:06:24,311] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 21:06:24,312] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-05-23 21:06:24,312] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 21:06:24,312] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [29]  [ 800/1349]  eta: 0:02:52  lr: 0.000214  min_lr: 0.000005  loss: 0.7676 (0.7886)  loss_scale: 65536.0000 (128044.7441)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
Epoch: [29]  [ 810/1349]  eta: 0:02:49  lr: 0.000214  min_lr: 0.000005  loss: 0.8219 (0.7896)  loss_scale: 131072.0000 (128082.0715)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
Epoch: [29]  [ 820/1349]  eta: 0:02:46  lr: 0.000214  min_lr: 0.000005  loss: 0.8689 (0.7904)  loss_scale: 131072.0000 (128118.4896)  weight_decay: 0.0500 (0.0500)  time: 0.3086  data: 0.0002  max mem: 41808
Epoch: [29]  [ 830/1349]  eta: 0:02:42  lr: 0.000214  min_lr: 0.000005  loss: 0.8385 (0.7903)  loss_scale: 131072.0000 (128154.0313)  weight_decay: 0.0500 (0.0500)  time: 0.3084  data: 0.0002  max mem: 41808
Epoch: [29]  [ 840/1349]  eta: 0:02:39  lr: 0.000214  min_lr: 0.000005  loss: 0.8280 (0.7901)  loss_scale: 131072.0000 (128188.7277)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0002  max mem: 41808
Epoch: [29]  [ 850/1349]  eta: 0:02:36  lr: 0.000214  min_lr: 0.000005  loss: 0.8060 (0.7897)  loss_scale: 131072.0000 (128222.6087)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [29]  [ 860/1349]  eta: 0:02:33  lr: 0.000213  min_lr: 0.000005  loss: 0.8334 (0.7895)  loss_scale: 131072.0000 (128255.7027)  weight_decay: 0.0500 (0.0500)  time: 0.3057  data: 0.0001  max mem: 41808
Epoch: [29]  [ 870/1349]  eta: 0:02:30  lr: 0.000213  min_lr: 0.000005  loss: 0.8512 (0.7901)  loss_scale: 131072.0000 (128288.0367)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
[2025-05-23 21:06:48,306] [INFO] [logging.py:96:log_dist] [Rank 0] step=40000, skipped=244, lr=[5.064484452408036e-06, 5.064484452408036e-06, 6.7526459365440485e-06, 6.7526459365440485e-06, 9.003527915392065e-06, 9.003527915392065e-06, 1.200470388718942e-05, 1.200470388718942e-05, 1.6006271849585895e-05, 1.6006271849585895e-05, 2.134169579944786e-05, 2.134169579944786e-05, 2.845559439926381e-05, 2.845559439926381e-05, 3.794079253235175e-05, 3.794079253235175e-05, 5.0587723376469e-05, 5.0587723376469e-05, 6.745029783529199e-05, 6.745029783529199e-05, 8.9933730447056e-05, 8.9933730447056e-05, 0.00011991164059607466, 0.00011991164059607466, 0.00015988218746143288, 0.00015988218746143288, 0.00021317624994857717, 0.00021317624994857717], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-05-23 21:06:48,307] [INFO] [timer.py:260:stop] epoch=0/micro_step=40000/global_step=40000, RunningAvgSamplesPerSec=209.61973459611323, CurrSamplesPerSec=214.88038730996325, MemAllocated=1.37GB, MaxMemAllocated=40.83GB
Epoch: [29]  [ 880/1349]  eta: 0:02:26  lr: 0.000213  min_lr: 0.000005  loss: 0.8329 (0.7899)  loss_scale: 131072.0000 (128319.6368)  weight_decay: 0.0500 (0.0500)  time: 0.3078  data: 0.0001  max mem: 41808
Epoch: [29]  [ 890/1349]  eta: 0:02:23  lr: 0.000213  min_lr: 0.000005  loss: 0.7777 (0.7901)  loss_scale: 131072.0000 (128350.5275)  weight_decay: 0.0500 (0.0500)  time: 0.3060  data: 0.0001  max mem: 41808
Epoch: [29]  [ 900/1349]  eta: 0:02:20  lr: 0.000213  min_lr: 0.000005  loss: 0.8460 (0.7907)  loss_scale: 131072.0000 (128380.7325)  weight_decay: 0.0500 (0.0500)  time: 0.3061  data: 0.0001  max mem: 41808
[2025-05-23 21:06:55,340] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 40022
[2025-05-23 21:06:55,340] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 40022
[2025-05-23 21:06:55,340] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 21:06:55,340] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 21:06:55,340] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [29]  [ 910/1349]  eta: 0:02:17  lr: 0.000213  min_lr: 0.000005  loss: 0.8460 (0.7915)  loss_scale: 65536.0000 (127690.8891)  weight_decay: 0.0500 (0.0500)  time: 0.3056  data: 0.0001  max mem: 41808
Epoch: [29]  [ 920/1349]  eta: 0:02:14  lr: 0.000213  min_lr: 0.000005  loss: 0.8449 (0.7920)  loss_scale: 65536.0000 (127016.0261)  weight_decay: 0.0500 (0.0500)  time: 0.3063  data: 0.0001  max mem: 41808
Epoch: [29]  [ 930/1349]  eta: 0:02:11  lr: 0.000213  min_lr: 0.000005  loss: 0.8675 (0.7926)  loss_scale: 65536.0000 (126355.6606)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
Epoch: [29]  [ 940/1349]  eta: 0:02:07  lr: 0.000212  min_lr: 0.000005  loss: 0.8623 (0.7934)  loss_scale: 65536.0000 (125709.3305)  weight_decay: 0.0500 (0.0500)  time: 0.3059  data: 0.0001  max mem: 41808
Epoch: [29]  [ 950/1349]  eta: 0:02:04  lr: 0.000212  min_lr: 0.000005  loss: 0.8327 (0.7923)  loss_scale: 65536.0000 (125076.5931)  weight_decay: 0.0500 (0.0500)  time: 0.3064  data: 0.0001  max mem: 41808
Epoch: [29]  [ 960/1349]  eta: 0:02:01  lr: 0.000212  min_lr: 0.000005  loss: 0.8349 (0.7934)  loss_scale: 65536.0000 (124457.0239)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
Epoch: [29]  [ 970/1349]  eta: 0:01:58  lr: 0.000212  min_lr: 0.000005  loss: 0.8449 (0.7934)  loss_scale: 65536.0000 (123850.2163)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0001  max mem: 41808
Epoch: [29]  [ 980/1349]  eta: 0:01:55  lr: 0.000212  min_lr: 0.000005  loss: 0.8385 (0.7940)  loss_scale: 65536.0000 (123255.7798)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
Epoch: [29]  [ 990/1349]  eta: 0:01:52  lr: 0.000212  min_lr: 0.000005  loss: 0.8162 (0.7934)  loss_scale: 65536.0000 (122673.3401)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0002  max mem: 41808
Epoch: [29]  [1000/1349]  eta: 0:01:49  lr: 0.000212  min_lr: 0.000005  loss: 0.6723 (0.7927)  loss_scale: 65536.0000 (122102.5375)  weight_decay: 0.0500 (0.0500)  time: 0.3063  data: 0.0001  max mem: 41808
Epoch: [29]  [1010/1349]  eta: 0:01:45  lr: 0.000211  min_lr: 0.000005  loss: 0.7886 (0.7926)  loss_scale: 65536.0000 (121543.0267)  weight_decay: 0.0500 (0.0500)  time: 0.3064  data: 0.0001  max mem: 41808
Epoch: [29]  [1020/1349]  eta: 0:01:42  lr: 0.000211  min_lr: 0.000005  loss: 0.7754 (0.7917)  loss_scale: 65536.0000 (120994.4760)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
[2025-05-23 21:07:34,894] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 21:07:34,894] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 21:07:34,894] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-05-23 21:07:34,894] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [29]  [1030/1349]  eta: 0:01:39  lr: 0.000211  min_lr: 0.000005  loss: 0.7754 (0.7924)  loss_scale: 65536.0000 (120520.1319)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0002  max mem: 41808
Epoch: [29]  [1040/1349]  eta: 0:01:36  lr: 0.000211  min_lr: 0.000005  loss: 0.8887 (0.7929)  loss_scale: 131072.0000 (120621.4947)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0002  max mem: 41808
Epoch: [29]  [1050/1349]  eta: 0:01:33  lr: 0.000211  min_lr: 0.000005  loss: 0.8303 (0.7928)  loss_scale: 131072.0000 (120720.9286)  weight_decay: 0.0500 (0.0500)  time: 0.3078  data: 0.0001  max mem: 41808
Epoch: [29]  [1060/1349]  eta: 0:01:30  lr: 0.000211  min_lr: 0.000005  loss: 0.8083 (0.7926)  loss_scale: 131072.0000 (120818.4882)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0001  max mem: 41808
Epoch: [29]  [1070/1349]  eta: 0:01:27  lr: 0.000211  min_lr: 0.000005  loss: 0.8067 (0.7927)  loss_scale: 131072.0000 (120914.2260)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0001  max mem: 41808
Epoch: [29]  [1080/1349]  eta: 0:01:23  lr: 0.000211  min_lr: 0.000005  loss: 0.8067 (0.7927)  loss_scale: 131072.0000 (121008.1924)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [29]  [1090/1349]  eta: 0:01:20  lr: 0.000210  min_lr: 0.000005  loss: 0.7631 (0.7922)  loss_scale: 131072.0000 (121100.4363)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
Epoch: [29]  [1100/1349]  eta: 0:01:17  lr: 0.000210  min_lr: 0.000005  loss: 0.7891 (0.7924)  loss_scale: 131072.0000 (121191.0045)  weight_decay: 0.0500 (0.0500)  time: 0.3092  data: 0.0002  max mem: 41808
Epoch: [29]  [1110/1349]  eta: 0:01:14  lr: 0.000210  min_lr: 0.000005  loss: 0.8106 (0.7920)  loss_scale: 131072.0000 (121279.9424)  weight_decay: 0.0500 (0.0500)  time: 0.3096  data: 0.0002  max mem: 41808
Epoch: [29]  [1120/1349]  eta: 0:01:11  lr: 0.000210  min_lr: 0.000005  loss: 0.7887 (0.7920)  loss_scale: 131072.0000 (121367.2935)  weight_decay: 0.0500 (0.0500)  time: 0.3079  data: 0.0001  max mem: 41808
Epoch: [29]  [1130/1349]  eta: 0:01:08  lr: 0.000210  min_lr: 0.000005  loss: 0.7887 (0.7920)  loss_scale: 131072.0000 (121453.0999)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
Epoch: [29]  [1140/1349]  eta: 0:01:05  lr: 0.000210  min_lr: 0.000005  loss: 0.8121 (0.7925)  loss_scale: 131072.0000 (121537.4023)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
Epoch: [29]  [1150/1349]  eta: 0:01:02  lr: 0.000210  min_lr: 0.000005  loss: 0.8889 (0.7930)  loss_scale: 131072.0000 (121620.2398)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
[2025-05-23 21:08:14,325] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 21:08:14,325] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 21:08:14,325] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 21:08:14,325] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [29]  [1160/1349]  eta: 0:00:58  lr: 0.000210  min_lr: 0.000005  loss: 0.8358 (0.7926)  loss_scale: 131072.0000 (122040.3376)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0001  max mem: 41808
[2025-05-23 21:08:16,778] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 40287
[2025-05-23 21:08:16,778] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 40287
[2025-05-23 21:08:16,778] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 21:08:16,778] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 21:08:16,778] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [29]  [1170/1349]  eta: 0:00:55  lr: 0.000209  min_lr: 0.000005  loss: 0.7344 (0.7927)  loss_scale: 131072.0000 (122677.1238)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0002  max mem: 41808
Epoch: [29]  [1180/1349]  eta: 0:00:52  lr: 0.000209  min_lr: 0.000005  loss: 0.7622 (0.7922)  loss_scale: 131072.0000 (122748.2066)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [29]  [1190/1349]  eta: 0:00:49  lr: 0.000209  min_lr: 0.000005  loss: 0.7457 (0.7915)  loss_scale: 131072.0000 (122818.0957)  weight_decay: 0.0500 (0.0500)  time: 0.3083  data: 0.0001  max mem: 41808
Epoch: [29]  [1200/1349]  eta: 0:00:46  lr: 0.000209  min_lr: 0.000005  loss: 0.7470 (0.7917)  loss_scale: 131072.0000 (122886.8210)  weight_decay: 0.0500 (0.0500)  time: 0.3097  data: 0.0001  max mem: 41808
Epoch: [29]  [1210/1349]  eta: 0:00:43  lr: 0.000209  min_lr: 0.000005  loss: 0.7261 (0.7909)  loss_scale: 131072.0000 (122954.4112)  weight_decay: 0.0500 (0.0500)  time: 0.3089  data: 0.0001  max mem: 41808
[2025-05-23 21:08:30,973] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 40333
[2025-05-23 21:08:30,973] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 21:08:30,973] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 40333
[2025-05-23 21:08:30,973] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 21:08:30,973] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [29]  [1220/1349]  eta: 0:00:40  lr: 0.000209  min_lr: 0.000005  loss: 0.7253 (0.7909)  loss_scale: 131072.0000 (122537.8280)  weight_decay: 0.0500 (0.0500)  time: 0.3081  data: 0.0001  max mem: 41808
Epoch: [29]  [1230/1349]  eta: 0:00:37  lr: 0.000209  min_lr: 0.000005  loss: 0.7937 (0.7909)  loss_scale: 65536.0000 (122074.7750)  weight_decay: 0.0500 (0.0500)  time: 0.3095  data: 0.0001  max mem: 41808
Epoch: [29]  [1240/1349]  eta: 0:00:33  lr: 0.000209  min_lr: 0.000005  loss: 0.8058 (0.7906)  loss_scale: 65536.0000 (121619.1845)  weight_decay: 0.0500 (0.0500)  time: 0.3114  data: 0.0001  max mem: 41808
Epoch: [29]  [1250/1349]  eta: 0:00:30  lr: 0.000208  min_lr: 0.000005  loss: 0.7773 (0.7905)  loss_scale: 65536.0000 (121170.8777)  weight_decay: 0.0500 (0.0500)  time: 0.3105  data: 0.0001  max mem: 41808
Epoch: [29]  [1260/1349]  eta: 0:00:27  lr: 0.000208  min_lr: 0.000005  loss: 0.7822 (0.7908)  loss_scale: 65536.0000 (120729.6812)  weight_decay: 0.0500 (0.0500)  time: 0.3086  data: 0.0001  max mem: 41808
Epoch: [29]  [1270/1349]  eta: 0:00:24  lr: 0.000208  min_lr: 0.000005  loss: 0.8494 (0.7905)  loss_scale: 65536.0000 (120295.4272)  weight_decay: 0.0500 (0.0500)  time: 0.3085  data: 0.0001  max mem: 41808
Epoch: [29]  [1280/1349]  eta: 0:00:21  lr: 0.000208  min_lr: 0.000005  loss: 0.8494 (0.7907)  loss_scale: 65536.0000 (119867.9532)  weight_decay: 0.0500 (0.0500)  time: 0.3079  data: 0.0001  max mem: 41808
Epoch: [29]  [1290/1349]  eta: 0:00:18  lr: 0.000208  min_lr: 0.000005  loss: 0.8212 (0.7907)  loss_scale: 65536.0000 (119447.1015)  weight_decay: 0.0500 (0.0500)  time: 0.3080  data: 0.0001  max mem: 41808
Epoch: [29]  [1300/1349]  eta: 0:00:15  lr: 0.000208  min_lr: 0.000005  loss: 0.7769 (0.7909)  loss_scale: 65536.0000 (119032.7194)  weight_decay: 0.0500 (0.0500)  time: 0.3081  data: 0.0001  max mem: 41808
Epoch: [29]  [1310/1349]  eta: 0:00:12  lr: 0.000208  min_lr: 0.000005  loss: 0.7769 (0.7907)  loss_scale: 65536.0000 (118624.6590)  weight_decay: 0.0500 (0.0500)  time: 0.3084  data: 0.0001  max mem: 41808
Epoch: [29]  [1320/1349]  eta: 0:00:09  lr: 0.000208  min_lr: 0.000005  loss: 0.7296 (0.7904)  loss_scale: 65536.0000 (118222.7767)  weight_decay: 0.0500 (0.0500)  time: 0.3089  data: 0.0001  max mem: 41808
Epoch: [29]  [1330/1349]  eta: 0:00:05  lr: 0.000207  min_lr: 0.000005  loss: 0.7965 (0.7904)  loss_scale: 65536.0000 (117826.9331)  weight_decay: 0.0500 (0.0500)  time: 0.3062  data: 0.0001  max mem: 41808
Epoch: [29]  [1340/1349]  eta: 0:00:02  lr: 0.000207  min_lr: 0.000005  loss: 0.7672 (0.7899)  loss_scale: 65536.0000 (117436.9933)  weight_decay: 0.0500 (0.0500)  time: 0.3028  data: 0.0001  max mem: 41808
[2025-05-23 21:09:10,715] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 21:09:10,715] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-05-23 21:09:10,716] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 21:09:10,716] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [29]  [1348/1349]  eta: 0:00:00  lr: 0.000207  min_lr: 0.000005  loss: 0.7770 (0.7903)  loss_scale: 65536.0000 (117517.8532)  weight_decay: 0.0500 (0.0500)  time: 0.3024  data: 0.0001  max mem: 41808
Epoch: [29] Total time: 0:07:00 (0.3116 s / it)
Averaged stats: lr: 0.000207  min_lr: 0.000005  loss: 0.7770 (0.7918)  loss_scale: 65536.0000 (117517.8532)  weight_decay: 0.0500 (0.0500)  total_time: 420.4104 (420.4043)
[2025-05-23 21:09:13,177] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-29 is about to be saved!
[2025-05-23 21:09:13,181] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /project/mmendoscope/Natural_Comparison/Cholec75/unified_base_st_settingE_videomae-st_Cholec80_0.0005_0.75_online_key_frame_frame16_Fixed_Stride_4/checkpoint-29/mp_rank_00_model_states.pt
[2025-05-23 21:09:13,181] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /project/mmendoscope/Natural_Comparison/Cholec75/unified_base_st_settingE_videomae-st_Cholec80_0.0005_0.75_online_key_frame_frame16_Fixed_Stride_4/checkpoint-29/mp_rank_00_model_states.pt...
[2025-05-23 21:09:13,181] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-29 is ready now!
[2025-05-23 21:09:15,966] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /project/mmendoscope/Natural_Comparison/Cholec75/unified_base_st_settingE_videomae-st_Cholec80_0.0005_0.75_online_key_frame_frame16_Fixed_Stride_4/checkpoint-29/mp_rank_00_model_states.pt.
[2025-05-23 21:09:15,967] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-29 is ready now!
Val:  [  0/346]  eta: 0:56:49  loss: 2.8435 (2.8435)  acc1: 3.1250 (3.1250)  acc5: 89.0625 (89.0625)  time: 9.8537  data: 8.9392  max mem: 41808
Val:  [ 10/346]  eta: 0:11:33  loss: 0.1569 (0.5511)  acc1: 100.0000 (85.1562)  acc5: 100.0000 (98.5795)  time: 2.0654  data: 1.2413  max mem: 41808
Val:  [ 20/346]  eta: 0:08:01  loss: 0.1410 (0.4262)  acc1: 100.0000 (89.1741)  acc5: 100.0000 (99.2188)  time: 1.0594  data: 0.2359  max mem: 41808
Val:  [ 30/346]  eta: 0:06:42  loss: 0.1136 (0.3684)  acc1: 100.0000 (91.1542)  acc5: 100.0000 (99.4708)  time: 0.8371  data: 0.0003  max mem: 41808
Val:  [ 40/346]  eta: 0:05:56  loss: 0.1152 (0.4288)  acc1: 99.2188 (89.5960)  acc5: 100.0000 (99.2569)  time: 0.8390  data: 0.0003  max mem: 41808
Val:  [ 50/346]  eta: 0:05:26  loss: 0.1354 (0.3756)  acc1: 99.2188 (91.3297)  acc5: 100.0000 (99.3873)  time: 0.8413  data: 0.0003  max mem: 41808
Val:  [ 60/346]  eta: 0:05:03  loss: 0.1451 (0.3663)  acc1: 97.6562 (91.3550)  acc5: 100.0000 (99.4877)  time: 0.8478  data: 0.0003  max mem: 41808
Val:  [ 70/346]  eta: 0:04:44  loss: 0.2298 (0.3863)  acc1: 95.3125 (90.7680)  acc5: 100.0000 (99.4388)  time: 0.8497  data: 0.0003  max mem: 41808
Val:  [ 80/346]  eta: 0:04:28  loss: 0.2310 (0.3857)  acc1: 92.9688 (90.8951)  acc5: 100.0000 (99.3634)  time: 0.8490  data: 0.0003  max mem: 41808
Val:  [ 90/346]  eta: 0:04:13  loss: 0.2719 (0.3827)  acc1: 93.7500 (90.9083)  acc5: 100.0000 (99.3561)  time: 0.8471  data: 0.0003  max mem: 41808
Val:  [100/346]  eta: 0:04:00  loss: 0.1838 (0.3597)  acc1: 98.4375 (91.7698)  acc5: 100.0000 (99.4199)  time: 0.8508  data: 0.0003  max mem: 41808
Val:  [110/346]  eta: 0:03:48  loss: 0.1441 (0.3717)  acc1: 98.4375 (91.4062)  acc5: 100.0000 (99.4299)  time: 0.8546  data: 0.0003  max mem: 41808
Val:  [120/346]  eta: 0:03:36  loss: 0.1930 (0.3771)  acc1: 95.3125 (91.2448)  acc5: 100.0000 (99.3995)  time: 0.8474  data: 0.0003  max mem: 41808
Val:  [130/346]  eta: 0:03:25  loss: 0.1192 (0.3740)  acc1: 100.0000 (91.3108)  acc5: 100.0000 (99.4454)  time: 0.8516  data: 0.0003  max mem: 41808
Val:  [140/346]  eta: 0:03:14  loss: 0.2361 (0.3710)  acc1: 95.3125 (91.3896)  acc5: 100.0000 (99.4847)  time: 0.8547  data: 0.0003  max mem: 41808
Val:  [150/346]  eta: 0:03:03  loss: 0.2793 (0.3723)  acc1: 95.3125 (91.4114)  acc5: 100.0000 (99.4981)  time: 0.8565  data: 0.0003  max mem: 41808
Val:  [160/346]  eta: 0:02:53  loss: 0.2196 (0.3662)  acc1: 96.8750 (91.5906)  acc5: 100.0000 (99.5148)  time: 0.8686  data: 0.0003  max mem: 41808
Val:  [170/346]  eta: 0:02:43  loss: 0.2120 (0.3653)  acc1: 97.6562 (91.5844)  acc5: 100.0000 (99.5431)  time: 0.8667  data: 0.0003  max mem: 41808
Val:  [180/346]  eta: 0:02:33  loss: 0.2182 (0.3781)  acc1: 96.8750 (91.0825)  acc5: 100.0000 (99.5684)  time: 0.8596  data: 0.0003  max mem: 41808
Val:  [190/346]  eta: 0:02:23  loss: 0.2654 (0.3756)  acc1: 92.1875 (91.1813)  acc5: 100.0000 (99.5173)  time: 0.8574  data: 0.0002  max mem: 41808
Val:  [200/346]  eta: 0:02:14  loss: 0.4398 (0.3904)  acc1: 88.2812 (90.6172)  acc5: 100.0000 (99.4597)  time: 0.8748  data: 0.0003  max mem: 41808
Val:  [210/346]  eta: 0:02:04  loss: 0.1931 (0.3828)  acc1: 96.0938 (90.8768)  acc5: 100.0000 (99.4853)  time: 0.8771  data: 0.0003  max mem: 41808
Val:  [220/346]  eta: 0:01:55  loss: 0.1398 (0.3796)  acc1: 100.0000 (91.0139)  acc5: 100.0000 (99.4768)  time: 0.8555  data: 0.0002  max mem: 41808
Val:  [230/346]  eta: 0:01:45  loss: 0.1381 (0.3712)  acc1: 98.4375 (91.2845)  acc5: 100.0000 (99.4927)  time: 0.8601  data: 0.0003  max mem: 41808
Val:  [240/346]  eta: 0:01:36  loss: 0.1670 (0.3761)  acc1: 97.6562 (91.1793)  acc5: 100.0000 (99.5073)  time: 0.8680  data: 0.0003  max mem: 41808
Val:  [250/346]  eta: 0:01:27  loss: 0.1900 (0.3754)  acc1: 97.6562 (91.2506)  acc5: 100.0000 (99.5051)  time: 0.8649  data: 0.0003  max mem: 41808
Val:  [260/346]  eta: 0:01:17  loss: 0.1483 (0.3721)  acc1: 98.4375 (91.3554)  acc5: 100.0000 (99.4971)  time: 0.8627  data: 0.0003  max mem: 41808
Val:  [270/346]  eta: 0:01:08  loss: 0.1377 (0.3690)  acc1: 98.4375 (91.4697)  acc5: 100.0000 (99.5042)  time: 0.8565  data: 0.0003  max mem: 41808
Val:  [280/346]  eta: 0:00:59  loss: 0.1232 (0.3674)  acc1: 100.0000 (91.5369)  acc5: 100.0000 (99.5190)  time: 0.8694  data: 0.0003  max mem: 41808
Val:  [290/346]  eta: 0:00:50  loss: 0.1103 (0.3590)  acc1: 100.0000 (91.7955)  acc5: 100.0000 (99.5355)  time: 0.8709  data: 0.0003  max mem: 41808
Val:  [300/346]  eta: 0:00:41  loss: 0.1135 (0.3594)  acc1: 100.0000 (91.8060)  acc5: 100.0000 (99.5302)  time: 0.8608  data: 0.0003  max mem: 41808
Val:  [310/346]  eta: 0:00:32  loss: 0.1319 (0.3606)  acc1: 98.4375 (91.7680)  acc5: 100.0000 (99.4951)  time: 0.8531  data: 0.0003  max mem: 41808
Val:  [320/346]  eta: 0:00:23  loss: 0.1456 (0.3617)  acc1: 98.4375 (91.7105)  acc5: 100.0000 (99.5084)  time: 0.8402  data: 0.0003  max mem: 41808
Val:  [330/346]  eta: 0:00:14  loss: 0.4012 (0.3748)  acc1: 88.2812 (91.3000)  acc5: 100.0000 (99.4335)  time: 0.8507  data: 0.0002  max mem: 41808
Val:  [340/346]  eta: 0:00:05  loss: 0.4752 (0.3826)  acc1: 88.2812 (91.1222)  acc5: 100.0000 (99.4364)  time: 0.8392  data: 0.0002  max mem: 41808
Val:  [345/346]  eta: 0:00:00  loss: 0.1983 (0.3792)  acc1: 95.4023 (91.2243)  acc5: 100.0000 (99.4440)  time: 0.7977  data: 0.0001  max mem: 41808
Val: Total time: 0:05:08 (0.8923 s / it)
* Acc@1 91.310 Acc@5 99.455 loss 0.376
Accuracy of the network on the 88494 val videos: 91.3%
Max accuracy: 91.50%   Max Epoch: 25
Epoch: [30]  [   0/1349]  eta: 1:55:09  lr: 0.000207  min_lr: 0.000005  loss: 0.8861 (0.8861)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 5.1223  data: 4.4257  max mem: 41808
Epoch: [30]  [  10/1349]  eta: 0:16:43  lr: 0.000207  min_lr: 0.000005  loss: 0.8116 (0.7920)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7498  data: 0.4024  max mem: 41808
Epoch: [30]  [  20/1349]  eta: 0:11:58  lr: 0.000207  min_lr: 0.000005  loss: 0.7993 (0.7761)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3119  data: 0.0001  max mem: 41808
Epoch: [30]  [  30/1349]  eta: 0:10:13  lr: 0.000207  min_lr: 0.000005  loss: 0.7712 (0.7844)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3089  data: 0.0001  max mem: 41808
Epoch: [30]  [  40/1349]  eta: 0:09:18  lr: 0.000207  min_lr: 0.000005  loss: 0.7967 (0.7835)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3061  data: 0.0001  max mem: 41808
Epoch: [30]  [  50/1349]  eta: 0:08:43  lr: 0.000207  min_lr: 0.000005  loss: 0.8236 (0.7936)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
Epoch: [30]  [  60/1349]  eta: 0:08:19  lr: 0.000206  min_lr: 0.000005  loss: 0.8482 (0.7909)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3078  data: 0.0001  max mem: 41808
[2025-05-23 21:14:49,279] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 40533
[2025-05-23 21:14:49,279] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 40533
[2025-05-23 21:14:49,279] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 21:14:49,279] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 21:14:49,280] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [30]  [  70/1349]  eta: 0:08:01  lr: 0.000206  min_lr: 0.000005  loss: 0.8469 (0.7960)  loss_scale: 131072.0000 (123687.6620)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0002  max mem: 41808
Epoch: [30]  [  80/1349]  eta: 0:07:46  lr: 0.000206  min_lr: 0.000005  loss: 0.8292 (0.7938)  loss_scale: 65536.0000 (116508.4444)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0002  max mem: 41808
Epoch: [30]  [  90/1349]  eta: 0:07:34  lr: 0.000206  min_lr: 0.000005  loss: 0.7999 (0.7949)  loss_scale: 65536.0000 (110907.0769)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0002  max mem: 41808
Epoch: [30]  [ 100/1349]  eta: 0:07:24  lr: 0.000206  min_lr: 0.000005  loss: 0.7999 (0.7967)  loss_scale: 65536.0000 (106414.8911)  weight_decay: 0.0500 (0.0500)  time: 0.3063  data: 0.0001  max mem: 41808
Epoch: [30]  [ 110/1349]  eta: 0:07:14  lr: 0.000206  min_lr: 0.000005  loss: 0.7880 (0.7920)  loss_scale: 65536.0000 (102732.1081)  weight_decay: 0.0500 (0.0500)  time: 0.3061  data: 0.0001  max mem: 41808
Epoch: [30]  [ 120/1349]  eta: 0:07:06  lr: 0.000206  min_lr: 0.000005  loss: 0.7453 (0.7887)  loss_scale: 65536.0000 (99658.0496)  weight_decay: 0.0500 (0.0500)  time: 0.3061  data: 0.0001  max mem: 41808
Epoch: [30]  [ 130/1349]  eta: 0:06:59  lr: 0.000206  min_lr: 0.000005  loss: 0.7648 (0.7913)  loss_scale: 65536.0000 (97053.3130)  weight_decay: 0.0500 (0.0500)  time: 0.3066  data: 0.0002  max mem: 41808
Epoch: [30]  [ 140/1349]  eta: 0:06:52  lr: 0.000205  min_lr: 0.000005  loss: 0.7970 (0.7931)  loss_scale: 65536.0000 (94818.0426)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
Epoch: [30]  [ 150/1349]  eta: 0:06:46  lr: 0.000205  min_lr: 0.000005  loss: 0.8025 (0.7954)  loss_scale: 65536.0000 (92878.8344)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0001  max mem: 41808
Epoch: [30]  [ 160/1349]  eta: 0:06:40  lr: 0.000205  min_lr: 0.000005  loss: 0.8339 (0.7974)  loss_scale: 65536.0000 (91180.5217)  weight_decay: 0.0500 (0.0500)  time: 0.3064  data: 0.0001  max mem: 41808
Epoch: [30]  [ 170/1349]  eta: 0:06:35  lr: 0.000205  min_lr: 0.000005  loss: 0.8276 (0.7983)  loss_scale: 65536.0000 (89680.8421)  weight_decay: 0.0500 (0.0500)  time: 0.3062  data: 0.0001  max mem: 41808
Epoch: [30]  [ 180/1349]  eta: 0:06:30  lr: 0.000205  min_lr: 0.000005  loss: 0.8033 (0.7988)  loss_scale: 65536.0000 (88346.8729)  weight_decay: 0.0500 (0.0500)  time: 0.3098  data: 0.0001  max mem: 41808
Epoch: [30]  [ 190/1349]  eta: 0:06:25  lr: 0.000205  min_lr: 0.000005  loss: 0.8262 (0.8015)  loss_scale: 65536.0000 (87152.5864)  weight_decay: 0.0500 (0.0500)  time: 0.3115  data: 0.0002  max mem: 41808
[2025-05-23 21:15:28,937] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 21:15:28,938] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-05-23 21:15:28,938] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 21:15:28,938] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [30]  [ 200/1349]  eta: 0:06:21  lr: 0.000205  min_lr: 0.000005  loss: 0.8781 (0.8016)  loss_scale: 65536.0000 (89011.5821)  weight_decay: 0.0500 (0.0500)  time: 0.3088  data: 0.0002  max mem: 41808
Epoch: [30]  [ 210/1349]  eta: 0:06:16  lr: 0.000205  min_lr: 0.000005  loss: 0.8548 (0.8036)  loss_scale: 131072.0000 (91004.9668)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
Epoch: [30]  [ 220/1349]  eta: 0:06:11  lr: 0.000204  min_lr: 0.000005  loss: 0.8469 (0.8043)  loss_scale: 131072.0000 (92817.9548)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [30]  [ 230/1349]  eta: 0:06:07  lr: 0.000204  min_lr: 0.000005  loss: 0.8234 (0.8046)  loss_scale: 131072.0000 (94473.9740)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0001  max mem: 41808
Epoch: [30]  [ 240/1349]  eta: 0:06:03  lr: 0.000204  min_lr: 0.000005  loss: 0.7899 (0.8050)  loss_scale: 131072.0000 (95992.5643)  weight_decay: 0.0500 (0.0500)  time: 0.3080  data: 0.0001  max mem: 41808
Epoch: [30]  [ 250/1349]  eta: 0:05:59  lr: 0.000204  min_lr: 0.000005  loss: 0.7732 (0.8033)  loss_scale: 131072.0000 (97390.1514)  weight_decay: 0.0500 (0.0500)  time: 0.3078  data: 0.0001  max mem: 41808
Epoch: [30]  [ 260/1349]  eta: 0:05:55  lr: 0.000204  min_lr: 0.000005  loss: 0.7637 (0.8012)  loss_scale: 131072.0000 (98680.6437)  weight_decay: 0.0500 (0.0500)  time: 0.3082  data: 0.0001  max mem: 41808
Epoch: [30]  [ 270/1349]  eta: 0:05:51  lr: 0.000204  min_lr: 0.000005  loss: 0.7899 (0.8016)  loss_scale: 131072.0000 (99875.8967)  weight_decay: 0.0500 (0.0500)  time: 0.3087  data: 0.0001  max mem: 41808
Epoch: [30]  [ 280/1349]  eta: 0:05:47  lr: 0.000204  min_lr: 0.000005  loss: 0.7899 (0.7988)  loss_scale: 131072.0000 (100986.0783)  weight_decay: 0.0500 (0.0500)  time: 0.3089  data: 0.0002  max mem: 41808
Epoch: [30]  [ 290/1349]  eta: 0:05:43  lr: 0.000203  min_lr: 0.000005  loss: 0.7809 (0.8007)  loss_scale: 131072.0000 (102019.9588)  weight_decay: 0.0500 (0.0500)  time: 0.3082  data: 0.0002  max mem: 41808
Epoch: [30]  [ 300/1349]  eta: 0:05:39  lr: 0.000203  min_lr: 0.000005  loss: 0.7837 (0.8008)  loss_scale: 131072.0000 (102985.1429)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
Epoch: [30]  [ 310/1349]  eta: 0:05:35  lr: 0.000203  min_lr: 0.000005  loss: 0.8506 (0.8004)  loss_scale: 131072.0000 (103888.2572)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
[2025-05-23 21:16:08,358] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 21:16:08,358] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 21:16:08,358] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 21:16:08,358] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [30]  [ 320/1349]  eta: 0:05:32  lr: 0.000203  min_lr: 0.000005  loss: 0.7026 (0.7985)  loss_scale: 131072.0000 (105143.4268)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
[2025-05-23 21:16:10,831] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 40798
[2025-05-23 21:16:10,831] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 40798
[2025-05-23 21:16:10,831] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 21:16:10,831] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 21:16:10,831] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [30]  [ 330/1349]  eta: 0:05:28  lr: 0.000203  min_lr: 0.000005  loss: 0.7209 (0.7983)  loss_scale: 131072.0000 (108698.6828)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [30]  [ 340/1349]  eta: 0:05:24  lr: 0.000203  min_lr: 0.000005  loss: 0.7209 (0.7948)  loss_scale: 131072.0000 (109354.7918)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
Epoch: [30]  [ 350/1349]  eta: 0:05:21  lr: 0.000203  min_lr: 0.000005  loss: 0.7140 (0.7932)  loss_scale: 131072.0000 (109973.5157)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
Epoch: [30]  [ 360/1349]  eta: 0:05:17  lr: 0.000203  min_lr: 0.000005  loss: 0.7636 (0.7924)  loss_scale: 131072.0000 (110557.9612)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
Epoch: [30]  [ 370/1349]  eta: 0:05:13  lr: 0.000202  min_lr: 0.000005  loss: 0.8141 (0.7932)  loss_scale: 131072.0000 (111110.9003)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
Epoch: [30]  [ 380/1349]  eta: 0:05:10  lr: 0.000202  min_lr: 0.000005  loss: 0.8285 (0.7937)  loss_scale: 131072.0000 (111634.8136)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0001  max mem: 41808
Epoch: [30]  [ 390/1349]  eta: 0:05:06  lr: 0.000202  min_lr: 0.000005  loss: 0.8088 (0.7932)  loss_scale: 131072.0000 (112131.9284)  weight_decay: 0.0500 (0.0500)  time: 0.3059  data: 0.0001  max mem: 41808
Epoch: [30]  [ 400/1349]  eta: 0:05:03  lr: 0.000202  min_lr: 0.000005  loss: 0.7492 (0.7922)  loss_scale: 131072.0000 (112604.2494)  weight_decay: 0.0500 (0.0500)  time: 0.3060  data: 0.0001  max mem: 41808
Epoch: [30]  [ 410/1349]  eta: 0:04:59  lr: 0.000202  min_lr: 0.000005  loss: 0.7492 (0.7933)  loss_scale: 131072.0000 (113053.5864)  weight_decay: 0.0500 (0.0500)  time: 0.3064  data: 0.0001  max mem: 41808
Epoch: [30]  [ 420/1349]  eta: 0:04:56  lr: 0.000202  min_lr: 0.000005  loss: 0.8049 (0.7933)  loss_scale: 131072.0000 (113481.5772)  weight_decay: 0.0500 (0.0500)  time: 0.3066  data: 0.0001  max mem: 41808
Epoch: [30]  [ 430/1349]  eta: 0:04:52  lr: 0.000202  min_lr: 0.000005  loss: 0.7474 (0.7911)  loss_scale: 131072.0000 (113889.7077)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [30]  [ 440/1349]  eta: 0:04:49  lr: 0.000202  min_lr: 0.000005  loss: 0.8158 (0.7925)  loss_scale: 131072.0000 (114279.3288)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [30]  [ 450/1349]  eta: 0:04:46  lr: 0.000201  min_lr: 0.000005  loss: 0.8736 (0.7933)  loss_scale: 131072.0000 (114651.6718)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
[2025-05-23 21:16:50,396] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 21:16:50,396] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 21:16:50,396] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 21:16:50,396] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 21:16:51,314] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 40930
[2025-05-23 21:16:51,314] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 40930
[2025-05-23 21:16:51,314] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 21:16:51,314] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 21:16:51,314] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [30]  [ 460/1349]  eta: 0:04:42  lr: 0.000201  min_lr: 0.000005  loss: 0.8352 (0.7945)  loss_scale: 131072.0000 (115860.8243)  weight_decay: 0.0500 (0.0500)  time: 0.3059  data: 0.0001  max mem: 41808
Epoch: [30]  [ 470/1349]  eta: 0:04:39  lr: 0.000201  min_lr: 0.000005  loss: 0.7932 (0.7932)  loss_scale: 131072.0000 (116183.7792)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
Epoch: [30]  [ 480/1349]  eta: 0:04:35  lr: 0.000201  min_lr: 0.000005  loss: 0.7829 (0.7930)  loss_scale: 131072.0000 (116493.3056)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
[2025-05-23 21:16:59,613] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 40957
[2025-05-23 21:16:59,613] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 40957
[2025-05-23 21:16:59,613] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 21:16:59,613] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 21:16:59,613] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [30]  [ 490/1349]  eta: 0:04:32  lr: 0.000201  min_lr: 0.000005  loss: 0.8412 (0.7942)  loss_scale: 131072.0000 (116256.3259)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [30]  [ 500/1349]  eta: 0:04:29  lr: 0.000201  min_lr: 0.000005  loss: 0.8412 (0.7952)  loss_scale: 65536.0000 (115243.9441)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
Epoch: [30]  [ 510/1349]  eta: 0:04:25  lr: 0.000201  min_lr: 0.000005  loss: 0.8737 (0.7960)  loss_scale: 65536.0000 (114271.1859)  weight_decay: 0.0500 (0.0500)  time: 0.3063  data: 0.0001  max mem: 41808
Epoch: [30]  [ 520/1349]  eta: 0:04:22  lr: 0.000201  min_lr: 0.000005  loss: 0.8737 (0.7970)  loss_scale: 65536.0000 (113335.7697)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
[2025-05-23 21:17:12,523] [INFO] [logging.py:96:log_dist] [Rank 0] step=41000, skipped=251, lr=[4.762503226308375e-06, 4.762503226308375e-06, 6.3500043017445e-06, 6.3500043017445e-06, 8.466672402326001e-06, 8.466672402326001e-06, 1.1288896536434667e-05, 1.1288896536434667e-05, 1.5051862048579557e-05, 1.5051862048579557e-05, 2.0069149398106076e-05, 2.0069149398106076e-05, 2.6758865864141434e-05, 2.6758865864141434e-05, 3.567848781885524e-05, 3.567848781885524e-05, 4.757131709180699e-05, 4.757131709180699e-05, 6.3428422789076e-05, 6.3428422789076e-05, 8.457123038543465e-05, 8.457123038543465e-05, 0.00011276164051391288, 0.00011276164051391288, 0.0001503488540185505, 0.0001503488540185505, 0.00020046513869140066, 0.00020046513869140066], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-05-23 21:17:12,524] [INFO] [timer.py:260:stop] epoch=0/micro_step=41000/global_step=41000, RunningAvgSamplesPerSec=209.70145242819203, CurrSamplesPerSec=213.63847021951557, MemAllocated=1.37GB, MaxMemAllocated=40.83GB
Epoch: [30]  [ 530/1349]  eta: 0:04:19  lr: 0.000200  min_lr: 0.000005  loss: 0.8467 (0.7979)  loss_scale: 65536.0000 (112435.5857)  weight_decay: 0.0500 (0.0500)  time: 0.3081  data: 0.0002  max mem: 41808
Epoch: [30]  [ 540/1349]  eta: 0:04:15  lr: 0.000200  min_lr: 0.000005  loss: 0.8295 (0.7985)  loss_scale: 65536.0000 (111568.6802)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
Epoch: [30]  [ 550/1349]  eta: 0:04:12  lr: 0.000200  min_lr: 0.000005  loss: 0.7963 (0.7978)  loss_scale: 65536.0000 (110733.2414)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
Epoch: [30]  [ 560/1349]  eta: 0:04:09  lr: 0.000200  min_lr: 0.000005  loss: 0.7749 (0.7973)  loss_scale: 65536.0000 (109927.5865)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [30]  [ 570/1349]  eta: 0:04:06  lr: 0.000200  min_lr: 0.000005  loss: 0.7798 (0.7965)  loss_scale: 65536.0000 (109150.1506)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [30]  [ 580/1349]  eta: 0:04:02  lr: 0.000200  min_lr: 0.000005  loss: 0.7811 (0.7969)  loss_scale: 65536.0000 (108399.4768)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0001  max mem: 41808
Epoch: [30]  [ 590/1349]  eta: 0:03:59  lr: 0.000200  min_lr: 0.000005  loss: 0.8664 (0.7970)  loss_scale: 65536.0000 (107674.2064)  weight_decay: 0.0500 (0.0500)  time: 0.3061  data: 0.0001  max mem: 41808
Epoch: [30]  [ 600/1349]  eta: 0:03:56  lr: 0.000200  min_lr: 0.000005  loss: 0.8036 (0.7964)  loss_scale: 65536.0000 (106973.0715)  weight_decay: 0.0500 (0.0500)  time: 0.3059  data: 0.0001  max mem: 41808
Epoch: [30]  [ 610/1349]  eta: 0:03:52  lr: 0.000199  min_lr: 0.000005  loss: 0.7598 (0.7958)  loss_scale: 65536.0000 (106294.8871)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
[2025-05-23 21:17:39,220] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 21:17:39,220] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-05-23 21:17:39,220] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 21:17:39,221] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [30]  [ 620/1349]  eta: 0:03:49  lr: 0.000199  min_lr: 0.000005  loss: 0.8037 (0.7963)  loss_scale: 65536.0000 (106166.2093)  weight_decay: 0.0500 (0.0500)  time: 0.3079  data: 0.0001  max mem: 41808
Epoch: [30]  [ 630/1349]  eta: 0:03:46  lr: 0.000199  min_lr: 0.000005  loss: 0.8500 (0.7974)  loss_scale: 131072.0000 (106560.9128)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [30]  [ 640/1349]  eta: 0:03:43  lr: 0.000199  min_lr: 0.000005  loss: 0.8177 (0.7968)  loss_scale: 131072.0000 (106943.3011)  weight_decay: 0.0500 (0.0500)  time: 0.3063  data: 0.0001  max mem: 41808
Epoch: [30]  [ 650/1349]  eta: 0:03:39  lr: 0.000199  min_lr: 0.000005  loss: 0.7952 (0.7963)  loss_scale: 131072.0000 (107313.9416)  weight_decay: 0.0500 (0.0500)  time: 0.3064  data: 0.0001  max mem: 41808
Epoch: [30]  [ 660/1349]  eta: 0:03:36  lr: 0.000199  min_lr: 0.000005  loss: 0.8038 (0.7967)  loss_scale: 131072.0000 (107673.3676)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
Epoch: [30]  [ 670/1349]  eta: 0:03:33  lr: 0.000199  min_lr: 0.000005  loss: 0.8483 (0.7981)  loss_scale: 131072.0000 (108022.0805)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
Epoch: [30]  [ 680/1349]  eta: 0:03:30  lr: 0.000199  min_lr: 0.000005  loss: 0.8483 (0.7976)  loss_scale: 131072.0000 (108360.5521)  weight_decay: 0.0500 (0.0500)  time: 0.3078  data: 0.0001  max mem: 41808
Epoch: [30]  [ 690/1349]  eta: 0:03:27  lr: 0.000198  min_lr: 0.000005  loss: 0.8272 (0.7979)  loss_scale: 131072.0000 (108689.2272)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
Epoch: [30]  [ 700/1349]  eta: 0:03:23  lr: 0.000198  min_lr: 0.000005  loss: 0.8879 (0.7992)  loss_scale: 131072.0000 (109008.5250)  weight_decay: 0.0500 (0.0500)  time: 0.3079  data: 0.0001  max mem: 41808
Epoch: [30]  [ 710/1349]  eta: 0:03:20  lr: 0.000198  min_lr: 0.000005  loss: 0.8879 (0.7997)  loss_scale: 131072.0000 (109318.8411)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
Epoch: [30]  [ 720/1349]  eta: 0:03:17  lr: 0.000198  min_lr: 0.000005  loss: 0.8418 (0.8007)  loss_scale: 131072.0000 (109620.5492)  weight_decay: 0.0500 (0.0500)  time: 0.3064  data: 0.0001  max mem: 41808
Epoch: [30]  [ 730/1349]  eta: 0:03:14  lr: 0.000198  min_lr: 0.000005  loss: 0.8592 (0.8007)  loss_scale: 131072.0000 (109914.0027)  weight_decay: 0.0500 (0.0500)  time: 0.3078  data: 0.0001  max mem: 41808
Epoch: [30]  [ 740/1349]  eta: 0:03:11  lr: 0.000198  min_lr: 0.000005  loss: 0.8044 (0.8002)  loss_scale: 131072.0000 (110199.5358)  weight_decay: 0.0500 (0.0500)  time: 0.3087  data: 0.0001  max mem: 41808
[2025-05-23 21:18:18,583] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 21:18:18,583] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 21:18:18,583] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 21:18:18,583] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [30]  [ 750/1349]  eta: 0:03:07  lr: 0.000198  min_lr: 0.000005  loss: 0.8027 (0.7996)  loss_scale: 131072.0000 (111699.1744)  weight_decay: 0.0500 (0.0500)  time: 0.3088  data: 0.0001  max mem: 41808
Epoch: [30]  [ 760/1349]  eta: 0:03:04  lr: 0.000198  min_lr: 0.000005  loss: 0.7603 (0.7993)  loss_scale: 262144.0000 (113676.1104)  weight_decay: 0.0500 (0.0500)  time: 0.3088  data: 0.0001  max mem: 41808
Epoch: [30]  [ 770/1349]  eta: 0:03:01  lr: 0.000197  min_lr: 0.000005  loss: 0.8014 (0.7998)  loss_scale: 262144.0000 (115601.7639)  weight_decay: 0.0500 (0.0500)  time: 0.3084  data: 0.0001  max mem: 41808
[2025-05-23 21:18:29,689] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 41250
[2025-05-23 21:18:29,689] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 41250
[2025-05-23 21:18:29,689] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 21:18:29,689] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 21:18:29,689] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [30]  [ 780/1349]  eta: 0:02:58  lr: 0.000197  min_lr: 0.000005  loss: 0.8750 (0.7994)  loss_scale: 262144.0000 (117310.2791)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0001  max mem: 41808
Epoch: [30]  [ 790/1349]  eta: 0:02:55  lr: 0.000197  min_lr: 0.000005  loss: 0.8085 (0.7995)  loss_scale: 131072.0000 (117484.2579)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0002  max mem: 41808
Epoch: [30]  [ 800/1349]  eta: 0:02:52  lr: 0.000197  min_lr: 0.000005  loss: 0.8912 (0.8008)  loss_scale: 131072.0000 (117653.8926)  weight_decay: 0.0500 (0.0500)  time: 0.3078  data: 0.0001  max mem: 41808
Epoch: [30]  [ 810/1349]  eta: 0:02:48  lr: 0.000197  min_lr: 0.000005  loss: 0.9088 (0.8009)  loss_scale: 131072.0000 (117819.3440)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
Epoch: [30]  [ 820/1349]  eta: 0:02:45  lr: 0.000197  min_lr: 0.000005  loss: 0.8045 (0.8007)  loss_scale: 131072.0000 (117980.7649)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [30]  [ 830/1349]  eta: 0:02:42  lr: 0.000197  min_lr: 0.000005  loss: 0.7829 (0.8007)  loss_scale: 131072.0000 (118138.3008)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
Epoch: [30]  [ 840/1349]  eta: 0:02:39  lr: 0.000197  min_lr: 0.000005  loss: 0.8569 (0.8010)  loss_scale: 131072.0000 (118292.0904)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0001  max mem: 41808
[2025-05-23 21:18:48,457] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 41311
[2025-05-23 21:18:48,457] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 41311
[2025-05-23 21:18:48,457] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 21:18:48,457] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 21:18:48,457] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [30]  [ 850/1349]  eta: 0:02:36  lr: 0.000196  min_lr: 0.000005  loss: 0.8370 (0.8005)  loss_scale: 65536.0000 (117672.1598)  weight_decay: 0.0500 (0.0500)  time: 0.3086  data: 0.0001  max mem: 41808
Epoch: [30]  [ 860/1349]  eta: 0:02:33  lr: 0.000196  min_lr: 0.000005  loss: 0.7810 (0.7993)  loss_scale: 65536.0000 (117066.6295)  weight_decay: 0.0500 (0.0500)  time: 0.3084  data: 0.0001  max mem: 41808
Epoch: [30]  [ 870/1349]  eta: 0:02:29  lr: 0.000196  min_lr: 0.000005  loss: 0.8095 (0.7996)  loss_scale: 65536.0000 (116475.0034)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
Epoch: [30]  [ 880/1349]  eta: 0:02:26  lr: 0.000196  min_lr: 0.000005  loss: 0.8425 (0.7997)  loss_scale: 65536.0000 (115896.8082)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [30]  [ 890/1349]  eta: 0:02:23  lr: 0.000196  min_lr: 0.000005  loss: 0.7972 (0.8002)  loss_scale: 65536.0000 (115331.5915)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [30]  [ 900/1349]  eta: 0:02:20  lr: 0.000196  min_lr: 0.000005  loss: 0.8725 (0.8004)  loss_scale: 65536.0000 (114778.9212)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
Epoch: [30]  [ 910/1349]  eta: 0:02:17  lr: 0.000196  min_lr: 0.000005  loss: 0.7856 (0.7995)  loss_scale: 65536.0000 (114238.3842)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
Epoch: [30]  [ 920/1349]  eta: 0:02:14  lr: 0.000196  min_lr: 0.000005  loss: 0.7495 (0.7985)  loss_scale: 65536.0000 (113709.5852)  weight_decay: 0.0500 (0.0500)  time: 0.3079  data: 0.0001  max mem: 41808
Epoch: [30]  [ 930/1349]  eta: 0:02:10  lr: 0.000195  min_lr: 0.000005  loss: 0.7495 (0.7975)  loss_scale: 65536.0000 (113192.1461)  weight_decay: 0.0500 (0.0500)  time: 0.3085  data: 0.0001  max mem: 41808
Epoch: [30]  [ 940/1349]  eta: 0:02:07  lr: 0.000195  min_lr: 0.000005  loss: 0.7722 (0.7975)  loss_scale: 65536.0000 (112685.7046)  weight_decay: 0.0500 (0.0500)  time: 0.3087  data: 0.0001  max mem: 41808
Epoch: [30]  [ 950/1349]  eta: 0:02:04  lr: 0.000195  min_lr: 0.000005  loss: 0.8439 (0.7975)  loss_scale: 65536.0000 (112189.9138)  weight_decay: 0.0500 (0.0500)  time: 0.3101  data: 0.0001  max mem: 41808
Epoch: [30]  [ 960/1349]  eta: 0:02:01  lr: 0.000195  min_lr: 0.000005  loss: 0.8720 (0.7987)  loss_scale: 65536.0000 (111704.4412)  weight_decay: 0.0500 (0.0500)  time: 0.3099  data: 0.0001  max mem: 41808
[2025-05-23 21:19:28,228] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 21:19:28,228] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-05-23 21:19:28,228] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 21:19:28,229] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [30]  [ 970/1349]  eta: 0:01:58  lr: 0.000195  min_lr: 0.000005  loss: 0.8616 (0.7987)  loss_scale: 65536.0000 (111296.4614)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0001  max mem: 41808
Epoch: [30]  [ 980/1349]  eta: 0:01:55  lr: 0.000195  min_lr: 0.000005  loss: 0.8616 (0.7992)  loss_scale: 131072.0000 (111498.0469)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [30]  [ 990/1349]  eta: 0:01:52  lr: 0.000195  min_lr: 0.000005  loss: 0.8447 (0.7991)  loss_scale: 131072.0000 (111695.5641)  weight_decay: 0.0500 (0.0500)  time: 0.3078  data: 0.0001  max mem: 41808
Epoch: [30]  [1000/1349]  eta: 0:01:48  lr: 0.000195  min_lr: 0.000005  loss: 0.8261 (0.7994)  loss_scale: 131072.0000 (111889.1349)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
Epoch: [30]  [1010/1349]  eta: 0:01:45  lr: 0.000194  min_lr: 0.000005  loss: 0.8322 (0.8003)  loss_scale: 131072.0000 (112078.8764)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
Epoch: [30]  [1020/1349]  eta: 0:01:42  lr: 0.000194  min_lr: 0.000005  loss: 0.8269 (0.8003)  loss_scale: 131072.0000 (112264.9011)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
Epoch: [30]  [1030/1349]  eta: 0:01:39  lr: 0.000194  min_lr: 0.000005  loss: 0.8211 (0.8002)  loss_scale: 131072.0000 (112447.3172)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [30]  [1040/1349]  eta: 0:01:36  lr: 0.000194  min_lr: 0.000005  loss: 0.8144 (0.7996)  loss_scale: 131072.0000 (112626.2286)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
Epoch: [30]  [1050/1349]  eta: 0:01:33  lr: 0.000194  min_lr: 0.000005  loss: 0.7876 (0.7997)  loss_scale: 131072.0000 (112801.7355)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0001  max mem: 41808
Epoch: [30]  [1060/1349]  eta: 0:01:30  lr: 0.000194  min_lr: 0.000005  loss: 0.7876 (0.7996)  loss_scale: 131072.0000 (112973.9340)  weight_decay: 0.0500 (0.0500)  time: 0.3078  data: 0.0001  max mem: 41808
Epoch: [30]  [1070/1349]  eta: 0:01:27  lr: 0.000194  min_lr: 0.000005  loss: 0.7443 (0.7993)  loss_scale: 131072.0000 (113142.9169)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [30]  [1080/1349]  eta: 0:01:23  lr: 0.000194  min_lr: 0.000005  loss: 0.7300 (0.7993)  loss_scale: 131072.0000 (113308.7734)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
Epoch: [30]  [1090/1349]  eta: 0:01:20  lr: 0.000193  min_lr: 0.000005  loss: 0.7894 (0.7992)  loss_scale: 131072.0000 (113471.5894)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
[2025-05-23 21:20:07,591] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 21:20:07,591] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 21:20:07,591] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 21:20:07,591] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [30]  [1100/1349]  eta: 0:01:17  lr: 0.000193  min_lr: 0.000005  loss: 0.7955 (0.7997)  loss_scale: 131072.0000 (113988.5922)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [30]  [1110/1349]  eta: 0:01:14  lr: 0.000193  min_lr: 0.000005  loss: 0.8314 (0.7998)  loss_scale: 262144.0000 (115322.1242)  weight_decay: 0.0500 (0.0500)  time: 0.3096  data: 0.0001  max mem: 41808
Epoch: [30]  [1120/1349]  eta: 0:01:11  lr: 0.000193  min_lr: 0.000005  loss: 0.8289 (0.8002)  loss_scale: 262144.0000 (116631.8644)  weight_decay: 0.0500 (0.0500)  time: 0.3104  data: 0.0001  max mem: 41808
[2025-05-23 21:20:15,638] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 41594
[2025-05-23 21:20:15,638] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 21:20:15,638] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 41594
[2025-05-23 21:20:15,639] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 21:20:15,639] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [30]  [1130/1349]  eta: 0:01:08  lr: 0.000193  min_lr: 0.000005  loss: 0.8096 (0.8002)  loss_scale: 262144.0000 (117107.2113)  weight_decay: 0.0500 (0.0500)  time: 0.3081  data: 0.0001  max mem: 41808
Epoch: [30]  [1140/1349]  eta: 0:01:05  lr: 0.000193  min_lr: 0.000005  loss: 0.7751 (0.7999)  loss_scale: 131072.0000 (117229.6021)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0001  max mem: 41808
Epoch: [30]  [1150/1349]  eta: 0:01:02  lr: 0.000193  min_lr: 0.000005  loss: 0.7612 (0.7998)  loss_scale: 131072.0000 (117349.8662)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
Epoch: [30]  [1160/1349]  eta: 0:00:58  lr: 0.000193  min_lr: 0.000005  loss: 0.8355 (0.8000)  loss_scale: 131072.0000 (117468.0586)  weight_decay: 0.0500 (0.0500)  time: 0.3066  data: 0.0001  max mem: 41808
Epoch: [30]  [1170/1349]  eta: 0:00:55  lr: 0.000192  min_lr: 0.000005  loss: 0.7676 (0.7989)  loss_scale: 131072.0000 (117584.2323)  weight_decay: 0.0500 (0.0500)  time: 0.3066  data: 0.0001  max mem: 41808
Epoch: [30]  [1180/1349]  eta: 0:00:52  lr: 0.000192  min_lr: 0.000005  loss: 0.6978 (0.7985)  loss_scale: 131072.0000 (117698.4386)  weight_decay: 0.0500 (0.0500)  time: 0.3088  data: 0.0001  max mem: 41808
Epoch: [30]  [1190/1349]  eta: 0:00:49  lr: 0.000192  min_lr: 0.000005  loss: 0.8013 (0.7989)  loss_scale: 131072.0000 (117810.7271)  weight_decay: 0.0500 (0.0500)  time: 0.3090  data: 0.0001  max mem: 41808
Epoch: [30]  [1200/1349]  eta: 0:00:46  lr: 0.000192  min_lr: 0.000005  loss: 0.8210 (0.7989)  loss_scale: 131072.0000 (117921.1457)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [30]  [1210/1349]  eta: 0:00:43  lr: 0.000192  min_lr: 0.000005  loss: 0.8081 (0.7990)  loss_scale: 131072.0000 (118029.7407)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
Epoch: [30]  [1220/1349]  eta: 0:00:40  lr: 0.000192  min_lr: 0.000005  loss: 0.8025 (0.7990)  loss_scale: 131072.0000 (118136.5569)  weight_decay: 0.0500 (0.0500)  time: 0.3085  data: 0.0001  max mem: 41808
Epoch: [30]  [1230/1349]  eta: 0:00:37  lr: 0.000192  min_lr: 0.000005  loss: 0.8626 (0.7994)  loss_scale: 131072.0000 (118241.6377)  weight_decay: 0.0500 (0.0500)  time: 0.3087  data: 0.0001  max mem: 41808
Epoch: [30]  [1240/1349]  eta: 0:00:33  lr: 0.000192  min_lr: 0.000005  loss: 0.8645 (0.7998)  loss_scale: 131072.0000 (118345.0250)  weight_decay: 0.0500 (0.0500)  time: 0.3084  data: 0.0001  max mem: 41808
Epoch: [30]  [1250/1349]  eta: 0:00:30  lr: 0.000191  min_lr: 0.000005  loss: 0.8195 (0.7995)  loss_scale: 131072.0000 (118446.7594)  weight_decay: 0.0500 (0.0500)  time: 0.3082  data: 0.0001  max mem: 41808
[2025-05-23 21:20:55,354] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 21:20:55,355] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 21:20:55,355] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 21:20:55,355] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 21:20:56,891] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 41728
[2025-05-23 21:20:56,891] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 41728
[2025-05-23 21:20:56,891] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 21:20:56,891] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 21:20:56,891] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
[2025-05-23 21:20:57,500] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 41730
[2025-05-23 21:20:57,500] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 41730
[2025-05-23 21:20:57,500] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 21:20:57,500] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 21:20:57,500] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [30]  [1260/1349]  eta: 0:00:27  lr: 0.000191  min_lr: 0.000005  loss: 0.8328 (0.8003)  loss_scale: 131072.0000 (119014.6233)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
Epoch: [30]  [1270/1349]  eta: 0:00:24  lr: 0.000191  min_lr: 0.000005  loss: 0.8143 (0.7994)  loss_scale: 65536.0000 (118593.8631)  weight_decay: 0.0500 (0.0500)  time: 0.3063  data: 0.0002  max mem: 41808
Epoch: [30]  [1280/1349]  eta: 0:00:21  lr: 0.000191  min_lr: 0.000005  loss: 0.6927 (0.7985)  loss_scale: 65536.0000 (118179.6721)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [30]  [1290/1349]  eta: 0:00:18  lr: 0.000191  min_lr: 0.000005  loss: 0.7599 (0.7989)  loss_scale: 65536.0000 (117771.8978)  weight_decay: 0.0500 (0.0500)  time: 0.3081  data: 0.0001  max mem: 41808
Epoch: [30]  [1300/1349]  eta: 0:00:15  lr: 0.000191  min_lr: 0.000005  loss: 0.9059 (0.7996)  loss_scale: 65536.0000 (117370.3920)  weight_decay: 0.0500 (0.0500)  time: 0.3083  data: 0.0001  max mem: 41808
Epoch: [30]  [1310/1349]  eta: 0:00:12  lr: 0.000191  min_lr: 0.000005  loss: 0.8389 (0.7992)  loss_scale: 65536.0000 (116975.0114)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0001  max mem: 41808
Epoch: [30]  [1320/1349]  eta: 0:00:09  lr: 0.000191  min_lr: 0.000005  loss: 0.7841 (0.7992)  loss_scale: 65536.0000 (116585.6170)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [30]  [1330/1349]  eta: 0:00:05  lr: 0.000190  min_lr: 0.000005  loss: 0.8047 (0.7992)  loss_scale: 65536.0000 (116202.0736)  weight_decay: 0.0500 (0.0500)  time: 0.3049  data: 0.0001  max mem: 41808
Epoch: [30]  [1340/1349]  eta: 0:00:02  lr: 0.000190  min_lr: 0.000005  loss: 0.8297 (0.7998)  loss_scale: 65536.0000 (115824.2506)  weight_decay: 0.0500 (0.0500)  time: 0.3024  data: 0.0001  max mem: 41808
Epoch: [30]  [1348/1349]  eta: 0:00:00  lr: 0.000190  min_lr: 0.000005  loss: 0.8597 (0.8000)  loss_scale: 65536.0000 (115526.0252)  weight_decay: 0.0500 (0.0500)  time: 0.3019  data: 0.0001  max mem: 41808
Epoch: [30] Total time: 0:07:00 (0.3114 s / it)
Averaged stats: lr: 0.000190  min_lr: 0.000005  loss: 0.8597 (0.7998)  loss_scale: 65536.0000 (115526.0252)  weight_decay: 0.0500 (0.0500)  total_time: 420.0118 (420.0060)
Val:  [  0/346]  eta: 0:48:32  loss: 3.2349 (3.2349)  acc1: 0.0000 (0.0000)  acc5: 82.8125 (82.8125)  time: 8.4188  data: 7.5114  max mem: 41808
Val:  [ 10/346]  eta: 0:11:21  loss: 0.1336 (0.5852)  acc1: 100.0000 (84.7301)  acc5: 100.0000 (98.2955)  time: 2.0284  data: 1.2390  max mem: 41808
Val:  [ 20/346]  eta: 0:08:09  loss: 0.1260 (0.4534)  acc1: 100.0000 (88.6161)  acc5: 100.0000 (99.0327)  time: 1.1551  data: 0.3865  max mem: 41808
Val:  [ 30/346]  eta: 0:06:39  loss: 0.1014 (0.3783)  acc1: 100.0000 (91.0534)  acc5: 100.0000 (99.3448)  time: 0.8446  data: 0.0808  max mem: 41808
Val:  [ 40/346]  eta: 0:05:49  loss: 0.1078 (0.4288)  acc1: 100.0000 (89.6532)  acc5: 100.0000 (99.2188)  time: 0.7636  data: 0.0003  max mem: 41808
Val:  [ 50/346]  eta: 0:05:19  loss: 0.1160 (0.3787)  acc1: 99.2188 (91.3450)  acc5: 100.0000 (99.3566)  time: 0.7916  data: 0.0003  max mem: 41808
Val:  [ 60/346]  eta: 0:04:53  loss: 0.1802 (0.3626)  acc1: 97.6562 (91.5087)  acc5: 100.0000 (99.4493)  time: 0.7884  data: 0.0003  max mem: 41808
Val:  [ 70/346]  eta: 0:04:34  loss: 0.2501 (0.3872)  acc1: 95.3125 (90.7790)  acc5: 100.0000 (99.5268)  time: 0.7730  data: 0.0003  max mem: 41808
Val:  [ 80/346]  eta: 0:04:20  loss: 0.2144 (0.3840)  acc1: 93.7500 (90.9144)  acc5: 100.0000 (99.5756)  time: 0.8309  data: 0.0497  max mem: 41808
Val:  [ 90/346]  eta: 0:04:07  loss: 0.2833 (0.3875)  acc1: 93.7500 (90.7795)  acc5: 100.0000 (99.6051)  time: 0.8813  data: 0.1164  max mem: 41808
Val:  [100/346]  eta: 0:03:55  loss: 0.1956 (0.3645)  acc1: 96.8750 (91.6151)  acc5: 100.0000 (99.6442)  time: 0.8827  data: 0.1304  max mem: 41808
Val:  [110/346]  eta: 0:03:45  loss: 0.1634 (0.3764)  acc1: 97.6562 (91.2233)  acc5: 100.0000 (99.6481)  time: 0.8874  data: 0.1452  max mem: 41808
Val:  [120/346]  eta: 0:03:34  loss: 0.1965 (0.3785)  acc1: 96.8750 (91.1028)  acc5: 100.0000 (99.6772)  time: 0.9026  data: 0.1570  max mem: 41808
Val:  [130/346]  eta: 0:03:23  loss: 0.1285 (0.3740)  acc1: 99.2188 (91.2154)  acc5: 100.0000 (99.6958)  time: 0.8835  data: 0.1402  max mem: 41808
Val:  [140/346]  eta: 0:03:13  loss: 0.2381 (0.3740)  acc1: 95.3125 (91.2068)  acc5: 100.0000 (99.7174)  time: 0.8687  data: 0.1371  max mem: 41808
Val:  [150/346]  eta: 0:03:03  loss: 0.2704 (0.3763)  acc1: 94.5312 (91.1734)  acc5: 100.0000 (99.6999)  time: 0.8878  data: 0.1489  max mem: 41808
Val:  [160/346]  eta: 0:02:53  loss: 0.2638 (0.3715)  acc1: 95.3125 (91.3092)  acc5: 100.0000 (99.6991)  time: 0.8971  data: 0.1515  max mem: 41808
Val:  [170/346]  eta: 0:02:43  loss: 0.1570 (0.3651)  acc1: 97.6562 (91.4062)  acc5: 100.0000 (99.7167)  time: 0.8906  data: 0.1514  max mem: 41808
Val:  [180/346]  eta: 0:02:34  loss: 0.1387 (0.3792)  acc1: 92.1875 (90.8710)  acc5: 100.0000 (99.6504)  time: 0.9120  data: 0.1540  max mem: 41808
Val:  [190/346]  eta: 0:02:24  loss: 0.2803 (0.3777)  acc1: 92.1875 (90.9236)  acc5: 100.0000 (99.6441)  time: 0.9052  data: 0.1547  max mem: 41808
Val:  [200/346]  eta: 0:02:15  loss: 0.4153 (0.3921)  acc1: 86.7188 (90.3335)  acc5: 100.0000 (99.6580)  time: 0.8861  data: 0.1497  max mem: 41808
Val:  [210/346]  eta: 0:02:05  loss: 0.2315 (0.3855)  acc1: 87.5000 (90.5398)  acc5: 100.0000 (99.6742)  time: 0.9019  data: 0.1446  max mem: 41808
Val:  [220/346]  eta: 0:01:56  loss: 0.1751 (0.3825)  acc1: 96.8750 (90.6604)  acc5: 100.0000 (99.6571)  time: 0.9043  data: 0.1585  max mem: 41808
Val:  [230/346]  eta: 0:01:46  loss: 0.1783 (0.3743)  acc1: 96.8750 (90.9395)  acc5: 100.0000 (99.6584)  time: 0.8870  data: 0.1538  max mem: 41808
Val:  [240/346]  eta: 0:01:37  loss: 0.1916 (0.3779)  acc1: 96.8750 (90.8973)  acc5: 100.0000 (99.6661)  time: 0.8674  data: 0.1468  max mem: 41808
Val:  [250/346]  eta: 0:01:28  loss: 0.2007 (0.3767)  acc1: 96.8750 (90.9643)  acc5: 100.0000 (99.6732)  time: 0.8883  data: 0.1583  max mem: 41808
Val:  [260/346]  eta: 0:01:19  loss: 0.1683 (0.3742)  acc1: 96.8750 (91.0441)  acc5: 100.0000 (99.6648)  time: 0.9255  data: 0.1547  max mem: 41808
Val:  [270/346]  eta: 0:01:09  loss: 0.1449 (0.3709)  acc1: 96.8750 (91.1554)  acc5: 100.0000 (99.6714)  time: 0.9281  data: 0.1582  max mem: 41808
Val:  [280/346]  eta: 0:01:00  loss: 0.1265 (0.3684)  acc1: 100.0000 (91.2561)  acc5: 100.0000 (99.6747)  time: 0.9313  data: 0.1559  max mem: 41808
Val:  [290/346]  eta: 0:00:51  loss: 0.1089 (0.3607)  acc1: 100.0000 (91.4975)  acc5: 100.0000 (99.6859)  time: 0.9104  data: 0.1391  max mem: 41808
Val:  [300/346]  eta: 0:00:42  loss: 0.1087 (0.3603)  acc1: 100.0000 (91.5646)  acc5: 100.0000 (99.6392)  time: 0.8855  data: 0.1377  max mem: 41808
Val:  [310/346]  eta: 0:00:33  loss: 0.1357 (0.3628)  acc1: 99.2188 (91.5067)  acc5: 100.0000 (99.6282)  time: 0.8839  data: 0.1420  max mem: 41808
Val:  [320/346]  eta: 0:00:23  loss: 0.1357 (0.3633)  acc1: 99.2188 (91.4793)  acc5: 100.0000 (99.6398)  time: 0.8797  data: 0.1388  max mem: 41808
Val:  [330/346]  eta: 0:00:14  loss: 0.3412 (0.3762)  acc1: 89.8438 (91.1207)  acc5: 100.0000 (99.6224)  time: 0.8916  data: 0.1419  max mem: 41808
Val:  [340/346]  eta: 0:00:05  loss: 0.4293 (0.3834)  acc1: 89.8438 (90.9412)  acc5: 100.0000 (99.6311)  time: 0.8919  data: 0.1574  max mem: 41808
Val:  [345/346]  eta: 0:00:00  loss: 0.1702 (0.3800)  acc1: 97.7011 (91.0502)  acc5: 100.0000 (99.6361)  time: 0.8931  data: 0.1819  max mem: 41808
Val: Total time: 0:05:16 (0.9154 s / it)
* Acc@1 91.156 Acc@5 99.655 loss 0.377
Accuracy of the network on the 88494 val videos: 91.2%
Max accuracy: 91.50%   Max Epoch: 25
Epoch: [31]  [   0/1349]  eta: 1:50:00  lr: 0.000190  min_lr: 0.000005  loss: 0.6509 (0.6509)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 4.8930  data: 4.5552  max mem: 41808
Epoch: [31]  [  10/1349]  eta: 0:16:24  lr: 0.000190  min_lr: 0.000005  loss: 0.7929 (0.7900)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7349  data: 0.4142  max mem: 41808
Epoch: [31]  [  20/1349]  eta: 0:11:47  lr: 0.000190  min_lr: 0.000005  loss: 0.8227 (0.7983)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3144  data: 0.0001  max mem: 41808
Epoch: [31]  [  30/1349]  eta: 0:10:06  lr: 0.000190  min_lr: 0.000005  loss: 0.8227 (0.7697)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3089  data: 0.0001  max mem: 41808
[2025-05-23 21:26:58,806] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 21:26:58,806] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-05-23 21:26:58,806] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 21:26:58,806] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [31]  [  40/1349]  eta: 0:09:13  lr: 0.000190  min_lr: 0.000005  loss: 0.7691 (0.7775)  loss_scale: 65536.0000 (67134.4390)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0001  max mem: 41808
Epoch: [31]  [  50/1349]  eta: 0:08:39  lr: 0.000190  min_lr: 0.000005  loss: 0.7893 (0.7703)  loss_scale: 131072.0000 (79671.2157)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [31]  [  60/1349]  eta: 0:08:16  lr: 0.000189  min_lr: 0.000004  loss: 0.7893 (0.7747)  loss_scale: 131072.0000 (88097.5738)  weight_decay: 0.0500 (0.0500)  time: 0.3086  data: 0.0001  max mem: 41808
Epoch: [31]  [  70/1349]  eta: 0:07:59  lr: 0.000189  min_lr: 0.000004  loss: 0.8321 (0.7746)  loss_scale: 131072.0000 (94150.3099)  weight_decay: 0.0500 (0.0500)  time: 0.3092  data: 0.0001  max mem: 41808
Epoch: [31]  [  80/1349]  eta: 0:07:44  lr: 0.000189  min_lr: 0.000004  loss: 0.7125 (0.7684)  loss_scale: 131072.0000 (98708.5432)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0001  max mem: 41808
Epoch: [31]  [  90/1349]  eta: 0:07:32  lr: 0.000189  min_lr: 0.000004  loss: 0.7680 (0.7788)  loss_scale: 131072.0000 (102264.9670)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
Epoch: [31]  [ 100/1349]  eta: 0:07:22  lr: 0.000189  min_lr: 0.000004  loss: 0.7780 (0.7742)  loss_scale: 131072.0000 (105117.1485)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
Epoch: [31]  [ 110/1349]  eta: 0:07:14  lr: 0.000189  min_lr: 0.000004  loss: 0.7482 (0.7711)  loss_scale: 131072.0000 (107455.4234)  weight_decay: 0.0500 (0.0500)  time: 0.3078  data: 0.0001  max mem: 41808
Epoch: [31]  [ 120/1349]  eta: 0:07:06  lr: 0.000189  min_lr: 0.000004  loss: 0.7577 (0.7696)  loss_scale: 131072.0000 (109407.2066)  weight_decay: 0.0500 (0.0500)  time: 0.3078  data: 0.0001  max mem: 41808
Epoch: [31]  [ 130/1349]  eta: 0:06:59  lr: 0.000189  min_lr: 0.000004  loss: 0.7319 (0.7661)  loss_scale: 131072.0000 (111061.0076)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
Epoch: [31]  [ 140/1349]  eta: 0:06:52  lr: 0.000188  min_lr: 0.000004  loss: 0.7250 (0.7684)  loss_scale: 131072.0000 (112480.2270)  weight_decay: 0.0500 (0.0500)  time: 0.3081  data: 0.0001  max mem: 41808
Epoch: [31]  [ 150/1349]  eta: 0:06:46  lr: 0.000188  min_lr: 0.000004  loss: 0.8458 (0.7773)  loss_scale: 131072.0000 (113711.4702)  weight_decay: 0.0500 (0.0500)  time: 0.3079  data: 0.0001  max mem: 41808
Epoch: [31]  [ 160/1349]  eta: 0:06:40  lr: 0.000188  min_lr: 0.000004  loss: 0.8600 (0.7771)  loss_scale: 131072.0000 (114789.7640)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
[2025-05-23 21:27:38,268] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 21:27:38,268] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 21:27:38,268] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 21:27:38,269] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 21:27:38,572] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 41988
[2025-05-23 21:27:38,572] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 41988
[2025-05-23 21:27:38,572] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 21:27:38,572] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 21:27:38,572] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [31]  [ 170/1349]  eta: 0:06:35  lr: 0.000188  min_lr: 0.000004  loss: 0.8600 (0.7824)  loss_scale: 131072.0000 (116508.4444)  weight_decay: 0.0500 (0.0500)  time: 0.3092  data: 0.0001  max mem: 41808
[2025-05-23 21:27:41,958] [INFO] [logging.py:96:log_dist] [Rank 0] step=42000, skipped=257, lr=[4.463704900336772e-06, 4.463704900336772e-06, 5.951606533782363e-06, 5.951606533782363e-06, 7.935475378376484e-06, 7.935475378376484e-06, 1.0580633837835311e-05, 1.0580633837835311e-05, 1.4107511783780415e-05, 1.4107511783780415e-05, 1.8810015711707222e-05, 1.8810015711707222e-05, 2.508002094894296e-05, 2.508002094894296e-05, 3.344002793192395e-05, 3.344002793192395e-05, 4.458670390923193e-05, 4.458670390923193e-05, 5.9448938545642575e-05, 5.9448938545642575e-05, 7.92652513941901e-05, 7.92652513941901e-05, 0.00010568700185892013, 0.00010568700185892013, 0.00014091600247856018, 0.00014091600247856018, 0.0001878880033047469, 0.0001878880033047469], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-05-23 21:27:41,959] [INFO] [timer.py:260:stop] epoch=0/micro_step=42000/global_step=42000, RunningAvgSamplesPerSec=209.78011639533028, CurrSamplesPerSec=212.91812063502135, MemAllocated=1.37GB, MaxMemAllocated=40.83GB
Epoch: [31]  [ 180/1349]  eta: 0:06:30  lr: 0.000188  min_lr: 0.000004  loss: 0.8077 (0.7817)  loss_scale: 131072.0000 (117313.0608)  weight_decay: 0.0500 (0.0500)  time: 0.3095  data: 0.0001  max mem: 41808
Epoch: [31]  [ 190/1349]  eta: 0:06:25  lr: 0.000188  min_lr: 0.000004  loss: 0.7862 (0.7819)  loss_scale: 131072.0000 (118033.4241)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
Epoch: [31]  [ 200/1349]  eta: 0:06:20  lr: 0.000188  min_lr: 0.000004  loss: 0.7687 (0.7810)  loss_scale: 131072.0000 (118682.1095)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
Epoch: [31]  [ 210/1349]  eta: 0:06:16  lr: 0.000188  min_lr: 0.000004  loss: 0.7374 (0.7800)  loss_scale: 131072.0000 (119269.3081)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0002  max mem: 41808
Epoch: [31]  [ 220/1349]  eta: 0:06:11  lr: 0.000187  min_lr: 0.000004  loss: 0.7374 (0.7782)  loss_scale: 131072.0000 (119803.3665)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0002  max mem: 41808
Epoch: [31]  [ 230/1349]  eta: 0:06:07  lr: 0.000187  min_lr: 0.000004  loss: 0.8181 (0.7805)  loss_scale: 131072.0000 (120291.1861)  weight_decay: 0.0500 (0.0500)  time: 0.3079  data: 0.0001  max mem: 41808
Epoch: [31]  [ 240/1349]  eta: 0:06:03  lr: 0.000187  min_lr: 0.000004  loss: 0.8366 (0.7805)  loss_scale: 131072.0000 (120738.5228)  weight_decay: 0.0500 (0.0500)  time: 0.3085  data: 0.0001  max mem: 41808
Epoch: [31]  [ 250/1349]  eta: 0:05:59  lr: 0.000187  min_lr: 0.000004  loss: 0.8650 (0.7825)  loss_scale: 131072.0000 (121150.2151)  weight_decay: 0.0500 (0.0500)  time: 0.3081  data: 0.0001  max mem: 41808
Epoch: [31]  [ 260/1349]  eta: 0:05:55  lr: 0.000187  min_lr: 0.000004  loss: 0.7769 (0.7801)  loss_scale: 131072.0000 (121530.3602)  weight_decay: 0.0500 (0.0500)  time: 0.3078  data: 0.0001  max mem: 41808
Epoch: [31]  [ 270/1349]  eta: 0:05:51  lr: 0.000187  min_lr: 0.000004  loss: 0.7154 (0.7788)  loss_scale: 131072.0000 (121882.4502)  weight_decay: 0.0500 (0.0500)  time: 0.3078  data: 0.0001  max mem: 41808
Epoch: [31]  [ 280/1349]  eta: 0:05:47  lr: 0.000187  min_lr: 0.000004  loss: 0.8132 (0.7817)  loss_scale: 131072.0000 (122209.4804)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
Epoch: [31]  [ 290/1349]  eta: 0:05:43  lr: 0.000187  min_lr: 0.000004  loss: 0.8554 (0.7832)  loss_scale: 131072.0000 (122514.0344)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
[2025-05-23 21:28:18,264] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 21:28:18,264] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 21:28:18,264] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 21:28:18,264] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [31]  [ 300/1349]  eta: 0:05:39  lr: 0.000186  min_lr: 0.000004  loss: 0.8625 (0.7833)  loss_scale: 131072.0000 (124104.7176)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [31]  [ 310/1349]  eta: 0:05:35  lr: 0.000186  min_lr: 0.000004  loss: 0.7944 (0.7827)  loss_scale: 262144.0000 (128543.2797)  weight_decay: 0.0500 (0.0500)  time: 0.3080  data: 0.0002  max mem: 41808
Epoch: [31]  [ 320/1349]  eta: 0:05:31  lr: 0.000186  min_lr: 0.000004  loss: 0.7709 (0.7820)  loss_scale: 262144.0000 (132705.2960)  weight_decay: 0.0500 (0.0500)  time: 0.3086  data: 0.0001  max mem: 41808
[2025-05-23 21:28:28,121] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 42149
[2025-05-23 21:28:28,121] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 42149
[2025-05-23 21:28:28,121] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 21:28:28,121] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 21:28:28,121] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [31]  [ 330/1349]  eta: 0:05:28  lr: 0.000186  min_lr: 0.000004  loss: 0.7841 (0.7824)  loss_scale: 262144.0000 (136219.8429)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [31]  [ 340/1349]  eta: 0:05:24  lr: 0.000186  min_lr: 0.000004  loss: 0.8101 (0.7831)  loss_scale: 131072.0000 (136068.8798)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
Epoch: [31]  [ 350/1349]  eta: 0:05:20  lr: 0.000186  min_lr: 0.000004  loss: 0.8430 (0.7862)  loss_scale: 131072.0000 (135926.5185)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [31]  [ 360/1349]  eta: 0:05:17  lr: 0.000186  min_lr: 0.000004  loss: 0.8207 (0.7863)  loss_scale: 131072.0000 (135792.0443)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0001  max mem: 41808
Epoch: [31]  [ 370/1349]  eta: 0:05:13  lr: 0.000186  min_lr: 0.000004  loss: 0.7881 (0.7872)  loss_scale: 131072.0000 (135664.8194)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
Epoch: [31]  [ 380/1349]  eta: 0:05:10  lr: 0.000185  min_lr: 0.000004  loss: 0.8051 (0.7884)  loss_scale: 131072.0000 (135544.2730)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
Epoch: [31]  [ 390/1349]  eta: 0:05:06  lr: 0.000185  min_lr: 0.000004  loss: 0.8187 (0.7900)  loss_scale: 131072.0000 (135429.8926)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
Epoch: [31]  [ 400/1349]  eta: 0:05:03  lr: 0.000185  min_lr: 0.000004  loss: 0.7919 (0.7885)  loss_scale: 131072.0000 (135321.2170)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0002  max mem: 41808
Epoch: [31]  [ 410/1349]  eta: 0:04:59  lr: 0.000185  min_lr: 0.000004  loss: 0.7622 (0.7876)  loss_scale: 131072.0000 (135217.8297)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
Epoch: [31]  [ 420/1349]  eta: 0:04:56  lr: 0.000185  min_lr: 0.000004  loss: 0.8077 (0.7885)  loss_scale: 131072.0000 (135119.3539)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0001  max mem: 41808
Epoch: [31]  [ 430/1349]  eta: 0:04:52  lr: 0.000185  min_lr: 0.000004  loss: 0.8173 (0.7878)  loss_scale: 131072.0000 (135025.4478)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
[2025-05-23 21:28:59,181] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 42250
[2025-05-23 21:28:59,181] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 21:28:59,181] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 42250
[2025-05-23 21:28:59,182] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 21:28:59,182] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [31]  [ 440/1349]  eta: 0:04:49  lr: 0.000185  min_lr: 0.000004  loss: 0.8173 (0.7880)  loss_scale: 65536.0000 (133449.7234)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
Epoch: [31]  [ 450/1349]  eta: 0:04:46  lr: 0.000185  min_lr: 0.000004  loss: 0.8396 (0.7881)  loss_scale: 65536.0000 (131943.8758)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
Epoch: [31]  [ 460/1349]  eta: 0:04:42  lr: 0.000184  min_lr: 0.000004  loss: 0.8371 (0.7886)  loss_scale: 65536.0000 (130503.3579)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0002  max mem: 41808
Epoch: [31]  [ 470/1349]  eta: 0:04:39  lr: 0.000184  min_lr: 0.000004  loss: 0.7993 (0.7891)  loss_scale: 65536.0000 (129124.0085)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
Epoch: [31]  [ 480/1349]  eta: 0:04:35  lr: 0.000184  min_lr: 0.000004  loss: 0.7940 (0.7894)  loss_scale: 65536.0000 (127802.0125)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [31]  [ 490/1349]  eta: 0:04:32  lr: 0.000184  min_lr: 0.000004  loss: 0.7777 (0.7893)  loss_scale: 65536.0000 (126533.8656)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [31]  [ 500/1349]  eta: 0:04:29  lr: 0.000184  min_lr: 0.000004  loss: 0.7841 (0.7881)  loss_scale: 65536.0000 (125316.3433)  weight_decay: 0.0500 (0.0500)  time: 0.3088  data: 0.0001  max mem: 41808
Epoch: [31]  [ 510/1349]  eta: 0:04:25  lr: 0.000184  min_lr: 0.000004  loss: 0.8238 (0.7890)  loss_scale: 65536.0000 (124146.4736)  weight_decay: 0.0500 (0.0500)  time: 0.3096  data: 0.0001  max mem: 41808
Epoch: [31]  [ 520/1349]  eta: 0:04:22  lr: 0.000184  min_lr: 0.000004  loss: 0.8300 (0.7888)  loss_scale: 65536.0000 (123021.5125)  weight_decay: 0.0500 (0.0500)  time: 0.3078  data: 0.0001  max mem: 41808
Epoch: [31]  [ 530/1349]  eta: 0:04:19  lr: 0.000184  min_lr: 0.000004  loss: 0.8081 (0.7893)  loss_scale: 65536.0000 (121938.9228)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
Epoch: [31]  [ 540/1349]  eta: 0:04:16  lr: 0.000183  min_lr: 0.000004  loss: 0.8031 (0.7888)  loss_scale: 65536.0000 (120896.3549)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [31]  [ 550/1349]  eta: 0:04:12  lr: 0.000183  min_lr: 0.000004  loss: 0.7433 (0.7868)  loss_scale: 65536.0000 (119891.6298)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
[2025-05-23 21:29:38,887] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 21:29:38,887] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-05-23 21:29:38,887] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 21:29:38,887] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [31]  [ 560/1349]  eta: 0:04:09  lr: 0.000183  min_lr: 0.000004  loss: 0.6820 (0.7852)  loss_scale: 65536.0000 (119039.5437)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [31]  [ 570/1349]  eta: 0:04:06  lr: 0.000183  min_lr: 0.000004  loss: 0.6890 (0.7844)  loss_scale: 131072.0000 (119250.2697)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
Epoch: [31]  [ 580/1349]  eta: 0:04:02  lr: 0.000183  min_lr: 0.000004  loss: 0.8408 (0.7858)  loss_scale: 131072.0000 (119453.7418)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
Epoch: [31]  [ 590/1349]  eta: 0:03:59  lr: 0.000183  min_lr: 0.000004  loss: 0.8582 (0.7866)  loss_scale: 131072.0000 (119650.3283)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [31]  [ 600/1349]  eta: 0:03:56  lr: 0.000183  min_lr: 0.000004  loss: 0.8207 (0.7864)  loss_scale: 131072.0000 (119840.3727)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0001  max mem: 41808
Epoch: [31]  [ 610/1349]  eta: 0:03:53  lr: 0.000183  min_lr: 0.000004  loss: 0.7913 (0.7875)  loss_scale: 131072.0000 (120024.1964)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0001  max mem: 41808
Epoch: [31]  [ 620/1349]  eta: 0:03:49  lr: 0.000182  min_lr: 0.000004  loss: 0.8191 (0.7877)  loss_scale: 131072.0000 (120202.0998)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0001  max mem: 41808
Epoch: [31]  [ 630/1349]  eta: 0:03:46  lr: 0.000182  min_lr: 0.000004  loss: 0.8420 (0.7878)  loss_scale: 131072.0000 (120374.3645)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0002  max mem: 41808
Epoch: [31]  [ 640/1349]  eta: 0:03:43  lr: 0.000182  min_lr: 0.000004  loss: 0.8552 (0.7881)  loss_scale: 131072.0000 (120541.2543)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0002  max mem: 41808
Epoch: [31]  [ 650/1349]  eta: 0:03:40  lr: 0.000182  min_lr: 0.000004  loss: 0.8314 (0.7876)  loss_scale: 131072.0000 (120703.0169)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [31]  [ 660/1349]  eta: 0:03:36  lr: 0.000182  min_lr: 0.000004  loss: 0.7962 (0.7880)  loss_scale: 131072.0000 (120859.8850)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [31]  [ 670/1349]  eta: 0:03:33  lr: 0.000182  min_lr: 0.000004  loss: 0.7962 (0.7875)  loss_scale: 131072.0000 (121012.0775)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
Epoch: [31]  [ 680/1349]  eta: 0:03:30  lr: 0.000182  min_lr: 0.000004  loss: 0.8142 (0.7883)  loss_scale: 131072.0000 (121159.8003)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
[2025-05-23 21:30:18,253] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 21:30:18,253] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 21:30:18,253] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 21:30:18,253] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 21:30:18,863] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 42509
[2025-05-23 21:30:18,863] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 42509
[2025-05-23 21:30:18,863] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 21:30:18,863] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 21:30:18,863] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [31]  [ 690/1349]  eta: 0:03:27  lr: 0.000182  min_lr: 0.000004  loss: 0.8230 (0.7886)  loss_scale: 131072.0000 (121682.6165)  weight_decay: 0.0500 (0.0500)  time: 0.3066  data: 0.0001  max mem: 41808
Epoch: [31]  [ 700/1349]  eta: 0:03:24  lr: 0.000181  min_lr: 0.000004  loss: 0.8146 (0.7885)  loss_scale: 131072.0000 (121816.5592)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [31]  [ 710/1349]  eta: 0:03:20  lr: 0.000181  min_lr: 0.000004  loss: 0.8038 (0.7884)  loss_scale: 131072.0000 (121946.7342)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0001  max mem: 41808
Epoch: [31]  [ 720/1349]  eta: 0:03:17  lr: 0.000181  min_lr: 0.000004  loss: 0.7650 (0.7881)  loss_scale: 131072.0000 (122073.2982)  weight_decay: 0.0500 (0.0500)  time: 0.3083  data: 0.0001  max mem: 41808
Epoch: [31]  [ 730/1349]  eta: 0:03:14  lr: 0.000181  min_lr: 0.000004  loss: 0.7585 (0.7877)  loss_scale: 131072.0000 (122196.3995)  weight_decay: 0.0500 (0.0500)  time: 0.3079  data: 0.0001  max mem: 41808
Epoch: [31]  [ 740/1349]  eta: 0:03:11  lr: 0.000181  min_lr: 0.000004  loss: 0.8155 (0.7886)  loss_scale: 131072.0000 (122316.1781)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
[2025-05-23 21:30:35,162] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 42562
[2025-05-23 21:30:35,162] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 42562
[2025-05-23 21:30:35,162] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 21:30:35,162] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 21:30:35,162] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [31]  [ 750/1349]  eta: 0:03:08  lr: 0.000181  min_lr: 0.000004  loss: 0.8181 (0.7884)  loss_scale: 131072.0000 (121734.6471)  weight_decay: 0.0500 (0.0500)  time: 0.3063  data: 0.0001  max mem: 41808
Epoch: [31]  [ 760/1349]  eta: 0:03:04  lr: 0.000181  min_lr: 0.000004  loss: 0.8417 (0.7889)  loss_scale: 65536.0000 (120996.1629)  weight_decay: 0.0500 (0.0500)  time: 0.3063  data: 0.0001  max mem: 41808
Epoch: [31]  [ 770/1349]  eta: 0:03:01  lr: 0.000181  min_lr: 0.000004  loss: 0.8417 (0.7889)  loss_scale: 65536.0000 (120276.8353)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
Epoch: [31]  [ 780/1349]  eta: 0:02:58  lr: 0.000180  min_lr: 0.000004  loss: 0.7867 (0.7887)  loss_scale: 65536.0000 (119575.9283)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [31]  [ 790/1349]  eta: 0:02:55  lr: 0.000180  min_lr: 0.000004  loss: 0.7531 (0.7883)  loss_scale: 65536.0000 (118892.7434)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
Epoch: [31]  [ 800/1349]  eta: 0:02:52  lr: 0.000180  min_lr: 0.000004  loss: 0.8204 (0.7895)  loss_scale: 65536.0000 (118226.6167)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [31]  [ 810/1349]  eta: 0:02:48  lr: 0.000180  min_lr: 0.000004  loss: 0.8929 (0.7902)  loss_scale: 65536.0000 (117576.9174)  weight_decay: 0.0500 (0.0500)  time: 0.3078  data: 0.0001  max mem: 41808
Epoch: [31]  [ 820/1349]  eta: 0:02:45  lr: 0.000180  min_lr: 0.000004  loss: 0.8935 (0.7909)  loss_scale: 65536.0000 (116943.0451)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [31]  [ 830/1349]  eta: 0:02:42  lr: 0.000180  min_lr: 0.000004  loss: 0.8397 (0.7912)  loss_scale: 65536.0000 (116324.4284)  weight_decay: 0.0500 (0.0500)  time: 0.3085  data: 0.0002  max mem: 41808
Epoch: [31]  [ 840/1349]  eta: 0:02:39  lr: 0.000180  min_lr: 0.000004  loss: 0.8339 (0.7915)  loss_scale: 65536.0000 (115720.5232)  weight_decay: 0.0500 (0.0500)  time: 0.3088  data: 0.0002  max mem: 41808
Epoch: [31]  [ 850/1349]  eta: 0:02:36  lr: 0.000180  min_lr: 0.000004  loss: 0.7869 (0.7910)  loss_scale: 65536.0000 (115130.8108)  weight_decay: 0.0500 (0.0500)  time: 0.3064  data: 0.0001  max mem: 41808
Epoch: [31]  [ 860/1349]  eta: 0:02:33  lr: 0.000179  min_lr: 0.000004  loss: 0.8140 (0.7920)  loss_scale: 65536.0000 (114554.7967)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0002  max mem: 41808
Epoch: [31]  [ 870/1349]  eta: 0:02:29  lr: 0.000179  min_lr: 0.000004  loss: 0.8437 (0.7924)  loss_scale: 65536.0000 (113992.0092)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0002  max mem: 41808
[2025-05-23 21:31:14,817] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 21:31:14,817] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-05-23 21:31:14,817] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 21:31:14,817] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [31]  [ 880/1349]  eta: 0:02:26  lr: 0.000179  min_lr: 0.000004  loss: 0.8437 (0.7926)  loss_scale: 65536.0000 (114111.4915)  weight_decay: 0.0500 (0.0500)  time: 0.3064  data: 0.0002  max mem: 41808
Epoch: [31]  [ 890/1349]  eta: 0:02:23  lr: 0.000179  min_lr: 0.000004  loss: 0.8074 (0.7922)  loss_scale: 131072.0000 (114301.8451)  weight_decay: 0.0500 (0.0500)  time: 0.3061  data: 0.0002  max mem: 41808
Epoch: [31]  [ 900/1349]  eta: 0:02:20  lr: 0.000179  min_lr: 0.000004  loss: 0.8074 (0.7922)  loss_scale: 131072.0000 (114487.9734)  weight_decay: 0.0500 (0.0500)  time: 0.3061  data: 0.0001  max mem: 41808
Epoch: [31]  [ 910/1349]  eta: 0:02:17  lr: 0.000179  min_lr: 0.000004  loss: 0.8465 (0.7930)  loss_scale: 131072.0000 (114670.0154)  weight_decay: 0.0500 (0.0500)  time: 0.3066  data: 0.0001  max mem: 41808
Epoch: [31]  [ 920/1349]  eta: 0:02:14  lr: 0.000179  min_lr: 0.000004  loss: 0.8683 (0.7935)  loss_scale: 131072.0000 (114848.1042)  weight_decay: 0.0500 (0.0500)  time: 0.3066  data: 0.0001  max mem: 41808
Epoch: [31]  [ 930/1349]  eta: 0:02:10  lr: 0.000179  min_lr: 0.000004  loss: 0.8333 (0.7929)  loss_scale: 131072.0000 (115022.3673)  weight_decay: 0.0500 (0.0500)  time: 0.3062  data: 0.0001  max mem: 41808
Epoch: [31]  [ 940/1349]  eta: 0:02:07  lr: 0.000178  min_lr: 0.000004  loss: 0.8431 (0.7935)  loss_scale: 131072.0000 (115192.9267)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0001  max mem: 41808
Epoch: [31]  [ 950/1349]  eta: 0:02:04  lr: 0.000178  min_lr: 0.000004  loss: 0.8144 (0.7928)  loss_scale: 131072.0000 (115359.8991)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
[2025-05-23 21:31:40,260] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 42774
[2025-05-23 21:31:40,260] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 42774
[2025-05-23 21:31:40,260] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 21:31:40,260] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 21:31:40,260] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [31]  [ 960/1349]  eta: 0:02:01  lr: 0.000178  min_lr: 0.000004  loss: 0.7153 (0.7921)  loss_scale: 131072.0000 (115114.2227)  weight_decay: 0.0500 (0.0500)  time: 0.3064  data: 0.0001  max mem: 41808
Epoch: [31]  [ 970/1349]  eta: 0:01:58  lr: 0.000178  min_lr: 0.000004  loss: 0.7600 (0.7914)  loss_scale: 65536.0000 (114603.6334)  weight_decay: 0.0500 (0.0500)  time: 0.3057  data: 0.0001  max mem: 41808
Epoch: [31]  [ 980/1349]  eta: 0:01:55  lr: 0.000178  min_lr: 0.000004  loss: 0.8137 (0.7912)  loss_scale: 65536.0000 (114103.4536)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0001  max mem: 41808
Epoch: [31]  [ 990/1349]  eta: 0:01:52  lr: 0.000178  min_lr: 0.000004  loss: 0.7459 (0.7910)  loss_scale: 65536.0000 (113613.3683)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
Epoch: [31]  [1000/1349]  eta: 0:01:48  lr: 0.000178  min_lr: 0.000004  loss: 0.7494 (0.7918)  loss_scale: 65536.0000 (113133.0749)  weight_decay: 0.0500 (0.0500)  time: 0.3066  data: 0.0001  max mem: 41808
Epoch: [31]  [1010/1349]  eta: 0:01:45  lr: 0.000178  min_lr: 0.000004  loss: 0.8270 (0.7918)  loss_scale: 65536.0000 (112662.2829)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
Epoch: [31]  [1020/1349]  eta: 0:01:42  lr: 0.000177  min_lr: 0.000004  loss: 0.8270 (0.7922)  loss_scale: 65536.0000 (112200.7130)  weight_decay: 0.0500 (0.0500)  time: 0.3064  data: 0.0001  max mem: 41808
Epoch: [31]  [1030/1349]  eta: 0:01:39  lr: 0.000177  min_lr: 0.000004  loss: 0.8344 (0.7919)  loss_scale: 65536.0000 (111748.0970)  weight_decay: 0.0500 (0.0500)  time: 0.3063  data: 0.0001  max mem: 41808
Epoch: [31]  [1040/1349]  eta: 0:01:36  lr: 0.000177  min_lr: 0.000004  loss: 0.7630 (0.7920)  loss_scale: 65536.0000 (111304.1768)  weight_decay: 0.0500 (0.0500)  time: 0.3066  data: 0.0001  max mem: 41808
Epoch: [31]  [1050/1349]  eta: 0:01:33  lr: 0.000177  min_lr: 0.000004  loss: 0.7816 (0.7920)  loss_scale: 65536.0000 (110868.7041)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
Epoch: [31]  [1060/1349]  eta: 0:01:30  lr: 0.000177  min_lr: 0.000004  loss: 0.7934 (0.7922)  loss_scale: 65536.0000 (110441.4402)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
Epoch: [31]  [1070/1349]  eta: 0:01:26  lr: 0.000177  min_lr: 0.000004  loss: 0.8091 (0.7924)  loss_scale: 65536.0000 (110022.1550)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0002  max mem: 41808
Epoch: [31]  [1080/1349]  eta: 0:01:23  lr: 0.000177  min_lr: 0.000004  loss: 0.8613 (0.7924)  loss_scale: 65536.0000 (109610.6272)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0002  max mem: 41808
[2025-05-23 21:32:19,829] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 21:32:19,829] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-05-23 21:32:19,829] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 21:32:19,829] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [31]  [1090/1349]  eta: 0:01:20  lr: 0.000177  min_lr: 0.000004  loss: 0.8310 (0.7919)  loss_scale: 65536.0000 (109627.1311)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
Epoch: [31]  [1100/1349]  eta: 0:01:17  lr: 0.000176  min_lr: 0.000004  loss: 0.7757 (0.7918)  loss_scale: 131072.0000 (109821.9074)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
Epoch: [31]  [1110/1349]  eta: 0:01:14  lr: 0.000176  min_lr: 0.000004  loss: 0.8254 (0.7912)  loss_scale: 131072.0000 (110013.1773)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [31]  [1120/1349]  eta: 0:01:11  lr: 0.000176  min_lr: 0.000004  loss: 0.7412 (0.7908)  loss_scale: 131072.0000 (110201.0348)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
Epoch: [31]  [1130/1349]  eta: 0:01:08  lr: 0.000176  min_lr: 0.000004  loss: 0.7412 (0.7907)  loss_scale: 131072.0000 (110385.5703)  weight_decay: 0.0500 (0.0500)  time: 0.3066  data: 0.0001  max mem: 41808
Epoch: [31]  [1140/1349]  eta: 0:01:05  lr: 0.000176  min_lr: 0.000004  loss: 0.8079 (0.7905)  loss_scale: 131072.0000 (110566.8712)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0001  max mem: 41808
Epoch: [31]  [1150/1349]  eta: 0:01:01  lr: 0.000176  min_lr: 0.000004  loss: 0.8352 (0.7906)  loss_scale: 131072.0000 (110745.0217)  weight_decay: 0.0500 (0.0500)  time: 0.3063  data: 0.0001  max mem: 41808
Epoch: [31]  [1160/1349]  eta: 0:00:58  lr: 0.000176  min_lr: 0.000004  loss: 0.7358 (0.7901)  loss_scale: 131072.0000 (110920.1034)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0002  max mem: 41808
Epoch: [31]  [1170/1349]  eta: 0:00:55  lr: 0.000176  min_lr: 0.000004  loss: 0.8085 (0.7907)  loss_scale: 131072.0000 (111092.1947)  weight_decay: 0.0500 (0.0500)  time: 0.3087  data: 0.0001  max mem: 41808
[2025-05-23 21:32:49,326] [INFO] [logging.py:96:log_dist] [Rank 0] step=43000, skipped=262, lr=[4.168889551974922e-06, 4.168889551974922e-06, 5.558519402633229e-06, 5.558519402633229e-06, 7.411359203510973e-06, 7.411359203510973e-06, 9.881812271347964e-06, 9.881812271347964e-06, 1.3175749695130618e-05, 1.3175749695130618e-05, 1.7567666260174157e-05, 1.7567666260174157e-05, 2.3423555013565545e-05, 2.3423555013565545e-05, 3.1231406684754056e-05, 3.1231406684754056e-05, 4.1641875579672076e-05, 4.1641875579672076e-05, 5.5522500772896106e-05, 5.5522500772896106e-05, 7.403000103052814e-05, 7.403000103052814e-05, 9.870666804070419e-05, 9.870666804070419e-05, 0.00013160889072093892, 0.00013160889072093892, 0.00017547852096125188, 0.00017547852096125188], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-05-23 21:32:49,327] [INFO] [timer.py:260:stop] epoch=0/micro_step=43000/global_step=43000, RunningAvgSamplesPerSec=209.86279513281224, CurrSamplesPerSec=214.1883906714286, MemAllocated=1.37GB, MaxMemAllocated=40.83GB
Epoch: [31]  [1180/1349]  eta: 0:00:52  lr: 0.000175  min_lr: 0.000004  loss: 0.8254 (0.7907)  loss_scale: 131072.0000 (111261.3717)  weight_decay: 0.0500 (0.0500)  time: 0.3082  data: 0.0001  max mem: 41808
Epoch: [31]  [1190/1349]  eta: 0:00:49  lr: 0.000175  min_lr: 0.000004  loss: 0.6544 (0.7897)  loss_scale: 131072.0000 (111427.7078)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
Epoch: [31]  [1200/1349]  eta: 0:00:46  lr: 0.000175  min_lr: 0.000004  loss: 0.6984 (0.7898)  loss_scale: 131072.0000 (111591.2739)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
Epoch: [31]  [1210/1349]  eta: 0:00:43  lr: 0.000175  min_lr: 0.000004  loss: 0.7872 (0.7901)  loss_scale: 131072.0000 (111752.1387)  weight_decay: 0.0500 (0.0500)  time: 0.3080  data: 0.0001  max mem: 41808
[2025-05-23 21:32:59,174] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 21:32:59,174] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 21:32:59,174] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 21:32:59,174] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [31]  [1220/1349]  eta: 0:00:40  lr: 0.000175  min_lr: 0.000004  loss: 0.7975 (0.7899)  loss_scale: 131072.0000 (112876.5012)  weight_decay: 0.0500 (0.0500)  time: 0.3085  data: 0.0001  max mem: 41808
Epoch: [31]  [1230/1349]  eta: 0:00:37  lr: 0.000175  min_lr: 0.000004  loss: 0.8039 (0.7901)  loss_scale: 262144.0000 (114089.0723)  weight_decay: 0.0500 (0.0500)  time: 0.3084  data: 0.0001  max mem: 41808
[2025-05-23 21:33:05,031] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 43050
[2025-05-23 21:33:05,031] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 43050
[2025-05-23 21:33:05,031] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 21:33:05,031] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 21:33:05,031] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [31]  [1240/1349]  eta: 0:00:33  lr: 0.000175  min_lr: 0.000004  loss: 0.8490 (0.7903)  loss_scale: 131072.0000 (114225.9210)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [31]  [1250/1349]  eta: 0:00:30  lr: 0.000175  min_lr: 0.000004  loss: 0.8565 (0.7908)  loss_scale: 131072.0000 (114360.5819)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
Epoch: [31]  [1260/1349]  eta: 0:00:27  lr: 0.000174  min_lr: 0.000004  loss: 0.8480 (0.7909)  loss_scale: 131072.0000 (114493.1071)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [31]  [1270/1349]  eta: 0:00:24  lr: 0.000174  min_lr: 0.000004  loss: 0.7363 (0.7906)  loss_scale: 131072.0000 (114623.5468)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0002  max mem: 41808
Epoch: [31]  [1280/1349]  eta: 0:00:21  lr: 0.000174  min_lr: 0.000004  loss: 0.7912 (0.7910)  loss_scale: 131072.0000 (114751.9500)  weight_decay: 0.0500 (0.0500)  time: 0.3083  data: 0.0001  max mem: 41808
Epoch: [31]  [1290/1349]  eta: 0:00:18  lr: 0.000174  min_lr: 0.000004  loss: 0.8146 (0.7909)  loss_scale: 131072.0000 (114878.3641)  weight_decay: 0.0500 (0.0500)  time: 0.3091  data: 0.0002  max mem: 41808
Epoch: [31]  [1300/1349]  eta: 0:00:15  lr: 0.000174  min_lr: 0.000004  loss: 0.8161 (0.7911)  loss_scale: 131072.0000 (115002.8347)  weight_decay: 0.0500 (0.0500)  time: 0.3086  data: 0.0002  max mem: 41808
Epoch: [31]  [1310/1349]  eta: 0:00:12  lr: 0.000174  min_lr: 0.000004  loss: 0.8403 (0.7917)  loss_scale: 131072.0000 (115125.4066)  weight_decay: 0.0500 (0.0500)  time: 0.3078  data: 0.0002  max mem: 41808
Epoch: [31]  [1320/1349]  eta: 0:00:09  lr: 0.000174  min_lr: 0.000004  loss: 0.8823 (0.7921)  loss_scale: 131072.0000 (115246.1226)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0002  max mem: 41808
Epoch: [31]  [1330/1349]  eta: 0:00:05  lr: 0.000174  min_lr: 0.000004  loss: 0.8886 (0.7924)  loss_scale: 131072.0000 (115365.0248)  weight_decay: 0.0500 (0.0500)  time: 0.3047  data: 0.0001  max mem: 41808
Epoch: [31]  [1340/1349]  eta: 0:00:02  lr: 0.000174  min_lr: 0.000004  loss: 0.8843 (0.7929)  loss_scale: 131072.0000 (115482.1536)  weight_decay: 0.0500 (0.0500)  time: 0.3022  data: 0.0001  max mem: 41808
Epoch: [31]  [1348/1349]  eta: 0:00:00  lr: 0.000173  min_lr: 0.000004  loss: 0.8246 (0.7926)  loss_scale: 131072.0000 (115574.6064)  weight_decay: 0.0500 (0.0500)  time: 0.3015  data: 0.0001  max mem: 41808
Epoch: [31] Total time: 0:06:59 (0.3111 s / it)
Averaged stats: lr: 0.000173  min_lr: 0.000004  loss: 0.8246 (0.7942)  loss_scale: 131072.0000 (115574.6064)  weight_decay: 0.0500 (0.0500)  total_time: 419.7255 (419.7202)
Val:  [  0/346]  eta: 1:30:58  loss: 2.6518 (2.6518)  acc1: 14.0625 (14.0625)  acc5: 78.1250 (78.1250)  time: 15.7755  data: 14.9904  max mem: 41808
Val:  [ 10/346]  eta: 0:12:11  loss: 0.1233 (0.5343)  acc1: 100.0000 (86.2926)  acc5: 100.0000 (97.3722)  time: 2.1781  data: 1.3631  max mem: 41808
Val:  [ 20/346]  eta: 0:08:22  loss: 0.1199 (0.4362)  acc1: 100.0000 (88.9881)  acc5: 100.0000 (98.4375)  time: 0.8302  data: 0.0374  max mem: 41808
Val:  [ 30/346]  eta: 0:06:50  loss: 0.1027 (0.3761)  acc1: 100.0000 (91.0282)  acc5: 100.0000 (98.7651)  time: 0.8133  data: 0.0373  max mem: 41808
Val:  [ 40/346]  eta: 0:05:57  loss: 0.1070 (0.4207)  acc1: 97.6562 (89.9009)  acc5: 100.0000 (98.6280)  time: 0.7752  data: 0.0003  max mem: 41808
Val:  [ 50/346]  eta: 0:05:21  loss: 0.1100 (0.3719)  acc1: 100.0000 (91.4982)  acc5: 100.0000 (98.8664)  time: 0.7545  data: 0.0003  max mem: 41808
Val:  [ 60/346]  eta: 0:04:57  loss: 0.1275 (0.3600)  acc1: 97.6562 (91.5984)  acc5: 100.0000 (99.0523)  time: 0.7762  data: 0.0003  max mem: 41808
Val:  [ 70/346]  eta: 0:04:36  loss: 0.2197 (0.3734)  acc1: 94.5312 (91.1752)  acc5: 100.0000 (99.1747)  time: 0.7853  data: 0.0128  max mem: 41808
Val:  [ 80/346]  eta: 0:04:21  loss: 0.1982 (0.3737)  acc1: 92.9688 (91.2230)  acc5: 100.0000 (99.2670)  time: 0.8136  data: 0.0848  max mem: 41808
Val:  [ 90/346]  eta: 0:04:09  loss: 0.2384 (0.3698)  acc1: 92.9688 (91.2260)  acc5: 100.0000 (99.3132)  time: 0.8878  data: 0.1423  max mem: 41808
Val:  [100/346]  eta: 0:03:57  loss: 0.1772 (0.3469)  acc1: 97.6562 (92.0173)  acc5: 100.0000 (99.3812)  time: 0.8971  data: 0.1392  max mem: 41808
Val:  [110/346]  eta: 0:03:45  loss: 0.1358 (0.3603)  acc1: 97.6562 (91.5470)  acc5: 100.0000 (99.3666)  time: 0.8647  data: 0.1378  max mem: 41808
Val:  [120/346]  eta: 0:03:34  loss: 0.1914 (0.3658)  acc1: 96.0938 (91.3804)  acc5: 100.0000 (99.4189)  time: 0.8710  data: 0.1443  max mem: 41808
Val:  [130/346]  eta: 0:03:24  loss: 0.1115 (0.3648)  acc1: 98.4375 (91.4122)  acc5: 100.0000 (99.4633)  time: 0.9018  data: 0.1560  max mem: 41808
Val:  [140/346]  eta: 0:03:14  loss: 0.2614 (0.3713)  acc1: 95.3125 (91.1680)  acc5: 100.0000 (99.5013)  time: 0.9056  data: 0.1568  max mem: 41808
Val:  [150/346]  eta: 0:03:04  loss: 0.2784 (0.3697)  acc1: 92.9688 (91.2459)  acc5: 100.0000 (99.5188)  time: 0.8904  data: 0.1500  max mem: 41808
Val:  [160/346]  eta: 0:02:53  loss: 0.2784 (0.3680)  acc1: 94.5312 (91.2122)  acc5: 100.0000 (99.5342)  time: 0.8663  data: 0.1445  max mem: 41808
Val:  [170/346]  eta: 0:02:43  loss: 0.2450 (0.3693)  acc1: 94.5312 (91.1413)  acc5: 100.0000 (99.5614)  time: 0.8682  data: 0.1464  max mem: 41808
Val:  [180/346]  eta: 0:02:34  loss: 0.2091 (0.3862)  acc1: 96.0938 (90.4178)  acc5: 100.0000 (99.5209)  time: 0.8858  data: 0.1498  max mem: 41808
Val:  [190/346]  eta: 0:02:24  loss: 0.2239 (0.3841)  acc1: 92.1875 (90.4900)  acc5: 100.0000 (99.5173)  time: 0.8774  data: 0.1489  max mem: 41808
Val:  [200/346]  eta: 0:02:15  loss: 0.3434 (0.3950)  acc1: 91.4062 (90.1081)  acc5: 100.0000 (99.5336)  time: 0.8904  data: 0.1499  max mem: 41808
Val:  [210/346]  eta: 0:02:05  loss: 0.3006 (0.3880)  acc1: 91.4062 (90.3288)  acc5: 100.0000 (99.5557)  time: 0.8996  data: 0.1462  max mem: 41808
Val:  [220/346]  eta: 0:01:56  loss: 0.1358 (0.3845)  acc1: 98.4375 (90.4801)  acc5: 100.0000 (99.5369)  time: 0.9001  data: 0.1571  max mem: 41808
Val:  [230/346]  eta: 0:01:47  loss: 0.1246 (0.3751)  acc1: 99.2188 (90.7873)  acc5: 100.0000 (99.5570)  time: 0.9192  data: 0.1738  max mem: 41808
Val:  [240/346]  eta: 0:01:37  loss: 0.1547 (0.3790)  acc1: 96.8750 (90.7255)  acc5: 100.0000 (99.5591)  time: 0.9239  data: 0.1636  max mem: 41808
Val:  [250/346]  eta: 0:01:28  loss: 0.1897 (0.3788)  acc1: 96.8750 (90.7869)  acc5: 100.0000 (99.5425)  time: 0.9159  data: 0.1530  max mem: 41808
Val:  [260/346]  eta: 0:01:19  loss: 0.1631 (0.3754)  acc1: 96.8750 (90.8914)  acc5: 100.0000 (99.5510)  time: 0.9103  data: 0.1552  max mem: 41808
Val:  [270/346]  eta: 0:01:10  loss: 0.1247 (0.3703)  acc1: 98.4375 (91.0545)  acc5: 100.0000 (99.5647)  time: 0.9209  data: 0.1532  max mem: 41808
Val:  [280/346]  eta: 0:01:00  loss: 0.1150 (0.3681)  acc1: 100.0000 (91.1449)  acc5: 100.0000 (99.5774)  time: 0.9171  data: 0.1577  max mem: 41808
Val:  [290/346]  eta: 0:00:51  loss: 0.1046 (0.3598)  acc1: 100.0000 (91.4116)  acc5: 100.0000 (99.5919)  time: 0.8970  data: 0.1558  max mem: 41808
Val:  [300/346]  eta: 0:00:42  loss: 0.1060 (0.3596)  acc1: 100.0000 (91.4737)  acc5: 100.0000 (99.5172)  time: 0.8887  data: 0.1462  max mem: 41808
Val:  [310/346]  eta: 0:00:33  loss: 0.1321 (0.3596)  acc1: 99.2188 (91.4866)  acc5: 100.0000 (99.5302)  time: 0.8885  data: 0.1409  max mem: 41808
Val:  [320/346]  eta: 0:00:23  loss: 0.1340 (0.3616)  acc1: 99.2188 (91.4062)  acc5: 100.0000 (99.5449)  time: 0.8926  data: 0.1397  max mem: 41808
Val:  [330/346]  eta: 0:00:14  loss: 0.3444 (0.3734)  acc1: 85.9375 (91.0711)  acc5: 100.0000 (99.4642)  time: 0.9038  data: 0.1486  max mem: 41808
Val:  [340/346]  eta: 0:00:05  loss: 0.3642 (0.3813)  acc1: 90.6250 (90.8587)  acc5: 100.0000 (99.4433)  time: 0.9064  data: 0.1589  max mem: 41808
Val:  [345/346]  eta: 0:00:00  loss: 0.2445 (0.3781)  acc1: 91.9540 (90.9621)  acc5: 100.0000 (99.4508)  time: 0.9047  data: 0.1718  max mem: 41808
Val: Total time: 0:05:17 (0.9186 s / it)
* Acc@1 91.041 Acc@5 99.451 loss 0.375
Accuracy of the network on the 88494 val videos: 91.0%
Max accuracy: 91.50%   Max Epoch: 25
Epoch: [32]  [   0/1349]  eta: 1:38:32  lr: 0.000173  min_lr: 0.000004  loss: 0.6360 (0.6360)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 4.3828  data: 3.5165  max mem: 41808
Epoch: [32]  [  10/1349]  eta: 0:15:28  lr: 0.000173  min_lr: 0.000004  loss: 0.8112 (0.8335)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6932  data: 0.3199  max mem: 41808
[2025-05-23 21:39:06,982] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 21:39:06,983] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 21:39:06,983] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 21:39:06,983] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [32]  [  20/1349]  eta: 0:11:22  lr: 0.000173  min_lr: 0.000004  loss: 0.8360 (0.7923)  loss_scale: 131072.0000 (193487.2381)  weight_decay: 0.0500 (0.0500)  time: 0.3202  data: 0.0002  max mem: 41808
[2025-05-23 21:39:12,295] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 43196
[2025-05-23 21:39:12,295] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 43196
[2025-05-23 21:39:12,295] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 21:39:12,295] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 21:39:12,295] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [32]  [  30/1349]  eta: 0:09:50  lr: 0.000173  min_lr: 0.000004  loss: 0.8360 (0.7967)  loss_scale: 262144.0000 (202950.1935)  weight_decay: 0.0500 (0.0500)  time: 0.3122  data: 0.0002  max mem: 41808
Epoch: [32]  [  40/1349]  eta: 0:09:01  lr: 0.000173  min_lr: 0.000004  loss: 0.8162 (0.8085)  loss_scale: 131072.0000 (185418.9268)  weight_decay: 0.0500 (0.0500)  time: 0.3081  data: 0.0001  max mem: 41808
Epoch: [32]  [  50/1349]  eta: 0:08:30  lr: 0.000173  min_lr: 0.000004  loss: 0.7934 (0.8003)  loss_scale: 131072.0000 (174762.6667)  weight_decay: 0.0500 (0.0500)  time: 0.3082  data: 0.0001  max mem: 41808
Epoch: [32]  [  60/1349]  eta: 0:08:08  lr: 0.000173  min_lr: 0.000004  loss: 0.7934 (0.8086)  loss_scale: 131072.0000 (167600.2623)  weight_decay: 0.0500 (0.0500)  time: 0.3083  data: 0.0001  max mem: 41808
Epoch: [32]  [  70/1349]  eta: 0:07:51  lr: 0.000173  min_lr: 0.000004  loss: 0.7728 (0.7940)  loss_scale: 131072.0000 (162455.4366)  weight_decay: 0.0500 (0.0500)  time: 0.3080  data: 0.0001  max mem: 41808
Epoch: [32]  [  80/1349]  eta: 0:07:38  lr: 0.000172  min_lr: 0.000004  loss: 0.7718 (0.7942)  loss_scale: 131072.0000 (158580.9383)  weight_decay: 0.0500 (0.0500)  time: 0.3078  data: 0.0001  max mem: 41808
Epoch: [32]  [  90/1349]  eta: 0:07:27  lr: 0.000172  min_lr: 0.000004  loss: 0.8054 (0.7927)  loss_scale: 131072.0000 (155557.9780)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0002  max mem: 41808
Epoch: [32]  [ 100/1349]  eta: 0:07:18  lr: 0.000172  min_lr: 0.000004  loss: 0.8336 (0.7958)  loss_scale: 131072.0000 (153133.6238)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0002  max mem: 41808
Epoch: [32]  [ 110/1349]  eta: 0:07:09  lr: 0.000172  min_lr: 0.000004  loss: 0.8292 (0.7886)  loss_scale: 131072.0000 (151146.0901)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0002  max mem: 41808
Epoch: [32]  [ 120/1349]  eta: 0:07:02  lr: 0.000172  min_lr: 0.000004  loss: 0.7642 (0.7892)  loss_scale: 131072.0000 (149487.0744)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
Epoch: [32]  [ 130/1349]  eta: 0:06:55  lr: 0.000172  min_lr: 0.000004  loss: 0.8080 (0.7945)  loss_scale: 131072.0000 (148081.3435)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [32]  [ 140/1349]  eta: 0:06:49  lr: 0.000172  min_lr: 0.000004  loss: 0.8080 (0.7928)  loss_scale: 131072.0000 (146875.0071)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
Epoch: [32]  [ 150/1349]  eta: 0:06:43  lr: 0.000172  min_lr: 0.000004  loss: 0.7859 (0.7920)  loss_scale: 131072.0000 (145828.4503)  weight_decay: 0.0500 (0.0500)  time: 0.3095  data: 0.0001  max mem: 41808
[2025-05-23 21:39:52,045] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 21:39:52,045] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 21:39:52,045] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 21:39:52,045] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 21:39:52,346] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 43326
[2025-05-23 21:39:52,346] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 43326
[2025-05-23 21:39:52,346] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 21:39:52,346] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 21:39:52,347] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [32]  [ 160/1349]  eta: 0:06:38  lr: 0.000171  min_lr: 0.000004  loss: 0.7962 (0.7943)  loss_scale: 131072.0000 (145726.0124)  weight_decay: 0.0500 (0.0500)  time: 0.3099  data: 0.0002  max mem: 41808
Epoch: [32]  [ 170/1349]  eta: 0:06:32  lr: 0.000171  min_lr: 0.000004  loss: 0.8464 (0.7933)  loss_scale: 131072.0000 (144869.0526)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0001  max mem: 41808
Epoch: [32]  [ 180/1349]  eta: 0:06:27  lr: 0.000171  min_lr: 0.000004  loss: 0.8333 (0.7967)  loss_scale: 131072.0000 (144106.7845)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [32]  [ 190/1349]  eta: 0:06:22  lr: 0.000171  min_lr: 0.000004  loss: 0.8195 (0.7943)  loss_scale: 131072.0000 (143424.3351)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
Epoch: [32]  [ 200/1349]  eta: 0:06:18  lr: 0.000171  min_lr: 0.000004  loss: 0.7379 (0.7925)  loss_scale: 131072.0000 (142809.7910)  weight_decay: 0.0500 (0.0500)  time: 0.3066  data: 0.0001  max mem: 41808
Epoch: [32]  [ 210/1349]  eta: 0:06:13  lr: 0.000171  min_lr: 0.000004  loss: 0.8148 (0.7952)  loss_scale: 131072.0000 (142253.4976)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [32]  [ 220/1349]  eta: 0:06:09  lr: 0.000171  min_lr: 0.000004  loss: 0.7498 (0.7914)  loss_scale: 131072.0000 (141747.5475)  weight_decay: 0.0500 (0.0500)  time: 0.3099  data: 0.0002  max mem: 41808
[2025-05-23 21:40:12,658] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 43392
[2025-05-23 21:40:12,658] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 43392
[2025-05-23 21:40:12,658] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 21:40:12,658] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 21:40:12,658] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [32]  [ 230/1349]  eta: 0:06:05  lr: 0.000171  min_lr: 0.000004  loss: 0.8321 (0.7942)  loss_scale: 131072.0000 (139299.4632)  weight_decay: 0.0500 (0.0500)  time: 0.3096  data: 0.0002  max mem: 41808
Epoch: [32]  [ 240/1349]  eta: 0:06:01  lr: 0.000170  min_lr: 0.000004  loss: 0.8431 (0.7964)  loss_scale: 65536.0000 (136238.7386)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [32]  [ 250/1349]  eta: 0:05:57  lr: 0.000170  min_lr: 0.000004  loss: 0.8410 (0.7960)  loss_scale: 65536.0000 (133421.8964)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [32]  [ 260/1349]  eta: 0:05:53  lr: 0.000170  min_lr: 0.000004  loss: 0.7925 (0.7955)  loss_scale: 65536.0000 (130820.9042)  weight_decay: 0.0500 (0.0500)  time: 0.3079  data: 0.0001  max mem: 41808
Epoch: [32]  [ 270/1349]  eta: 0:05:49  lr: 0.000170  min_lr: 0.000004  loss: 0.7925 (0.7948)  loss_scale: 65536.0000 (128411.8672)  weight_decay: 0.0500 (0.0500)  time: 0.3079  data: 0.0001  max mem: 41808
Epoch: [32]  [ 280/1349]  eta: 0:05:45  lr: 0.000170  min_lr: 0.000004  loss: 0.8205 (0.7959)  loss_scale: 65536.0000 (126174.2918)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [32]  [ 290/1349]  eta: 0:05:41  lr: 0.000170  min_lr: 0.000004  loss: 0.8038 (0.7955)  loss_scale: 65536.0000 (124090.5017)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
Epoch: [32]  [ 300/1349]  eta: 0:05:37  lr: 0.000170  min_lr: 0.000004  loss: 0.7676 (0.7936)  loss_scale: 65536.0000 (122145.1694)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
Epoch: [32]  [ 310/1349]  eta: 0:05:34  lr: 0.000170  min_lr: 0.000004  loss: 0.8695 (0.7952)  loss_scale: 65536.0000 (120324.9389)  weight_decay: 0.0500 (0.0500)  time: 0.3080  data: 0.0001  max mem: 41808
Epoch: [32]  [ 320/1349]  eta: 0:05:30  lr: 0.000169  min_lr: 0.000004  loss: 0.8718 (0.7946)  loss_scale: 65536.0000 (118618.1184)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
Epoch: [32]  [ 330/1349]  eta: 0:05:26  lr: 0.000169  min_lr: 0.000004  loss: 0.8433 (0.7942)  loss_scale: 65536.0000 (117014.4290)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
Epoch: [32]  [ 340/1349]  eta: 0:05:23  lr: 0.000169  min_lr: 0.000004  loss: 0.7835 (0.7928)  loss_scale: 65536.0000 (115504.7977)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0001  max mem: 41808
Epoch: [32]  [ 350/1349]  eta: 0:05:19  lr: 0.000169  min_lr: 0.000004  loss: 0.7977 (0.7934)  loss_scale: 65536.0000 (114081.1852)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
[2025-05-23 21:40:52,311] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 21:40:52,311] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 21:40:52,311] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-05-23 21:40:52,311] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [32]  [ 360/1349]  eta: 0:05:16  lr: 0.000169  min_lr: 0.000004  loss: 0.7904 (0.7920)  loss_scale: 65536.0000 (114188.7645)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
Epoch: [32]  [ 370/1349]  eta: 0:05:12  lr: 0.000169  min_lr: 0.000004  loss: 0.7669 (0.7920)  loss_scale: 131072.0000 (114643.8383)  weight_decay: 0.0500 (0.0500)  time: 0.3062  data: 0.0001  max mem: 41808
[2025-05-23 21:40:58,436] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 43541
[2025-05-23 21:40:58,436] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 43541
[2025-05-23 21:40:58,436] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 21:40:58,436] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 21:40:58,436] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [32]  [ 380/1349]  eta: 0:05:09  lr: 0.000169  min_lr: 0.000004  loss: 0.8343 (0.7915)  loss_scale: 131072.0000 (113698.9396)  weight_decay: 0.0500 (0.0500)  time: 0.3062  data: 0.0001  max mem: 41808
Epoch: [32]  [ 390/1349]  eta: 0:05:05  lr: 0.000169  min_lr: 0.000004  loss: 0.8517 (0.7929)  loss_scale: 65536.0000 (112467.1509)  weight_decay: 0.0500 (0.0500)  time: 0.3063  data: 0.0001  max mem: 41808
Epoch: [32]  [ 400/1349]  eta: 0:05:02  lr: 0.000169  min_lr: 0.000004  loss: 0.8250 (0.7912)  loss_scale: 65536.0000 (111296.7980)  weight_decay: 0.0500 (0.0500)  time: 0.3064  data: 0.0001  max mem: 41808
Epoch: [32]  [ 410/1349]  eta: 0:04:58  lr: 0.000168  min_lr: 0.000004  loss: 0.7804 (0.7911)  loss_scale: 65536.0000 (110183.3966)  weight_decay: 0.0500 (0.0500)  time: 0.3066  data: 0.0001  max mem: 41808
Epoch: [32]  [ 420/1349]  eta: 0:04:55  lr: 0.000168  min_lr: 0.000004  loss: 0.8493 (0.7922)  loss_scale: 65536.0000 (109122.8884)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [32]  [ 430/1349]  eta: 0:04:51  lr: 0.000168  min_lr: 0.000004  loss: 0.8448 (0.7909)  loss_scale: 65536.0000 (108111.5916)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
Epoch: [32]  [ 440/1349]  eta: 0:04:48  lr: 0.000168  min_lr: 0.000004  loss: 0.7544 (0.7903)  loss_scale: 65536.0000 (107146.1587)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
Epoch: [32]  [ 450/1349]  eta: 0:04:44  lr: 0.000168  min_lr: 0.000004  loss: 0.8518 (0.7911)  loss_scale: 65536.0000 (106223.5388)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
Epoch: [32]  [ 460/1349]  eta: 0:04:41  lr: 0.000168  min_lr: 0.000004  loss: 0.8547 (0.7918)  loss_scale: 65536.0000 (105340.9458)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
Epoch: [32]  [ 470/1349]  eta: 0:04:38  lr: 0.000168  min_lr: 0.000004  loss: 0.7525 (0.7921)  loss_scale: 65536.0000 (104495.8301)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0002  max mem: 41808
Epoch: [32]  [ 480/1349]  eta: 0:04:35  lr: 0.000168  min_lr: 0.000004  loss: 0.7863 (0.7914)  loss_scale: 65536.0000 (103685.8545)  weight_decay: 0.0500 (0.0500)  time: 0.3090  data: 0.0001  max mem: 41808
Epoch: [32]  [ 490/1349]  eta: 0:04:31  lr: 0.000167  min_lr: 0.000004  loss: 0.8596 (0.7922)  loss_scale: 65536.0000 (102908.8717)  weight_decay: 0.0500 (0.0500)  time: 0.3093  data: 0.0001  max mem: 41808
Epoch: [32]  [ 500/1349]  eta: 0:04:28  lr: 0.000167  min_lr: 0.000004  loss: 0.8596 (0.7925)  loss_scale: 65536.0000 (102162.9062)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
[2025-05-23 21:41:38,083] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 21:41:38,083] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-05-23 21:41:38,083] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 21:41:38,083] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [32]  [ 510/1349]  eta: 0:04:25  lr: 0.000167  min_lr: 0.000004  loss: 0.8319 (0.7927)  loss_scale: 65536.0000 (102600.3914)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [32]  [ 520/1349]  eta: 0:04:21  lr: 0.000167  min_lr: 0.000004  loss: 0.8157 (0.7927)  loss_scale: 131072.0000 (103146.8714)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [32]  [ 530/1349]  eta: 0:04:18  lr: 0.000167  min_lr: 0.000004  loss: 0.7827 (0.7920)  loss_scale: 131072.0000 (103672.7684)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
Epoch: [32]  [ 540/1349]  eta: 0:04:15  lr: 0.000167  min_lr: 0.000004  loss: 0.7898 (0.7918)  loss_scale: 131072.0000 (104179.2237)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [32]  [ 550/1349]  eta: 0:04:11  lr: 0.000167  min_lr: 0.000004  loss: 0.7713 (0.7898)  loss_scale: 131072.0000 (104667.2958)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
Epoch: [32]  [ 560/1349]  eta: 0:04:08  lr: 0.000167  min_lr: 0.000004  loss: 0.6754 (0.7890)  loss_scale: 131072.0000 (105137.9679)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0002  max mem: 41808
Epoch: [32]  [ 570/1349]  eta: 0:04:05  lr: 0.000166  min_lr: 0.000004  loss: 0.7294 (0.7890)  loss_scale: 131072.0000 (105592.1541)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [32]  [ 580/1349]  eta: 0:04:02  lr: 0.000166  min_lr: 0.000004  loss: 0.8061 (0.7880)  loss_scale: 131072.0000 (106030.7057)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
Epoch: [32]  [ 590/1349]  eta: 0:03:58  lr: 0.000166  min_lr: 0.000004  loss: 0.7831 (0.7878)  loss_scale: 131072.0000 (106454.4162)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
Epoch: [32]  [ 600/1349]  eta: 0:03:55  lr: 0.000166  min_lr: 0.000004  loss: 0.8154 (0.7879)  loss_scale: 131072.0000 (106864.0266)  weight_decay: 0.0500 (0.0500)  time: 0.3066  data: 0.0001  max mem: 41808
Epoch: [32]  [ 610/1349]  eta: 0:03:52  lr: 0.000166  min_lr: 0.000004  loss: 0.7504 (0.7868)  loss_scale: 131072.0000 (107260.2291)  weight_decay: 0.0500 (0.0500)  time: 0.3060  data: 0.0001  max mem: 41808
Epoch: [32]  [ 620/1349]  eta: 0:03:49  lr: 0.000166  min_lr: 0.000004  loss: 0.7716 (0.7875)  loss_scale: 131072.0000 (107643.6715)  weight_decay: 0.0500 (0.0500)  time: 0.3061  data: 0.0001  max mem: 41808
[2025-05-23 21:42:17,376] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 21:42:17,376] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 21:42:17,376] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 21:42:17,376] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [32]  [ 630/1349]  eta: 0:03:45  lr: 0.000166  min_lr: 0.000004  loss: 0.8188 (0.7874)  loss_scale: 131072.0000 (108222.6815)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
Epoch: [32]  [ 640/1349]  eta: 0:03:42  lr: 0.000166  min_lr: 0.000004  loss: 0.8058 (0.7879)  loss_scale: 262144.0000 (110623.9501)  weight_decay: 0.0500 (0.0500)  time: 0.3066  data: 0.0001  max mem: 41808
[2025-05-23 21:42:21,661] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 43812
[2025-05-23 21:42:21,661] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 43812
[2025-05-23 21:42:21,661] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 21:42:21,661] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 21:42:21,661] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [32]  [ 650/1349]  eta: 0:03:39  lr: 0.000165  min_lr: 0.000004  loss: 0.8205 (0.7882)  loss_scale: 262144.0000 (111542.0707)  weight_decay: 0.0500 (0.0500)  time: 0.3062  data: 0.0001  max mem: 41808
Epoch: [32]  [ 660/1349]  eta: 0:03:36  lr: 0.000165  min_lr: 0.000004  loss: 0.7974 (0.7882)  loss_scale: 131072.0000 (111837.5310)  weight_decay: 0.0500 (0.0500)  time: 0.3063  data: 0.0001  max mem: 41808
Epoch: [32]  [ 670/1349]  eta: 0:03:33  lr: 0.000165  min_lr: 0.000004  loss: 0.7624 (0.7878)  loss_scale: 131072.0000 (112124.1848)  weight_decay: 0.0500 (0.0500)  time: 0.3064  data: 0.0001  max mem: 41808
Epoch: [32]  [ 680/1349]  eta: 0:03:29  lr: 0.000165  min_lr: 0.000004  loss: 0.7865 (0.7876)  loss_scale: 131072.0000 (112402.4200)  weight_decay: 0.0500 (0.0500)  time: 0.3060  data: 0.0002  max mem: 41808
Epoch: [32]  [ 690/1349]  eta: 0:03:26  lr: 0.000165  min_lr: 0.000004  loss: 0.7776 (0.7873)  loss_scale: 131072.0000 (112672.6020)  weight_decay: 0.0500 (0.0500)  time: 0.3059  data: 0.0001  max mem: 41808
Epoch: [32]  [ 700/1349]  eta: 0:03:23  lr: 0.000165  min_lr: 0.000004  loss: 0.7678 (0.7874)  loss_scale: 131072.0000 (112935.0756)  weight_decay: 0.0500 (0.0500)  time: 0.3062  data: 0.0001  max mem: 41808
Epoch: [32]  [ 710/1349]  eta: 0:03:20  lr: 0.000165  min_lr: 0.000004  loss: 0.7735 (0.7866)  loss_scale: 131072.0000 (113190.1660)  weight_decay: 0.0500 (0.0500)  time: 0.3066  data: 0.0001  max mem: 41808
Epoch: [32]  [ 720/1349]  eta: 0:03:17  lr: 0.000165  min_lr: 0.000004  loss: 0.8162 (0.7867)  loss_scale: 131072.0000 (113438.1803)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0001  max mem: 41808
Epoch: [32]  [ 730/1349]  eta: 0:03:13  lr: 0.000164  min_lr: 0.000004  loss: 0.8273 (0.7869)  loss_scale: 131072.0000 (113679.4090)  weight_decay: 0.0500 (0.0500)  time: 0.3061  data: 0.0001  max mem: 41808
Epoch: [32]  [ 740/1349]  eta: 0:03:10  lr: 0.000164  min_lr: 0.000004  loss: 0.7836 (0.7869)  loss_scale: 131072.0000 (113914.1269)  weight_decay: 0.0500 (0.0500)  time: 0.3059  data: 0.0001  max mem: 41808
Epoch: [32]  [ 750/1349]  eta: 0:03:07  lr: 0.000164  min_lr: 0.000004  loss: 0.8020 (0.7880)  loss_scale: 131072.0000 (114142.5939)  weight_decay: 0.0500 (0.0500)  time: 0.3058  data: 0.0001  max mem: 41808
Epoch: [32]  [ 760/1349]  eta: 0:03:04  lr: 0.000164  min_lr: 0.000004  loss: 0.8541 (0.7888)  loss_scale: 131072.0000 (114365.0565)  weight_decay: 0.0500 (0.0500)  time: 0.3060  data: 0.0001  max mem: 41808
Epoch: [32]  [ 770/1349]  eta: 0:03:01  lr: 0.000164  min_lr: 0.000004  loss: 0.8193 (0.7885)  loss_scale: 131072.0000 (114581.7484)  weight_decay: 0.0500 (0.0500)  time: 0.3060  data: 0.0001  max mem: 41808
[2025-05-23 21:43:01,167] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 21:43:01,167] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 21:43:01,167] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 21:43:01,167] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [32]  [ 780/1349]  eta: 0:02:57  lr: 0.000164  min_lr: 0.000004  loss: 0.7534 (0.7884)  loss_scale: 131072.0000 (116135.4981)  weight_decay: 0.0500 (0.0500)  time: 0.3057  data: 0.0001  max mem: 41808
[2025-05-23 21:43:05,136] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 43954
[2025-05-23 21:43:05,136] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 43954
[2025-05-23 21:43:05,136] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 21:43:05,136] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 21:43:05,137] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [32]  [ 790/1349]  eta: 0:02:54  lr: 0.000164  min_lr: 0.000004  loss: 0.7576 (0.7881)  loss_scale: 262144.0000 (117152.8496)  weight_decay: 0.0500 (0.0500)  time: 0.3052  data: 0.0001  max mem: 41808
Epoch: [32]  [ 800/1349]  eta: 0:02:51  lr: 0.000164  min_lr: 0.000004  loss: 0.7014 (0.7867)  loss_scale: 131072.0000 (117326.6217)  weight_decay: 0.0500 (0.0500)  time: 0.3055  data: 0.0001  max mem: 41808
Epoch: [32]  [ 810/1349]  eta: 0:02:48  lr: 0.000164  min_lr: 0.000004  loss: 0.7263 (0.7866)  loss_scale: 131072.0000 (117496.1085)  weight_decay: 0.0500 (0.0500)  time: 0.3060  data: 0.0001  max mem: 41808
Epoch: [32]  [ 820/1349]  eta: 0:02:45  lr: 0.000163  min_lr: 0.000004  loss: 0.7524 (0.7863)  loss_scale: 131072.0000 (117661.4665)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
Epoch: [32]  [ 830/1349]  eta: 0:02:42  lr: 0.000163  min_lr: 0.000004  loss: 0.7554 (0.7863)  loss_scale: 131072.0000 (117822.8448)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
[2025-05-23 21:43:18,943] [INFO] [logging.py:96:log_dist] [Rank 0] step=44000, skipped=269, lr=[3.878846593682602e-06, 3.878846593682602e-06, 5.171795458243469e-06, 5.171795458243469e-06, 6.895727277657959e-06, 6.895727277657959e-06, 9.194303036877278e-06, 9.194303036877278e-06, 1.225907071583637e-05, 1.225907071583637e-05, 1.634542762111516e-05, 1.634542762111516e-05, 2.1793903494820215e-05, 2.1793903494820215e-05, 2.905853799309362e-05, 2.905853799309362e-05, 3.874471732412483e-05, 3.874471732412483e-05, 5.1659623098833104e-05, 5.1659623098833104e-05, 6.88794974651108e-05, 6.88794974651108e-05, 9.183932995348107e-05, 9.183932995348107e-05, 0.00012245243993797478, 0.00012245243993797478, 0.00016326991991729968, 0.00016326991991729968], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-05-23 21:43:18,944] [INFO] [timer.py:260:stop] epoch=0/micro_step=44000/global_step=44000, RunningAvgSamplesPerSec=209.93271153372456, CurrSamplesPerSec=214.83584595647346, MemAllocated=1.37GB, MaxMemAllocated=40.83GB
Epoch: [32]  [ 840/1349]  eta: 0:02:38  lr: 0.000163  min_lr: 0.000004  loss: 0.8567 (0.7865)  loss_scale: 131072.0000 (117980.3853)  weight_decay: 0.0500 (0.0500)  time: 0.3059  data: 0.0001  max mem: 41808
Epoch: [32]  [ 850/1349]  eta: 0:02:35  lr: 0.000163  min_lr: 0.000004  loss: 0.7945 (0.7863)  loss_scale: 131072.0000 (118134.2233)  weight_decay: 0.0500 (0.0500)  time: 0.3058  data: 0.0001  max mem: 41808
Epoch: [32]  [ 860/1349]  eta: 0:02:32  lr: 0.000163  min_lr: 0.000004  loss: 0.7983 (0.7868)  loss_scale: 131072.0000 (118284.4878)  weight_decay: 0.0500 (0.0500)  time: 0.3060  data: 0.0001  max mem: 41808
Epoch: [32]  [ 870/1349]  eta: 0:02:29  lr: 0.000163  min_lr: 0.000004  loss: 0.8650 (0.7870)  loss_scale: 131072.0000 (118431.3020)  weight_decay: 0.0500 (0.0500)  time: 0.3061  data: 0.0001  max mem: 41808
Epoch: [32]  [ 880/1349]  eta: 0:02:26  lr: 0.000163  min_lr: 0.000004  loss: 0.8583 (0.7876)  loss_scale: 131072.0000 (118574.7832)  weight_decay: 0.0500 (0.0500)  time: 0.3063  data: 0.0001  max mem: 41808
Epoch: [32]  [ 890/1349]  eta: 0:02:23  lr: 0.000163  min_lr: 0.000004  loss: 0.8600 (0.7882)  loss_scale: 131072.0000 (118715.0438)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
Epoch: [32]  [ 900/1349]  eta: 0:02:20  lr: 0.000162  min_lr: 0.000004  loss: 0.8306 (0.7875)  loss_scale: 131072.0000 (118852.1909)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
Epoch: [32]  [ 910/1349]  eta: 0:02:16  lr: 0.000162  min_lr: 0.000004  loss: 0.8306 (0.7884)  loss_scale: 131072.0000 (118986.3271)  weight_decay: 0.0500 (0.0500)  time: 0.3062  data: 0.0001  max mem: 41808
[2025-05-23 21:43:44,671] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 21:43:44,671] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 21:43:44,671] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 21:43:44,671] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [32]  [ 920/1349]  eta: 0:02:13  lr: 0.000162  min_lr: 0.000004  loss: 0.8371 (0.7882)  loss_scale: 131072.0000 (119971.4397)  weight_decay: 0.0500 (0.0500)  time: 0.3060  data: 0.0001  max mem: 41808
Epoch: [32]  [ 930/1349]  eta: 0:02:10  lr: 0.000162  min_lr: 0.000004  loss: 0.7381 (0.7869)  loss_scale: 262144.0000 (121498.5349)  weight_decay: 0.0500 (0.0500)  time: 0.3059  data: 0.0001  max mem: 41808
Epoch: [32]  [ 940/1349]  eta: 0:02:07  lr: 0.000162  min_lr: 0.000004  loss: 0.7381 (0.7869)  loss_scale: 262144.0000 (122993.1732)  weight_decay: 0.0500 (0.0500)  time: 0.3060  data: 0.0001  max mem: 41808
[2025-05-23 21:43:55,377] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 44118
[2025-05-23 21:43:55,377] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 44118
[2025-05-23 21:43:55,377] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 21:43:55,377] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 21:43:55,377] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [32]  [ 950/1349]  eta: 0:02:04  lr: 0.000162  min_lr: 0.000004  loss: 0.7618 (0.7860)  loss_scale: 262144.0000 (124318.5531)  weight_decay: 0.0500 (0.0500)  time: 0.3055  data: 0.0001  max mem: 41808
Epoch: [32]  [ 960/1349]  eta: 0:02:01  lr: 0.000162  min_lr: 0.000004  loss: 0.7210 (0.7856)  loss_scale: 131072.0000 (124388.8283)  weight_decay: 0.0500 (0.0500)  time: 0.3063  data: 0.0001  max mem: 41808
Epoch: [32]  [ 970/1349]  eta: 0:01:58  lr: 0.000162  min_lr: 0.000004  loss: 0.7251 (0.7851)  loss_scale: 131072.0000 (124457.6560)  weight_decay: 0.0500 (0.0500)  time: 0.3081  data: 0.0001  max mem: 41808
Epoch: [32]  [ 980/1349]  eta: 0:01:54  lr: 0.000161  min_lr: 0.000004  loss: 0.7654 (0.7852)  loss_scale: 131072.0000 (124525.0805)  weight_decay: 0.0500 (0.0500)  time: 0.3089  data: 0.0001  max mem: 41808
Epoch: [32]  [ 990/1349]  eta: 0:01:51  lr: 0.000161  min_lr: 0.000004  loss: 0.8206 (0.7854)  loss_scale: 131072.0000 (124591.1443)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0001  max mem: 41808
Epoch: [32]  [1000/1349]  eta: 0:01:48  lr: 0.000161  min_lr: 0.000004  loss: 0.8694 (0.7862)  loss_scale: 131072.0000 (124655.8881)  weight_decay: 0.0500 (0.0500)  time: 0.3064  data: 0.0001  max mem: 41808
Epoch: [32]  [1010/1349]  eta: 0:01:45  lr: 0.000161  min_lr: 0.000004  loss: 0.8451 (0.7869)  loss_scale: 131072.0000 (124719.3511)  weight_decay: 0.0500 (0.0500)  time: 0.3059  data: 0.0001  max mem: 41808
Epoch: [32]  [1020/1349]  eta: 0:01:42  lr: 0.000161  min_lr: 0.000004  loss: 0.8375 (0.7873)  loss_scale: 131072.0000 (124781.5710)  weight_decay: 0.0500 (0.0500)  time: 0.3057  data: 0.0001  max mem: 41808
Epoch: [32]  [1030/1349]  eta: 0:01:39  lr: 0.000161  min_lr: 0.000004  loss: 0.8483 (0.7879)  loss_scale: 131072.0000 (124842.5839)  weight_decay: 0.0500 (0.0500)  time: 0.3059  data: 0.0001  max mem: 41808
Epoch: [32]  [1040/1349]  eta: 0:01:36  lr: 0.000161  min_lr: 0.000004  loss: 0.7957 (0.7875)  loss_scale: 131072.0000 (124902.4246)  weight_decay: 0.0500 (0.0500)  time: 0.3061  data: 0.0001  max mem: 41808
Epoch: [32]  [1050/1349]  eta: 0:01:32  lr: 0.000161  min_lr: 0.000004  loss: 0.7566 (0.7872)  loss_scale: 131072.0000 (124961.1265)  weight_decay: 0.0500 (0.0500)  time: 0.3060  data: 0.0001  max mem: 41808
Epoch: [32]  [1060/1349]  eta: 0:01:29  lr: 0.000161  min_lr: 0.000004  loss: 0.7615 (0.7872)  loss_scale: 131072.0000 (125018.7220)  weight_decay: 0.0500 (0.0500)  time: 0.3059  data: 0.0001  max mem: 41808
Epoch: [32]  [1070/1349]  eta: 0:01:26  lr: 0.000160  min_lr: 0.000004  loss: 0.8099 (0.7869)  loss_scale: 131072.0000 (125075.2418)  weight_decay: 0.0500 (0.0500)  time: 0.3059  data: 0.0002  max mem: 41808
[2025-05-23 21:44:34,956] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 21:44:34,956] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 21:44:34,956] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 21:44:34,956] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [32]  [1080/1349]  eta: 0:01:23  lr: 0.000160  min_lr: 0.000004  loss: 0.7904 (0.7873)  loss_scale: 131072.0000 (125373.2174)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0002  max mem: 41808
[2025-05-23 21:44:36,183] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 44251
[2025-05-23 21:44:36,183] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 44251
[2025-05-23 21:44:36,183] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 21:44:36,183] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 21:44:36,183] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [32]  [1090/1349]  eta: 0:01:20  lr: 0.000160  min_lr: 0.000004  loss: 0.7578 (0.7870)  loss_scale: 131072.0000 (125665.7305)  weight_decay: 0.0500 (0.0500)  time: 0.3078  data: 0.0001  max mem: 41808
Epoch: [32]  [1100/1349]  eta: 0:01:17  lr: 0.000160  min_lr: 0.000004  loss: 0.7541 (0.7865)  loss_scale: 131072.0000 (125714.8338)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
Epoch: [32]  [1110/1349]  eta: 0:01:14  lr: 0.000160  min_lr: 0.000004  loss: 0.7155 (0.7857)  loss_scale: 131072.0000 (125763.0531)  weight_decay: 0.0500 (0.0500)  time: 0.3084  data: 0.0001  max mem: 41808
Epoch: [32]  [1120/1349]  eta: 0:01:11  lr: 0.000160  min_lr: 0.000004  loss: 0.7334 (0.7856)  loss_scale: 131072.0000 (125810.4121)  weight_decay: 0.0500 (0.0500)  time: 0.3085  data: 0.0001  max mem: 41808
Epoch: [32]  [1130/1349]  eta: 0:01:08  lr: 0.000160  min_lr: 0.000004  loss: 0.7556 (0.7856)  loss_scale: 131072.0000 (125856.9337)  weight_decay: 0.0500 (0.0500)  time: 0.3079  data: 0.0001  max mem: 41808
Epoch: [32]  [1140/1349]  eta: 0:01:04  lr: 0.000160  min_lr: 0.000004  loss: 0.7884 (0.7856)  loss_scale: 131072.0000 (125902.6398)  weight_decay: 0.0500 (0.0500)  time: 0.3080  data: 0.0001  max mem: 41808
Epoch: [32]  [1150/1349]  eta: 0:01:01  lr: 0.000159  min_lr: 0.000004  loss: 0.7313 (0.7853)  loss_scale: 131072.0000 (125947.5517)  weight_decay: 0.0500 (0.0500)  time: 0.3091  data: 0.0001  max mem: 41808
Epoch: [32]  [1160/1349]  eta: 0:00:58  lr: 0.000159  min_lr: 0.000004  loss: 0.7109 (0.7847)  loss_scale: 131072.0000 (125991.6899)  weight_decay: 0.0500 (0.0500)  time: 0.3090  data: 0.0001  max mem: 41808
Epoch: [32]  [1170/1349]  eta: 0:00:55  lr: 0.000159  min_lr: 0.000004  loss: 0.7721 (0.7851)  loss_scale: 131072.0000 (126035.0743)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
Epoch: [32]  [1180/1349]  eta: 0:00:52  lr: 0.000159  min_lr: 0.000004  loss: 0.8421 (0.7852)  loss_scale: 131072.0000 (126077.7240)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0001  max mem: 41808
Epoch: [32]  [1190/1349]  eta: 0:00:49  lr: 0.000159  min_lr: 0.000004  loss: 0.8015 (0.7851)  loss_scale: 131072.0000 (126119.6574)  weight_decay: 0.0500 (0.0500)  time: 0.3078  data: 0.0001  max mem: 41808
Epoch: [32]  [1200/1349]  eta: 0:00:46  lr: 0.000159  min_lr: 0.000004  loss: 0.7828 (0.7849)  loss_scale: 131072.0000 (126160.8926)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0001  max mem: 41808
Epoch: [32]  [1210/1349]  eta: 0:00:43  lr: 0.000159  min_lr: 0.000004  loss: 0.8223 (0.7852)  loss_scale: 131072.0000 (126201.4467)  weight_decay: 0.0500 (0.0500)  time: 0.3080  data: 0.0001  max mem: 41808
[2025-05-23 21:45:15,943] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 21:45:15,943] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 21:45:15,943] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 21:45:15,943] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 21:45:18,094] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 44387
[2025-05-23 21:45:18,094] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 21:45:18,094] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 44387
[2025-05-23 21:45:18,094] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 21:45:18,095] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [32]  [1220/1349]  eta: 0:00:40  lr: 0.000159  min_lr: 0.000004  loss: 0.7869 (0.7849)  loss_scale: 131072.0000 (126992.7731)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
Epoch: [32]  [1230/1349]  eta: 0:00:36  lr: 0.000158  min_lr: 0.000004  loss: 0.7512 (0.7848)  loss_scale: 131072.0000 (127025.9106)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
Epoch: [32]  [1240/1349]  eta: 0:00:33  lr: 0.000158  min_lr: 0.000004  loss: 0.7512 (0.7846)  loss_scale: 131072.0000 (127058.5141)  weight_decay: 0.0500 (0.0500)  time: 0.3078  data: 0.0001  max mem: 41808
Epoch: [32]  [1250/1349]  eta: 0:00:30  lr: 0.000158  min_lr: 0.000004  loss: 0.7789 (0.7849)  loss_scale: 131072.0000 (127090.5963)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
Epoch: [32]  [1260/1349]  eta: 0:00:27  lr: 0.000158  min_lr: 0.000004  loss: 0.8366 (0.7851)  loss_scale: 131072.0000 (127122.1697)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
Epoch: [32]  [1270/1349]  eta: 0:00:24  lr: 0.000158  min_lr: 0.000004  loss: 0.8150 (0.7855)  loss_scale: 131072.0000 (127153.2463)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [32]  [1280/1349]  eta: 0:00:21  lr: 0.000158  min_lr: 0.000004  loss: 0.8028 (0.7855)  loss_scale: 131072.0000 (127183.8376)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [32]  [1290/1349]  eta: 0:00:18  lr: 0.000158  min_lr: 0.000004  loss: 0.8237 (0.7860)  loss_scale: 131072.0000 (127213.9551)  weight_decay: 0.0500 (0.0500)  time: 0.3078  data: 0.0001  max mem: 41808
Epoch: [32]  [1300/1349]  eta: 0:00:15  lr: 0.000158  min_lr: 0.000004  loss: 0.8518 (0.7865)  loss_scale: 131072.0000 (127243.6095)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0001  max mem: 41808
Epoch: [32]  [1310/1349]  eta: 0:00:12  lr: 0.000158  min_lr: 0.000004  loss: 0.8239 (0.7865)  loss_scale: 131072.0000 (127272.8116)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [32]  [1320/1349]  eta: 0:00:09  lr: 0.000157  min_lr: 0.000004  loss: 0.8085 (0.7860)  loss_scale: 131072.0000 (127301.5715)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [32]  [1330/1349]  eta: 0:00:05  lr: 0.000157  min_lr: 0.000004  loss: 0.6830 (0.7855)  loss_scale: 131072.0000 (127329.8993)  weight_decay: 0.0500 (0.0500)  time: 0.3054  data: 0.0001  max mem: 41808
Epoch: [32]  [1340/1349]  eta: 0:00:02  lr: 0.000157  min_lr: 0.000004  loss: 0.7560 (0.7855)  loss_scale: 131072.0000 (127357.8046)  weight_decay: 0.0500 (0.0500)  time: 0.3030  data: 0.0001  max mem: 41808
[2025-05-23 21:45:57,645] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 21:45:57,645] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 21:45:57,645] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 21:45:57,645] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [32]  [1348/1349]  eta: 0:00:00  lr: 0.000157  min_lr: 0.000004  loss: 0.7560 (0.7856)  loss_scale: 131072.0000 (127476.9933)  weight_decay: 0.0500 (0.0500)  time: 0.3024  data: 0.0001  max mem: 41808
Epoch: [32] Total time: 0:06:58 (0.3106 s / it)
Averaged stats: lr: 0.000157  min_lr: 0.000004  loss: 0.7560 (0.7884)  loss_scale: 131072.0000 (127476.9933)  weight_decay: 0.0500 (0.0500)  total_time: 418.9468 (418.9382)
Val:  [  0/346]  eta: 1:36:05  loss: 3.2256 (3.2256)  acc1: 2.3438 (2.3438)  acc5: 62.5000 (62.5000)  time: 16.6634  data: 15.8993  max mem: 41808
Val:  [ 10/346]  eta: 0:12:22  loss: 0.1142 (0.6139)  acc1: 100.0000 (84.1619)  acc5: 100.0000 (96.2358)  time: 2.2093  data: 1.4456  max mem: 41808
Val:  [ 20/346]  eta: 0:08:37  loss: 0.1132 (0.4610)  acc1: 100.0000 (88.7649)  acc5: 100.0000 (97.8051)  time: 0.8339  data: 0.0438  max mem: 41808
Val:  [ 30/346]  eta: 0:07:00  loss: 0.1016 (0.3835)  acc1: 100.0000 (91.1038)  acc5: 100.0000 (98.5131)  time: 0.8455  data: 0.0438  max mem: 41808
Val:  [ 40/346]  eta: 0:06:08  loss: 0.1018 (0.4267)  acc1: 100.0000 (89.9009)  acc5: 100.0000 (98.6471)  time: 0.7997  data: 0.0003  max mem: 41808
Val:  [ 50/346]  eta: 0:05:32  loss: 0.1056 (0.3754)  acc1: 99.2188 (91.4982)  acc5: 100.0000 (98.8971)  time: 0.8056  data: 0.0003  max mem: 41808
Val:  [ 60/346]  eta: 0:05:05  loss: 0.1540 (0.3531)  acc1: 98.4375 (92.1619)  acc5: 100.0000 (99.0779)  time: 0.7959  data: 0.0003  max mem: 41808
Val:  [ 70/346]  eta: 0:04:46  loss: 0.1851 (0.3753)  acc1: 96.8750 (91.5273)  acc5: 100.0000 (99.1087)  time: 0.8225  data: 0.0003  max mem: 41808
Val:  [ 80/346]  eta: 0:04:29  loss: 0.1868 (0.3803)  acc1: 89.8438 (91.1941)  acc5: 100.0000 (99.0548)  time: 0.8341  data: 0.0003  max mem: 41808
Val:  [ 90/346]  eta: 0:04:12  loss: 0.2540 (0.3772)  acc1: 90.6250 (91.1659)  acc5: 100.0000 (99.1243)  time: 0.7995  data: 0.0003  max mem: 41808
Val:  [100/346]  eta: 0:03:57  loss: 0.1849 (0.3547)  acc1: 96.8750 (91.9245)  acc5: 100.0000 (99.2110)  time: 0.7788  data: 0.0003  max mem: 41808
Val:  [110/346]  eta: 0:03:43  loss: 0.1542 (0.3732)  acc1: 97.6562 (91.4062)  acc5: 100.0000 (99.1906)  time: 0.7759  data: 0.0077  max mem: 41808
Val:  [120/346]  eta: 0:03:33  loss: 0.1698 (0.3744)  acc1: 96.0938 (91.2900)  acc5: 100.0000 (99.2446)  time: 0.8507  data: 0.0822  max mem: 41808
Val:  [130/346]  eta: 0:03:23  loss: 0.1036 (0.3738)  acc1: 100.0000 (91.2333)  acc5: 100.0000 (99.2963)  time: 0.9143  data: 0.1533  max mem: 41808
Val:  [140/346]  eta: 0:03:13  loss: 0.2128 (0.3694)  acc1: 96.0938 (91.3619)  acc5: 100.0000 (99.3074)  time: 0.9070  data: 0.1543  max mem: 41808
Val:  [150/346]  eta: 0:03:03  loss: 0.2506 (0.3700)  acc1: 96.0938 (91.3907)  acc5: 100.0000 (99.2498)  time: 0.8867  data: 0.1492  max mem: 41808
Val:  [160/346]  eta: 0:02:52  loss: 0.1898 (0.3619)  acc1: 96.8750 (91.6392)  acc5: 100.0000 (99.2818)  time: 0.8489  data: 0.1412  max mem: 41808
Val:  [170/346]  eta: 0:02:43  loss: 0.1448 (0.3563)  acc1: 97.6562 (91.7535)  acc5: 100.0000 (99.3238)  time: 0.8634  data: 0.1378  max mem: 41808
Val:  [180/346]  eta: 0:02:33  loss: 0.1258 (0.3658)  acc1: 95.3125 (91.3933)  acc5: 100.0000 (99.3439)  time: 0.8951  data: 0.1505  max mem: 41808
Val:  [190/346]  eta: 0:02:24  loss: 0.2280 (0.3651)  acc1: 92.9688 (91.4349)  acc5: 100.0000 (99.3783)  time: 0.8985  data: 0.1605  max mem: 41808
Val:  [200/346]  eta: 0:02:14  loss: 0.3759 (0.3792)  acc1: 89.8438 (90.9049)  acc5: 100.0000 (99.3937)  time: 0.9016  data: 0.1632  max mem: 41808
Val:  [210/346]  eta: 0:02:05  loss: 0.2571 (0.3723)  acc1: 92.1875 (91.1026)  acc5: 100.0000 (99.4224)  time: 0.8916  data: 0.1601  max mem: 41808
Val:  [220/346]  eta: 0:01:55  loss: 0.1352 (0.3692)  acc1: 98.4375 (91.2260)  acc5: 100.0000 (99.4167)  time: 0.8916  data: 0.1477  max mem: 41808
Val:  [230/346]  eta: 0:01:46  loss: 0.1255 (0.3608)  acc1: 98.4375 (91.4840)  acc5: 100.0000 (99.4420)  time: 0.8998  data: 0.1474  max mem: 41808
Val:  [240/346]  eta: 0:01:37  loss: 0.1601 (0.3658)  acc1: 97.6562 (91.4095)  acc5: 100.0000 (99.4522)  time: 0.8945  data: 0.1544  max mem: 41808
Val:  [250/346]  eta: 0:01:28  loss: 0.1601 (0.3656)  acc1: 97.6562 (91.4561)  acc5: 100.0000 (99.4553)  time: 0.9067  data: 0.1569  max mem: 41808
Val:  [260/346]  eta: 0:01:19  loss: 0.1385 (0.3626)  acc1: 98.4375 (91.5380)  acc5: 100.0000 (99.4672)  time: 0.9311  data: 0.1584  max mem: 41808
Val:  [270/346]  eta: 0:01:09  loss: 0.1322 (0.3579)  acc1: 99.2188 (91.7032)  acc5: 100.0000 (99.4667)  time: 0.9227  data: 0.1703  max mem: 41808
Val:  [280/346]  eta: 0:01:00  loss: 0.1201 (0.3561)  acc1: 99.2188 (91.7816)  acc5: 100.0000 (99.4773)  time: 0.9161  data: 0.1761  max mem: 41808
Val:  [290/346]  eta: 0:00:51  loss: 0.1011 (0.3486)  acc1: 100.0000 (92.0049)  acc5: 100.0000 (99.4953)  time: 0.9123  data: 0.1673  max mem: 41808
Val:  [300/346]  eta: 0:00:42  loss: 0.1059 (0.3484)  acc1: 100.0000 (92.0499)  acc5: 100.0000 (99.5043)  time: 0.8963  data: 0.1680  max mem: 41808
Val:  [310/346]  eta: 0:00:32  loss: 0.1082 (0.3497)  acc1: 99.2188 (92.0091)  acc5: 100.0000 (99.4875)  time: 0.8931  data: 0.1563  max mem: 41808
Val:  [320/346]  eta: 0:00:23  loss: 0.1122 (0.3493)  acc1: 99.2188 (92.0001)  acc5: 100.0000 (99.4986)  time: 0.8885  data: 0.1426  max mem: 41808
Val:  [330/346]  eta: 0:00:14  loss: 0.3915 (0.3635)  acc1: 89.8438 (91.5904)  acc5: 100.0000 (99.4029)  time: 0.8746  data: 0.1447  max mem: 41808
Val:  [340/346]  eta: 0:00:05  loss: 0.4180 (0.3721)  acc1: 89.8438 (91.3536)  acc5: 100.0000 (99.4135)  time: 0.8757  data: 0.1547  max mem: 41808
Val:  [345/346]  eta: 0:00:00  loss: 0.1701 (0.3686)  acc1: 96.5517 (91.4616)  acc5: 100.0000 (99.4214)  time: 0.8715  data: 0.1734  max mem: 41808
Val: Total time: 0:05:16 (0.9136 s / it)
* Acc@1 91.518 Acc@5 99.423 loss 0.366
Accuracy of the network on the 88494 val videos: 91.5%
[2025-05-23 21:51:14,099] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2025-05-23 21:51:14,103] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /project/mmendoscope/Natural_Comparison/Cholec75/unified_base_st_settingE_videomae-st_Cholec80_0.0005_0.75_online_key_frame_frame16_Fixed_Stride_4/checkpoint-best/mp_rank_00_model_states.pt
[2025-05-23 21:51:14,103] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /project/mmendoscope/Natural_Comparison/Cholec75/unified_base_st_settingE_videomae-st_Cholec80_0.0005_0.75_online_key_frame_frame16_Fixed_Stride_4/checkpoint-best/mp_rank_00_model_states.pt...
[2025-05-23 21:51:14,103] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2025-05-23 21:51:17,183] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /project/mmendoscope/Natural_Comparison/Cholec75/unified_base_st_settingE_videomae-st_Cholec80_0.0005_0.75_online_key_frame_frame16_Fixed_Stride_4/checkpoint-best/mp_rank_00_model_states.pt.
[2025-05-23 21:51:17,183] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 91.52%   Max Epoch: 32
Epoch: [33]  [   0/1349]  eta: 1:44:37  lr: 0.000157  min_lr: 0.000004  loss: 0.8520 (0.8520)  loss_scale: 262144.0000 (262144.0000)  weight_decay: 0.0500 (0.0500)  time: 4.6532  data: 4.2839  max mem: 41808
[2025-05-23 21:51:25,266] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 44526
[2025-05-23 21:51:25,267] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 21:51:25,267] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
[2025-05-23 21:51:25,267] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 44526
[2025-05-23 21:51:25,267] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
Epoch: [33]  [  10/1349]  eta: 0:17:00  lr: 0.000157  min_lr: 0.000004  loss: 0.8520 (0.8070)  loss_scale: 262144.0000 (238312.7273)  weight_decay: 0.0500 (0.0500)  time: 0.7622  data: 0.3896  max mem: 41808
Epoch: [33]  [  20/1349]  eta: 0:12:05  lr: 0.000157  min_lr: 0.000004  loss: 0.8337 (0.8003)  loss_scale: 131072.0000 (187245.7143)  weight_decay: 0.0500 (0.0500)  time: 0.3408  data: 0.0001  max mem: 41808
[2025-05-23 21:51:29,580] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 44540
[2025-05-23 21:51:29,580] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 44540
[2025-05-23 21:51:29,580] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 21:51:29,580] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 21:51:29,580] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [33]  [  30/1349]  eta: 0:10:19  lr: 0.000157  min_lr: 0.000004  loss: 0.7676 (0.7813)  loss_scale: 131072.0000 (152212.6452)  weight_decay: 0.0500 (0.0500)  time: 0.3088  data: 0.0001  max mem: 41808
Epoch: [33]  [  40/1349]  eta: 0:09:23  lr: 0.000157  min_lr: 0.000004  loss: 0.7208 (0.7618)  loss_scale: 65536.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3089  data: 0.0001  max mem: 41808
Epoch: [33]  [  50/1349]  eta: 0:08:47  lr: 0.000156  min_lr: 0.000004  loss: 0.7488 (0.7760)  loss_scale: 65536.0000 (118221.8039)  weight_decay: 0.0500 (0.0500)  time: 0.3079  data: 0.0001  max mem: 41808
Epoch: [33]  [  60/1349]  eta: 0:08:22  lr: 0.000156  min_lr: 0.000004  loss: 0.7405 (0.7593)  loss_scale: 65536.0000 (109584.7869)  weight_decay: 0.0500 (0.0500)  time: 0.3078  data: 0.0002  max mem: 41808
Epoch: [33]  [  70/1349]  eta: 0:08:04  lr: 0.000156  min_lr: 0.000004  loss: 0.8395 (0.7819)  loss_scale: 65536.0000 (103380.7324)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0002  max mem: 41808
Epoch: [33]  [  80/1349]  eta: 0:07:49  lr: 0.000156  min_lr: 0.000004  loss: 0.8512 (0.7837)  loss_scale: 65536.0000 (98708.5432)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0002  max mem: 41808
Epoch: [33]  [  90/1349]  eta: 0:07:36  lr: 0.000156  min_lr: 0.000004  loss: 0.7931 (0.7862)  loss_scale: 65536.0000 (95063.2088)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [33]  [ 100/1349]  eta: 0:07:26  lr: 0.000156  min_lr: 0.000004  loss: 0.8262 (0.7877)  loss_scale: 65536.0000 (92139.7228)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [33]  [ 110/1349]  eta: 0:07:17  lr: 0.000156  min_lr: 0.000004  loss: 0.7561 (0.7794)  loss_scale: 65536.0000 (89742.9910)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
Epoch: [33]  [ 120/1349]  eta: 0:07:09  lr: 0.000156  min_lr: 0.000004  loss: 0.8001 (0.7827)  loss_scale: 65536.0000 (87742.4132)  weight_decay: 0.0500 (0.0500)  time: 0.3079  data: 0.0002  max mem: 41808
Epoch: [33]  [ 130/1349]  eta: 0:07:02  lr: 0.000155  min_lr: 0.000004  loss: 0.8484 (0.7840)  loss_scale: 65536.0000 (86047.2672)  weight_decay: 0.0500 (0.0500)  time: 0.3102  data: 0.0002  max mem: 41808
Epoch: [33]  [ 140/1349]  eta: 0:06:55  lr: 0.000155  min_lr: 0.000004  loss: 0.8069 (0.7856)  loss_scale: 65536.0000 (84592.5674)  weight_decay: 0.0500 (0.0500)  time: 0.3092  data: 0.0001  max mem: 41808
Epoch: [33]  [ 150/1349]  eta: 0:06:48  lr: 0.000155  min_lr: 0.000004  loss: 0.7949 (0.7845)  loss_scale: 65536.0000 (83330.5430)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
[2025-05-23 21:52:09,309] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 21:52:09,309] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 21:52:09,309] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-05-23 21:52:09,309] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [33]  [ 160/1349]  eta: 0:06:42  lr: 0.000155  min_lr: 0.000004  loss: 0.6424 (0.7761)  loss_scale: 65536.0000 (85888.7950)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
Epoch: [33]  [ 170/1349]  eta: 0:06:37  lr: 0.000155  min_lr: 0.000004  loss: 0.7546 (0.7794)  loss_scale: 131072.0000 (88531.0877)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [33]  [ 180/1349]  eta: 0:06:32  lr: 0.000155  min_lr: 0.000004  loss: 0.8490 (0.7803)  loss_scale: 131072.0000 (90881.4144)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [33]  [ 190/1349]  eta: 0:06:27  lr: 0.000155  min_lr: 0.000004  loss: 0.8095 (0.7779)  loss_scale: 131072.0000 (92985.6335)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
Epoch: [33]  [ 200/1349]  eta: 0:06:22  lr: 0.000155  min_lr: 0.000004  loss: 0.7819 (0.7785)  loss_scale: 131072.0000 (94880.4776)  weight_decay: 0.0500 (0.0500)  time: 0.3078  data: 0.0001  max mem: 41808
Epoch: [33]  [ 210/1349]  eta: 0:06:17  lr: 0.000155  min_lr: 0.000004  loss: 0.7985 (0.7790)  loss_scale: 131072.0000 (96595.7156)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
Epoch: [33]  [ 220/1349]  eta: 0:06:12  lr: 0.000154  min_lr: 0.000004  loss: 0.7985 (0.7804)  loss_scale: 131072.0000 (98155.7285)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
Epoch: [33]  [ 230/1349]  eta: 0:06:08  lr: 0.000154  min_lr: 0.000004  loss: 0.7746 (0.7782)  loss_scale: 131072.0000 (99580.6753)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0001  max mem: 41808
Epoch: [33]  [ 240/1349]  eta: 0:06:04  lr: 0.000154  min_lr: 0.000004  loss: 0.7591 (0.7780)  loss_scale: 131072.0000 (100887.3693)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
Epoch: [33]  [ 250/1349]  eta: 0:06:00  lr: 0.000154  min_lr: 0.000004  loss: 0.7890 (0.7784)  loss_scale: 131072.0000 (102089.9442)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
Epoch: [33]  [ 260/1349]  eta: 0:05:55  lr: 0.000154  min_lr: 0.000004  loss: 0.7672 (0.7745)  loss_scale: 131072.0000 (103200.3678)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0001  max mem: 41808
Epoch: [33]  [ 270/1349]  eta: 0:05:52  lr: 0.000154  min_lr: 0.000004  loss: 0.7337 (0.7770)  loss_scale: 131072.0000 (104228.8413)  weight_decay: 0.0500 (0.0500)  time: 0.3087  data: 0.0001  max mem: 41808
[2025-05-23 21:52:48,733] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 21:52:48,733] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 21:52:48,733] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 21:52:48,733] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [33]  [ 280/1349]  eta: 0:05:48  lr: 0.000154  min_lr: 0.000004  loss: 0.8227 (0.7791)  loss_scale: 131072.0000 (105650.5623)  weight_decay: 0.0500 (0.0500)  time: 0.3101  data: 0.0001  max mem: 41808
[2025-05-23 21:52:49,046] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 44798
[2025-05-23 21:52:49,046] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 21:52:49,046] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 44798
[2025-05-23 21:52:49,046] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 21:52:49,046] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [33]  [ 290/1349]  eta: 0:05:44  lr: 0.000154  min_lr: 0.000004  loss: 0.8505 (0.7808)  loss_scale: 131072.0000 (106524.1512)  weight_decay: 0.0500 (0.0500)  time: 0.3101  data: 0.0001  max mem: 41808
Epoch: [33]  [ 300/1349]  eta: 0:05:40  lr: 0.000153  min_lr: 0.000004  loss: 0.8369 (0.7814)  loss_scale: 131072.0000 (107339.6944)  weight_decay: 0.0500 (0.0500)  time: 0.3086  data: 0.0001  max mem: 41808
Epoch: [33]  [ 310/1349]  eta: 0:05:36  lr: 0.000153  min_lr: 0.000004  loss: 0.7975 (0.7815)  loss_scale: 131072.0000 (108102.7910)  weight_decay: 0.0500 (0.0500)  time: 0.3080  data: 0.0002  max mem: 41808
Epoch: [33]  [ 320/1349]  eta: 0:05:32  lr: 0.000153  min_lr: 0.000004  loss: 0.7975 (0.7813)  loss_scale: 131072.0000 (108818.3427)  weight_decay: 0.0500 (0.0500)  time: 0.3078  data: 0.0002  max mem: 41808
Epoch: [33]  [ 330/1349]  eta: 0:05:29  lr: 0.000153  min_lr: 0.000004  loss: 0.7617 (0.7807)  loss_scale: 131072.0000 (109490.6586)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
Epoch: [33]  [ 340/1349]  eta: 0:05:25  lr: 0.000153  min_lr: 0.000004  loss: 0.7989 (0.7825)  loss_scale: 131072.0000 (110123.5425)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
Epoch: [33]  [ 350/1349]  eta: 0:05:21  lr: 0.000153  min_lr: 0.000004  loss: 0.8165 (0.7820)  loss_scale: 131072.0000 (110720.3647)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
Epoch: [33]  [ 360/1349]  eta: 0:05:18  lr: 0.000153  min_lr: 0.000004  loss: 0.7356 (0.7813)  loss_scale: 131072.0000 (111284.1219)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0002  max mem: 41808
Epoch: [33]  [ 370/1349]  eta: 0:05:14  lr: 0.000153  min_lr: 0.000004  loss: 0.8296 (0.7832)  loss_scale: 131072.0000 (111817.4879)  weight_decay: 0.0500 (0.0500)  time: 0.3064  data: 0.0002  max mem: 41808
Epoch: [33]  [ 380/1349]  eta: 0:05:10  lr: 0.000153  min_lr: 0.000004  loss: 0.7943 (0.7818)  loss_scale: 131072.0000 (112322.8556)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [33]  [ 390/1349]  eta: 0:05:07  lr: 0.000152  min_lr: 0.000004  loss: 0.7717 (0.7820)  loss_scale: 131072.0000 (112802.3734)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [33]  [ 400/1349]  eta: 0:05:03  lr: 0.000152  min_lr: 0.000004  loss: 0.7815 (0.7812)  loss_scale: 131072.0000 (113257.9751)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
[2025-05-23 21:53:28,704] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 21:53:28,704] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 21:53:28,704] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 21:53:28,704] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [33]  [ 410/1349]  eta: 0:05:00  lr: 0.000152  min_lr: 0.000004  loss: 0.6588 (0.7790)  loss_scale: 131072.0000 (114010.3163)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
[2025-05-23 21:53:29,008] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 44928
[2025-05-23 21:53:29,008] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 44928
[2025-05-23 21:53:29,008] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 21:53:29,008] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 21:53:29,008] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [33]  [ 420/1349]  eta: 0:04:56  lr: 0.000152  min_lr: 0.000004  loss: 0.7598 (0.7793)  loss_scale: 131072.0000 (114415.5819)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
Epoch: [33]  [ 430/1349]  eta: 0:04:53  lr: 0.000152  min_lr: 0.000004  loss: 0.8120 (0.7791)  loss_scale: 131072.0000 (114802.0418)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0001  max mem: 41808
Epoch: [33]  [ 440/1349]  eta: 0:04:49  lr: 0.000152  min_lr: 0.000004  loss: 0.7979 (0.7794)  loss_scale: 131072.0000 (115170.9751)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
Epoch: [33]  [ 450/1349]  eta: 0:04:46  lr: 0.000152  min_lr: 0.000004  loss: 0.7921 (0.7801)  loss_scale: 131072.0000 (115523.5477)  weight_decay: 0.0500 (0.0500)  time: 0.3066  data: 0.0001  max mem: 41808
Epoch: [33]  [ 460/1349]  eta: 0:04:43  lr: 0.000152  min_lr: 0.000004  loss: 0.8529 (0.7830)  loss_scale: 131072.0000 (115860.8243)  weight_decay: 0.0500 (0.0500)  time: 0.3066  data: 0.0001  max mem: 41808
Epoch: [33]  [ 470/1349]  eta: 0:04:39  lr: 0.000151  min_lr: 0.000004  loss: 0.8631 (0.7832)  loss_scale: 131072.0000 (116183.7792)  weight_decay: 0.0500 (0.0500)  time: 0.3088  data: 0.0001  max mem: 41808
Epoch: [33]  [ 480/1349]  eta: 0:04:36  lr: 0.000151  min_lr: 0.000004  loss: 0.7779 (0.7838)  loss_scale: 131072.0000 (116493.3056)  weight_decay: 0.0500 (0.0500)  time: 0.3092  data: 0.0002  max mem: 41808
[2025-05-23 21:53:50,848] [INFO] [logging.py:96:log_dist] [Rank 0] step=45000, skipped=276, lr=[3.5943526591270144e-06, 3.5943526591270144e-06, 4.792470212169353e-06, 4.792470212169353e-06, 6.389960282892471e-06, 6.389960282892471e-06, 8.519947043856626e-06, 8.519947043856626e-06, 1.1359929391808836e-05, 1.1359929391808836e-05, 1.5146572522411782e-05, 1.5146572522411782e-05, 2.0195430029882376e-05, 2.0195430029882376e-05, 2.6927240039843167e-05, 2.6927240039843167e-05, 3.590298671979089e-05, 3.590298671979089e-05, 4.787064895972119e-05, 4.787064895972119e-05, 6.382753194629491e-05, 6.382753194629491e-05, 8.510337592839322e-05, 8.510337592839322e-05, 0.0001134711679045243, 0.0001134711679045243, 0.00015129489053936573, 0.00015129489053936573], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-05-23 21:53:50,848] [INFO] [timer.py:260:stop] epoch=0/micro_step=45000/global_step=45000, RunningAvgSamplesPerSec=209.99833034557946, CurrSamplesPerSec=213.1062532549189, MemAllocated=1.37GB, MaxMemAllocated=40.83GB
Epoch: [33]  [ 490/1349]  eta: 0:04:33  lr: 0.000151  min_lr: 0.000004  loss: 0.8458 (0.7846)  loss_scale: 131072.0000 (116790.2240)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0002  max mem: 41808
Epoch: [33]  [ 500/1349]  eta: 0:04:29  lr: 0.000151  min_lr: 0.000004  loss: 0.8059 (0.7833)  loss_scale: 131072.0000 (117075.2894)  weight_decay: 0.0500 (0.0500)  time: 0.3063  data: 0.0001  max mem: 41808
Epoch: [33]  [ 510/1349]  eta: 0:04:26  lr: 0.000151  min_lr: 0.000004  loss: 0.7815 (0.7842)  loss_scale: 131072.0000 (117349.1977)  weight_decay: 0.0500 (0.0500)  time: 0.3057  data: 0.0001  max mem: 41808
Epoch: [33]  [ 520/1349]  eta: 0:04:22  lr: 0.000151  min_lr: 0.000004  loss: 0.7815 (0.7831)  loss_scale: 131072.0000 (117612.5912)  weight_decay: 0.0500 (0.0500)  time: 0.3058  data: 0.0001  max mem: 41808
Epoch: [33]  [ 530/1349]  eta: 0:04:19  lr: 0.000151  min_lr: 0.000004  loss: 0.7419 (0.7829)  loss_scale: 131072.0000 (117866.0640)  weight_decay: 0.0500 (0.0500)  time: 0.3057  data: 0.0001  max mem: 41808
[2025-05-23 21:54:08,601] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 21:54:08,601] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 21:54:08,601] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 21:54:08,601] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [33]  [ 540/1349]  eta: 0:04:16  lr: 0.000151  min_lr: 0.000004  loss: 0.7543 (0.7827)  loss_scale: 131072.0000 (118352.4436)  weight_decay: 0.0500 (0.0500)  time: 0.3061  data: 0.0001  max mem: 41808
[2025-05-23 21:54:10,434] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 45063
[2025-05-23 21:54:10,434] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 45063
[2025-05-23 21:54:10,434] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 21:54:10,434] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 21:54:10,434] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [33]  [ 550/1349]  eta: 0:04:12  lr: 0.000150  min_lr: 0.000004  loss: 0.8393 (0.7838)  loss_scale: 131072.0000 (119772.6897)  weight_decay: 0.0500 (0.0500)  time: 0.3060  data: 0.0001  max mem: 41808
Epoch: [33]  [ 560/1349]  eta: 0:04:09  lr: 0.000150  min_lr: 0.000004  loss: 0.8393 (0.7838)  loss_scale: 131072.0000 (119974.1034)  weight_decay: 0.0500 (0.0500)  time: 0.3058  data: 0.0001  max mem: 41808
Epoch: [33]  [ 570/1349]  eta: 0:04:06  lr: 0.000150  min_lr: 0.000004  loss: 0.7395 (0.7826)  loss_scale: 131072.0000 (120168.4623)  weight_decay: 0.0500 (0.0500)  time: 0.3064  data: 0.0001  max mem: 41808
Epoch: [33]  [ 580/1349]  eta: 0:04:03  lr: 0.000150  min_lr: 0.000004  loss: 0.7344 (0.7821)  loss_scale: 131072.0000 (120356.1308)  weight_decay: 0.0500 (0.0500)  time: 0.3064  data: 0.0001  max mem: 41808
Epoch: [33]  [ 590/1349]  eta: 0:03:59  lr: 0.000150  min_lr: 0.000004  loss: 0.7754 (0.7813)  loss_scale: 131072.0000 (120537.4484)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
[2025-05-23 21:54:27,001] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 45117
[2025-05-23 21:54:27,001] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 45117
[2025-05-23 21:54:27,001] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 21:54:27,001] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 21:54:27,001] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [33]  [ 600/1349]  eta: 0:03:56  lr: 0.000150  min_lr: 0.000004  loss: 0.8039 (0.7817)  loss_scale: 131072.0000 (120603.6872)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
Epoch: [33]  [ 610/1349]  eta: 0:03:53  lr: 0.000150  min_lr: 0.000004  loss: 0.8363 (0.7828)  loss_scale: 65536.0000 (119702.4157)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
Epoch: [33]  [ 620/1349]  eta: 0:03:49  lr: 0.000150  min_lr: 0.000004  loss: 0.8201 (0.7826)  loss_scale: 65536.0000 (118830.1707)  weight_decay: 0.0500 (0.0500)  time: 0.3080  data: 0.0001  max mem: 41808
Epoch: [33]  [ 630/1349]  eta: 0:03:46  lr: 0.000150  min_lr: 0.000004  loss: 0.7965 (0.7832)  loss_scale: 65536.0000 (117985.5721)  weight_decay: 0.0500 (0.0500)  time: 0.3080  data: 0.0001  max mem: 41808
Epoch: [33]  [ 640/1349]  eta: 0:03:43  lr: 0.000149  min_lr: 0.000004  loss: 0.8111 (0.7833)  loss_scale: 65536.0000 (117167.3261)  weight_decay: 0.0500 (0.0500)  time: 0.3080  data: 0.0001  max mem: 41808
Epoch: [33]  [ 650/1349]  eta: 0:03:40  lr: 0.000149  min_lr: 0.000004  loss: 0.8478 (0.7832)  loss_scale: 65536.0000 (116374.2181)  weight_decay: 0.0500 (0.0500)  time: 0.3090  data: 0.0001  max mem: 41808
Epoch: [33]  [ 660/1349]  eta: 0:03:37  lr: 0.000149  min_lr: 0.000004  loss: 0.8213 (0.7827)  loss_scale: 65536.0000 (115605.1074)  weight_decay: 0.0500 (0.0500)  time: 0.3081  data: 0.0001  max mem: 41808
Epoch: [33]  [ 670/1349]  eta: 0:03:33  lr: 0.000149  min_lr: 0.000004  loss: 0.7934 (0.7821)  loss_scale: 65536.0000 (114858.9210)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0002  max mem: 41808
Epoch: [33]  [ 680/1349]  eta: 0:03:30  lr: 0.000149  min_lr: 0.000004  loss: 0.7934 (0.7816)  loss_scale: 65536.0000 (114134.6490)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0002  max mem: 41808
Epoch: [33]  [ 690/1349]  eta: 0:03:27  lr: 0.000149  min_lr: 0.000004  loss: 0.8111 (0.7816)  loss_scale: 65536.0000 (113431.3401)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [33]  [ 700/1349]  eta: 0:03:24  lr: 0.000149  min_lr: 0.000004  loss: 0.7827 (0.7810)  loss_scale: 65536.0000 (112748.0970)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0001  max mem: 41808
Epoch: [33]  [ 710/1349]  eta: 0:03:20  lr: 0.000149  min_lr: 0.000004  loss: 0.7795 (0.7807)  loss_scale: 65536.0000 (112084.0731)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0001  max mem: 41808
Epoch: [33]  [ 720/1349]  eta: 0:03:17  lr: 0.000148  min_lr: 0.000004  loss: 0.7795 (0.7802)  loss_scale: 65536.0000 (111438.4688)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
[2025-05-23 21:55:06,707] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 21:55:06,707] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-05-23 21:55:06,707] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 21:55:06,707] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [33]  [ 730/1349]  eta: 0:03:14  lr: 0.000148  min_lr: 0.000004  loss: 0.7504 (0.7803)  loss_scale: 65536.0000 (110989.8331)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [33]  [ 740/1349]  eta: 0:03:11  lr: 0.000148  min_lr: 0.000004  loss: 0.7504 (0.7803)  loss_scale: 131072.0000 (111260.8475)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0001  max mem: 41808
Epoch: [33]  [ 750/1349]  eta: 0:03:08  lr: 0.000148  min_lr: 0.000004  loss: 0.7985 (0.7806)  loss_scale: 131072.0000 (111524.6445)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
[2025-05-23 21:55:16,245] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 45277
[2025-05-23 21:55:16,245] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 45277
[2025-05-23 21:55:16,245] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 21:55:16,245] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 21:55:16,245] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [33]  [ 760/1349]  eta: 0:03:04  lr: 0.000148  min_lr: 0.000004  loss: 0.7368 (0.7798)  loss_scale: 131072.0000 (111695.3903)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [33]  [ 770/1349]  eta: 0:03:01  lr: 0.000148  min_lr: 0.000004  loss: 0.7424 (0.7800)  loss_scale: 65536.0000 (111096.6952)  weight_decay: 0.0500 (0.0500)  time: 0.3081  data: 0.0001  max mem: 41808
Epoch: [33]  [ 780/1349]  eta: 0:02:58  lr: 0.000148  min_lr: 0.000004  loss: 0.7702 (0.7796)  loss_scale: 65536.0000 (110513.3316)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [33]  [ 790/1349]  eta: 0:02:55  lr: 0.000148  min_lr: 0.000004  loss: 0.7544 (0.7794)  loss_scale: 65536.0000 (109944.7181)  weight_decay: 0.0500 (0.0500)  time: 0.3062  data: 0.0001  max mem: 41808
Epoch: [33]  [ 800/1349]  eta: 0:02:52  lr: 0.000148  min_lr: 0.000004  loss: 0.8071 (0.7805)  loss_scale: 65536.0000 (109390.3021)  weight_decay: 0.0500 (0.0500)  time: 0.3085  data: 0.0002  max mem: 41808
Epoch: [33]  [ 810/1349]  eta: 0:02:49  lr: 0.000147  min_lr: 0.000004  loss: 0.7943 (0.7796)  loss_scale: 65536.0000 (108849.5586)  weight_decay: 0.0500 (0.0500)  time: 0.3084  data: 0.0002  max mem: 41808
Epoch: [33]  [ 820/1349]  eta: 0:02:45  lr: 0.000147  min_lr: 0.000003  loss: 0.7594 (0.7796)  loss_scale: 65536.0000 (108321.9878)  weight_decay: 0.0500 (0.0500)  time: 0.3066  data: 0.0002  max mem: 41808
Epoch: [33]  [ 830/1349]  eta: 0:02:42  lr: 0.000147  min_lr: 0.000003  loss: 0.8203 (0.7803)  loss_scale: 65536.0000 (107807.1143)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
Epoch: [33]  [ 840/1349]  eta: 0:02:39  lr: 0.000147  min_lr: 0.000003  loss: 0.8562 (0.7806)  loss_scale: 65536.0000 (107304.4851)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
Epoch: [33]  [ 850/1349]  eta: 0:02:36  lr: 0.000147  min_lr: 0.000003  loss: 0.8562 (0.7813)  loss_scale: 65536.0000 (106813.6686)  weight_decay: 0.0500 (0.0500)  time: 0.3064  data: 0.0001  max mem: 41808
Epoch: [33]  [ 860/1349]  eta: 0:02:33  lr: 0.000147  min_lr: 0.000003  loss: 0.8267 (0.7815)  loss_scale: 65536.0000 (106334.2532)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0001  max mem: 41808
Epoch: [33]  [ 870/1349]  eta: 0:02:29  lr: 0.000147  min_lr: 0.000003  loss: 0.8343 (0.7823)  loss_scale: 65536.0000 (105865.8462)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
Epoch: [33]  [ 880/1349]  eta: 0:02:26  lr: 0.000147  min_lr: 0.000003  loss: 0.8139 (0.7818)  loss_scale: 65536.0000 (105408.0726)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
[2025-05-23 21:55:55,860] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 21:55:55,860] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-05-23 21:55:55,860] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 21:55:55,860] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [33]  [ 890/1349]  eta: 0:02:23  lr: 0.000146  min_lr: 0.000003  loss: 0.8393 (0.7823)  loss_scale: 65536.0000 (105107.6813)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
Epoch: [33]  [ 900/1349]  eta: 0:02:20  lr: 0.000146  min_lr: 0.000003  loss: 0.8426 (0.7824)  loss_scale: 131072.0000 (105395.8535)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0001  max mem: 41808
Epoch: [33]  [ 910/1349]  eta: 0:02:17  lr: 0.000146  min_lr: 0.000003  loss: 0.8480 (0.7828)  loss_scale: 131072.0000 (105677.6992)  weight_decay: 0.0500 (0.0500)  time: 0.3060  data: 0.0001  max mem: 41808
Epoch: [33]  [ 920/1349]  eta: 0:02:14  lr: 0.000146  min_lr: 0.000003  loss: 0.7812 (0.7815)  loss_scale: 131072.0000 (105953.4245)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0001  max mem: 41808
Epoch: [33]  [ 930/1349]  eta: 0:02:11  lr: 0.000146  min_lr: 0.000003  loss: 0.7291 (0.7814)  loss_scale: 131072.0000 (106223.2266)  weight_decay: 0.0500 (0.0500)  time: 0.3093  data: 0.0001  max mem: 41808
Epoch: [33]  [ 940/1349]  eta: 0:02:07  lr: 0.000146  min_lr: 0.000003  loss: 0.7705 (0.7813)  loss_scale: 131072.0000 (106487.2944)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0001  max mem: 41808
Epoch: [33]  [ 950/1349]  eta: 0:02:04  lr: 0.000146  min_lr: 0.000003  loss: 0.7781 (0.7818)  loss_scale: 131072.0000 (106745.8086)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
Epoch: [33]  [ 960/1349]  eta: 0:02:01  lr: 0.000146  min_lr: 0.000003  loss: 0.7781 (0.7808)  loss_scale: 131072.0000 (106998.9428)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [33]  [ 970/1349]  eta: 0:01:58  lr: 0.000146  min_lr: 0.000003  loss: 0.8036 (0.7814)  loss_scale: 131072.0000 (107246.8630)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [33]  [ 980/1349]  eta: 0:01:55  lr: 0.000145  min_lr: 0.000003  loss: 0.8219 (0.7812)  loss_scale: 131072.0000 (107489.7288)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
Epoch: [33]  [ 990/1349]  eta: 0:01:52  lr: 0.000145  min_lr: 0.000003  loss: 0.7962 (0.7813)  loss_scale: 131072.0000 (107727.6932)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [33]  [1000/1349]  eta: 0:01:49  lr: 0.000145  min_lr: 0.000003  loss: 0.7642 (0.7810)  loss_scale: 131072.0000 (107960.9031)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [33]  [1010/1349]  eta: 0:01:45  lr: 0.000145  min_lr: 0.000003  loss: 0.8146 (0.7815)  loss_scale: 131072.0000 (108189.4995)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
[2025-05-23 21:56:35,197] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 21:56:35,197] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 21:56:35,197] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 21:56:35,197] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 21:56:36,112] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 45537
[2025-05-23 21:56:36,112] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 45537
[2025-05-23 21:56:36,113] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 21:56:36,113] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 21:56:36,113] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [33]  [1020/1349]  eta: 0:01:42  lr: 0.000145  min_lr: 0.000003  loss: 0.8440 (0.7818)  loss_scale: 131072.0000 (108798.7463)  weight_decay: 0.0500 (0.0500)  time: 0.3066  data: 0.0001  max mem: 41808
Epoch: [33]  [1030/1349]  eta: 0:01:39  lr: 0.000145  min_lr: 0.000003  loss: 0.8073 (0.7819)  loss_scale: 131072.0000 (109014.7818)  weight_decay: 0.0500 (0.0500)  time: 0.3064  data: 0.0002  max mem: 41808
Epoch: [33]  [1040/1349]  eta: 0:01:36  lr: 0.000145  min_lr: 0.000003  loss: 0.7399 (0.7812)  loss_scale: 131072.0000 (109226.6667)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
Epoch: [33]  [1050/1349]  eta: 0:01:33  lr: 0.000145  min_lr: 0.000003  loss: 0.7567 (0.7815)  loss_scale: 131072.0000 (109434.5195)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0001  max mem: 41808
Epoch: [33]  [1060/1349]  eta: 0:01:30  lr: 0.000144  min_lr: 0.000003  loss: 0.7790 (0.7812)  loss_scale: 131072.0000 (109638.4543)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0001  max mem: 41808
Epoch: [33]  [1070/1349]  eta: 0:01:27  lr: 0.000144  min_lr: 0.000003  loss: 0.7756 (0.7813)  loss_scale: 131072.0000 (109838.5808)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
Epoch: [33]  [1080/1349]  eta: 0:01:23  lr: 0.000144  min_lr: 0.000003  loss: 0.8123 (0.7816)  loss_scale: 131072.0000 (110035.0046)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
Epoch: [33]  [1090/1349]  eta: 0:01:20  lr: 0.000144  min_lr: 0.000003  loss: 0.7984 (0.7814)  loss_scale: 131072.0000 (110227.8277)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [33]  [1100/1349]  eta: 0:01:17  lr: 0.000144  min_lr: 0.000003  loss: 0.8055 (0.7823)  loss_scale: 131072.0000 (110417.1480)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
Epoch: [33]  [1110/1349]  eta: 0:01:14  lr: 0.000144  min_lr: 0.000003  loss: 0.8406 (0.7829)  loss_scale: 131072.0000 (110603.0603)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
Epoch: [33]  [1120/1349]  eta: 0:01:11  lr: 0.000144  min_lr: 0.000003  loss: 0.8406 (0.7831)  loss_scale: 131072.0000 (110785.6557)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
[2025-05-23 21:57:09,592] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 45646
[2025-05-23 21:57:09,592] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 45646
[2025-05-23 21:57:09,592] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 21:57:09,592] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 21:57:09,592] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [33]  [1130/1349]  eta: 0:01:08  lr: 0.000144  min_lr: 0.000003  loss: 0.8249 (0.7837)  loss_scale: 131072.0000 (110849.1317)  weight_decay: 0.0500 (0.0500)  time: 0.3084  data: 0.0001  max mem: 41808
Epoch: [33]  [1140/1349]  eta: 0:01:05  lr: 0.000144  min_lr: 0.000003  loss: 0.7994 (0.7835)  loss_scale: 65536.0000 (110451.9965)  weight_decay: 0.0500 (0.0500)  time: 0.3082  data: 0.0001  max mem: 41808
Epoch: [33]  [1150/1349]  eta: 0:01:02  lr: 0.000143  min_lr: 0.000003  loss: 0.7858 (0.7835)  loss_scale: 65536.0000 (110061.7619)  weight_decay: 0.0500 (0.0500)  time: 0.3066  data: 0.0001  max mem: 41808
Epoch: [33]  [1160/1349]  eta: 0:00:58  lr: 0.000143  min_lr: 0.000003  loss: 0.8186 (0.7834)  loss_scale: 65536.0000 (109678.2498)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
Epoch: [33]  [1170/1349]  eta: 0:00:55  lr: 0.000143  min_lr: 0.000003  loss: 0.7276 (0.7826)  loss_scale: 65536.0000 (109301.2878)  weight_decay: 0.0500 (0.0500)  time: 0.3085  data: 0.0001  max mem: 41808
Epoch: [33]  [1180/1349]  eta: 0:00:52  lr: 0.000143  min_lr: 0.000003  loss: 0.6495 (0.7818)  loss_scale: 65536.0000 (108930.7096)  weight_decay: 0.0500 (0.0500)  time: 0.3089  data: 0.0001  max mem: 41808
Epoch: [33]  [1190/1349]  eta: 0:00:49  lr: 0.000143  min_lr: 0.000003  loss: 0.7233 (0.7818)  loss_scale: 65536.0000 (108566.3543)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
Epoch: [33]  [1200/1349]  eta: 0:00:46  lr: 0.000143  min_lr: 0.000003  loss: 0.7849 (0.7820)  loss_scale: 65536.0000 (108208.0666)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
Epoch: [33]  [1210/1349]  eta: 0:00:43  lr: 0.000143  min_lr: 0.000003  loss: 0.7849 (0.7816)  loss_scale: 65536.0000 (107855.6961)  weight_decay: 0.0500 (0.0500)  time: 0.3079  data: 0.0002  max mem: 41808
Epoch: [33]  [1220/1349]  eta: 0:00:40  lr: 0.000143  min_lr: 0.000003  loss: 0.8142 (0.7818)  loss_scale: 65536.0000 (107509.0975)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [33]  [1230/1349]  eta: 0:00:37  lr: 0.000143  min_lr: 0.000003  loss: 0.8465 (0.7820)  loss_scale: 65536.0000 (107168.1300)  weight_decay: 0.0500 (0.0500)  time: 0.3061  data: 0.0001  max mem: 41808
Epoch: [33]  [1240/1349]  eta: 0:00:33  lr: 0.000142  min_lr: 0.000003  loss: 0.8693 (0.7827)  loss_scale: 65536.0000 (106832.6575)  weight_decay: 0.0500 (0.0500)  time: 0.3064  data: 0.0001  max mem: 41808
Epoch: [33]  [1250/1349]  eta: 0:00:30  lr: 0.000142  min_lr: 0.000003  loss: 0.8410 (0.7827)  loss_scale: 65536.0000 (106502.5484)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
[2025-05-23 21:57:49,292] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 21:57:49,292] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 21:57:49,292] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-05-23 21:57:49,292] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [33]  [1260/1349]  eta: 0:00:27  lr: 0.000142  min_lr: 0.000003  loss: 0.7523 (0.7821)  loss_scale: 65536.0000 (106333.5892)  weight_decay: 0.0500 (0.0500)  time: 0.3079  data: 0.0001  max mem: 41808
Epoch: [33]  [1270/1349]  eta: 0:00:24  lr: 0.000142  min_lr: 0.000003  loss: 0.7483 (0.7819)  loss_scale: 131072.0000 (106528.2266)  weight_decay: 0.0500 (0.0500)  time: 0.3089  data: 0.0001  max mem: 41808
Epoch: [33]  [1280/1349]  eta: 0:00:21  lr: 0.000142  min_lr: 0.000003  loss: 0.7816 (0.7818)  loss_scale: 131072.0000 (106719.8251)  weight_decay: 0.0500 (0.0500)  time: 0.3082  data: 0.0001  max mem: 41808
Epoch: [33]  [1290/1349]  eta: 0:00:18  lr: 0.000142  min_lr: 0.000003  loss: 0.7769 (0.7815)  loss_scale: 131072.0000 (106908.4555)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [33]  [1300/1349]  eta: 0:00:15  lr: 0.000142  min_lr: 0.000003  loss: 0.7958 (0.7818)  loss_scale: 131072.0000 (107094.1860)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
Epoch: [33]  [1310/1349]  eta: 0:00:12  lr: 0.000142  min_lr: 0.000003  loss: 0.8378 (0.7821)  loss_scale: 131072.0000 (107277.0831)  weight_decay: 0.0500 (0.0500)  time: 0.3079  data: 0.0001  max mem: 41808
Epoch: [33]  [1320/1349]  eta: 0:00:09  lr: 0.000141  min_lr: 0.000003  loss: 0.8363 (0.7823)  loss_scale: 131072.0000 (107457.2112)  weight_decay: 0.0500 (0.0500)  time: 0.3080  data: 0.0001  max mem: 41808
[2025-05-23 21:58:09,300] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 45840
[2025-05-23 21:58:09,300] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 45840
[2025-05-23 21:58:09,301] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 21:58:09,301] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 21:58:09,301] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [33]  [1330/1349]  eta: 0:00:05  lr: 0.000141  min_lr: 0.000003  loss: 0.8346 (0.7828)  loss_scale: 131072.0000 (107240.7273)  weight_decay: 0.0500 (0.0500)  time: 0.3052  data: 0.0001  max mem: 41808
Epoch: [33]  [1340/1349]  eta: 0:00:02  lr: 0.000141  min_lr: 0.000003  loss: 0.8562 (0.7832)  loss_scale: 65536.0000 (106929.7301)  weight_decay: 0.0500 (0.0500)  time: 0.3021  data: 0.0001  max mem: 41808
Epoch: [33]  [1348/1349]  eta: 0:00:00  lr: 0.000141  min_lr: 0.000003  loss: 0.7824 (0.7832)  loss_scale: 65536.0000 (106684.2520)  weight_decay: 0.0500 (0.0500)  time: 0.3018  data: 0.0001  max mem: 41808
Epoch: [33] Total time: 0:06:59 (0.3113 s / it)
Averaged stats: lr: 0.000141  min_lr: 0.000003  loss: 0.7824 (0.7849)  loss_scale: 65536.0000 (106684.2520)  weight_decay: 0.0500 (0.0500)  total_time: 419.9716 (419.9659)
Val:  [  0/346]  eta: 1:00:03  loss: 3.4479 (3.4479)  acc1: 1.5625 (1.5625)  acc5: 57.0312 (57.0312)  time: 10.4143  data: 9.3126  max mem: 41808
Val:  [ 10/346]  eta: 0:11:42  loss: 0.1386 (0.6240)  acc1: 99.2188 (84.6591)  acc5: 100.0000 (95.9517)  time: 2.0916  data: 1.2894  max mem: 41808
Val:  [ 20/346]  eta: 0:08:14  loss: 0.1315 (0.4769)  acc1: 99.2188 (88.3929)  acc5: 100.0000 (97.7679)  time: 1.0721  data: 0.2727  max mem: 41808
Val:  [ 30/346]  eta: 0:06:42  loss: 0.1051 (0.3996)  acc1: 100.0000 (90.7258)  acc5: 100.0000 (98.4627)  time: 0.8249  data: 0.0293  max mem: 41808
Val:  [ 40/346]  eta: 0:05:49  loss: 0.1051 (0.4483)  acc1: 100.0000 (89.2721)  acc5: 100.0000 (98.2851)  time: 0.7518  data: 0.0003  max mem: 41808
Val:  [ 50/346]  eta: 0:05:18  loss: 0.1130 (0.3931)  acc1: 99.2188 (91.0080)  acc5: 100.0000 (98.6060)  time: 0.7700  data: 0.0276  max mem: 41808
Val:  [ 60/346]  eta: 0:04:59  loss: 0.1618 (0.3757)  acc1: 98.4375 (91.4959)  acc5: 100.0000 (98.8345)  time: 0.8455  data: 0.1026  max mem: 41808
Val:  [ 70/346]  eta: 0:04:42  loss: 0.2129 (0.4059)  acc1: 95.3125 (90.7020)  acc5: 100.0000 (98.5145)  time: 0.8843  data: 0.1388  max mem: 41808
Val:  [ 80/346]  eta: 0:04:28  loss: 0.2132 (0.4075)  acc1: 93.7500 (90.5382)  acc5: 100.0000 (98.6786)  time: 0.8921  data: 0.1395  max mem: 41808
Val:  [ 90/346]  eta: 0:04:15  loss: 0.2581 (0.4043)  acc1: 93.7500 (90.5391)  acc5: 100.0000 (98.7380)  time: 0.9109  data: 0.1565  max mem: 41808
Val:  [100/346]  eta: 0:04:03  loss: 0.1677 (0.3787)  acc1: 98.4375 (91.3908)  acc5: 100.0000 (98.8629)  time: 0.9210  data: 0.1529  max mem: 41808
Val:  [110/346]  eta: 0:03:51  loss: 0.1498 (0.3913)  acc1: 98.4375 (90.8854)  acc5: 100.0000 (98.9231)  time: 0.9112  data: 0.1506  max mem: 41808
Val:  [120/346]  eta: 0:03:39  loss: 0.2164 (0.3928)  acc1: 95.3125 (90.7348)  acc5: 100.0000 (99.0057)  time: 0.8855  data: 0.1564  max mem: 41808
Val:  [130/346]  eta: 0:03:28  loss: 0.1171 (0.3887)  acc1: 96.8750 (90.8457)  acc5: 100.0000 (99.0816)  time: 0.8856  data: 0.1545  max mem: 41808
Val:  [140/346]  eta: 0:03:18  loss: 0.2133 (0.3856)  acc1: 96.0938 (90.8854)  acc5: 100.0000 (99.1467)  time: 0.9008  data: 0.1512  max mem: 41808
Val:  [150/346]  eta: 0:03:08  loss: 0.2605 (0.3874)  acc1: 95.3125 (90.8578)  acc5: 100.0000 (99.1515)  time: 0.9101  data: 0.1546  max mem: 41808
Val:  [160/346]  eta: 0:02:57  loss: 0.2713 (0.3821)  acc1: 92.9688 (90.9259)  acc5: 100.0000 (99.1945)  time: 0.8993  data: 0.1550  max mem: 41808
Val:  [170/346]  eta: 0:02:47  loss: 0.1709 (0.3761)  acc1: 93.7500 (91.0270)  acc5: 100.0000 (99.2416)  time: 0.9036  data: 0.1570  max mem: 41808
Val:  [180/346]  eta: 0:02:37  loss: 0.1701 (0.3942)  acc1: 96.0938 (90.2106)  acc5: 100.0000 (99.2749)  time: 0.9068  data: 0.1449  max mem: 41808
Val:  [190/346]  eta: 0:02:27  loss: 0.2507 (0.3902)  acc1: 92.1875 (90.3632)  acc5: 100.0000 (99.2965)  time: 0.8949  data: 0.1377  max mem: 41808
Val:  [200/346]  eta: 0:02:17  loss: 0.3169 (0.4011)  acc1: 90.6250 (89.9992)  acc5: 100.0000 (99.2771)  time: 0.8914  data: 0.1451  max mem: 41808
Val:  [210/346]  eta: 0:02:08  loss: 0.2006 (0.3935)  acc1: 95.3125 (90.2547)  acc5: 100.0000 (99.3113)  time: 0.8868  data: 0.1515  max mem: 41808
Val:  [220/346]  eta: 0:01:58  loss: 0.1527 (0.3905)  acc1: 98.4375 (90.3846)  acc5: 100.0000 (99.2930)  time: 0.9080  data: 0.1614  max mem: 41808
Val:  [230/346]  eta: 0:01:48  loss: 0.1411 (0.3820)  acc1: 97.6562 (90.6690)  acc5: 100.0000 (99.3236)  time: 0.9134  data: 0.1658  max mem: 41808
Val:  [240/346]  eta: 0:01:39  loss: 0.1708 (0.3872)  acc1: 97.6562 (90.5829)  acc5: 100.0000 (99.3517)  time: 0.9335  data: 0.1798  max mem: 41808
Val:  [250/346]  eta: 0:01:30  loss: 0.2025 (0.3874)  acc1: 95.3125 (90.6157)  acc5: 100.0000 (99.3526)  time: 0.9489  data: 0.1741  max mem: 41808
Val:  [260/346]  eta: 0:01:21  loss: 0.1621 (0.3832)  acc1: 97.6562 (90.7597)  acc5: 100.0000 (99.3654)  time: 1.0298  data: 0.1962  max mem: 41808
Val:  [270/346]  eta: 0:01:13  loss: 0.1341 (0.3797)  acc1: 98.4375 (90.8931)  acc5: 100.0000 (99.3398)  time: 1.2593  data: 0.2126  max mem: 41808
Val:  [280/346]  eta: 0:01:04  loss: 0.1125 (0.3779)  acc1: 100.0000 (90.9864)  acc5: 100.0000 (99.3383)  time: 1.4232  data: 0.1749  max mem: 41808
Val:  [290/346]  eta: 0:00:55  loss: 0.1031 (0.3697)  acc1: 100.0000 (91.2371)  acc5: 100.0000 (99.3610)  time: 1.4564  data: 0.1449  max mem: 41808
Val:  [300/346]  eta: 0:00:46  loss: 0.1039 (0.3691)  acc1: 100.0000 (91.3050)  acc5: 100.0000 (99.3667)  time: 1.4401  data: 0.1455  max mem: 41808
Val:  [310/346]  eta: 0:00:36  loss: 0.1164 (0.3685)  acc1: 99.2188 (91.3108)  acc5: 100.0000 (99.3418)  time: 1.4285  data: 0.1600  max mem: 41808
Val:  [320/346]  eta: 0:00:27  loss: 0.1278 (0.3686)  acc1: 98.4375 (91.2821)  acc5: 100.0000 (99.3599)  time: 1.4618  data: 0.1681  max mem: 41808
Val:  [330/346]  eta: 0:00:16  loss: 0.4348 (0.3820)  acc1: 88.2812 (90.9012)  acc5: 100.0000 (99.3132)  time: 1.4581  data: 0.1681  max mem: 41808
Val:  [340/346]  eta: 0:00:06  loss: 0.4386 (0.3893)  acc1: 89.8438 (90.7144)  acc5: 100.0000 (99.3333)  time: 1.4239  data: 0.1641  max mem: 41808
Val:  [345/346]  eta: 0:00:01  loss: 0.1981 (0.3860)  acc1: 94.5312 (90.8084)  acc5: 100.0000 (99.3423)  time: 1.3969  data: 0.1524  max mem: 41808
Val: Total time: 0:06:09 (1.0669 s / it)
* Acc@1 90.952 Acc@5 99.364 loss 0.383
Accuracy of the network on the 88494 val videos: 91.0%
Max accuracy: 91.52%   Max Epoch: 32
Epoch: [34]  [   0/1349]  eta: 2:31:10  lr: 0.000141  min_lr: 0.000003  loss: 0.8802 (0.8802)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 6.7236  data: 5.9863  max mem: 41808
Epoch: [34]  [  10/1349]  eta: 0:28:03  lr: 0.000141  min_lr: 0.000003  loss: 0.8297 (0.8002)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 1.2574  data: 0.5778  max mem: 41808
Epoch: [34]  [  20/1349]  eta: 0:21:34  lr: 0.000141  min_lr: 0.000003  loss: 0.7899 (0.7740)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6864  data: 0.0186  max mem: 41808
Epoch: [34]  [  30/1349]  eta: 0:19:11  lr: 0.000141  min_lr: 0.000003  loss: 0.7295 (0.7693)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6617  data: 0.0002  max mem: 41808
Epoch: [34]  [  40/1349]  eta: 0:17:55  lr: 0.000141  min_lr: 0.000003  loss: 0.7346 (0.7555)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6615  data: 0.0002  max mem: 41808
Epoch: [34]  [  50/1349]  eta: 0:17:06  lr: 0.000141  min_lr: 0.000003  loss: 0.7848 (0.7738)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6614  data: 0.0002  max mem: 41808
Epoch: [34]  [  60/1349]  eta: 0:16:30  lr: 0.000140  min_lr: 0.000003  loss: 0.8276 (0.7765)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6606  data: 0.0002  max mem: 41808
Epoch: [34]  [  70/1349]  eta: 0:16:04  lr: 0.000140  min_lr: 0.000003  loss: 0.8191 (0.7828)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6615  data: 0.0002  max mem: 41808
Epoch: [34]  [  80/1349]  eta: 0:15:42  lr: 0.000140  min_lr: 0.000003  loss: 0.8218 (0.7799)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6624  data: 0.0002  max mem: 41808
Epoch: [34]  [  90/1349]  eta: 0:15:23  lr: 0.000140  min_lr: 0.000003  loss: 0.7791 (0.7776)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6615  data: 0.0003  max mem: 41808
Epoch: [34]  [ 100/1349]  eta: 0:15:07  lr: 0.000140  min_lr: 0.000003  loss: 0.7791 (0.7788)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6612  data: 0.0003  max mem: 41808
[2025-05-23 22:05:41,669] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 22:05:41,669] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-05-23 22:05:41,669] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 22:05:41,669] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [34]  [ 110/1349]  eta: 0:14:52  lr: 0.000140  min_lr: 0.000003  loss: 0.8407 (0.7846)  loss_scale: 65536.0000 (70259.3153)  weight_decay: 0.0500 (0.0500)  time: 0.6603  data: 0.0002  max mem: 41808
[2025-05-23 22:05:51,536] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 45984
[2025-05-23 22:05:51,536] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 22:05:51,536] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
[2025-05-23 22:05:51,538] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 45984
[2025-05-23 22:05:51,538] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
Epoch: [34]  [ 120/1349]  eta: 0:14:37  lr: 0.000140  min_lr: 0.000003  loss: 0.8243 (0.7787)  loss_scale: 131072.0000 (73660.2975)  weight_decay: 0.0500 (0.0500)  time: 0.6510  data: 0.0002  max mem: 41808
Epoch: [34]  [ 130/1349]  eta: 0:14:25  lr: 0.000140  min_lr: 0.000003  loss: 0.8243 (0.7870)  loss_scale: 65536.0000 (73040.1221)  weight_decay: 0.0500 (0.0500)  time: 0.6519  data: 0.0002  max mem: 41808
[2025-05-23 22:06:01,308] [INFO] [logging.py:96:log_dist] [Rank 0] step=46000, skipped=283, lr=[3.3161695236292843e-06, 3.3161695236292843e-06, 4.421559364839046e-06, 4.421559364839046e-06, 5.895412486452061e-06, 5.895412486452061e-06, 7.860549981936081e-06, 7.860549981936081e-06, 1.0480733309248109e-05, 1.0480733309248109e-05, 1.3974311078997479e-05, 1.3974311078997479e-05, 1.863241477199664e-05, 1.863241477199664e-05, 2.4843219695995516e-05, 2.4843219695995516e-05, 3.3124292927994025e-05, 3.3124292927994025e-05, 4.416572390399203e-05, 4.416572390399203e-05, 5.888763187198937e-05, 5.888763187198937e-05, 7.851684249598584e-05, 7.851684249598584e-05, 0.00010468912332798111, 0.00010468912332798111, 0.00013958549777064148, 0.00013958549777064148], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-05-23 22:06:01,311] [INFO] [timer.py:260:stop] epoch=0/micro_step=46000/global_step=46000, RunningAvgSamplesPerSec=209.3657639150033, CurrSamplesPerSec=98.8059297812312, MemAllocated=1.37GB, MaxMemAllocated=40.83GB
Epoch: [34]  [ 140/1349]  eta: 0:14:14  lr: 0.000140  min_lr: 0.000003  loss: 0.8347 (0.7851)  loss_scale: 65536.0000 (72507.9149)  weight_decay: 0.0500 (0.0500)  time: 0.6609  data: 0.0002  max mem: 41808
Epoch: [34]  [ 150/1349]  eta: 0:14:03  lr: 0.000139  min_lr: 0.000003  loss: 0.8223 (0.7888)  loss_scale: 65536.0000 (72046.1987)  weight_decay: 0.0500 (0.0500)  time: 0.6634  data: 0.0002  max mem: 41808
Epoch: [34]  [ 160/1349]  eta: 0:13:53  lr: 0.000139  min_lr: 0.000003  loss: 0.8224 (0.7881)  loss_scale: 65536.0000 (71641.8385)  weight_decay: 0.0500 (0.0500)  time: 0.6630  data: 0.0002  max mem: 41808
Epoch: [34]  [ 170/1349]  eta: 0:13:43  lr: 0.000139  min_lr: 0.000003  loss: 0.7759 (0.7881)  loss_scale: 65536.0000 (71284.7719)  weight_decay: 0.0500 (0.0500)  time: 0.6594  data: 0.0002  max mem: 41808
Epoch: [34]  [ 180/1349]  eta: 0:13:34  lr: 0.000139  min_lr: 0.000003  loss: 0.7950 (0.7895)  loss_scale: 65536.0000 (70967.1602)  weight_decay: 0.0500 (0.0500)  time: 0.6601  data: 0.0002  max mem: 41808
Epoch: [34]  [ 190/1349]  eta: 0:13:25  lr: 0.000139  min_lr: 0.000003  loss: 0.8273 (0.7905)  loss_scale: 65536.0000 (70682.8063)  weight_decay: 0.0500 (0.0500)  time: 0.6619  data: 0.0002  max mem: 41808
Epoch: [34]  [ 200/1349]  eta: 0:13:16  lr: 0.000139  min_lr: 0.000003  loss: 0.8528 (0.7921)  loss_scale: 65536.0000 (70426.7463)  weight_decay: 0.0500 (0.0500)  time: 0.6619  data: 0.0002  max mem: 41808
Epoch: [34]  [ 210/1349]  eta: 0:13:07  lr: 0.000139  min_lr: 0.000003  loss: 0.7341 (0.7902)  loss_scale: 65536.0000 (70194.9573)  weight_decay: 0.0500 (0.0500)  time: 0.6618  data: 0.0001  max mem: 41808
Epoch: [34]  [ 220/1349]  eta: 0:12:59  lr: 0.000139  min_lr: 0.000003  loss: 0.7329 (0.7889)  loss_scale: 65536.0000 (69984.1448)  weight_decay: 0.0500 (0.0500)  time: 0.6632  data: 0.0001  max mem: 41808
Epoch: [34]  [ 230/1349]  eta: 0:12:51  lr: 0.000138  min_lr: 0.000003  loss: 0.7947 (0.7896)  loss_scale: 65536.0000 (69791.5844)  weight_decay: 0.0500 (0.0500)  time: 0.6624  data: 0.0002  max mem: 41808
Epoch: [34]  [ 240/1349]  eta: 0:12:43  lr: 0.000138  min_lr: 0.000003  loss: 0.8127 (0.7899)  loss_scale: 65536.0000 (69615.0041)  weight_decay: 0.0500 (0.0500)  time: 0.6628  data: 0.0002  max mem: 41808
[2025-05-23 22:07:16,812] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 22:07:16,812] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-05-23 22:07:16,826] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 22:07:16,826] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [34]  [ 250/1349]  eta: 0:12:35  lr: 0.000138  min_lr: 0.000003  loss: 0.7970 (0.7867)  loss_scale: 65536.0000 (70496.8924)  weight_decay: 0.0500 (0.0500)  time: 0.6639  data: 0.0002  max mem: 41808
Epoch: [34]  [ 260/1349]  eta: 0:12:27  lr: 0.000138  min_lr: 0.000003  loss: 0.7258 (0.7830)  loss_scale: 131072.0000 (72817.7778)  weight_decay: 0.0500 (0.0500)  time: 0.6635  data: 0.0002  max mem: 41808
Epoch: [34]  [ 270/1349]  eta: 0:12:19  lr: 0.000138  min_lr: 0.000003  loss: 0.7258 (0.7824)  loss_scale: 131072.0000 (74967.3801)  weight_decay: 0.0500 (0.0500)  time: 0.6633  data: 0.0001  max mem: 41808
Epoch: [34]  [ 280/1349]  eta: 0:12:11  lr: 0.000138  min_lr: 0.000003  loss: 0.8032 (0.7819)  loss_scale: 131072.0000 (76963.9858)  weight_decay: 0.0500 (0.0500)  time: 0.6616  data: 0.0001  max mem: 41808
Epoch: [34]  [ 290/1349]  eta: 0:12:04  lr: 0.000138  min_lr: 0.000003  loss: 0.7634 (0.7806)  loss_scale: 131072.0000 (78823.3677)  weight_decay: 0.0500 (0.0500)  time: 0.6618  data: 0.0002  max mem: 41808
[2025-05-23 22:07:48,538] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 46161
[2025-05-23 22:07:48,539] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 22:07:48,541] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 46161
[2025-05-23 22:07:48,541] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 22:07:48,541] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [34]  [ 300/1349]  eta: 0:11:55  lr: 0.000138  min_lr: 0.000003  loss: 0.7843 (0.7818)  loss_scale: 131072.0000 (79252.8372)  weight_decay: 0.0500 (0.0500)  time: 0.6497  data: 0.0001  max mem: 41808
Epoch: [34]  [ 310/1349]  eta: 0:11:48  lr: 0.000138  min_lr: 0.000003  loss: 0.8357 (0.7833)  loss_scale: 65536.0000 (78811.7814)  weight_decay: 0.0500 (0.0500)  time: 0.6487  data: 0.0001  max mem: 41808
Epoch: [34]  [ 320/1349]  eta: 0:11:40  lr: 0.000137  min_lr: 0.000003  loss: 0.8342 (0.7831)  loss_scale: 65536.0000 (78398.2056)  weight_decay: 0.0500 (0.0500)  time: 0.6610  data: 0.0002  max mem: 41808
Epoch: [34]  [ 330/1349]  eta: 0:11:33  lr: 0.000137  min_lr: 0.000003  loss: 0.8005 (0.7829)  loss_scale: 65536.0000 (78009.6193)  weight_decay: 0.0500 (0.0500)  time: 0.6610  data: 0.0001  max mem: 41808
Epoch: [34]  [ 340/1349]  eta: 0:11:25  lr: 0.000137  min_lr: 0.000003  loss: 0.8552 (0.7849)  loss_scale: 65536.0000 (77643.8240)  weight_decay: 0.0500 (0.0500)  time: 0.6604  data: 0.0001  max mem: 41808
Epoch: [34]  [ 350/1349]  eta: 0:11:18  lr: 0.000137  min_lr: 0.000003  loss: 0.8605 (0.7867)  loss_scale: 65536.0000 (77298.8718)  weight_decay: 0.0500 (0.0500)  time: 0.6605  data: 0.0001  max mem: 41808
Epoch: [34]  [ 360/1349]  eta: 0:11:11  lr: 0.000137  min_lr: 0.000003  loss: 0.8160 (0.7860)  loss_scale: 65536.0000 (76973.0305)  weight_decay: 0.0500 (0.0500)  time: 0.6606  data: 0.0002  max mem: 41808
Epoch: [34]  [ 370/1349]  eta: 0:11:03  lr: 0.000137  min_lr: 0.000003  loss: 0.7194 (0.7859)  loss_scale: 65536.0000 (76664.7547)  weight_decay: 0.0500 (0.0500)  time: 0.6613  data: 0.0002  max mem: 41808
Epoch: [34]  [ 380/1349]  eta: 0:10:56  lr: 0.000137  min_lr: 0.000003  loss: 0.8040 (0.7872)  loss_scale: 65536.0000 (76372.6614)  weight_decay: 0.0500 (0.0500)  time: 0.6611  data: 0.0002  max mem: 41808
Epoch: [34]  [ 390/1349]  eta: 0:10:49  lr: 0.000137  min_lr: 0.000003  loss: 0.8106 (0.7871)  loss_scale: 65536.0000 (76095.5090)  weight_decay: 0.0500 (0.0500)  time: 0.6600  data: 0.0002  max mem: 41808
Epoch: [34]  [ 400/1349]  eta: 0:10:42  lr: 0.000137  min_lr: 0.000003  loss: 0.7601 (0.7856)  loss_scale: 65536.0000 (75832.1796)  weight_decay: 0.0500 (0.0500)  time: 0.6610  data: 0.0002  max mem: 41808
Epoch: [34]  [ 410/1349]  eta: 0:10:35  lr: 0.000136  min_lr: 0.000003  loss: 0.7376 (0.7839)  loss_scale: 65536.0000 (75581.6642)  weight_decay: 0.0500 (0.0500)  time: 0.6621  data: 0.0001  max mem: 41808
Epoch: [34]  [ 420/1349]  eta: 0:10:28  lr: 0.000136  min_lr: 0.000003  loss: 0.7538 (0.7833)  loss_scale: 65536.0000 (75343.0499)  weight_decay: 0.0500 (0.0500)  time: 0.6642  data: 0.0002  max mem: 41808
[2025-05-23 22:09:13,697] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 22:09:13,697] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 22:09:13,698] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-05-23 22:09:13,698] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [34]  [ 430/1349]  eta: 0:10:21  lr: 0.000136  min_lr: 0.000003  loss: 0.8254 (0.7851)  loss_scale: 65536.0000 (76179.8979)  weight_decay: 0.0500 (0.0500)  time: 0.6645  data: 0.0002  max mem: 41808
Epoch: [34]  [ 440/1349]  eta: 0:10:14  lr: 0.000136  min_lr: 0.000003  loss: 0.7840 (0.7840)  loss_scale: 131072.0000 (77424.6168)  weight_decay: 0.0500 (0.0500)  time: 0.6628  data: 0.0002  max mem: 41808
Epoch: [34]  [ 450/1349]  eta: 0:10:07  lr: 0.000136  min_lr: 0.000003  loss: 0.7223 (0.7826)  loss_scale: 131072.0000 (78614.1375)  weight_decay: 0.0500 (0.0500)  time: 0.6635  data: 0.0002  max mem: 41808
Epoch: [34]  [ 460/1349]  eta: 0:10:00  lr: 0.000136  min_lr: 0.000003  loss: 0.7416 (0.7829)  loss_scale: 131072.0000 (79752.0521)  weight_decay: 0.0500 (0.0500)  time: 0.6626  data: 0.0002  max mem: 41808
Epoch: [34]  [ 470/1349]  eta: 0:09:53  lr: 0.000136  min_lr: 0.000003  loss: 0.8233 (0.7826)  loss_scale: 131072.0000 (80841.6476)  weight_decay: 0.0500 (0.0500)  time: 0.6605  data: 0.0001  max mem: 41808
Epoch: [34]  [ 480/1349]  eta: 0:09:46  lr: 0.000136  min_lr: 0.000003  loss: 0.8041 (0.7827)  loss_scale: 131072.0000 (81885.9376)  weight_decay: 0.0500 (0.0500)  time: 0.6596  data: 0.0002  max mem: 41808
Epoch: [34]  [ 490/1349]  eta: 0:09:41  lr: 0.000135  min_lr: 0.000003  loss: 0.8047 (0.7833)  loss_scale: 131072.0000 (82887.6904)  weight_decay: 0.0500 (0.0500)  time: 0.7186  data: 0.0001  max mem: 41808
Epoch: [34]  [ 500/1349]  eta: 0:09:36  lr: 0.000135  min_lr: 0.000003  loss: 0.8195 (0.7832)  loss_scale: 131072.0000 (83849.4531)  weight_decay: 0.0500 (0.0500)  time: 0.7770  data: 0.0001  max mem: 41808
Epoch: [34]  [ 510/1349]  eta: 0:09:23  lr: 0.000135  min_lr: 0.000003  loss: 0.8616 (0.7847)  loss_scale: 131072.0000 (84773.5734)  weight_decay: 0.0500 (0.0500)  time: 0.5412  data: 0.0001  max mem: 41808
Epoch: [34]  [ 520/1349]  eta: 0:09:10  lr: 0.000135  min_lr: 0.000003  loss: 0.8098 (0.7842)  loss_scale: 131072.0000 (85662.2188)  weight_decay: 0.0500 (0.0500)  time: 0.3167  data: 0.0001  max mem: 41808
Epoch: [34]  [ 530/1349]  eta: 0:08:58  lr: 0.000135  min_lr: 0.000003  loss: 0.7642 (0.7844)  loss_scale: 131072.0000 (86517.3936)  weight_decay: 0.0500 (0.0500)  time: 0.3171  data: 0.0001  max mem: 41808
[2025-05-23 22:10:17,243] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 46401
[2025-05-23 22:10:17,243] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 46401
[2025-05-23 22:10:17,243] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 22:10:17,243] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 22:10:17,243] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [34]  [ 540/1349]  eta: 0:08:46  lr: 0.000135  min_lr: 0.000003  loss: 0.7902 (0.7855)  loss_scale: 131072.0000 (86614.1220)  weight_decay: 0.0500 (0.0500)  time: 0.3061  data: 0.0001  max mem: 41808
Epoch: [34]  [ 550/1349]  eta: 0:08:35  lr: 0.000135  min_lr: 0.000003  loss: 0.8634 (0.7872)  loss_scale: 65536.0000 (86231.5789)  weight_decay: 0.0500 (0.0500)  time: 0.3060  data: 0.0001  max mem: 41808
Epoch: [34]  [ 560/1349]  eta: 0:08:24  lr: 0.000135  min_lr: 0.000003  loss: 0.8438 (0.7879)  loss_scale: 65536.0000 (85862.6738)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
Epoch: [34]  [ 570/1349]  eta: 0:08:13  lr: 0.000135  min_lr: 0.000003  loss: 0.7727 (0.7863)  loss_scale: 65536.0000 (85506.6900)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [34]  [ 580/1349]  eta: 0:08:02  lr: 0.000134  min_lr: 0.000003  loss: 0.7292 (0.7860)  loss_scale: 65536.0000 (85162.9604)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0002  max mem: 41808
Epoch: [34]  [ 590/1349]  eta: 0:07:52  lr: 0.000134  min_lr: 0.000003  loss: 0.8234 (0.7858)  loss_scale: 65536.0000 (84830.8629)  weight_decay: 0.0500 (0.0500)  time: 0.3086  data: 0.0002  max mem: 41808
Epoch: [34]  [ 600/1349]  eta: 0:07:42  lr: 0.000134  min_lr: 0.000003  loss: 0.8031 (0.7855)  loss_scale: 65536.0000 (84509.8170)  weight_decay: 0.0500 (0.0500)  time: 0.3083  data: 0.0001  max mem: 41808
Epoch: [34]  [ 610/1349]  eta: 0:07:32  lr: 0.000134  min_lr: 0.000003  loss: 0.7298 (0.7848)  loss_scale: 65536.0000 (84199.2799)  weight_decay: 0.0500 (0.0500)  time: 0.3082  data: 0.0001  max mem: 41808
Epoch: [34]  [ 620/1349]  eta: 0:07:22  lr: 0.000134  min_lr: 0.000003  loss: 0.8203 (0.7862)  loss_scale: 65536.0000 (83898.7440)  weight_decay: 0.0500 (0.0500)  time: 0.3091  data: 0.0001  max mem: 41808
Epoch: [34]  [ 630/1349]  eta: 0:07:13  lr: 0.000134  min_lr: 0.000003  loss: 0.8239 (0.7864)  loss_scale: 65536.0000 (83607.7338)  weight_decay: 0.0500 (0.0500)  time: 0.3084  data: 0.0001  max mem: 41808
Epoch: [34]  [ 640/1349]  eta: 0:07:03  lr: 0.000134  min_lr: 0.000003  loss: 0.8219 (0.7871)  loss_scale: 65536.0000 (83325.8034)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0002  max mem: 41808
Epoch: [34]  [ 650/1349]  eta: 0:06:54  lr: 0.000134  min_lr: 0.000003  loss: 0.8302 (0.7873)  loss_scale: 65536.0000 (83052.5346)  weight_decay: 0.0500 (0.0500)  time: 0.3079  data: 0.0001  max mem: 41808
Epoch: [34]  [ 660/1349]  eta: 0:06:45  lr: 0.000134  min_lr: 0.000003  loss: 0.7905 (0.7877)  loss_scale: 65536.0000 (82787.5340)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
[2025-05-23 22:10:56,956] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 22:10:56,956] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-05-23 22:10:56,956] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 22:10:56,956] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [34]  [ 670/1349]  eta: 0:06:37  lr: 0.000133  min_lr: 0.000003  loss: 0.8284 (0.7877)  loss_scale: 65536.0000 (83214.1162)  weight_decay: 0.0500 (0.0500)  time: 0.3079  data: 0.0001  max mem: 41808
Epoch: [34]  [ 680/1349]  eta: 0:06:28  lr: 0.000133  min_lr: 0.000003  loss: 0.8238 (0.7872)  loss_scale: 131072.0000 (83916.8752)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0002  max mem: 41808
Epoch: [34]  [ 690/1349]  eta: 0:06:20  lr: 0.000133  min_lr: 0.000003  loss: 0.7662 (0.7865)  loss_scale: 131072.0000 (84599.2938)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0002  max mem: 41808
Epoch: [34]  [ 700/1349]  eta: 0:06:11  lr: 0.000133  min_lr: 0.000003  loss: 0.8022 (0.7865)  loss_scale: 131072.0000 (85262.2425)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0002  max mem: 41808
Epoch: [34]  [ 710/1349]  eta: 0:06:03  lr: 0.000133  min_lr: 0.000003  loss: 0.7944 (0.7863)  loss_scale: 131072.0000 (85906.5429)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0001  max mem: 41808
[2025-05-23 22:11:14,185] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 46586
[2025-05-23 22:11:14,185] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 46586
[2025-05-23 22:11:14,185] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 22:11:14,185] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 22:11:14,185] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [34]  [ 720/1349]  eta: 0:05:55  lr: 0.000133  min_lr: 0.000003  loss: 0.7635 (0.7865)  loss_scale: 131072.0000 (86442.0749)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [34]  [ 730/1349]  eta: 0:05:47  lr: 0.000133  min_lr: 0.000003  loss: 0.8390 (0.7880)  loss_scale: 65536.0000 (86156.0821)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0002  max mem: 41808
Epoch: [34]  [ 740/1349]  eta: 0:05:40  lr: 0.000133  min_lr: 0.000003  loss: 0.8916 (0.7890)  loss_scale: 65536.0000 (85877.8084)  weight_decay: 0.0500 (0.0500)  time: 0.3087  data: 0.0002  max mem: 41808
Epoch: [34]  [ 750/1349]  eta: 0:05:32  lr: 0.000133  min_lr: 0.000003  loss: 0.8424 (0.7894)  loss_scale: 65536.0000 (85606.9454)  weight_decay: 0.0500 (0.0500)  time: 0.3079  data: 0.0002  max mem: 41808
Epoch: [34]  [ 760/1349]  eta: 0:05:25  lr: 0.000132  min_lr: 0.000003  loss: 0.8414 (0.7894)  loss_scale: 65536.0000 (85343.2011)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [34]  [ 770/1349]  eta: 0:05:17  lr: 0.000132  min_lr: 0.000003  loss: 0.7895 (0.7883)  loss_scale: 65536.0000 (85086.2983)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [34]  [ 780/1349]  eta: 0:05:10  lr: 0.000132  min_lr: 0.000003  loss: 0.8019 (0.7882)  loss_scale: 65536.0000 (84835.9744)  weight_decay: 0.0500 (0.0500)  time: 0.3079  data: 0.0001  max mem: 41808
Epoch: [34]  [ 790/1349]  eta: 0:05:03  lr: 0.000132  min_lr: 0.000003  loss: 0.8030 (0.7882)  loss_scale: 65536.0000 (84591.9798)  weight_decay: 0.0500 (0.0500)  time: 0.3097  data: 0.0001  max mem: 41808
Epoch: [34]  [ 800/1349]  eta: 0:04:56  lr: 0.000132  min_lr: 0.000003  loss: 0.8174 (0.7877)  loss_scale: 65536.0000 (84354.0774)  weight_decay: 0.0500 (0.0500)  time: 0.3088  data: 0.0002  max mem: 41808
Epoch: [34]  [ 810/1349]  eta: 0:04:49  lr: 0.000132  min_lr: 0.000003  loss: 0.8566 (0.7884)  loss_scale: 65536.0000 (84122.0419)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [34]  [ 820/1349]  eta: 0:04:42  lr: 0.000132  min_lr: 0.000003  loss: 0.7926 (0.7869)  loss_scale: 65536.0000 (83895.6590)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
Epoch: [34]  [ 830/1349]  eta: 0:04:35  lr: 0.000132  min_lr: 0.000003  loss: 0.7926 (0.7876)  loss_scale: 65536.0000 (83674.7244)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
Epoch: [34]  [ 840/1349]  eta: 0:04:29  lr: 0.000131  min_lr: 0.000003  loss: 0.8064 (0.7874)  loss_scale: 65536.0000 (83459.0440)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
[2025-05-23 22:11:53,892] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 22:11:53,892] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-05-23 22:11:53,892] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 22:11:53,892] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [34]  [ 850/1349]  eta: 0:04:22  lr: 0.000131  min_lr: 0.000003  loss: 0.7979 (0.7873)  loss_scale: 65536.0000 (83402.4536)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [34]  [ 860/1349]  eta: 0:04:16  lr: 0.000131  min_lr: 0.000003  loss: 0.7577 (0.7862)  loss_scale: 131072.0000 (83956.1069)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
Epoch: [34]  [ 870/1349]  eta: 0:04:09  lr: 0.000131  min_lr: 0.000003  loss: 0.7577 (0.7859)  loss_scale: 131072.0000 (84497.0471)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0002  max mem: 41808
Epoch: [34]  [ 880/1349]  eta: 0:04:03  lr: 0.000131  min_lr: 0.000003  loss: 0.7974 (0.7860)  loss_scale: 131072.0000 (85025.7072)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0002  max mem: 41808
[2025-05-23 22:12:03,717] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 46747
[2025-05-23 22:12:03,718] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 22:12:03,718] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 46747
[2025-05-23 22:12:03,718] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 22:12:03,718] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [34]  [ 890/1349]  eta: 0:03:56  lr: 0.000131  min_lr: 0.000003  loss: 0.7982 (0.7859)  loss_scale: 65536.0000 (84806.9675)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0002  max mem: 41808
Epoch: [34]  [ 900/1349]  eta: 0:03:50  lr: 0.000131  min_lr: 0.000003  loss: 0.7924 (0.7854)  loss_scale: 65536.0000 (84593.0832)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0002  max mem: 41808
Epoch: [34]  [ 910/1349]  eta: 0:03:44  lr: 0.000131  min_lr: 0.000003  loss: 0.8045 (0.7857)  loss_scale: 65536.0000 (84383.8946)  weight_decay: 0.0500 (0.0500)  time: 0.3063  data: 0.0001  max mem: 41808
Epoch: [34]  [ 920/1349]  eta: 0:03:38  lr: 0.000131  min_lr: 0.000003  loss: 0.8431 (0.7863)  loss_scale: 65536.0000 (84179.2486)  weight_decay: 0.0500 (0.0500)  time: 0.3062  data: 0.0001  max mem: 41808
Epoch: [34]  [ 930/1349]  eta: 0:03:32  lr: 0.000130  min_lr: 0.000003  loss: 0.8390 (0.7860)  loss_scale: 65536.0000 (83978.9989)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
Epoch: [34]  [ 940/1349]  eta: 0:03:26  lr: 0.000130  min_lr: 0.000003  loss: 0.8297 (0.7865)  loss_scale: 65536.0000 (83783.0053)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
Epoch: [34]  [ 950/1349]  eta: 0:03:20  lr: 0.000130  min_lr: 0.000003  loss: 0.8133 (0.7862)  loss_scale: 65536.0000 (83591.1335)  weight_decay: 0.0500 (0.0500)  time: 0.3063  data: 0.0001  max mem: 41808
Epoch: [34]  [ 960/1349]  eta: 0:03:14  lr: 0.000130  min_lr: 0.000003  loss: 0.7269 (0.7855)  loss_scale: 65536.0000 (83403.2549)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0002  max mem: 41808
Epoch: [34]  [ 970/1349]  eta: 0:03:09  lr: 0.000130  min_lr: 0.000003  loss: 0.6844 (0.7844)  loss_scale: 65536.0000 (83219.2461)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [34]  [ 980/1349]  eta: 0:03:03  lr: 0.000130  min_lr: 0.000003  loss: 0.7030 (0.7844)  loss_scale: 65536.0000 (83038.9888)  weight_decay: 0.0500 (0.0500)  time: 0.3066  data: 0.0001  max mem: 41808
Epoch: [34]  [ 990/1349]  eta: 0:02:57  lr: 0.000130  min_lr: 0.000003  loss: 0.7922 (0.7839)  loss_scale: 65536.0000 (82862.3693)  weight_decay: 0.0500 (0.0500)  time: 0.3078  data: 0.0001  max mem: 41808
Epoch: [34]  [1000/1349]  eta: 0:02:52  lr: 0.000130  min_lr: 0.000003  loss: 0.8134 (0.7847)  loss_scale: 65536.0000 (82689.2787)  weight_decay: 0.0500 (0.0500)  time: 0.3091  data: 0.0002  max mem: 41808
[2025-05-23 22:12:43,349] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 22:12:43,349] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-05-23 22:12:43,349] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 22:12:43,349] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [34]  [1010/1349]  eta: 0:02:46  lr: 0.000130  min_lr: 0.000003  loss: 0.8198 (0.7845)  loss_scale: 65536.0000 (82584.4352)  weight_decay: 0.0500 (0.0500)  time: 0.3082  data: 0.0002  max mem: 41808
Epoch: [34]  [1020/1349]  eta: 0:02:41  lr: 0.000129  min_lr: 0.000003  loss: 0.7882 (0.7843)  loss_scale: 131072.0000 (83059.3379)  weight_decay: 0.0500 (0.0500)  time: 0.3082  data: 0.0001  max mem: 41808
Epoch: [34]  [1030/1349]  eta: 0:02:35  lr: 0.000129  min_lr: 0.000003  loss: 0.7941 (0.7843)  loss_scale: 131072.0000 (83525.0281)  weight_decay: 0.0500 (0.0500)  time: 0.3082  data: 0.0002  max mem: 41808
Epoch: [34]  [1040/1349]  eta: 0:02:30  lr: 0.000129  min_lr: 0.000003  loss: 0.7941 (0.7843)  loss_scale: 131072.0000 (83981.7714)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [34]  [1050/1349]  eta: 0:02:24  lr: 0.000129  min_lr: 0.000003  loss: 0.8078 (0.7845)  loss_scale: 131072.0000 (84429.8230)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [34]  [1060/1349]  eta: 0:02:19  lr: 0.000129  min_lr: 0.000003  loss: 0.8633 (0.7855)  loss_scale: 131072.0000 (84869.4288)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [34]  [1070/1349]  eta: 0:02:14  lr: 0.000129  min_lr: 0.000003  loss: 0.8514 (0.7857)  loss_scale: 131072.0000 (85300.8254)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [34]  [1080/1349]  eta: 0:02:09  lr: 0.000129  min_lr: 0.000003  loss: 0.7977 (0.7860)  loss_scale: 131072.0000 (85724.2405)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0001  max mem: 41808
Epoch: [34]  [1090/1349]  eta: 0:02:03  lr: 0.000129  min_lr: 0.000003  loss: 0.7977 (0.7866)  loss_scale: 131072.0000 (86139.8937)  weight_decay: 0.0500 (0.0500)  time: 0.3066  data: 0.0001  max mem: 41808
Epoch: [34]  [1100/1349]  eta: 0:01:58  lr: 0.000129  min_lr: 0.000003  loss: 0.7733 (0.7862)  loss_scale: 131072.0000 (86547.9964)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
Epoch: [34]  [1110/1349]  eta: 0:01:53  lr: 0.000128  min_lr: 0.000003  loss: 0.7460 (0.7859)  loss_scale: 131072.0000 (86948.7525)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0001  max mem: 41808
Epoch: [34]  [1120/1349]  eta: 0:01:48  lr: 0.000128  min_lr: 0.000003  loss: 0.7582 (0.7853)  loss_scale: 131072.0000 (87342.3586)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [34]  [1130/1349]  eta: 0:01:43  lr: 0.000128  min_lr: 0.000003  loss: 0.8511 (0.7860)  loss_scale: 131072.0000 (87729.0044)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
[2025-05-23 22:13:21,148] [INFO] [logging.py:96:log_dist] [Rank 0] step=47000, skipped=287, lr=[3.0450420643963832e-06, 3.0450420643963832e-06, 4.060056085861844e-06, 4.060056085861844e-06, 5.413408114482459e-06, 5.413408114482459e-06, 7.217877485976612e-06, 7.217877485976612e-06, 9.623836647968817e-06, 9.623836647968817e-06, 1.2831782197291755e-05, 1.2831782197291755e-05, 1.710904292972234e-05, 1.710904292972234e-05, 2.2812057239629785e-05, 2.2812057239629785e-05, 3.041607631950638e-05, 3.041607631950638e-05, 4.055476842600851e-05, 4.055476842600851e-05, 5.4073024568011344e-05, 5.4073024568011344e-05, 7.209736609068179e-05, 7.209736609068179e-05, 9.612982145424238e-05, 9.612982145424238e-05, 0.0001281730952723232, 0.0001281730952723232], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-05-23 22:13:21,149] [INFO] [timer.py:260:stop] epoch=0/micro_step=47000/global_step=47000, RunningAvgSamplesPerSec=207.5374647677421, CurrSamplesPerSec=215.28761722362174, MemAllocated=1.37GB, MaxMemAllocated=40.83GB
[2025-05-23 22:13:22,674] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 22:13:22,674] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 22:13:22,674] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 22:13:22,674] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [34]  [1140/1349]  eta: 0:01:38  lr: 0.000128  min_lr: 0.000003  loss: 0.8511 (0.7856)  loss_scale: 131072.0000 (88453.4969)  weight_decay: 0.0500 (0.0500)  time: 0.3059  data: 0.0001  max mem: 41808
Epoch: [34]  [1150/1349]  eta: 0:01:33  lr: 0.000128  min_lr: 0.000003  loss: 0.8064 (0.7861)  loss_scale: 262144.0000 (89962.5369)  weight_decay: 0.0500 (0.0500)  time: 0.3059  data: 0.0001  max mem: 41808
[2025-05-23 22:13:28,480] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 47023
[2025-05-23 22:13:28,480] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 47023
[2025-05-23 22:13:28,480] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 22:13:28,480] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 22:13:28,480] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [34]  [1160/1349]  eta: 0:01:28  lr: 0.000128  min_lr: 0.000003  loss: 0.7833 (0.7859)  loss_scale: 262144.0000 (90993.9983)  weight_decay: 0.0500 (0.0500)  time: 0.3053  data: 0.0001  max mem: 41808
Epoch: [34]  [1170/1349]  eta: 0:01:23  lr: 0.000128  min_lr: 0.000003  loss: 0.7833 (0.7863)  loss_scale: 131072.0000 (91336.2528)  weight_decay: 0.0500 (0.0500)  time: 0.3057  data: 0.0001  max mem: 41808
Epoch: [34]  [1180/1349]  eta: 0:01:18  lr: 0.000128  min_lr: 0.000003  loss: 0.7966 (0.7861)  loss_scale: 131072.0000 (91672.7113)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
Epoch: [34]  [1190/1349]  eta: 0:01:13  lr: 0.000128  min_lr: 0.000003  loss: 0.7777 (0.7863)  loss_scale: 131072.0000 (92003.5197)  weight_decay: 0.0500 (0.0500)  time: 0.3083  data: 0.0001  max mem: 41808
Epoch: [34]  [1200/1349]  eta: 0:01:08  lr: 0.000127  min_lr: 0.000003  loss: 0.8509 (0.7865)  loss_scale: 131072.0000 (92328.8193)  weight_decay: 0.0500 (0.0500)  time: 0.3081  data: 0.0001  max mem: 41808
Epoch: [34]  [1210/1349]  eta: 0:01:04  lr: 0.000127  min_lr: 0.000003  loss: 0.7896 (0.7865)  loss_scale: 131072.0000 (92648.7465)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [34]  [1220/1349]  eta: 0:00:59  lr: 0.000127  min_lr: 0.000003  loss: 0.7836 (0.7869)  loss_scale: 131072.0000 (92963.4333)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
Epoch: [34]  [1230/1349]  eta: 0:00:54  lr: 0.000127  min_lr: 0.000003  loss: 0.8015 (0.7871)  loss_scale: 131072.0000 (93273.0073)  weight_decay: 0.0500 (0.0500)  time: 0.3078  data: 0.0001  max mem: 41808
Epoch: [34]  [1240/1349]  eta: 0:00:49  lr: 0.000127  min_lr: 0.000003  loss: 0.8066 (0.7873)  loss_scale: 131072.0000 (93577.5923)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [34]  [1250/1349]  eta: 0:00:45  lr: 0.000127  min_lr: 0.000003  loss: 0.8066 (0.7873)  loss_scale: 131072.0000 (93877.3078)  weight_decay: 0.0500 (0.0500)  time: 0.3079  data: 0.0001  max mem: 41808
Epoch: [34]  [1260/1349]  eta: 0:00:40  lr: 0.000127  min_lr: 0.000003  loss: 0.7969 (0.7875)  loss_scale: 131072.0000 (94172.2696)  weight_decay: 0.0500 (0.0500)  time: 0.3093  data: 0.0001  max mem: 41808
Epoch: [34]  [1270/1349]  eta: 0:00:35  lr: 0.000127  min_lr: 0.000003  loss: 0.7411 (0.7870)  loss_scale: 131072.0000 (94462.5901)  weight_decay: 0.0500 (0.0500)  time: 0.3093  data: 0.0002  max mem: 41808
Epoch: [34]  [1280/1349]  eta: 0:00:31  lr: 0.000127  min_lr: 0.000003  loss: 0.7743 (0.7869)  loss_scale: 131072.0000 (94748.3778)  weight_decay: 0.0500 (0.0500)  time: 0.3081  data: 0.0001  max mem: 41808
[2025-05-23 22:14:08,211] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 22:14:08,211] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 22:14:08,211] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 22:14:08,211] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 22:14:08,517] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 47153
[2025-05-23 22:14:08,517] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 47153
[2025-05-23 22:14:08,518] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 22:14:08,518] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 22:14:08,518] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [34]  [1290/1349]  eta: 0:00:26  lr: 0.000126  min_lr: 0.000003  loss: 0.8385 (0.7875)  loss_scale: 131072.0000 (95131.2657)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0001  max mem: 41808
Epoch: [34]  [1300/1349]  eta: 0:00:22  lr: 0.000126  min_lr: 0.000003  loss: 0.8385 (0.7874)  loss_scale: 131072.0000 (95407.5204)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [34]  [1310/1349]  eta: 0:00:17  lr: 0.000126  min_lr: 0.000003  loss: 0.8418 (0.7878)  loss_scale: 131072.0000 (95679.5606)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [34]  [1320/1349]  eta: 0:00:13  lr: 0.000126  min_lr: 0.000003  loss: 0.8457 (0.7880)  loss_scale: 131072.0000 (95947.4822)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0002  max mem: 41808
Epoch: [34]  [1330/1349]  eta: 0:00:08  lr: 0.000126  min_lr: 0.000003  loss: 0.8661 (0.7883)  loss_scale: 131072.0000 (96211.3779)  weight_decay: 0.0500 (0.0500)  time: 0.3048  data: 0.0001  max mem: 41808
Epoch: [34]  [1340/1349]  eta: 0:00:04  lr: 0.000126  min_lr: 0.000003  loss: 0.7415 (0.7877)  loss_scale: 131072.0000 (96471.3378)  weight_decay: 0.0500 (0.0500)  time: 0.3026  data: 0.0001  max mem: 41808
Epoch: [34]  [1348/1349]  eta: 0:00:00  lr: 0.000126  min_lr: 0.000003  loss: 0.7038 (0.7875)  loss_scale: 131072.0000 (96676.5308)  weight_decay: 0.0500 (0.0500)  time: 0.3021  data: 0.0001  max mem: 41808
Epoch: [34] Total time: 0:10:01 (0.4458 s / it)
Averaged stats: lr: 0.000126  min_lr: 0.000003  loss: 0.7038 (0.7869)  loss_scale: 131072.0000 (96676.5308)  weight_decay: 0.0500 (0.0500)  total_time: 601.4334 (601.4232)
Val:  [  0/346]  eta: 1:33:43  loss: 3.2893 (3.2893)  acc1: 0.0000 (0.0000)  acc5: 82.8125 (82.8125)  time: 16.2541  data: 15.4800  max mem: 41808
Val:  [ 10/346]  eta: 0:12:08  loss: 0.1331 (0.6170)  acc1: 100.0000 (84.2330)  acc5: 100.0000 (97.8693)  time: 2.1684  data: 1.4075  max mem: 41808
Val:  [ 20/346]  eta: 0:08:41  loss: 0.1221 (0.4739)  acc1: 100.0000 (88.3185)  acc5: 100.0000 (98.6607)  time: 0.8676  data: 0.1058  max mem: 41808
Val:  [ 30/346]  eta: 0:07:02  loss: 0.1048 (0.3929)  acc1: 100.0000 (90.8266)  acc5: 100.0000 (99.0927)  time: 0.8789  data: 0.1058  max mem: 41808
Val:  [ 40/346]  eta: 0:06:07  loss: 0.1035 (0.4428)  acc1: 100.0000 (89.3483)  acc5: 100.0000 (98.5328)  time: 0.7790  data: 0.0003  max mem: 41808
Val:  [ 50/346]  eta: 0:05:30  loss: 0.1065 (0.3866)  acc1: 99.2188 (91.1305)  acc5: 100.0000 (98.8051)  time: 0.7729  data: 0.0003  max mem: 41808
Val:  [ 60/346]  eta: 0:05:02  loss: 0.1529 (0.3595)  acc1: 98.4375 (91.9185)  acc5: 100.0000 (99.0010)  time: 0.7635  data: 0.0003  max mem: 41808
Val:  [ 70/346]  eta: 0:04:40  loss: 0.1902 (0.3869)  acc1: 96.8750 (91.0982)  acc5: 100.0000 (99.1417)  time: 0.7673  data: 0.0003  max mem: 41808
Val:  [ 80/346]  eta: 0:04:26  loss: 0.1852 (0.3925)  acc1: 96.8750 (90.7890)  acc5: 100.0000 (99.0548)  time: 0.8357  data: 0.0760  max mem: 41808
Val:  [ 90/346]  eta: 0:04:13  loss: 0.2541 (0.3859)  acc1: 93.7500 (90.8482)  acc5: 100.0000 (99.1501)  time: 0.8956  data: 0.1466  max mem: 41808
Val:  [100/346]  eta: 0:04:01  loss: 0.1957 (0.3662)  acc1: 96.0938 (91.4759)  acc5: 100.0000 (99.2342)  time: 0.9022  data: 0.1463  max mem: 41808
Val:  [110/346]  eta: 0:03:50  loss: 0.1944 (0.3822)  acc1: 96.8750 (90.9136)  acc5: 100.0000 (99.2680)  time: 0.9165  data: 0.1550  max mem: 41808
Val:  [120/346]  eta: 0:03:39  loss: 0.2156 (0.3849)  acc1: 95.3125 (90.8445)  acc5: 100.0000 (99.3156)  time: 0.9276  data: 0.1671  max mem: 41808
Val:  [130/346]  eta: 0:03:29  loss: 0.1059 (0.3814)  acc1: 98.4375 (90.9351)  acc5: 100.0000 (99.3619)  time: 0.9276  data: 0.1669  max mem: 41808
Val:  [140/346]  eta: 0:03:18  loss: 0.2575 (0.3789)  acc1: 95.3125 (91.0018)  acc5: 100.0000 (99.4071)  time: 0.8960  data: 0.1589  max mem: 41808
Val:  [150/346]  eta: 0:03:07  loss: 0.2667 (0.3800)  acc1: 95.3125 (90.9975)  acc5: 100.0000 (99.4412)  time: 0.8766  data: 0.1501  max mem: 41808
Val:  [160/346]  eta: 0:02:57  loss: 0.2048 (0.3740)  acc1: 96.0938 (91.1442)  acc5: 100.0000 (99.4662)  time: 0.9078  data: 0.1498  max mem: 41808
Val:  [170/346]  eta: 0:02:48  loss: 0.1541 (0.3666)  acc1: 97.6562 (91.3103)  acc5: 100.0000 (99.4974)  time: 0.9388  data: 0.1613  max mem: 41808
Val:  [180/346]  eta: 0:02:38  loss: 0.1521 (0.3775)  acc1: 96.8750 (90.8667)  acc5: 100.0000 (99.5252)  time: 0.9352  data: 0.1578  max mem: 41808
Val:  [190/346]  eta: 0:02:28  loss: 0.3294 (0.3750)  acc1: 91.4062 (90.9522)  acc5: 100.0000 (99.5501)  time: 0.9188  data: 0.1491  max mem: 41808
Val:  [200/346]  eta: 0:02:18  loss: 0.4494 (0.3910)  acc1: 87.5000 (90.3685)  acc5: 100.0000 (99.5647)  time: 0.8991  data: 0.1538  max mem: 41808
Val:  [210/346]  eta: 0:02:08  loss: 0.1965 (0.3826)  acc1: 95.3125 (90.6509)  acc5: 100.0000 (99.5853)  time: 0.9032  data: 0.1697  max mem: 41808
Val:  [220/346]  eta: 0:01:58  loss: 0.1347 (0.3809)  acc1: 96.8750 (90.7346)  acc5: 100.0000 (99.5793)  time: 0.9053  data: 0.1653  max mem: 41808
Val:  [230/346]  eta: 0:01:49  loss: 0.1614 (0.3719)  acc1: 97.6562 (91.0410)  acc5: 100.0000 (99.5942)  time: 0.9111  data: 0.1514  max mem: 41808
Val:  [240/346]  eta: 0:01:39  loss: 0.1661 (0.3760)  acc1: 97.6562 (90.9719)  acc5: 100.0000 (99.6110)  time: 0.8974  data: 0.1453  max mem: 41808
Val:  [250/346]  eta: 0:01:29  loss: 0.2015 (0.3771)  acc1: 96.0938 (90.9923)  acc5: 100.0000 (99.5705)  time: 0.8602  data: 0.1410  max mem: 41808
Val:  [260/346]  eta: 0:01:20  loss: 0.1551 (0.3742)  acc1: 96.0938 (91.0680)  acc5: 100.0000 (99.5869)  time: 0.8878  data: 0.1470  max mem: 41808
Val:  [270/346]  eta: 0:01:11  loss: 0.1383 (0.3705)  acc1: 98.4375 (91.1987)  acc5: 100.0000 (99.5993)  time: 0.9254  data: 0.1528  max mem: 41808
Val:  [280/346]  eta: 0:01:01  loss: 0.1139 (0.3684)  acc1: 100.0000 (91.2978)  acc5: 100.0000 (99.5857)  time: 0.9380  data: 0.1593  max mem: 41808
Val:  [290/346]  eta: 0:00:52  loss: 0.1002 (0.3597)  acc1: 100.0000 (91.5727)  acc5: 100.0000 (99.6000)  time: 0.9325  data: 0.1606  max mem: 41808
Val:  [300/346]  eta: 0:00:42  loss: 0.1045 (0.3602)  acc1: 100.0000 (91.5801)  acc5: 100.0000 (99.5899)  time: 0.9126  data: 0.1512  max mem: 41808
Val:  [310/346]  eta: 0:00:33  loss: 0.1118 (0.3615)  acc1: 99.2188 (91.5444)  acc5: 100.0000 (99.6031)  time: 0.8870  data: 0.1461  max mem: 41808
Val:  [320/346]  eta: 0:00:24  loss: 0.1210 (0.3614)  acc1: 99.2188 (91.5474)  acc5: 100.0000 (99.6155)  time: 0.8975  data: 0.1610  max mem: 41808
Val:  [330/346]  eta: 0:00:14  loss: 0.3884 (0.3765)  acc1: 86.7188 (91.1041)  acc5: 100.0000 (99.5799)  time: 0.9037  data: 0.1674  max mem: 41808
Val:  [340/346]  eta: 0:00:05  loss: 0.4078 (0.3845)  acc1: 88.2812 (90.8976)  acc5: 100.0000 (99.5853)  time: 0.8932  data: 0.1642  max mem: 41808
Val:  [345/346]  eta: 0:00:00  loss: 0.1819 (0.3812)  acc1: 95.4023 (90.9960)  acc5: 100.0000 (99.5909)  time: 0.8732  data: 0.1730  max mem: 41808
Val: Total time: 0:05:21 (0.9297 s / it)
* Acc@1 91.058 Acc@5 99.615 loss 0.378
Accuracy of the network on the 88494 val videos: 91.1%
Max accuracy: 91.52%   Max Epoch: 32
Epoch: [35]  [   0/1349]  eta: 1:29:51  lr: 0.000126  min_lr: 0.000003  loss: 0.4861 (0.4861)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 3.9963  data: 3.6563  max mem: 41808
Epoch: [35]  [  10/1349]  eta: 0:15:41  lr: 0.000126  min_lr: 0.000003  loss: 0.8482 (0.8155)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7035  data: 0.3684  max mem: 41808
Epoch: [35]  [  20/1349]  eta: 0:11:27  lr: 0.000126  min_lr: 0.000003  loss: 0.7818 (0.7670)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3436  data: 0.0199  max mem: 41808
Epoch: [35]  [  30/1349]  eta: 0:09:53  lr: 0.000125  min_lr: 0.000003  loss: 0.7818 (0.7743)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3103  data: 0.0001  max mem: 41808
Epoch: [35]  [  40/1349]  eta: 0:09:03  lr: 0.000125  min_lr: 0.000003  loss: 0.8123 (0.7761)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [35]  [  50/1349]  eta: 0:08:31  lr: 0.000125  min_lr: 0.000003  loss: 0.7782 (0.7658)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
Epoch: [35]  [  60/1349]  eta: 0:08:11  lr: 0.000125  min_lr: 0.000003  loss: 0.7648 (0.7682)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3118  data: 0.0001  max mem: 41808
[2025-05-23 22:20:14,841] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 22:20:14,841] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 22:20:14,841] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 22:20:14,841] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [35]  [  70/1349]  eta: 0:07:54  lr: 0.000125  min_lr: 0.000003  loss: 0.7839 (0.7729)  loss_scale: 131072.0000 (138456.3380)  weight_decay: 0.0500 (0.0500)  time: 0.3118  data: 0.0001  max mem: 41808
[2025-05-23 22:20:16,685] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 47288
[2025-05-23 22:20:16,685] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 47288
[2025-05-23 22:20:16,685] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 22:20:16,685] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 22:20:16,685] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [35]  [  80/1349]  eta: 0:07:40  lr: 0.000125  min_lr: 0.000003  loss: 0.8413 (0.7827)  loss_scale: 131072.0000 (140781.0370)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
Epoch: [35]  [  90/1349]  eta: 0:07:28  lr: 0.000125  min_lr: 0.000003  loss: 0.8172 (0.7812)  loss_scale: 131072.0000 (139714.1099)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0001  max mem: 41808
Epoch: [35]  [ 100/1349]  eta: 0:07:19  lr: 0.000125  min_lr: 0.000003  loss: 0.7607 (0.7768)  loss_scale: 131072.0000 (138858.4554)  weight_decay: 0.0500 (0.0500)  time: 0.3088  data: 0.0001  max mem: 41808
Epoch: [35]  [ 110/1349]  eta: 0:07:11  lr: 0.000125  min_lr: 0.000003  loss: 0.8005 (0.7802)  loss_scale: 131072.0000 (138156.9730)  weight_decay: 0.0500 (0.0500)  time: 0.3092  data: 0.0001  max mem: 41808
Epoch: [35]  [ 120/1349]  eta: 0:07:03  lr: 0.000124  min_lr: 0.000003  loss: 0.8306 (0.7817)  loss_scale: 131072.0000 (137571.4380)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [35]  [ 130/1349]  eta: 0:06:56  lr: 0.000124  min_lr: 0.000003  loss: 0.8505 (0.7880)  loss_scale: 131072.0000 (137075.2977)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0001  max mem: 41808
[2025-05-23 22:20:35,135] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 47348
[2025-05-23 22:20:35,135] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 47348
[2025-05-23 22:20:35,135] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 22:20:35,135] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 22:20:35,135] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [35]  [ 140/1349]  eta: 0:06:50  lr: 0.000124  min_lr: 0.000003  loss: 0.8967 (0.7904)  loss_scale: 131072.0000 (132931.1773)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
Epoch: [35]  [ 150/1349]  eta: 0:06:44  lr: 0.000124  min_lr: 0.000003  loss: 0.7994 (0.7866)  loss_scale: 65536.0000 (128467.9205)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0002  max mem: 41808
Epoch: [35]  [ 160/1349]  eta: 0:06:38  lr: 0.000124  min_lr: 0.000003  loss: 0.7867 (0.7843)  loss_scale: 65536.0000 (124559.1056)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0002  max mem: 41808
Epoch: [35]  [ 170/1349]  eta: 0:06:33  lr: 0.000124  min_lr: 0.000003  loss: 0.7786 (0.7817)  loss_scale: 65536.0000 (121107.4620)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [35]  [ 180/1349]  eta: 0:06:28  lr: 0.000124  min_lr: 0.000003  loss: 0.7906 (0.7835)  loss_scale: 65536.0000 (118037.2155)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
Epoch: [35]  [ 190/1349]  eta: 0:06:23  lr: 0.000124  min_lr: 0.000003  loss: 0.8001 (0.7834)  loss_scale: 65536.0000 (115288.4607)  weight_decay: 0.0500 (0.0500)  time: 0.3066  data: 0.0001  max mem: 41808
Epoch: [35]  [ 200/1349]  eta: 0:06:18  lr: 0.000124  min_lr: 0.000003  loss: 0.7629 (0.7843)  loss_scale: 65536.0000 (112813.2139)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
Epoch: [35]  [ 210/1349]  eta: 0:06:14  lr: 0.000123  min_lr: 0.000003  loss: 0.7752 (0.7852)  loss_scale: 65536.0000 (110572.5877)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0001  max mem: 41808
Epoch: [35]  [ 220/1349]  eta: 0:06:09  lr: 0.000123  min_lr: 0.000003  loss: 0.8029 (0.7847)  loss_scale: 65536.0000 (108534.7330)  weight_decay: 0.0500 (0.0500)  time: 0.3066  data: 0.0001  max mem: 41808
Epoch: [35]  [ 230/1349]  eta: 0:06:05  lr: 0.000123  min_lr: 0.000003  loss: 0.8091 (0.7879)  loss_scale: 65536.0000 (106673.3160)  weight_decay: 0.0500 (0.0500)  time: 0.3063  data: 0.0001  max mem: 41808
Epoch: [35]  [ 240/1349]  eta: 0:06:01  lr: 0.000123  min_lr: 0.000003  loss: 0.8501 (0.7893)  loss_scale: 65536.0000 (104966.3734)  weight_decay: 0.0500 (0.0500)  time: 0.3064  data: 0.0001  max mem: 41808
Epoch: [35]  [ 250/1349]  eta: 0:05:57  lr: 0.000123  min_lr: 0.000003  loss: 0.7311 (0.7854)  loss_scale: 65536.0000 (103395.4422)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0001  max mem: 41808
Epoch: [35]  [ 260/1349]  eta: 0:05:53  lr: 0.000123  min_lr: 0.000003  loss: 0.6908 (0.7832)  loss_scale: 65536.0000 (101944.8889)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0001  max mem: 41808
[2025-05-23 22:21:14,728] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 22:21:14,729] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-05-23 22:21:14,729] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 22:21:14,729] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [35]  [ 270/1349]  eta: 0:05:49  lr: 0.000123  min_lr: 0.000003  loss: 0.7764 (0.7840)  loss_scale: 65536.0000 (102777.8598)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [35]  [ 280/1349]  eta: 0:05:45  lr: 0.000123  min_lr: 0.000003  loss: 0.8098 (0.7849)  loss_scale: 131072.0000 (103784.7687)  weight_decay: 0.0500 (0.0500)  time: 0.3084  data: 0.0002  max mem: 41808
Epoch: [35]  [ 290/1349]  eta: 0:05:41  lr: 0.000123  min_lr: 0.000003  loss: 0.8192 (0.7850)  loss_scale: 131072.0000 (104722.4742)  weight_decay: 0.0500 (0.0500)  time: 0.3095  data: 0.0002  max mem: 41808
[2025-05-23 22:21:23,997] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 47507
[2025-05-23 22:21:23,997] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 47507
[2025-05-23 22:21:23,997] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 22:21:23,997] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 22:21:23,997] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [35]  [ 300/1349]  eta: 0:05:37  lr: 0.000122  min_lr: 0.000003  loss: 0.8192 (0.7857)  loss_scale: 131072.0000 (103638.3256)  weight_decay: 0.0500 (0.0500)  time: 0.3084  data: 0.0002  max mem: 41808
Epoch: [35]  [ 310/1349]  eta: 0:05:34  lr: 0.000122  min_lr: 0.000003  loss: 0.7770 (0.7845)  loss_scale: 65536.0000 (102413.1704)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0002  max mem: 41808
Epoch: [35]  [ 320/1349]  eta: 0:05:30  lr: 0.000122  min_lr: 0.000003  loss: 0.7299 (0.7830)  loss_scale: 65536.0000 (101264.3489)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [35]  [ 330/1349]  eta: 0:05:26  lr: 0.000122  min_lr: 0.000003  loss: 0.7379 (0.7826)  loss_scale: 65536.0000 (100184.9426)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [35]  [ 340/1349]  eta: 0:05:23  lr: 0.000122  min_lr: 0.000003  loss: 0.7767 (0.7844)  loss_scale: 65536.0000 (99168.8446)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [35]  [ 350/1349]  eta: 0:05:19  lr: 0.000122  min_lr: 0.000003  loss: 0.8157 (0.7845)  loss_scale: 65536.0000 (98210.6439)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [35]  [ 360/1349]  eta: 0:05:16  lr: 0.000122  min_lr: 0.000003  loss: 0.7310 (0.7834)  loss_scale: 65536.0000 (97305.5291)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [35]  [ 370/1349]  eta: 0:05:12  lr: 0.000122  min_lr: 0.000003  loss: 0.7732 (0.7835)  loss_scale: 65536.0000 (96449.2075)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
Epoch: [35]  [ 380/1349]  eta: 0:05:09  lr: 0.000122  min_lr: 0.000003  loss: 0.7849 (0.7826)  loss_scale: 65536.0000 (95637.8373)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [35]  [ 390/1349]  eta: 0:05:05  lr: 0.000121  min_lr: 0.000003  loss: 0.8052 (0.7844)  loss_scale: 65536.0000 (94867.9693)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [35]  [ 400/1349]  eta: 0:05:02  lr: 0.000121  min_lr: 0.000003  loss: 0.8257 (0.7853)  loss_scale: 65536.0000 (94136.4988)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [35]  [ 410/1349]  eta: 0:04:58  lr: 0.000121  min_lr: 0.000003  loss: 0.8079 (0.7852)  loss_scale: 65536.0000 (93440.6229)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [35]  [ 420/1349]  eta: 0:04:55  lr: 0.000121  min_lr: 0.000003  loss: 0.8085 (0.7861)  loss_scale: 65536.0000 (92777.8052)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
[2025-05-23 22:22:03,647] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 22:22:03,647] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 22:22:03,647] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-05-23 22:22:03,647] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [35]  [ 430/1349]  eta: 0:04:51  lr: 0.000121  min_lr: 0.000003  loss: 0.8184 (0.7852)  loss_scale: 65536.0000 (93666.3016)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
Epoch: [35]  [ 440/1349]  eta: 0:04:48  lr: 0.000121  min_lr: 0.000003  loss: 0.7922 (0.7844)  loss_scale: 131072.0000 (94514.5034)  weight_decay: 0.0500 (0.0500)  time: 0.3091  data: 0.0001  max mem: 41808
Epoch: [35]  [ 450/1349]  eta: 0:04:45  lr: 0.000121  min_lr: 0.000003  loss: 0.7922 (0.7835)  loss_scale: 131072.0000 (95325.0909)  weight_decay: 0.0500 (0.0500)  time: 0.3087  data: 0.0001  max mem: 41808
Epoch: [35]  [ 460/1349]  eta: 0:04:41  lr: 0.000121  min_lr: 0.000003  loss: 0.8022 (0.7829)  loss_scale: 131072.0000 (96100.5119)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
Epoch: [35]  [ 470/1349]  eta: 0:04:38  lr: 0.000121  min_lr: 0.000003  loss: 0.7956 (0.7832)  loss_scale: 131072.0000 (96843.0064)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [35]  [ 480/1349]  eta: 0:04:35  lr: 0.000120  min_lr: 0.000003  loss: 0.7305 (0.7811)  loss_scale: 131072.0000 (97554.6279)  weight_decay: 0.0500 (0.0500)  time: 0.3081  data: 0.0001  max mem: 41808
Epoch: [35]  [ 490/1349]  eta: 0:04:31  lr: 0.000120  min_lr: 0.000003  loss: 0.6895 (0.7795)  loss_scale: 131072.0000 (98237.2627)  weight_decay: 0.0500 (0.0500)  time: 0.3082  data: 0.0001  max mem: 41808
Epoch: [35]  [ 500/1349]  eta: 0:04:28  lr: 0.000120  min_lr: 0.000003  loss: 0.7310 (0.7785)  loss_scale: 131072.0000 (98892.6467)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
Epoch: [35]  [ 510/1349]  eta: 0:04:25  lr: 0.000120  min_lr: 0.000003  loss: 0.7866 (0.7789)  loss_scale: 131072.0000 (99522.3796)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
Epoch: [35]  [ 520/1349]  eta: 0:04:21  lr: 0.000120  min_lr: 0.000003  loss: 0.8022 (0.7780)  loss_scale: 131072.0000 (100127.9386)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [35]  [ 530/1349]  eta: 0:04:18  lr: 0.000120  min_lr: 0.000003  loss: 0.7537 (0.7775)  loss_scale: 131072.0000 (100710.6893)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [35]  [ 540/1349]  eta: 0:04:15  lr: 0.000120  min_lr: 0.000003  loss: 0.7350 (0.7775)  loss_scale: 131072.0000 (101271.8965)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
[2025-05-23 22:22:43,044] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 22:22:43,044] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 22:22:43,044] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 22:22:43,044] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [35]  [ 550/1349]  eta: 0:04:12  lr: 0.000120  min_lr: 0.000003  loss: 0.7760 (0.7774)  loss_scale: 131072.0000 (102288.4936)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [35]  [ 560/1349]  eta: 0:04:08  lr: 0.000120  min_lr: 0.000003  loss: 0.8100 (0.7779)  loss_scale: 262144.0000 (105137.9679)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [35]  [ 570/1349]  eta: 0:04:05  lr: 0.000119  min_lr: 0.000003  loss: 0.8323 (0.7787)  loss_scale: 262144.0000 (107887.6357)  weight_decay: 0.0500 (0.0500)  time: 0.3083  data: 0.0001  max mem: 41808
[2025-05-23 22:22:52,593] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 47795
[2025-05-23 22:22:52,593] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 47795
[2025-05-23 22:22:52,594] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 22:22:52,594] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 22:22:52,594] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [35]  [ 580/1349]  eta: 0:04:02  lr: 0.000119  min_lr: 0.000003  loss: 0.8256 (0.7793)  loss_scale: 262144.0000 (110317.0534)  weight_decay: 0.0500 (0.0500)  time: 0.3082  data: 0.0001  max mem: 41808
Epoch: [35]  [ 590/1349]  eta: 0:03:59  lr: 0.000119  min_lr: 0.000003  loss: 0.8352 (0.7798)  loss_scale: 131072.0000 (110668.2369)  weight_decay: 0.0500 (0.0500)  time: 0.3079  data: 0.0001  max mem: 41808
Epoch: [35]  [ 600/1349]  eta: 0:03:55  lr: 0.000119  min_lr: 0.000003  loss: 0.8381 (0.7790)  loss_scale: 131072.0000 (111007.7338)  weight_decay: 0.0500 (0.0500)  time: 0.3089  data: 0.0002  max mem: 41808
Epoch: [35]  [ 610/1349]  eta: 0:03:52  lr: 0.000119  min_lr: 0.000003  loss: 0.8303 (0.7798)  loss_scale: 131072.0000 (111336.1178)  weight_decay: 0.0500 (0.0500)  time: 0.3086  data: 0.0002  max mem: 41808
Epoch: [35]  [ 620/1349]  eta: 0:03:49  lr: 0.000119  min_lr: 0.000003  loss: 0.8069 (0.7789)  loss_scale: 131072.0000 (111653.9259)  weight_decay: 0.0500 (0.0500)  time: 0.3089  data: 0.0001  max mem: 41808
Epoch: [35]  [ 630/1349]  eta: 0:03:46  lr: 0.000119  min_lr: 0.000003  loss: 0.7642 (0.7785)  loss_scale: 131072.0000 (111961.6609)  weight_decay: 0.0500 (0.0500)  time: 0.3082  data: 0.0001  max mem: 41808
Epoch: [35]  [ 640/1349]  eta: 0:03:42  lr: 0.000119  min_lr: 0.000003  loss: 0.7625 (0.7781)  loss_scale: 131072.0000 (112259.7941)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0002  max mem: 41808
Epoch: [35]  [ 650/1349]  eta: 0:03:39  lr: 0.000119  min_lr: 0.000003  loss: 0.7602 (0.7778)  loss_scale: 131072.0000 (112548.7680)  weight_decay: 0.0500 (0.0500)  time: 0.3064  data: 0.0002  max mem: 41808
Epoch: [35]  [ 660/1349]  eta: 0:03:36  lr: 0.000118  min_lr: 0.000003  loss: 0.7972 (0.7784)  loss_scale: 131072.0000 (112828.9985)  weight_decay: 0.0500 (0.0500)  time: 0.3066  data: 0.0001  max mem: 41808
Epoch: [35]  [ 670/1349]  eta: 0:03:33  lr: 0.000118  min_lr: 0.000003  loss: 0.8133 (0.7783)  loss_scale: 131072.0000 (113100.8763)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
Epoch: [35]  [ 680/1349]  eta: 0:03:30  lr: 0.000118  min_lr: 0.000003  loss: 0.8353 (0.7793)  loss_scale: 131072.0000 (113364.7695)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0001  max mem: 41808
Epoch: [35]  [ 690/1349]  eta: 0:03:26  lr: 0.000118  min_lr: 0.000003  loss: 0.8687 (0.7799)  loss_scale: 131072.0000 (113621.0246)  weight_decay: 0.0500 (0.0500)  time: 0.3061  data: 0.0001  max mem: 41808
Epoch: [35]  [ 700/1349]  eta: 0:03:23  lr: 0.000118  min_lr: 0.000003  loss: 0.8768 (0.7811)  loss_scale: 131072.0000 (113869.9686)  weight_decay: 0.0500 (0.0500)  time: 0.3061  data: 0.0001  max mem: 41808
[2025-05-23 22:23:32,226] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 22:23:32,226] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 22:23:32,226] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 22:23:32,226] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [35]  [ 710/1349]  eta: 0:03:20  lr: 0.000118  min_lr: 0.000003  loss: 0.8768 (0.7814)  loss_scale: 131072.0000 (114480.6076)  weight_decay: 0.0500 (0.0500)  time: 0.3060  data: 0.0001  max mem: 41808
Epoch: [35]  [ 720/1349]  eta: 0:03:17  lr: 0.000118  min_lr: 0.000003  loss: 0.8289 (0.7821)  loss_scale: 262144.0000 (116528.6436)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
Epoch: [35]  [ 730/1349]  eta: 0:03:14  lr: 0.000118  min_lr: 0.000003  loss: 0.7529 (0.7817)  loss_scale: 262144.0000 (118520.6457)  weight_decay: 0.0500 (0.0500)  time: 0.3085  data: 0.0001  max mem: 41808
[2025-05-23 22:23:39,008] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 47946
[2025-05-23 22:23:39,008] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 47946
[2025-05-23 22:23:39,008] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 22:23:39,008] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 22:23:39,008] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [35]  [ 740/1349]  eta: 0:03:10  lr: 0.000118  min_lr: 0.000003  loss: 0.7447 (0.7809)  loss_scale: 131072.0000 (118690.0297)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
[2025-05-23 22:23:42,383] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 47957
[2025-05-23 22:23:42,383] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 47957
[2025-05-23 22:23:42,383] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 22:23:42,383] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 22:23:42,384] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [35]  [ 750/1349]  eta: 0:03:07  lr: 0.000117  min_lr: 0.000003  loss: 0.7503 (0.7799)  loss_scale: 131072.0000 (118069.5180)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0002  max mem: 41808
Epoch: [35]  [ 760/1349]  eta: 0:03:04  lr: 0.000117  min_lr: 0.000003  loss: 0.7668 (0.7801)  loss_scale: 65536.0000 (117379.1958)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0002  max mem: 41808
Epoch: [35]  [ 770/1349]  eta: 0:03:01  lr: 0.000117  min_lr: 0.000003  loss: 0.7795 (0.7799)  loss_scale: 65536.0000 (116706.7808)  weight_decay: 0.0500 (0.0500)  time: 0.3095  data: 0.0002  max mem: 41808
Epoch: [35]  [ 780/1349]  eta: 0:02:58  lr: 0.000117  min_lr: 0.000003  loss: 0.7587 (0.7799)  loss_scale: 65536.0000 (116051.5851)  weight_decay: 0.0500 (0.0500)  time: 0.3102  data: 0.0002  max mem: 41808
[2025-05-23 22:23:55,348] [INFO] [logging.py:96:log_dist] [Rank 0] step=48000, skipped=295, lr=[2.781696266000336e-06, 2.781696266000336e-06, 3.708928354667115e-06, 3.708928354667115e-06, 4.9452378062228195e-06, 4.9452378062228195e-06, 6.593650408297093e-06, 6.593650408297093e-06, 8.791533877729458e-06, 8.791533877729458e-06, 1.1722045170305942e-05, 1.1722045170305942e-05, 1.5629393560407925e-05, 1.5629393560407925e-05, 2.0839191413877233e-05, 2.0839191413877233e-05, 2.7785588551836308e-05, 2.7785588551836308e-05, 3.704745140244841e-05, 3.704745140244841e-05, 4.939660186993122e-05, 4.939660186993122e-05, 6.586213582657495e-05, 6.586213582657495e-05, 8.781618110209994e-05, 8.781618110209994e-05, 0.00011708824146946659, 0.00011708824146946659], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-05-23 22:23:55,348] [INFO] [timer.py:260:stop] epoch=0/micro_step=48000/global_step=48000, RunningAvgSamplesPerSec=207.65043872087816, CurrSamplesPerSec=215.25585199666733, MemAllocated=1.37GB, MaxMemAllocated=40.83GB
Epoch: [35]  [ 790/1349]  eta: 0:02:55  lr: 0.000117  min_lr: 0.000003  loss: 0.7614 (0.7797)  loss_scale: 65536.0000 (115412.9558)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
Epoch: [35]  [ 800/1349]  eta: 0:02:51  lr: 0.000117  min_lr: 0.000003  loss: 0.8278 (0.7801)  loss_scale: 65536.0000 (114790.2722)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [35]  [ 810/1349]  eta: 0:02:48  lr: 0.000117  min_lr: 0.000003  loss: 0.8278 (0.7803)  loss_scale: 65536.0000 (114182.9445)  weight_decay: 0.0500 (0.0500)  time: 0.3086  data: 0.0001  max mem: 41808
Epoch: [35]  [ 820/1349]  eta: 0:02:45  lr: 0.000117  min_lr: 0.000003  loss: 0.7748 (0.7802)  loss_scale: 65536.0000 (113590.4117)  weight_decay: 0.0500 (0.0500)  time: 0.3088  data: 0.0001  max mem: 41808
Epoch: [35]  [ 830/1349]  eta: 0:02:42  lr: 0.000117  min_lr: 0.000003  loss: 0.7870 (0.7803)  loss_scale: 65536.0000 (113012.1396)  weight_decay: 0.0500 (0.0500)  time: 0.3090  data: 0.0001  max mem: 41808
Epoch: [35]  [ 840/1349]  eta: 0:02:39  lr: 0.000116  min_lr: 0.000003  loss: 0.8456 (0.7806)  loss_scale: 65536.0000 (112447.6195)  weight_decay: 0.0500 (0.0500)  time: 0.3090  data: 0.0001  max mem: 41808
Epoch: [35]  [ 850/1349]  eta: 0:02:36  lr: 0.000116  min_lr: 0.000003  loss: 0.8130 (0.7806)  loss_scale: 65536.0000 (111896.3666)  weight_decay: 0.0500 (0.0500)  time: 0.3080  data: 0.0001  max mem: 41808
Epoch: [35]  [ 860/1349]  eta: 0:02:32  lr: 0.000116  min_lr: 0.000003  loss: 0.7626 (0.7802)  loss_scale: 65536.0000 (111357.9187)  weight_decay: 0.0500 (0.0500)  time: 0.3081  data: 0.0001  max mem: 41808
Epoch: [35]  [ 870/1349]  eta: 0:02:29  lr: 0.000116  min_lr: 0.000003  loss: 0.7552 (0.7802)  loss_scale: 65536.0000 (110831.8347)  weight_decay: 0.0500 (0.0500)  time: 0.3088  data: 0.0001  max mem: 41808
[2025-05-23 22:24:22,184] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 22:24:22,184] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 22:24:22,184] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-05-23 22:24:22,184] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [35]  [ 880/1349]  eta: 0:02:26  lr: 0.000116  min_lr: 0.000003  loss: 0.7552 (0.7798)  loss_scale: 65536.0000 (111061.5755)  weight_decay: 0.0500 (0.0500)  time: 0.3084  data: 0.0001  max mem: 41808
Epoch: [35]  [ 890/1349]  eta: 0:02:23  lr: 0.000116  min_lr: 0.000003  loss: 0.7444 (0.7797)  loss_scale: 131072.0000 (111286.1594)  weight_decay: 0.0500 (0.0500)  time: 0.3081  data: 0.0001  max mem: 41808
Epoch: [35]  [ 900/1349]  eta: 0:02:20  lr: 0.000116  min_lr: 0.000003  loss: 0.7399 (0.7788)  loss_scale: 131072.0000 (111505.7580)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
Epoch: [35]  [ 910/1349]  eta: 0:02:17  lr: 0.000116  min_lr: 0.000003  loss: 0.7847 (0.7792)  loss_scale: 131072.0000 (111720.5357)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [35]  [ 920/1349]  eta: 0:02:14  lr: 0.000116  min_lr: 0.000003  loss: 0.8289 (0.7790)  loss_scale: 131072.0000 (111930.6493)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
Epoch: [35]  [ 930/1349]  eta: 0:02:10  lr: 0.000115  min_lr: 0.000003  loss: 0.7873 (0.7783)  loss_scale: 131072.0000 (112136.2492)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
Epoch: [35]  [ 940/1349]  eta: 0:02:07  lr: 0.000115  min_lr: 0.000003  loss: 0.8058 (0.7788)  loss_scale: 131072.0000 (112337.4793)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
Epoch: [35]  [ 950/1349]  eta: 0:02:04  lr: 0.000115  min_lr: 0.000003  loss: 0.8197 (0.7786)  loss_scale: 131072.0000 (112534.4774)  weight_decay: 0.0500 (0.0500)  time: 0.3064  data: 0.0001  max mem: 41808
Epoch: [35]  [ 960/1349]  eta: 0:02:01  lr: 0.000115  min_lr: 0.000003  loss: 0.8423 (0.7796)  loss_scale: 131072.0000 (112727.3757)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
Epoch: [35]  [ 970/1349]  eta: 0:01:58  lr: 0.000115  min_lr: 0.000003  loss: 0.8492 (0.7800)  loss_scale: 131072.0000 (112916.3007)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [35]  [ 980/1349]  eta: 0:01:55  lr: 0.000115  min_lr: 0.000003  loss: 0.8413 (0.7800)  loss_scale: 131072.0000 (113101.3741)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
Epoch: [35]  [ 990/1349]  eta: 0:01:52  lr: 0.000115  min_lr: 0.000003  loss: 0.7803 (0.7800)  loss_scale: 131072.0000 (113282.7124)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
[2025-05-23 22:25:01,518] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 22:25:01,518] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 22:25:01,518] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 22:25:01,518] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [35]  [1000/1349]  eta: 0:01:48  lr: 0.000115  min_lr: 0.000003  loss: 0.7688 (0.7799)  loss_scale: 131072.0000 (113722.3097)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [35]  [1010/1349]  eta: 0:01:45  lr: 0.000115  min_lr: 0.000003  loss: 0.8302 (0.7807)  loss_scale: 262144.0000 (115190.3778)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
[2025-05-23 22:25:05,199] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 48226
[2025-05-23 22:25:05,199] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 48226
[2025-05-23 22:25:05,199] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 22:25:05,199] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 22:25:05,199] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [35]  [1020/1349]  eta: 0:01:42  lr: 0.000115  min_lr: 0.000003  loss: 0.8640 (0.7811)  loss_scale: 131072.0000 (115345.9275)  weight_decay: 0.0500 (0.0500)  time: 0.3066  data: 0.0001  max mem: 41808
Epoch: [35]  [1030/1349]  eta: 0:01:39  lr: 0.000114  min_lr: 0.000003  loss: 0.8082 (0.7813)  loss_scale: 131072.0000 (115498.4597)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
Epoch: [35]  [1040/1349]  eta: 0:01:36  lr: 0.000114  min_lr: 0.000003  loss: 0.8052 (0.7811)  loss_scale: 131072.0000 (115648.0615)  weight_decay: 0.0500 (0.0500)  time: 0.3083  data: 0.0001  max mem: 41808
Epoch: [35]  [1050/1349]  eta: 0:01:33  lr: 0.000114  min_lr: 0.000003  loss: 0.7881 (0.7805)  loss_scale: 131072.0000 (115794.8164)  weight_decay: 0.0500 (0.0500)  time: 0.3081  data: 0.0001  max mem: 41808
Epoch: [35]  [1060/1349]  eta: 0:01:30  lr: 0.000114  min_lr: 0.000003  loss: 0.7130 (0.7801)  loss_scale: 131072.0000 (115938.8049)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
Epoch: [35]  [1070/1349]  eta: 0:01:26  lr: 0.000114  min_lr: 0.000003  loss: 0.7678 (0.7801)  loss_scale: 131072.0000 (116080.1046)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0002  max mem: 41808
Epoch: [35]  [1080/1349]  eta: 0:01:23  lr: 0.000114  min_lr: 0.000003  loss: 0.7429 (0.7798)  loss_scale: 131072.0000 (116218.7900)  weight_decay: 0.0500 (0.0500)  time: 0.3062  data: 0.0002  max mem: 41808
Epoch: [35]  [1090/1349]  eta: 0:01:20  lr: 0.000114  min_lr: 0.000003  loss: 0.7429 (0.7798)  loss_scale: 131072.0000 (116354.9331)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0001  max mem: 41808
Epoch: [35]  [1100/1349]  eta: 0:01:17  lr: 0.000114  min_lr: 0.000003  loss: 0.7918 (0.7797)  loss_scale: 131072.0000 (116488.6031)  weight_decay: 0.0500 (0.0500)  time: 0.3093  data: 0.0002  max mem: 41808
Epoch: [35]  [1110/1349]  eta: 0:01:14  lr: 0.000114  min_lr: 0.000003  loss: 0.7568 (0.7794)  loss_scale: 131072.0000 (116619.8668)  weight_decay: 0.0500 (0.0500)  time: 0.3102  data: 0.0001  max mem: 41808
Epoch: [35]  [1120/1349]  eta: 0:01:11  lr: 0.000113  min_lr: 0.000003  loss: 0.7200 (0.7794)  loss_scale: 131072.0000 (116748.7886)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
Epoch: [35]  [1130/1349]  eta: 0:01:08  lr: 0.000113  min_lr: 0.000003  loss: 0.7570 (0.7792)  loss_scale: 131072.0000 (116875.4306)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
[2025-05-23 22:25:44,879] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 22:25:44,879] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 22:25:44,879] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 22:25:44,879] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [35]  [1140/1349]  eta: 0:01:05  lr: 0.000113  min_lr: 0.000003  loss: 0.7988 (0.7798)  loss_scale: 131072.0000 (117114.7274)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
Epoch: [35]  [1150/1349]  eta: 0:01:01  lr: 0.000113  min_lr: 0.000003  loss: 0.8247 (0.7794)  loss_scale: 262144.0000 (118374.7559)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [35]  [1160/1349]  eta: 0:00:58  lr: 0.000113  min_lr: 0.000003  loss: 0.7493 (0.7790)  loss_scale: 262144.0000 (119613.0784)  weight_decay: 0.0500 (0.0500)  time: 0.3082  data: 0.0001  max mem: 41808
[2025-05-23 22:25:52,272] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 48379
[2025-05-23 22:25:52,272] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 48379
[2025-05-23 22:25:52,273] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 22:25:52,273] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 22:25:52,273] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [35]  [1170/1349]  eta: 0:00:55  lr: 0.000113  min_lr: 0.000003  loss: 0.7691 (0.7791)  loss_scale: 262144.0000 (120046.7293)  weight_decay: 0.0500 (0.0500)  time: 0.3080  data: 0.0001  max mem: 41808
Epoch: [35]  [1180/1349]  eta: 0:00:52  lr: 0.000113  min_lr: 0.000003  loss: 0.7691 (0.7790)  loss_scale: 131072.0000 (120140.0847)  weight_decay: 0.0500 (0.0500)  time: 0.3083  data: 0.0002  max mem: 41808
Epoch: [35]  [1190/1349]  eta: 0:00:49  lr: 0.000113  min_lr: 0.000003  loss: 0.8243 (0.7798)  loss_scale: 131072.0000 (120231.8724)  weight_decay: 0.0500 (0.0500)  time: 0.3092  data: 0.0001  max mem: 41808
Epoch: [35]  [1200/1349]  eta: 0:00:46  lr: 0.000113  min_lr: 0.000003  loss: 0.8289 (0.7801)  loss_scale: 131072.0000 (120322.1316)  weight_decay: 0.0500 (0.0500)  time: 0.3080  data: 0.0002  max mem: 41808
Epoch: [35]  [1210/1349]  eta: 0:00:43  lr: 0.000112  min_lr: 0.000003  loss: 0.8102 (0.7802)  loss_scale: 131072.0000 (120410.9001)  weight_decay: 0.0500 (0.0500)  time: 0.3081  data: 0.0002  max mem: 41808
Epoch: [35]  [1220/1349]  eta: 0:00:40  lr: 0.000112  min_lr: 0.000003  loss: 0.8415 (0.7807)  loss_scale: 131072.0000 (120498.2146)  weight_decay: 0.0500 (0.0500)  time: 0.3082  data: 0.0002  max mem: 41808
Epoch: [35]  [1230/1349]  eta: 0:00:37  lr: 0.000112  min_lr: 0.000003  loss: 0.8777 (0.7812)  loss_scale: 131072.0000 (120584.1105)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0002  max mem: 41808
Epoch: [35]  [1240/1349]  eta: 0:00:33  lr: 0.000112  min_lr: 0.000003  loss: 0.8463 (0.7811)  loss_scale: 131072.0000 (120668.6221)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
Epoch: [35]  [1250/1349]  eta: 0:00:30  lr: 0.000112  min_lr: 0.000003  loss: 0.8308 (0.7815)  loss_scale: 131072.0000 (120751.7826)  weight_decay: 0.0500 (0.0500)  time: 0.3081  data: 0.0001  max mem: 41808
Epoch: [35]  [1260/1349]  eta: 0:00:27  lr: 0.000112  min_lr: 0.000003  loss: 0.8369 (0.7816)  loss_scale: 131072.0000 (120833.6241)  weight_decay: 0.0500 (0.0500)  time: 0.3087  data: 0.0001  max mem: 41808
Epoch: [35]  [1270/1349]  eta: 0:00:24  lr: 0.000112  min_lr: 0.000003  loss: 0.8309 (0.7816)  loss_scale: 131072.0000 (120914.1778)  weight_decay: 0.0500 (0.0500)  time: 0.3078  data: 0.0001  max mem: 41808
Epoch: [35]  [1280/1349]  eta: 0:00:21  lr: 0.000112  min_lr: 0.000003  loss: 0.8309 (0.7816)  loss_scale: 131072.0000 (120993.4738)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [35]  [1290/1349]  eta: 0:00:18  lr: 0.000112  min_lr: 0.000003  loss: 0.7969 (0.7815)  loss_scale: 131072.0000 (121071.5414)  weight_decay: 0.0500 (0.0500)  time: 0.3079  data: 0.0001  max mem: 41808
[2025-05-23 22:26:32,024] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 22:26:32,024] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 22:26:32,024] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 22:26:32,024] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 22:26:33,247] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 48512
[2025-05-23 22:26:33,247] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 48512
[2025-05-23 22:26:33,247] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 22:26:33,247] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 22:26:33,247] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [35]  [1300/1349]  eta: 0:00:15  lr: 0.000112  min_lr: 0.000003  loss: 0.8346 (0.7821)  loss_scale: 131072.0000 (121551.3974)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
Epoch: [35]  [1310/1349]  eta: 0:00:12  lr: 0.000111  min_lr: 0.000003  loss: 0.8255 (0.7817)  loss_scale: 131072.0000 (121624.0183)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
Epoch: [35]  [1320/1349]  eta: 0:00:09  lr: 0.000111  min_lr: 0.000003  loss: 0.7553 (0.7815)  loss_scale: 131072.0000 (121695.5397)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0001  max mem: 41808
[2025-05-23 22:26:42,740] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 48543
[2025-05-23 22:26:42,740] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 48543
[2025-05-23 22:26:42,741] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 22:26:42,741] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 22:26:42,741] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [35]  [1330/1349]  eta: 0:00:05  lr: 0.000111  min_lr: 0.000003  loss: 0.7143 (0.7812)  loss_scale: 131072.0000 (121618.2720)  weight_decay: 0.0500 (0.0500)  time: 0.3043  data: 0.0001  max mem: 41808
Epoch: [35]  [1340/1349]  eta: 0:00:02  lr: 0.000111  min_lr: 0.000003  loss: 0.7431 (0.7812)  loss_scale: 65536.0000 (121200.0597)  weight_decay: 0.0500 (0.0500)  time: 0.3017  data: 0.0001  max mem: 41808
Epoch: [35]  [1348/1349]  eta: 0:00:00  lr: 0.000111  min_lr: 0.000003  loss: 0.7488 (0.7807)  loss_scale: 65536.0000 (120869.9540)  weight_decay: 0.0500 (0.0500)  time: 0.3018  data: 0.0001  max mem: 41808
Epoch: [35] Total time: 0:06:59 (0.3111 s / it)
Averaged stats: lr: 0.000111  min_lr: 0.000003  loss: 0.7488 (0.7839)  loss_scale: 65536.0000 (120869.9540)  weight_decay: 0.0500 (0.0500)  total_time: 419.6838 (419.6782)
Val:  [  0/346]  eta: 1:35:15  loss: 3.0759 (3.0759)  acc1: 6.2500 (6.2500)  acc5: 78.1250 (78.1250)  time: 16.5182  data: 15.6923  max mem: 41808
Val:  [ 10/346]  eta: 0:12:27  loss: 0.1322 (0.5854)  acc1: 100.0000 (84.9432)  acc5: 100.0000 (97.3722)  time: 2.2244  data: 1.4268  max mem: 41808
Val:  [ 20/346]  eta: 0:08:28  loss: 0.1198 (0.4711)  acc1: 100.0000 (88.3929)  acc5: 100.0000 (98.5119)  time: 0.8123  data: 0.0259  max mem: 41808
Val:  [ 30/346]  eta: 0:06:52  loss: 0.1053 (0.3974)  acc1: 100.0000 (90.7258)  acc5: 100.0000 (98.9667)  time: 0.8014  data: 0.0259  max mem: 41808
Val:  [ 40/346]  eta: 0:05:58  loss: 0.1016 (0.4437)  acc1: 100.0000 (89.3674)  acc5: 100.0000 (98.8948)  time: 0.7634  data: 0.0002  max mem: 41808
Val:  [ 50/346]  eta: 0:05:25  loss: 0.1090 (0.3890)  acc1: 99.2188 (91.0999)  acc5: 100.0000 (99.0809)  time: 0.7827  data: 0.0003  max mem: 41808
Val:  [ 60/346]  eta: 0:04:59  loss: 0.1424 (0.3695)  acc1: 98.4375 (91.4191)  acc5: 100.0000 (99.2316)  time: 0.7912  data: 0.0003  max mem: 41808
Val:  [ 70/346]  eta: 0:04:41  loss: 0.1955 (0.3905)  acc1: 95.3125 (90.8671)  acc5: 100.0000 (99.3068)  time: 0.8104  data: 0.0469  max mem: 41808
Val:  [ 80/346]  eta: 0:04:27  loss: 0.2363 (0.3921)  acc1: 93.7500 (90.7600)  acc5: 100.0000 (99.3731)  time: 0.8755  data: 0.1171  max mem: 41808
Val:  [ 90/346]  eta: 0:04:13  loss: 0.2693 (0.3904)  acc1: 93.7500 (90.7452)  acc5: 100.0000 (99.4248)  time: 0.8965  data: 0.1385  max mem: 41808
Val:  [100/346]  eta: 0:04:02  loss: 0.2008 (0.3665)  acc1: 97.6562 (91.5842)  acc5: 100.0000 (99.4817)  time: 0.9038  data: 0.1407  max mem: 41808
Val:  [110/346]  eta: 0:03:50  loss: 0.1505 (0.3780)  acc1: 97.6562 (91.1669)  acc5: 100.0000 (99.5214)  time: 0.9116  data: 0.1389  max mem: 41808
Val:  [120/346]  eta: 0:03:39  loss: 0.1789 (0.3825)  acc1: 95.3125 (91.0253)  acc5: 100.0000 (99.5416)  time: 0.9003  data: 0.1398  max mem: 41808
Val:  [130/346]  eta: 0:03:28  loss: 0.1045 (0.3763)  acc1: 100.0000 (91.1916)  acc5: 100.0000 (99.5766)  time: 0.8990  data: 0.1376  max mem: 41808
Val:  [140/346]  eta: 0:03:17  loss: 0.2486 (0.3715)  acc1: 95.3125 (91.3065)  acc5: 100.0000 (99.6066)  time: 0.8953  data: 0.1357  max mem: 41808
Val:  [150/346]  eta: 0:03:06  loss: 0.2593 (0.3705)  acc1: 93.7500 (91.3804)  acc5: 100.0000 (99.6016)  time: 0.8772  data: 0.1474  max mem: 41808
Val:  [160/346]  eta: 0:02:56  loss: 0.2381 (0.3624)  acc1: 96.0938 (91.6489)  acc5: 100.0000 (99.6118)  time: 0.8744  data: 0.1521  max mem: 41808
Val:  [170/346]  eta: 0:02:46  loss: 0.1493 (0.3582)  acc1: 99.2188 (91.7580)  acc5: 100.0000 (99.6345)  time: 0.8896  data: 0.1549  max mem: 41808
Val:  [180/346]  eta: 0:02:36  loss: 0.1150 (0.3755)  acc1: 99.2188 (91.1214)  acc5: 100.0000 (99.6504)  time: 0.9023  data: 0.1561  max mem: 41808
Val:  [190/346]  eta: 0:02:26  loss: 0.2781 (0.3736)  acc1: 92.9688 (91.1895)  acc5: 100.0000 (99.6523)  time: 0.9040  data: 0.1534  max mem: 41808
Val:  [200/346]  eta: 0:02:17  loss: 0.4249 (0.3867)  acc1: 89.8438 (90.6639)  acc5: 100.0000 (99.6618)  time: 0.9158  data: 0.1484  max mem: 41808
Val:  [210/346]  eta: 0:02:07  loss: 0.2053 (0.3803)  acc1: 89.8438 (90.8620)  acc5: 100.0000 (99.6779)  time: 0.9045  data: 0.1484  max mem: 41808
Val:  [220/346]  eta: 0:01:58  loss: 0.1606 (0.3776)  acc1: 97.6562 (90.9502)  acc5: 100.0000 (99.6642)  time: 0.9062  data: 0.1633  max mem: 41808
Val:  [230/346]  eta: 0:01:48  loss: 0.1384 (0.3689)  acc1: 97.6562 (91.2338)  acc5: 100.0000 (99.6652)  time: 0.9078  data: 0.1576  max mem: 41808
Val:  [240/346]  eta: 0:01:38  loss: 0.1595 (0.3735)  acc1: 97.6562 (91.1404)  acc5: 100.0000 (99.6758)  time: 0.8873  data: 0.1505  max mem: 41808
Val:  [250/346]  eta: 0:01:29  loss: 0.2049 (0.3738)  acc1: 96.0938 (91.1728)  acc5: 100.0000 (99.6670)  time: 0.8893  data: 0.1520  max mem: 41808
Val:  [260/346]  eta: 0:01:20  loss: 0.1517 (0.3701)  acc1: 96.8750 (91.2895)  acc5: 100.0000 (99.6797)  time: 0.8914  data: 0.1454  max mem: 41808
Val:  [270/346]  eta: 0:01:10  loss: 0.1404 (0.3665)  acc1: 98.4375 (91.4091)  acc5: 100.0000 (99.6887)  time: 0.8835  data: 0.1494  max mem: 41808
Val:  [280/346]  eta: 0:01:01  loss: 0.1137 (0.3653)  acc1: 99.2188 (91.4702)  acc5: 100.0000 (99.6719)  time: 0.8943  data: 0.1547  max mem: 41808
Val:  [290/346]  eta: 0:00:51  loss: 0.1008 (0.3572)  acc1: 100.0000 (91.7204)  acc5: 100.0000 (99.6832)  time: 0.9021  data: 0.1595  max mem: 41808
Val:  [300/346]  eta: 0:00:42  loss: 0.1059 (0.3568)  acc1: 100.0000 (91.7748)  acc5: 100.0000 (99.6418)  time: 0.8928  data: 0.1465  max mem: 41808
Val:  [310/346]  eta: 0:00:33  loss: 0.1162 (0.3577)  acc1: 99.2188 (91.7504)  acc5: 100.0000 (99.6508)  time: 0.8873  data: 0.1355  max mem: 41808
Val:  [320/346]  eta: 0:00:23  loss: 0.1162 (0.3577)  acc1: 99.2188 (91.7324)  acc5: 100.0000 (99.6617)  time: 0.8773  data: 0.1420  max mem: 41808
Val:  [330/346]  eta: 0:00:14  loss: 0.4109 (0.3710)  acc1: 87.5000 (91.3449)  acc5: 100.0000 (99.6294)  time: 0.8866  data: 0.1496  max mem: 41808
Val:  [340/346]  eta: 0:00:05  loss: 0.4290 (0.3783)  acc1: 89.0625 (91.1726)  acc5: 100.0000 (99.6288)  time: 0.9136  data: 0.1613  max mem: 41808
Val:  [345/346]  eta: 0:00:00  loss: 0.1565 (0.3748)  acc1: 94.2529 (91.2762)  acc5: 100.0000 (99.6339)  time: 0.9132  data: 0.1783  max mem: 41808
Val: Total time: 0:05:19 (0.9231 s / it)
* Acc@1 91.333 Acc@5 99.644 loss 0.372
Accuracy of the network on the 88494 val videos: 91.3%
Max accuracy: 91.52%   Max Epoch: 32
Epoch: [36]  [   0/1349]  eta: 1:51:43  lr: 0.000111  min_lr: 0.000003  loss: 0.8482 (0.8482)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 4.9696  data: 4.6348  max mem: 41808
Epoch: [36]  [  10/1349]  eta: 0:16:24  lr: 0.000111  min_lr: 0.000003  loss: 0.7643 (0.7297)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7354  data: 0.4214  max mem: 41808
Epoch: [36]  [  20/1349]  eta: 0:11:47  lr: 0.000111  min_lr: 0.000003  loss: 0.7242 (0.7249)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3102  data: 0.0001  max mem: 41808
Epoch: [36]  [  30/1349]  eta: 0:10:06  lr: 0.000111  min_lr: 0.000003  loss: 0.7940 (0.7449)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3083  data: 0.0001  max mem: 41808
Epoch: [36]  [  40/1349]  eta: 0:09:13  lr: 0.000111  min_lr: 0.000003  loss: 0.8212 (0.7527)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3086  data: 0.0001  max mem: 41808
Epoch: [36]  [  50/1349]  eta: 0:08:39  lr: 0.000110  min_lr: 0.000003  loss: 0.8198 (0.7564)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3078  data: 0.0001  max mem: 41808
Epoch: [36]  [  60/1349]  eta: 0:08:16  lr: 0.000110  min_lr: 0.000003  loss: 0.7828 (0.7608)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
Epoch: [36]  [  70/1349]  eta: 0:07:58  lr: 0.000110  min_lr: 0.000003  loss: 0.7644 (0.7588)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
Epoch: [36]  [  80/1349]  eta: 0:07:44  lr: 0.000110  min_lr: 0.000003  loss: 0.7644 (0.7651)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3086  data: 0.0001  max mem: 41808
Epoch: [36]  [  90/1349]  eta: 0:07:33  lr: 0.000110  min_lr: 0.000003  loss: 0.7811 (0.7645)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3111  data: 0.0001  max mem: 41808
Epoch: [36]  [ 100/1349]  eta: 0:07:23  lr: 0.000110  min_lr: 0.000003  loss: 0.7741 (0.7632)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3093  data: 0.0001  max mem: 41808
[2025-05-23 22:32:46,823] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 22:32:46,824] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-05-23 22:32:46,823] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 22:32:46,824] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [36]  [ 110/1349]  eta: 0:07:14  lr: 0.000110  min_lr: 0.000003  loss: 0.7207 (0.7591)  loss_scale: 65536.0000 (67307.2432)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0002  max mem: 41808
Epoch: [36]  [ 120/1349]  eta: 0:07:06  lr: 0.000110  min_lr: 0.000003  loss: 0.7702 (0.7633)  loss_scale: 131072.0000 (72577.0579)  weight_decay: 0.0500 (0.0500)  time: 0.3088  data: 0.0002  max mem: 41808
Epoch: [36]  [ 130/1349]  eta: 0:06:59  lr: 0.000110  min_lr: 0.000003  loss: 0.8470 (0.7667)  loss_scale: 131072.0000 (77042.3206)  weight_decay: 0.0500 (0.0500)  time: 0.3087  data: 0.0002  max mem: 41808
Epoch: [36]  [ 140/1349]  eta: 0:06:53  lr: 0.000109  min_lr: 0.000003  loss: 0.8801 (0.7743)  loss_scale: 131072.0000 (80874.2128)  weight_decay: 0.0500 (0.0500)  time: 0.3078  data: 0.0002  max mem: 41808
Epoch: [36]  [ 150/1349]  eta: 0:06:46  lr: 0.000109  min_lr: 0.000003  loss: 0.8585 (0.7738)  loss_scale: 131072.0000 (84198.5695)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [36]  [ 160/1349]  eta: 0:06:40  lr: 0.000109  min_lr: 0.000003  loss: 0.7874 (0.7709)  loss_scale: 131072.0000 (87109.9627)  weight_decay: 0.0500 (0.0500)  time: 0.3061  data: 0.0001  max mem: 41808
Epoch: [36]  [ 170/1349]  eta: 0:06:35  lr: 0.000109  min_lr: 0.000003  loss: 0.7874 (0.7731)  loss_scale: 131072.0000 (89680.8421)  weight_decay: 0.0500 (0.0500)  time: 0.3062  data: 0.0001  max mem: 41808
Epoch: [36]  [ 180/1349]  eta: 0:06:30  lr: 0.000109  min_lr: 0.000003  loss: 0.8237 (0.7755)  loss_scale: 131072.0000 (91967.6464)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
Epoch: [36]  [ 190/1349]  eta: 0:06:25  lr: 0.000109  min_lr: 0.000003  loss: 0.8237 (0.7798)  loss_scale: 131072.0000 (94014.9948)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
[2025-05-23 22:33:13,878] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 48760
[2025-05-23 22:33:13,878] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 48760
[2025-05-23 22:33:13,878] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 22:33:13,878] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 22:33:13,878] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [36]  [ 200/1349]  eta: 0:06:20  lr: 0.000109  min_lr: 0.000003  loss: 0.8599 (0.7805)  loss_scale: 131072.0000 (94228.3781)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
Epoch: [36]  [ 210/1349]  eta: 0:06:15  lr: 0.000109  min_lr: 0.000003  loss: 0.7638 (0.7765)  loss_scale: 65536.0000 (92868.5498)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0002  max mem: 41808
Epoch: [36]  [ 220/1349]  eta: 0:06:11  lr: 0.000109  min_lr: 0.000003  loss: 0.7918 (0.7793)  loss_scale: 65536.0000 (91631.7828)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0002  max mem: 41808
Epoch: [36]  [ 230/1349]  eta: 0:06:07  lr: 0.000109  min_lr: 0.000003  loss: 0.8049 (0.7767)  loss_scale: 65536.0000 (90502.0952)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0002  max mem: 41808
Epoch: [36]  [ 240/1349]  eta: 0:06:02  lr: 0.000108  min_lr: 0.000003  loss: 0.7270 (0.7766)  loss_scale: 65536.0000 (89466.1577)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [36]  [ 250/1349]  eta: 0:05:58  lr: 0.000108  min_lr: 0.000003  loss: 0.7398 (0.7743)  loss_scale: 65536.0000 (88512.7649)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
Epoch: [36]  [ 260/1349]  eta: 0:05:54  lr: 0.000108  min_lr: 0.000003  loss: 0.7764 (0.7756)  loss_scale: 65536.0000 (87632.4291)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
Epoch: [36]  [ 270/1349]  eta: 0:05:50  lr: 0.000108  min_lr: 0.000003  loss: 0.7938 (0.7774)  loss_scale: 65536.0000 (86817.0627)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0002  max mem: 41808
Epoch: [36]  [ 280/1349]  eta: 0:05:46  lr: 0.000108  min_lr: 0.000003  loss: 0.7857 (0.7768)  loss_scale: 65536.0000 (86059.7295)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0002  max mem: 41808
Epoch: [36]  [ 290/1349]  eta: 0:05:42  lr: 0.000108  min_lr: 0.000003  loss: 0.7363 (0.7738)  loss_scale: 65536.0000 (85354.4467)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
Epoch: [36]  [ 300/1349]  eta: 0:05:39  lr: 0.000108  min_lr: 0.000003  loss: 0.7277 (0.7725)  loss_scale: 65536.0000 (84696.0266)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0001  max mem: 41808
Epoch: [36]  [ 310/1349]  eta: 0:05:35  lr: 0.000108  min_lr: 0.000003  loss: 0.7985 (0.7729)  loss_scale: 65536.0000 (84079.9486)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
Epoch: [36]  [ 320/1349]  eta: 0:05:31  lr: 0.000108  min_lr: 0.000003  loss: 0.8063 (0.7737)  loss_scale: 65536.0000 (83502.2555)  weight_decay: 0.0500 (0.0500)  time: 0.3080  data: 0.0001  max mem: 41808
[2025-05-23 22:33:53,541] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 22:33:53,541] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 22:33:53,541] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-05-23 22:33:53,541] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [36]  [ 330/1349]  eta: 0:05:27  lr: 0.000107  min_lr: 0.000003  loss: 0.8039 (0.7728)  loss_scale: 65536.0000 (84147.4320)  weight_decay: 0.0500 (0.0500)  time: 0.3082  data: 0.0001  max mem: 41808
Epoch: [36]  [ 340/1349]  eta: 0:05:24  lr: 0.000107  min_lr: 0.000003  loss: 0.7797 (0.7722)  loss_scale: 131072.0000 (85523.5191)  weight_decay: 0.0500 (0.0500)  time: 0.3079  data: 0.0001  max mem: 41808
Epoch: [36]  [ 350/1349]  eta: 0:05:20  lr: 0.000107  min_lr: 0.000003  loss: 0.7797 (0.7743)  loss_scale: 131072.0000 (86821.1966)  weight_decay: 0.0500 (0.0500)  time: 0.3080  data: 0.0001  max mem: 41808
[2025-05-23 22:34:02,469] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 48918
[2025-05-23 22:34:02,469] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 48918
[2025-05-23 22:34:02,469] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 22:34:02,469] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 22:34:02,469] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [36]  [ 360/1349]  eta: 0:05:17  lr: 0.000107  min_lr: 0.000003  loss: 0.8369 (0.7764)  loss_scale: 131072.0000 (86776.1994)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0002  max mem: 41808
Epoch: [36]  [ 370/1349]  eta: 0:05:13  lr: 0.000107  min_lr: 0.000003  loss: 0.8648 (0.7783)  loss_scale: 65536.0000 (86203.6873)  weight_decay: 0.0500 (0.0500)  time: 0.3078  data: 0.0002  max mem: 41808
Epoch: [36]  [ 380/1349]  eta: 0:05:10  lr: 0.000107  min_lr: 0.000003  loss: 0.7930 (0.7778)  loss_scale: 65536.0000 (85661.2283)  weight_decay: 0.0500 (0.0500)  time: 0.3081  data: 0.0002  max mem: 41808
Epoch: [36]  [ 390/1349]  eta: 0:05:06  lr: 0.000107  min_lr: 0.000003  loss: 0.7337 (0.7757)  loss_scale: 65536.0000 (85146.5166)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0002  max mem: 41808
Epoch: [36]  [ 400/1349]  eta: 0:05:03  lr: 0.000107  min_lr: 0.000003  loss: 0.7534 (0.7753)  loss_scale: 65536.0000 (84657.4763)  weight_decay: 0.0500 (0.0500)  time: 0.3082  data: 0.0001  max mem: 41808
Epoch: [36]  [ 410/1349]  eta: 0:04:59  lr: 0.000107  min_lr: 0.000003  loss: 0.7913 (0.7754)  loss_scale: 65536.0000 (84192.2336)  weight_decay: 0.0500 (0.0500)  time: 0.3081  data: 0.0001  max mem: 41808
Epoch: [36]  [ 420/1349]  eta: 0:04:56  lr: 0.000107  min_lr: 0.000003  loss: 0.7918 (0.7750)  loss_scale: 65536.0000 (83749.0926)  weight_decay: 0.0500 (0.0500)  time: 0.3093  data: 0.0001  max mem: 41808
Epoch: [36]  [ 430/1349]  eta: 0:04:52  lr: 0.000106  min_lr: 0.000003  loss: 0.7489 (0.7738)  loss_scale: 65536.0000 (83326.5151)  weight_decay: 0.0500 (0.0500)  time: 0.3103  data: 0.0001  max mem: 41808
[2025-05-23 22:34:27,465] [INFO] [logging.py:96:log_dist] [Rank 0] step=49000, skipped=301, lr=[2.5268372764452736e-06, 2.5268372764452736e-06, 3.3691163685936982e-06, 3.3691163685936982e-06, 4.492155158124931e-06, 4.492155158124931e-06, 5.9895402108332415e-06, 5.9895402108332415e-06, 7.986053614444321e-06, 7.986053614444321e-06, 1.0648071485925763e-05, 1.0648071485925763e-05, 1.4197428647901016e-05, 1.4197428647901016e-05, 1.8929904863868023e-05, 1.8929904863868023e-05, 2.523987315182403e-05, 2.523987315182403e-05, 3.365316420243204e-05, 3.365316420243204e-05, 4.487088560324272e-05, 4.487088560324272e-05, 5.982784747099029e-05, 5.982784747099029e-05, 7.977046329465373e-05, 7.977046329465373e-05, 0.00010636061772620496, 0.00010636061772620496], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-05-23 22:34:27,465] [INFO] [timer.py:260:stop] epoch=0/micro_step=49000/global_step=49000, RunningAvgSamplesPerSec=207.75984606237643, CurrSamplesPerSec=213.51595073237274, MemAllocated=1.37GB, MaxMemAllocated=40.83GB
Epoch: [36]  [ 440/1349]  eta: 0:04:49  lr: 0.000106  min_lr: 0.000003  loss: 0.7102 (0.7726)  loss_scale: 65536.0000 (82923.1020)  weight_decay: 0.0500 (0.0500)  time: 0.3083  data: 0.0001  max mem: 41808
Epoch: [36]  [ 450/1349]  eta: 0:04:46  lr: 0.000106  min_lr: 0.000003  loss: 0.7014 (0.7722)  loss_scale: 65536.0000 (82537.5787)  weight_decay: 0.0500 (0.0500)  time: 0.3079  data: 0.0001  max mem: 41808
Epoch: [36]  [ 460/1349]  eta: 0:04:42  lr: 0.000106  min_lr: 0.000003  loss: 0.8425 (0.7735)  loss_scale: 65536.0000 (82168.7809)  weight_decay: 0.0500 (0.0500)  time: 0.3088  data: 0.0001  max mem: 41808
Epoch: [36]  [ 470/1349]  eta: 0:04:39  lr: 0.000106  min_lr: 0.000003  loss: 0.8269 (0.7725)  loss_scale: 65536.0000 (81815.6433)  weight_decay: 0.0500 (0.0500)  time: 0.3086  data: 0.0002  max mem: 41808
Epoch: [36]  [ 480/1349]  eta: 0:04:36  lr: 0.000106  min_lr: 0.000003  loss: 0.7973 (0.7733)  loss_scale: 65536.0000 (81477.1892)  weight_decay: 0.0500 (0.0500)  time: 0.3085  data: 0.0001  max mem: 41808
[2025-05-23 22:34:42,269] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 22:34:42,269] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-05-23 22:34:42,269] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 22:34:42,269] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [36]  [ 490/1349]  eta: 0:04:32  lr: 0.000106  min_lr: 0.000003  loss: 0.8188 (0.7735)  loss_scale: 65536.0000 (82220.3177)  weight_decay: 0.0500 (0.0500)  time: 0.3085  data: 0.0002  max mem: 41808
Epoch: [36]  [ 500/1349]  eta: 0:04:29  lr: 0.000106  min_lr: 0.000003  loss: 0.8188 (0.7734)  loss_scale: 131072.0000 (83195.4012)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
Epoch: [36]  [ 510/1349]  eta: 0:04:25  lr: 0.000106  min_lr: 0.000003  loss: 0.8201 (0.7738)  loss_scale: 131072.0000 (84132.3209)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [36]  [ 520/1349]  eta: 0:04:22  lr: 0.000105  min_lr: 0.000003  loss: 0.8360 (0.7745)  loss_scale: 131072.0000 (85033.2745)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
Epoch: [36]  [ 530/1349]  eta: 0:04:19  lr: 0.000105  min_lr: 0.000003  loss: 0.8360 (0.7753)  loss_scale: 131072.0000 (85900.2938)  weight_decay: 0.0500 (0.0500)  time: 0.3087  data: 0.0001  max mem: 41808
Epoch: [36]  [ 540/1349]  eta: 0:04:16  lr: 0.000105  min_lr: 0.000003  loss: 0.7959 (0.7754)  loss_scale: 131072.0000 (86735.2606)  weight_decay: 0.0500 (0.0500)  time: 0.3092  data: 0.0001  max mem: 41808
Epoch: [36]  [ 550/1349]  eta: 0:04:12  lr: 0.000105  min_lr: 0.000002  loss: 0.7425 (0.7750)  loss_scale: 131072.0000 (87539.9201)  weight_decay: 0.0500 (0.0500)  time: 0.3078  data: 0.0001  max mem: 41808
Epoch: [36]  [ 560/1349]  eta: 0:04:09  lr: 0.000105  min_lr: 0.000002  loss: 0.7041 (0.7742)  loss_scale: 131072.0000 (88315.8930)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [36]  [ 570/1349]  eta: 0:04:06  lr: 0.000105  min_lr: 0.000002  loss: 0.6271 (0.7722)  loss_scale: 131072.0000 (89064.6865)  weight_decay: 0.0500 (0.0500)  time: 0.3080  data: 0.0001  max mem: 41808
Epoch: [36]  [ 580/1349]  eta: 0:04:02  lr: 0.000105  min_lr: 0.000002  loss: 0.6738 (0.7725)  loss_scale: 131072.0000 (89787.7040)  weight_decay: 0.0500 (0.0500)  time: 0.3086  data: 0.0001  max mem: 41808
Epoch: [36]  [ 590/1349]  eta: 0:03:59  lr: 0.000105  min_lr: 0.000002  loss: 0.8143 (0.7730)  loss_scale: 131072.0000 (90486.2538)  weight_decay: 0.0500 (0.0500)  time: 0.3079  data: 0.0001  max mem: 41808
Epoch: [36]  [ 600/1349]  eta: 0:03:56  lr: 0.000105  min_lr: 0.000002  loss: 0.8157 (0.7728)  loss_scale: 131072.0000 (91161.5574)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [36]  [ 610/1349]  eta: 0:03:53  lr: 0.000105  min_lr: 0.000002  loss: 0.8157 (0.7734)  loss_scale: 131072.0000 (91814.7561)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
[2025-05-23 22:35:21,691] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 22:35:21,691] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 22:35:21,692] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 22:35:21,692] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 22:35:23,240] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 49180
[2025-05-23 22:35:23,240] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 49180
[2025-05-23 22:35:23,240] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 22:35:23,240] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 22:35:23,240] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [36]  [ 620/1349]  eta: 0:03:49  lr: 0.000104  min_lr: 0.000002  loss: 0.8345 (0.7740)  loss_scale: 131072.0000 (93502.2480)  weight_decay: 0.0500 (0.0500)  time: 0.3079  data: 0.0001  max mem: 41808
Epoch: [36]  [ 630/1349]  eta: 0:03:46  lr: 0.000104  min_lr: 0.000002  loss: 0.7987 (0.7731)  loss_scale: 131072.0000 (94097.6482)  weight_decay: 0.0500 (0.0500)  time: 0.3081  data: 0.0001  max mem: 41808
Epoch: [36]  [ 640/1349]  eta: 0:03:43  lr: 0.000104  min_lr: 0.000002  loss: 0.7936 (0.7736)  loss_scale: 131072.0000 (94674.4711)  weight_decay: 0.0500 (0.0500)  time: 0.3083  data: 0.0001  max mem: 41808
Epoch: [36]  [ 650/1349]  eta: 0:03:40  lr: 0.000104  min_lr: 0.000002  loss: 0.7936 (0.7728)  loss_scale: 131072.0000 (95233.5730)  weight_decay: 0.0500 (0.0500)  time: 0.3080  data: 0.0001  max mem: 41808
Epoch: [36]  [ 660/1349]  eta: 0:03:37  lr: 0.000104  min_lr: 0.000002  loss: 0.8165 (0.7740)  loss_scale: 131072.0000 (95775.7579)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
Epoch: [36]  [ 670/1349]  eta: 0:03:33  lr: 0.000104  min_lr: 0.000002  loss: 0.8518 (0.7744)  loss_scale: 131072.0000 (96301.7824)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
Epoch: [36]  [ 680/1349]  eta: 0:03:30  lr: 0.000104  min_lr: 0.000002  loss: 0.7917 (0.7740)  loss_scale: 131072.0000 (96812.3583)  weight_decay: 0.0500 (0.0500)  time: 0.3079  data: 0.0001  max mem: 41808
Epoch: [36]  [ 690/1349]  eta: 0:03:27  lr: 0.000104  min_lr: 0.000002  loss: 0.7787 (0.7741)  loss_scale: 131072.0000 (97308.1563)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
[2025-05-23 22:35:46,932] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 49257
[2025-05-23 22:35:46,932] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 49257
[2025-05-23 22:35:46,932] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 22:35:46,932] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 22:35:46,932] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [36]  [ 700/1349]  eta: 0:03:24  lr: 0.000104  min_lr: 0.000002  loss: 0.8402 (0.7753)  loss_scale: 131072.0000 (97041.8944)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
Epoch: [36]  [ 710/1349]  eta: 0:03:20  lr: 0.000103  min_lr: 0.000002  loss: 0.8402 (0.7759)  loss_scale: 65536.0000 (96598.7736)  weight_decay: 0.0500 (0.0500)  time: 0.3059  data: 0.0001  max mem: 41808
Epoch: [36]  [ 720/1349]  eta: 0:03:17  lr: 0.000103  min_lr: 0.000002  loss: 0.8275 (0.7768)  loss_scale: 65536.0000 (96167.9445)  weight_decay: 0.0500 (0.0500)  time: 0.3059  data: 0.0001  max mem: 41808
Epoch: [36]  [ 730/1349]  eta: 0:03:14  lr: 0.000103  min_lr: 0.000002  loss: 0.8296 (0.7774)  loss_scale: 65536.0000 (95748.9029)  weight_decay: 0.0500 (0.0500)  time: 0.3061  data: 0.0001  max mem: 41808
Epoch: [36]  [ 740/1349]  eta: 0:03:11  lr: 0.000103  min_lr: 0.000002  loss: 0.7997 (0.7777)  loss_scale: 65536.0000 (95341.1714)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
Epoch: [36]  [ 750/1349]  eta: 0:03:08  lr: 0.000103  min_lr: 0.000002  loss: 0.7842 (0.7781)  loss_scale: 65536.0000 (94944.2983)  weight_decay: 0.0500 (0.0500)  time: 0.3095  data: 0.0001  max mem: 41808
Epoch: [36]  [ 760/1349]  eta: 0:03:04  lr: 0.000103  min_lr: 0.000002  loss: 0.7840 (0.7773)  loss_scale: 65536.0000 (94557.8555)  weight_decay: 0.0500 (0.0500)  time: 0.3085  data: 0.0001  max mem: 41808
Epoch: [36]  [ 770/1349]  eta: 0:03:01  lr: 0.000103  min_lr: 0.000002  loss: 0.7854 (0.7781)  loss_scale: 65536.0000 (94181.4371)  weight_decay: 0.0500 (0.0500)  time: 0.3060  data: 0.0001  max mem: 41808
Epoch: [36]  [ 780/1349]  eta: 0:02:58  lr: 0.000103  min_lr: 0.000002  loss: 0.8617 (0.7789)  loss_scale: 65536.0000 (93814.6581)  weight_decay: 0.0500 (0.0500)  time: 0.3063  data: 0.0001  max mem: 41808
Epoch: [36]  [ 790/1349]  eta: 0:02:55  lr: 0.000103  min_lr: 0.000002  loss: 0.8141 (0.7795)  loss_scale: 65536.0000 (93457.1530)  weight_decay: 0.0500 (0.0500)  time: 0.3062  data: 0.0001  max mem: 41808
Epoch: [36]  [ 800/1349]  eta: 0:02:52  lr: 0.000103  min_lr: 0.000002  loss: 0.7989 (0.7796)  loss_scale: 65536.0000 (93108.5743)  weight_decay: 0.0500 (0.0500)  time: 0.3064  data: 0.0001  max mem: 41808
Epoch: [36]  [ 810/1349]  eta: 0:02:48  lr: 0.000102  min_lr: 0.000002  loss: 0.7966 (0.7799)  loss_scale: 65536.0000 (92768.5919)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [36]  [ 820/1349]  eta: 0:02:45  lr: 0.000102  min_lr: 0.000002  loss: 0.7711 (0.7801)  loss_scale: 65536.0000 (92436.8916)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
[2025-05-23 22:36:26,529] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 22:36:26,529] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-05-23 22:36:26,529] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 22:36:26,529] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [36]  [ 830/1349]  eta: 0:02:42  lr: 0.000102  min_lr: 0.000002  loss: 0.7776 (0.7802)  loss_scale: 65536.0000 (92822.9507)  weight_decay: 0.0500 (0.0500)  time: 0.3062  data: 0.0001  max mem: 41808
Epoch: [36]  [ 840/1349]  eta: 0:02:39  lr: 0.000102  min_lr: 0.000002  loss: 0.7776 (0.7796)  loss_scale: 131072.0000 (93277.7551)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0001  max mem: 41808
Epoch: [36]  [ 850/1349]  eta: 0:02:36  lr: 0.000102  min_lr: 0.000002  loss: 0.7908 (0.7805)  loss_scale: 131072.0000 (93721.8707)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
Epoch: [36]  [ 860/1349]  eta: 0:02:33  lr: 0.000102  min_lr: 0.000002  loss: 0.8150 (0.7803)  loss_scale: 131072.0000 (94155.6702)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
Epoch: [36]  [ 870/1349]  eta: 0:02:29  lr: 0.000102  min_lr: 0.000002  loss: 0.8006 (0.7806)  loss_scale: 131072.0000 (94579.5086)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [36]  [ 880/1349]  eta: 0:02:26  lr: 0.000102  min_lr: 0.000002  loss: 0.8449 (0.7813)  loss_scale: 131072.0000 (94993.7253)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
Epoch: [36]  [ 890/1349]  eta: 0:02:23  lr: 0.000102  min_lr: 0.000002  loss: 0.8201 (0.7815)  loss_scale: 131072.0000 (95398.6442)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [36]  [ 900/1349]  eta: 0:02:20  lr: 0.000102  min_lr: 0.000002  loss: 0.8201 (0.7824)  loss_scale: 131072.0000 (95794.5749)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [36]  [ 910/1349]  eta: 0:02:17  lr: 0.000101  min_lr: 0.000002  loss: 0.8123 (0.7818)  loss_scale: 131072.0000 (96181.8134)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
Epoch: [36]  [ 920/1349]  eta: 0:02:14  lr: 0.000101  min_lr: 0.000002  loss: 0.7960 (0.7821)  loss_scale: 131072.0000 (96560.6428)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0002  max mem: 41808
Epoch: [36]  [ 930/1349]  eta: 0:02:11  lr: 0.000101  min_lr: 0.000002  loss: 0.8269 (0.7820)  loss_scale: 131072.0000 (96931.3340)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0002  max mem: 41808
Epoch: [36]  [ 940/1349]  eta: 0:02:07  lr: 0.000101  min_lr: 0.000002  loss: 0.8638 (0.7824)  loss_scale: 131072.0000 (97294.1467)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
[2025-05-23 22:37:05,848] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 22:37:05,848] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 22:37:05,848] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 22:37:05,848] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [36]  [ 950/1349]  eta: 0:02:04  lr: 0.000101  min_lr: 0.000002  loss: 0.8880 (0.7828)  loss_scale: 131072.0000 (97787.1546)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
[2025-05-23 22:37:06,764] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 49517
[2025-05-23 22:37:06,764] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 22:37:06,764] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 49517
[2025-05-23 22:37:06,764] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 22:37:06,764] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [36]  [ 960/1349]  eta: 0:02:01  lr: 0.000101  min_lr: 0.000002  loss: 0.8880 (0.7838)  loss_scale: 131072.0000 (98406.2934)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [36]  [ 970/1349]  eta: 0:01:58  lr: 0.000101  min_lr: 0.000002  loss: 0.8572 (0.7838)  loss_scale: 131072.0000 (98742.7065)  weight_decay: 0.0500 (0.0500)  time: 0.3081  data: 0.0001  max mem: 41808
Epoch: [36]  [ 980/1349]  eta: 0:01:55  lr: 0.000101  min_lr: 0.000002  loss: 0.7933 (0.7834)  loss_scale: 131072.0000 (99072.2610)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0002  max mem: 41808
Epoch: [36]  [ 990/1349]  eta: 0:01:52  lr: 0.000101  min_lr: 0.000002  loss: 0.7459 (0.7830)  loss_scale: 131072.0000 (99395.1645)  weight_decay: 0.0500 (0.0500)  time: 0.3062  data: 0.0003  max mem: 41808
Epoch: [36]  [1000/1349]  eta: 0:01:48  lr: 0.000100  min_lr: 0.000002  loss: 0.7842 (0.7832)  loss_scale: 131072.0000 (99711.6164)  weight_decay: 0.0500 (0.0500)  time: 0.3063  data: 0.0003  max mem: 41808
Epoch: [36]  [1010/1349]  eta: 0:01:45  lr: 0.000100  min_lr: 0.000002  loss: 0.8433 (0.7831)  loss_scale: 131072.0000 (100021.8081)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0003  max mem: 41808
Epoch: [36]  [1020/1349]  eta: 0:01:42  lr: 0.000100  min_lr: 0.000002  loss: 0.8383 (0.7835)  loss_scale: 131072.0000 (100325.9236)  weight_decay: 0.0500 (0.0500)  time: 0.3088  data: 0.0002  max mem: 41808
Epoch: [36]  [1030/1349]  eta: 0:01:39  lr: 0.000100  min_lr: 0.000002  loss: 0.7932 (0.7828)  loss_scale: 131072.0000 (100624.1397)  weight_decay: 0.0500 (0.0500)  time: 0.3091  data: 0.0002  max mem: 41808
Epoch: [36]  [1040/1349]  eta: 0:01:36  lr: 0.000100  min_lr: 0.000002  loss: 0.7932 (0.7827)  loss_scale: 131072.0000 (100916.6263)  weight_decay: 0.0500 (0.0500)  time: 0.3089  data: 0.0002  max mem: 41808
Epoch: [36]  [1050/1349]  eta: 0:01:33  lr: 0.000100  min_lr: 0.000002  loss: 0.7993 (0.7823)  loss_scale: 131072.0000 (101203.5471)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0002  max mem: 41808
Epoch: [36]  [1060/1349]  eta: 0:01:30  lr: 0.000100  min_lr: 0.000002  loss: 0.8063 (0.7827)  loss_scale: 131072.0000 (101485.0594)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0001  max mem: 41808
Epoch: [36]  [1070/1349]  eta: 0:01:27  lr: 0.000100  min_lr: 0.000002  loss: 0.8132 (0.7825)  loss_scale: 131072.0000 (101761.3147)  weight_decay: 0.0500 (0.0500)  time: 0.3058  data: 0.0002  max mem: 41808
Epoch: [36]  [1080/1349]  eta: 0:01:23  lr: 0.000100  min_lr: 0.000002  loss: 0.7790 (0.7824)  loss_scale: 131072.0000 (102032.4588)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0001  max mem: 41808
[2025-05-23 22:37:46,468] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 22:37:46,468] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 22:37:46,468] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 22:37:46,468] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 22:37:47,997] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 49651
[2025-05-23 22:37:47,997] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 49651
[2025-05-23 22:37:47,997] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 22:37:47,997] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 22:37:47,997] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [36]  [1090/1349]  eta: 0:01:20  lr: 0.000100  min_lr: 0.000002  loss: 0.8172 (0.7828)  loss_scale: 131072.0000 (102899.3291)  weight_decay: 0.0500 (0.0500)  time: 0.3082  data: 0.0001  max mem: 41808
Epoch: [36]  [1100/1349]  eta: 0:01:17  lr: 0.000099  min_lr: 0.000002  loss: 0.8167 (0.7825)  loss_scale: 131072.0000 (103155.2116)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
Epoch: [36]  [1110/1349]  eta: 0:01:14  lr: 0.000099  min_lr: 0.000002  loss: 0.7859 (0.7824)  loss_scale: 131072.0000 (103406.4878)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
Epoch: [36]  [1120/1349]  eta: 0:01:11  lr: 0.000099  min_lr: 0.000002  loss: 0.7880 (0.7825)  loss_scale: 131072.0000 (103653.2810)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [36]  [1130/1349]  eta: 0:01:08  lr: 0.000099  min_lr: 0.000002  loss: 0.7637 (0.7818)  loss_scale: 131072.0000 (103895.7100)  weight_decay: 0.0500 (0.0500)  time: 0.3079  data: 0.0001  max mem: 41808
[2025-05-23 22:38:02,456] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 49698
[2025-05-23 22:38:02,456] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 49698
[2025-05-23 22:38:02,456] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 22:38:02,456] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 22:38:02,456] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [36]  [1140/1349]  eta: 0:01:05  lr: 0.000099  min_lr: 0.000002  loss: 0.7625 (0.7818)  loss_scale: 131072.0000 (103731.8282)  weight_decay: 0.0500 (0.0500)  time: 0.3096  data: 0.0001  max mem: 41808
Epoch: [36]  [1150/1349]  eta: 0:01:02  lr: 0.000099  min_lr: 0.000002  loss: 0.8083 (0.7824)  loss_scale: 65536.0000 (103399.9791)  weight_decay: 0.0500 (0.0500)  time: 0.3088  data: 0.0001  max mem: 41808
Epoch: [36]  [1160/1349]  eta: 0:00:58  lr: 0.000099  min_lr: 0.000002  loss: 0.8203 (0.7827)  loss_scale: 65536.0000 (103073.8467)  weight_decay: 0.0500 (0.0500)  time: 0.3079  data: 0.0002  max mem: 41808
Epoch: [36]  [1170/1349]  eta: 0:00:55  lr: 0.000099  min_lr: 0.000002  loss: 0.8082 (0.7828)  loss_scale: 65536.0000 (102753.2844)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0002  max mem: 41808
Epoch: [36]  [1180/1349]  eta: 0:00:52  lr: 0.000099  min_lr: 0.000002  loss: 0.7969 (0.7826)  loss_scale: 65536.0000 (102438.1507)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
Epoch: [36]  [1190/1349]  eta: 0:00:49  lr: 0.000099  min_lr: 0.000002  loss: 0.6707 (0.7817)  loss_scale: 65536.0000 (102128.3090)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
Epoch: [36]  [1200/1349]  eta: 0:00:46  lr: 0.000098  min_lr: 0.000002  loss: 0.7153 (0.7818)  loss_scale: 65536.0000 (101823.6270)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [36]  [1210/1349]  eta: 0:00:43  lr: 0.000098  min_lr: 0.000002  loss: 0.8033 (0.7820)  loss_scale: 65536.0000 (101523.9769)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [36]  [1220/1349]  eta: 0:00:40  lr: 0.000098  min_lr: 0.000002  loss: 0.8224 (0.7818)  loss_scale: 65536.0000 (101229.2351)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
Epoch: [36]  [1230/1349]  eta: 0:00:37  lr: 0.000098  min_lr: 0.000002  loss: 0.7712 (0.7813)  loss_scale: 65536.0000 (100939.2819)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0001  max mem: 41808
Epoch: [36]  [1240/1349]  eta: 0:00:33  lr: 0.000098  min_lr: 0.000002  loss: 0.8058 (0.7814)  loss_scale: 65536.0000 (100654.0016)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0001  max mem: 41808
Epoch: [36]  [1250/1349]  eta: 0:00:30  lr: 0.000098  min_lr: 0.000002  loss: 0.8458 (0.7815)  loss_scale: 65536.0000 (100373.2822)  weight_decay: 0.0500 (0.0500)  time: 0.3064  data: 0.0001  max mem: 41808
Epoch: [36]  [1260/1349]  eta: 0:00:27  lr: 0.000098  min_lr: 0.000002  loss: 0.8074 (0.7812)  loss_scale: 65536.0000 (100097.0151)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
[2025-05-23 22:38:42,100] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 22:38:42,100] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-05-23 22:38:42,100] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 22:38:42,100] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [36]  [1270/1349]  eta: 0:00:24  lr: 0.000098  min_lr: 0.000002  loss: 0.7868 (0.7810)  loss_scale: 65536.0000 (100237.5956)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0001  max mem: 41808
Epoch: [36]  [1280/1349]  eta: 0:00:21  lr: 0.000098  min_lr: 0.000002  loss: 0.7562 (0.7811)  loss_scale: 131072.0000 (100478.3013)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [36]  [1290/1349]  eta: 0:00:18  lr: 0.000097  min_lr: 0.000002  loss: 0.7562 (0.7811)  loss_scale: 131072.0000 (100715.2781)  weight_decay: 0.0500 (0.0500)  time: 0.3062  data: 0.0001  max mem: 41808
Epoch: [36]  [1300/1349]  eta: 0:00:15  lr: 0.000097  min_lr: 0.000002  loss: 0.8236 (0.7814)  loss_scale: 131072.0000 (100948.6118)  weight_decay: 0.0500 (0.0500)  time: 0.3062  data: 0.0001  max mem: 41808
Epoch: [36]  [1310/1349]  eta: 0:00:12  lr: 0.000097  min_lr: 0.000002  loss: 0.8694 (0.7814)  loss_scale: 131072.0000 (101178.3860)  weight_decay: 0.0500 (0.0500)  time: 0.3066  data: 0.0001  max mem: 41808
Epoch: [36]  [1320/1349]  eta: 0:00:09  lr: 0.000097  min_lr: 0.000002  loss: 0.8018 (0.7815)  loss_scale: 131072.0000 (101404.6813)  weight_decay: 0.0500 (0.0500)  time: 0.3063  data: 0.0001  max mem: 41808
Epoch: [36]  [1330/1349]  eta: 0:00:05  lr: 0.000097  min_lr: 0.000002  loss: 0.7757 (0.7816)  loss_scale: 131072.0000 (101627.5763)  weight_decay: 0.0500 (0.0500)  time: 0.3044  data: 0.0001  max mem: 41808
Epoch: [36]  [1340/1349]  eta: 0:00:02  lr: 0.000097  min_lr: 0.000002  loss: 0.7734 (0.7815)  loss_scale: 131072.0000 (101847.1469)  weight_decay: 0.0500 (0.0500)  time: 0.3023  data: 0.0001  max mem: 41808
Epoch: [36]  [1348/1349]  eta: 0:00:00  lr: 0.000097  min_lr: 0.000002  loss: 0.7299 (0.7811)  loss_scale: 131072.0000 (102020.4596)  weight_decay: 0.0500 (0.0500)  time: 0.3019  data: 0.0001  max mem: 41808
Epoch: [36] Total time: 0:06:59 (0.3112 s / it)
Averaged stats: lr: 0.000097  min_lr: 0.000002  loss: 0.7299 (0.7826)  loss_scale: 131072.0000 (102020.4596)  weight_decay: 0.0500 (0.0500)  total_time: 419.8364 (419.8299)
Val:  [  0/346]  eta: 1:19:05  loss: 3.0663 (3.0663)  acc1: 1.5625 (1.5625)  acc5: 93.7500 (93.7500)  time: 13.7162  data: 12.8868  max mem: 41808
Val:  [ 10/346]  eta: 0:11:08  loss: 0.1367 (0.5969)  acc1: 100.0000 (84.0199)  acc5: 100.0000 (99.3608)  time: 1.9907  data: 1.2566  max mem: 41808
Val:  [ 20/346]  eta: 0:08:12  loss: 0.1214 (0.4563)  acc1: 100.0000 (88.6161)  acc5: 100.0000 (99.4048)  time: 0.8990  data: 0.1316  max mem: 41808
Val:  [ 30/346]  eta: 0:06:41  loss: 0.1029 (0.3859)  acc1: 100.0000 (90.8014)  acc5: 100.0000 (99.5968)  time: 0.8727  data: 0.0850  max mem: 41808
Val:  [ 40/346]  eta: 0:05:48  loss: 0.0991 (0.4307)  acc1: 100.0000 (89.7485)  acc5: 100.0000 (98.8567)  time: 0.7472  data: 0.0003  max mem: 41808
Val:  [ 50/346]  eta: 0:05:14  loss: 0.1053 (0.3767)  acc1: 99.2188 (91.4828)  acc5: 100.0000 (99.0349)  time: 0.7446  data: 0.0003  max mem: 41808
Val:  [ 60/346]  eta: 0:04:53  loss: 0.1473 (0.3534)  acc1: 98.4375 (92.0850)  acc5: 100.0000 (99.1675)  time: 0.7948  data: 0.0496  max mem: 41808
Val:  [ 70/346]  eta: 0:04:37  loss: 0.1845 (0.3778)  acc1: 96.8750 (91.4283)  acc5: 100.0000 (99.1197)  time: 0.8580  data: 0.1207  max mem: 41808
Val:  [ 80/346]  eta: 0:04:24  loss: 0.1731 (0.3771)  acc1: 96.8750 (91.4931)  acc5: 100.0000 (98.9005)  time: 0.8973  data: 0.1469  max mem: 41808
Val:  [ 90/346]  eta: 0:04:11  loss: 0.2625 (0.3789)  acc1: 94.5312 (91.3633)  acc5: 100.0000 (98.9784)  time: 0.8930  data: 0.1473  max mem: 41808
Val:  [100/346]  eta: 0:03:58  loss: 0.1796 (0.3566)  acc1: 96.8750 (92.1179)  acc5: 100.0000 (99.0795)  time: 0.8749  data: 0.1395  max mem: 41808
Val:  [110/346]  eta: 0:03:47  loss: 0.1531 (0.3683)  acc1: 97.6562 (91.6878)  acc5: 100.0000 (99.1484)  time: 0.8875  data: 0.1360  max mem: 41808
Val:  [120/346]  eta: 0:03:36  loss: 0.1755 (0.3727)  acc1: 96.8750 (91.5289)  acc5: 100.0000 (99.2058)  time: 0.9019  data: 0.1456  max mem: 41808
Val:  [130/346]  eta: 0:03:26  loss: 0.1051 (0.3685)  acc1: 100.0000 (91.5434)  acc5: 100.0000 (99.2545)  time: 0.9024  data: 0.1508  max mem: 41808
Val:  [140/346]  eta: 0:03:16  loss: 0.1895 (0.3610)  acc1: 96.0938 (91.7996)  acc5: 100.0000 (99.2963)  time: 0.9146  data: 0.1518  max mem: 41808
Val:  [150/346]  eta: 0:03:05  loss: 0.2220 (0.3611)  acc1: 96.0938 (91.8409)  acc5: 100.0000 (99.2912)  time: 0.8882  data: 0.1496  max mem: 41808
Val:  [160/346]  eta: 0:02:55  loss: 0.2401 (0.3557)  acc1: 94.5312 (91.9740)  acc5: 100.0000 (99.3207)  time: 0.8785  data: 0.1453  max mem: 41808
Val:  [170/346]  eta: 0:02:45  loss: 0.1753 (0.3526)  acc1: 96.0938 (92.0002)  acc5: 100.0000 (99.3604)  time: 0.9033  data: 0.1568  max mem: 41808
Val:  [180/346]  eta: 0:02:35  loss: 0.1596 (0.3680)  acc1: 96.0938 (91.3717)  acc5: 100.0000 (99.3698)  time: 0.9005  data: 0.1605  max mem: 41808
Val:  [190/346]  eta: 0:02:26  loss: 0.3242 (0.3677)  acc1: 91.4062 (91.3981)  acc5: 100.0000 (99.3742)  time: 0.9082  data: 0.1537  max mem: 41808
Val:  [200/346]  eta: 0:02:16  loss: 0.4315 (0.3886)  acc1: 89.0625 (90.5822)  acc5: 100.0000 (99.4014)  time: 0.9039  data: 0.1514  max mem: 41808
Val:  [210/346]  eta: 0:02:07  loss: 0.2245 (0.3792)  acc1: 90.6250 (90.9138)  acc5: 100.0000 (99.4298)  time: 0.9113  data: 0.1517  max mem: 41808
Val:  [220/346]  eta: 0:01:57  loss: 0.1588 (0.3764)  acc1: 98.4375 (91.0280)  acc5: 100.0000 (99.4096)  time: 0.8990  data: 0.1447  max mem: 41808
Val:  [230/346]  eta: 0:01:47  loss: 0.1486 (0.3672)  acc1: 98.4375 (91.3420)  acc5: 100.0000 (99.4284)  time: 0.8689  data: 0.1380  max mem: 41808
Val:  [240/346]  eta: 0:01:38  loss: 0.1630 (0.3717)  acc1: 98.4375 (91.2539)  acc5: 100.0000 (99.4457)  time: 0.8926  data: 0.1486  max mem: 41808
Val:  [250/346]  eta: 0:01:28  loss: 0.2016 (0.3723)  acc1: 96.8750 (91.2786)  acc5: 100.0000 (99.4615)  time: 0.9012  data: 0.1526  max mem: 41808
Val:  [260/346]  eta: 0:01:19  loss: 0.1523 (0.3696)  acc1: 97.6562 (91.3524)  acc5: 100.0000 (99.4732)  time: 0.9104  data: 0.1538  max mem: 41808
Val:  [270/346]  eta: 0:01:10  loss: 0.1336 (0.3653)  acc1: 99.2188 (91.5043)  acc5: 100.0000 (99.4523)  time: 0.9082  data: 0.1569  max mem: 41808
Val:  [280/346]  eta: 0:01:01  loss: 0.1137 (0.3639)  acc1: 99.2188 (91.5758)  acc5: 100.0000 (99.4467)  time: 0.9089  data: 0.1575  max mem: 41808
Val:  [290/346]  eta: 0:00:51  loss: 0.1006 (0.3560)  acc1: 100.0000 (91.8116)  acc5: 100.0000 (99.4657)  time: 0.9065  data: 0.1615  max mem: 41808
Val:  [300/346]  eta: 0:00:42  loss: 0.1035 (0.3560)  acc1: 100.0000 (91.8475)  acc5: 100.0000 (99.4575)  time: 0.8780  data: 0.1520  max mem: 41808
Val:  [310/346]  eta: 0:00:33  loss: 0.1212 (0.3582)  acc1: 99.2188 (91.7881)  acc5: 100.0000 (99.4524)  time: 0.9070  data: 0.1464  max mem: 41808
Val:  [320/346]  eta: 0:00:23  loss: 0.1193 (0.3555)  acc1: 99.2188 (91.8541)  acc5: 100.0000 (99.4621)  time: 0.9161  data: 0.1460  max mem: 41808
Val:  [330/346]  eta: 0:00:14  loss: 0.3416 (0.3685)  acc1: 89.8438 (91.4676)  acc5: 100.0000 (99.4029)  time: 0.8686  data: 0.1174  max mem: 41808
Val:  [340/346]  eta: 0:00:05  loss: 0.4565 (0.3773)  acc1: 88.2812 (91.2207)  acc5: 100.0000 (99.4089)  time: 0.8691  data: 0.1310  max mem: 41808
Val:  [345/346]  eta: 0:00:00  loss: 0.1566 (0.3739)  acc1: 94.2529 (91.3214)  acc5: 100.0000 (99.4169)  time: 0.8714  data: 0.1733  max mem: 41808
Val: Total time: 0:05:17 (0.9191 s / it)
* Acc@1 91.394 Acc@5 99.442 loss 0.371
Accuracy of the network on the 88494 val videos: 91.4%
Max accuracy: 91.52%   Max Epoch: 32
Epoch: [37]  [   0/1349]  eta: 1:44:38  lr: 0.000097  min_lr: 0.000002  loss: 0.5313 (0.5313)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 4.6542  data: 3.9484  max mem: 41808
Epoch: [37]  [  10/1349]  eta: 0:16:28  lr: 0.000097  min_lr: 0.000002  loss: 0.7909 (0.7736)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7385  data: 0.3859  max mem: 41808
Epoch: [37]  [  20/1349]  eta: 0:11:49  lr: 0.000097  min_lr: 0.000002  loss: 0.7989 (0.7828)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3277  data: 0.0149  max mem: 41808
Epoch: [37]  [  30/1349]  eta: 0:10:08  lr: 0.000097  min_lr: 0.000002  loss: 0.8129 (0.7797)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3086  data: 0.0001  max mem: 41808
Epoch: [37]  [  40/1349]  eta: 0:09:14  lr: 0.000096  min_lr: 0.000002  loss: 0.8129 (0.7815)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  time: 0.3081  data: 0.0001  max mem: 41808
[2025-05-23 22:44:44,351] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 22:44:44,352] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 22:44:44,352] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 22:44:44,352] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 22:44:45,269] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 49958
[2025-05-23 22:44:45,269] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 49958
[2025-05-23 22:44:45,270] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 22:44:45,270] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 22:44:45,270] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [37]  [  50/1349]  eta: 0:08:40  lr: 0.000096  min_lr: 0.000002  loss: 0.8456 (0.7923)  loss_scale: 131072.0000 (138782.1176)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0002  max mem: 41808
Epoch: [37]  [  60/1349]  eta: 0:08:16  lr: 0.000096  min_lr: 0.000002  loss: 0.8189 (0.7796)  loss_scale: 131072.0000 (137518.1639)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0002  max mem: 41808
Epoch: [37]  [  70/1349]  eta: 0:07:59  lr: 0.000096  min_lr: 0.000002  loss: 0.7900 (0.7824)  loss_scale: 131072.0000 (136610.2535)  weight_decay: 0.0500 (0.0500)  time: 0.3092  data: 0.0003  max mem: 41808
Epoch: [37]  [  80/1349]  eta: 0:07:45  lr: 0.000096  min_lr: 0.000002  loss: 0.8156 (0.7819)  loss_scale: 131072.0000 (135926.5185)  weight_decay: 0.0500 (0.0500)  time: 0.3091  data: 0.0002  max mem: 41808
[2025-05-23 22:44:57,279] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 49997
[2025-05-23 22:44:57,279] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 49997
[2025-05-23 22:44:57,279] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 22:44:57,279] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 22:44:57,279] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
[2025-05-23 22:44:57,892] [INFO] [logging.py:96:log_dist] [Rank 0] step=50000, skipped=308, lr=[2.2811475190275343e-06, 2.2811475190275343e-06, 3.041530025370046e-06, 3.041530025370046e-06, 4.055373367160061e-06, 4.055373367160061e-06, 5.407164489546749e-06, 5.407164489546749e-06, 7.209552652728997e-06, 7.209552652728997e-06, 9.61273687030533e-06, 9.61273687030533e-06, 1.281698249374044e-05, 1.281698249374044e-05, 1.708930999165392e-05, 1.708930999165392e-05, 2.278574665553856e-05, 2.278574665553856e-05, 3.038099554071808e-05, 3.038099554071808e-05, 4.0507994054290776e-05, 4.0507994054290776e-05, 5.4010658739054366e-05, 5.4010658739054366e-05, 7.201421165207249e-05, 7.201421165207249e-05, 9.601894886942998e-05, 9.601894886942998e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-05-23 22:44:57,893] [INFO] [timer.py:260:stop] epoch=0/micro_step=50000/global_step=50000, RunningAvgSamplesPerSec=207.86376758215363, CurrSamplesPerSec=213.9331937585026, MemAllocated=1.37GB, MaxMemAllocated=40.83GB
Epoch: [37]  [  90/1349]  eta: 0:07:32  lr: 0.000096  min_lr: 0.000002  loss: 0.7569 (0.7787)  loss_scale: 131072.0000 (130351.8242)  weight_decay: 0.0500 (0.0500)  time: 0.3059  data: 0.0002  max mem: 41808
Epoch: [37]  [ 100/1349]  eta: 0:07:22  lr: 0.000096  min_lr: 0.000002  loss: 0.7628 (0.7803)  loss_scale: 65536.0000 (123934.4158)  weight_decay: 0.0500 (0.0500)  time: 0.3058  data: 0.0002  max mem: 41808
Epoch: [37]  [ 110/1349]  eta: 0:07:13  lr: 0.000096  min_lr: 0.000002  loss: 0.7461 (0.7751)  loss_scale: 65536.0000 (118673.2973)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0002  max mem: 41808
Epoch: [37]  [ 120/1349]  eta: 0:07:05  lr: 0.000096  min_lr: 0.000002  loss: 0.7461 (0.7736)  loss_scale: 65536.0000 (114281.7851)  weight_decay: 0.0500 (0.0500)  time: 0.3064  data: 0.0002  max mem: 41808
Epoch: [37]  [ 130/1349]  eta: 0:06:58  lr: 0.000096  min_lr: 0.000002  loss: 0.7866 (0.7764)  loss_scale: 65536.0000 (110560.7328)  weight_decay: 0.0500 (0.0500)  time: 0.3064  data: 0.0002  max mem: 41808
Epoch: [37]  [ 140/1349]  eta: 0:06:52  lr: 0.000095  min_lr: 0.000002  loss: 0.7279 (0.7761)  loss_scale: 65536.0000 (107367.4894)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0002  max mem: 41808
Epoch: [37]  [ 150/1349]  eta: 0:06:46  lr: 0.000095  min_lr: 0.000002  loss: 0.7042 (0.7720)  loss_scale: 65536.0000 (104597.1921)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [37]  [ 160/1349]  eta: 0:06:40  lr: 0.000095  min_lr: 0.000002  loss: 0.7013 (0.7670)  loss_scale: 65536.0000 (102171.0311)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [37]  [ 170/1349]  eta: 0:06:34  lr: 0.000095  min_lr: 0.000002  loss: 0.6924 (0.7628)  loss_scale: 65536.0000 (100028.6316)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0002  max mem: 41808
Epoch: [37]  [ 180/1349]  eta: 0:06:29  lr: 0.000095  min_lr: 0.000002  loss: 0.7479 (0.7660)  loss_scale: 65536.0000 (98122.9613)  weight_decay: 0.0500 (0.0500)  time: 0.3065  data: 0.0002  max mem: 41808
Epoch: [37]  [ 190/1349]  eta: 0:06:24  lr: 0.000095  min_lr: 0.000002  loss: 0.8266 (0.7683)  loss_scale: 65536.0000 (96416.8377)  weight_decay: 0.0500 (0.0500)  time: 0.3063  data: 0.0002  max mem: 41808
Epoch: [37]  [ 200/1349]  eta: 0:06:20  lr: 0.000095  min_lr: 0.000002  loss: 0.7969 (0.7681)  loss_scale: 65536.0000 (94880.4776)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
Epoch: [37]  [ 210/1349]  eta: 0:06:15  lr: 0.000095  min_lr: 0.000002  loss: 0.8149 (0.7719)  loss_scale: 65536.0000 (93489.7441)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
[2025-05-23 22:45:36,862] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 22:45:36,862] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 22:45:36,863] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-05-23 22:45:36,863] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [37]  [ 220/1349]  eta: 0:06:11  lr: 0.000095  min_lr: 0.000002  loss: 0.8130 (0.7723)  loss_scale: 65536.0000 (94597.2127)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [37]  [ 230/1349]  eta: 0:06:06  lr: 0.000095  min_lr: 0.000002  loss: 0.8130 (0.7739)  loss_scale: 131072.0000 (96176.2078)  weight_decay: 0.0500 (0.0500)  time: 0.3079  data: 0.0001  max mem: 41808
Epoch: [37]  [ 240/1349]  eta: 0:06:02  lr: 0.000094  min_lr: 0.000002  loss: 0.8283 (0.7722)  loss_scale: 131072.0000 (97624.1660)  weight_decay: 0.0500 (0.0500)  time: 0.3082  data: 0.0001  max mem: 41808
Epoch: [37]  [ 250/1349]  eta: 0:05:58  lr: 0.000094  min_lr: 0.000002  loss: 0.7319 (0.7711)  loss_scale: 131072.0000 (98956.7490)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
Epoch: [37]  [ 260/1349]  eta: 0:05:54  lr: 0.000094  min_lr: 0.000002  loss: 0.7319 (0.7704)  loss_scale: 131072.0000 (100187.2184)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0001  max mem: 41808
Epoch: [37]  [ 270/1349]  eta: 0:05:50  lr: 0.000094  min_lr: 0.000002  loss: 0.8309 (0.7730)  loss_scale: 131072.0000 (101326.8782)  weight_decay: 0.0500 (0.0500)  time: 0.3083  data: 0.0001  max mem: 41808
Epoch: [37]  [ 280/1349]  eta: 0:05:46  lr: 0.000094  min_lr: 0.000002  loss: 0.8322 (0.7747)  loss_scale: 131072.0000 (102385.4235)  weight_decay: 0.0500 (0.0500)  time: 0.3082  data: 0.0001  max mem: 41808
Epoch: [37]  [ 290/1349]  eta: 0:05:42  lr: 0.000094  min_lr: 0.000002  loss: 0.7295 (0.7734)  loss_scale: 131072.0000 (103371.2165)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0002  max mem: 41808
Epoch: [37]  [ 300/1349]  eta: 0:05:38  lr: 0.000094  min_lr: 0.000002  loss: 0.7295 (0.7728)  loss_scale: 131072.0000 (104291.5083)  weight_decay: 0.0500 (0.0500)  time: 0.3064  data: 0.0002  max mem: 41808
Epoch: [37]  [ 310/1349]  eta: 0:05:35  lr: 0.000094  min_lr: 0.000002  loss: 0.7533 (0.7727)  loss_scale: 131072.0000 (105152.6174)  weight_decay: 0.0500 (0.0500)  time: 0.3066  data: 0.0002  max mem: 41808
Epoch: [37]  [ 320/1349]  eta: 0:05:31  lr: 0.000094  min_lr: 0.000002  loss: 0.8308 (0.7756)  loss_scale: 131072.0000 (105960.0748)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0002  max mem: 41808
Epoch: [37]  [ 330/1349]  eta: 0:05:27  lr: 0.000094  min_lr: 0.000002  loss: 0.8670 (0.7776)  loss_scale: 131072.0000 (106718.7432)  weight_decay: 0.0500 (0.0500)  time: 0.3081  data: 0.0002  max mem: 41808
[2025-05-23 22:46:14,082] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 50247
[2025-05-23 22:46:14,082] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 50247
[2025-05-23 22:46:14,083] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 22:46:14,083] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 22:46:14,083] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [37]  [ 340/1349]  eta: 0:05:24  lr: 0.000093  min_lr: 0.000002  loss: 0.8381 (0.7783)  loss_scale: 131072.0000 (106087.6012)  weight_decay: 0.0500 (0.0500)  time: 0.3081  data: 0.0002  max mem: 41808
Epoch: [37]  [ 350/1349]  eta: 0:05:20  lr: 0.000093  min_lr: 0.000002  loss: 0.8120 (0.7781)  loss_scale: 65536.0000 (104932.2849)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [37]  [ 360/1349]  eta: 0:05:16  lr: 0.000093  min_lr: 0.000002  loss: 0.8015 (0.7786)  loss_scale: 65536.0000 (103840.9751)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [37]  [ 370/1349]  eta: 0:05:13  lr: 0.000093  min_lr: 0.000002  loss: 0.7855 (0.7776)  loss_scale: 65536.0000 (102808.4960)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
Epoch: [37]  [ 380/1349]  eta: 0:05:09  lr: 0.000093  min_lr: 0.000002  loss: 0.7572 (0.7776)  loss_scale: 65536.0000 (101830.2152)  weight_decay: 0.0500 (0.0500)  time: 0.3081  data: 0.0002  max mem: 41808
Epoch: [37]  [ 390/1349]  eta: 0:05:06  lr: 0.000093  min_lr: 0.000002  loss: 0.7572 (0.7780)  loss_scale: 65536.0000 (100901.9744)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
Epoch: [37]  [ 400/1349]  eta: 0:05:02  lr: 0.000093  min_lr: 0.000002  loss: 0.7837 (0.7788)  loss_scale: 65536.0000 (100020.0299)  weight_decay: 0.0500 (0.0500)  time: 0.3096  data: 0.0001  max mem: 41808
Epoch: [37]  [ 410/1349]  eta: 0:04:59  lr: 0.000093  min_lr: 0.000002  loss: 0.7826 (0.7778)  loss_scale: 65536.0000 (99181.0024)  weight_decay: 0.0500 (0.0500)  time: 0.3099  data: 0.0001  max mem: 41808
Epoch: [37]  [ 420/1349]  eta: 0:04:56  lr: 0.000093  min_lr: 0.000002  loss: 0.7641 (0.7776)  loss_scale: 65536.0000 (98381.8337)  weight_decay: 0.0500 (0.0500)  time: 0.3078  data: 0.0001  max mem: 41808
Epoch: [37]  [ 430/1349]  eta: 0:04:52  lr: 0.000093  min_lr: 0.000002  loss: 0.7641 (0.7767)  loss_scale: 65536.0000 (97619.7494)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
Epoch: [37]  [ 440/1349]  eta: 0:04:49  lr: 0.000092  min_lr: 0.000002  loss: 0.7816 (0.7778)  loss_scale: 65536.0000 (96892.2268)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [37]  [ 450/1349]  eta: 0:04:45  lr: 0.000092  min_lr: 0.000002  loss: 0.8498 (0.7773)  loss_scale: 65536.0000 (96196.9667)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0001  max mem: 41808
Epoch: [37]  [ 460/1349]  eta: 0:04:42  lr: 0.000092  min_lr: 0.000002  loss: 0.8459 (0.7783)  loss_scale: 65536.0000 (95531.8698)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0002  max mem: 41808
[2025-05-23 22:46:53,813] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 22:46:53,813] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-05-23 22:46:53,813] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 22:46:53,813] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [37]  [ 470/1349]  eta: 0:04:39  lr: 0.000092  min_lr: 0.000002  loss: 0.8223 (0.7785)  loss_scale: 65536.0000 (96008.1529)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
Epoch: [37]  [ 480/1349]  eta: 0:04:35  lr: 0.000092  min_lr: 0.000002  loss: 0.8223 (0.7799)  loss_scale: 131072.0000 (96737.1310)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0002  max mem: 41808
Epoch: [37]  [ 490/1349]  eta: 0:04:32  lr: 0.000092  min_lr: 0.000002  loss: 0.8462 (0.7803)  loss_scale: 131072.0000 (97436.4155)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0002  max mem: 41808
Epoch: [37]  [ 500/1349]  eta: 0:04:29  lr: 0.000092  min_lr: 0.000002  loss: 0.8031 (0.7809)  loss_scale: 131072.0000 (98107.7844)  weight_decay: 0.0500 (0.0500)  time: 0.3078  data: 0.0001  max mem: 41808
Epoch: [37]  [ 510/1349]  eta: 0:04:25  lr: 0.000092  min_lr: 0.000002  loss: 0.8118 (0.7809)  loss_scale: 131072.0000 (98752.8767)  weight_decay: 0.0500 (0.0500)  time: 0.3083  data: 0.0001  max mem: 41808
Epoch: [37]  [ 520/1349]  eta: 0:04:22  lr: 0.000092  min_lr: 0.000002  loss: 0.7922 (0.7819)  loss_scale: 131072.0000 (99373.2054)  weight_decay: 0.0500 (0.0500)  time: 0.3083  data: 0.0001  max mem: 41808
Epoch: [37]  [ 530/1349]  eta: 0:04:19  lr: 0.000092  min_lr: 0.000002  loss: 0.7712 (0.7813)  loss_scale: 131072.0000 (99970.1695)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [37]  [ 540/1349]  eta: 0:04:15  lr: 0.000091  min_lr: 0.000002  loss: 0.8000 (0.7822)  loss_scale: 131072.0000 (100545.0647)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
Epoch: [37]  [ 550/1349]  eta: 0:04:12  lr: 0.000091  min_lr: 0.000002  loss: 0.8618 (0.7814)  loss_scale: 131072.0000 (101099.0926)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
Epoch: [37]  [ 560/1349]  eta: 0:04:09  lr: 0.000091  min_lr: 0.000002  loss: 0.7618 (0.7816)  loss_scale: 131072.0000 (101633.3690)  weight_decay: 0.0500 (0.0500)  time: 0.3078  data: 0.0001  max mem: 41808
Epoch: [37]  [ 570/1349]  eta: 0:04:05  lr: 0.000091  min_lr: 0.000002  loss: 0.7687 (0.7820)  loss_scale: 131072.0000 (102148.9317)  weight_decay: 0.0500 (0.0500)  time: 0.3078  data: 0.0001  max mem: 41808
Epoch: [37]  [ 580/1349]  eta: 0:04:02  lr: 0.000091  min_lr: 0.000002  loss: 0.8268 (0.7824)  loss_scale: 131072.0000 (102646.7470)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [37]  [ 590/1349]  eta: 0:03:59  lr: 0.000091  min_lr: 0.000002  loss: 0.7722 (0.7826)  loss_scale: 131072.0000 (103127.7157)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
[2025-05-23 22:47:33,182] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 22:47:33,182] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 22:47:33,182] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 22:47:33,182] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [37]  [ 600/1349]  eta: 0:03:56  lr: 0.000091  min_lr: 0.000002  loss: 0.7722 (0.7827)  loss_scale: 131072.0000 (105773.5774)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
[2025-05-23 22:47:38,105] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 50520
[2025-05-23 22:47:38,106] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 22:47:38,105] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 50520
[2025-05-23 22:47:38,106] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 22:47:38,106] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [37]  [ 610/1349]  eta: 0:03:52  lr: 0.000091  min_lr: 0.000002  loss: 0.8386 (0.7835)  loss_scale: 262144.0000 (107474.7496)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [37]  [ 620/1349]  eta: 0:03:49  lr: 0.000091  min_lr: 0.000002  loss: 0.8386 (0.7836)  loss_scale: 131072.0000 (107854.7375)  weight_decay: 0.0500 (0.0500)  time: 0.3080  data: 0.0001  max mem: 41808
Epoch: [37]  [ 630/1349]  eta: 0:03:46  lr: 0.000091  min_lr: 0.000002  loss: 0.7729 (0.7829)  loss_scale: 131072.0000 (108222.6815)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0001  max mem: 41808
Epoch: [37]  [ 640/1349]  eta: 0:03:43  lr: 0.000090  min_lr: 0.000002  loss: 0.7691 (0.7827)  loss_scale: 131072.0000 (108579.1451)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [37]  [ 650/1349]  eta: 0:03:39  lr: 0.000090  min_lr: 0.000002  loss: 0.7849 (0.7828)  loss_scale: 131072.0000 (108924.6575)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [37]  [ 660/1349]  eta: 0:03:36  lr: 0.000090  min_lr: 0.000002  loss: 0.8103 (0.7824)  loss_scale: 131072.0000 (109259.7156)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [37]  [ 670/1349]  eta: 0:03:33  lr: 0.000090  min_lr: 0.000002  loss: 0.7402 (0.7805)  loss_scale: 131072.0000 (109584.7869)  weight_decay: 0.0500 (0.0500)  time: 0.3080  data: 0.0001  max mem: 41808
Epoch: [37]  [ 680/1349]  eta: 0:03:30  lr: 0.000090  min_lr: 0.000002  loss: 0.7587 (0.7812)  loss_scale: 131072.0000 (109900.3113)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
Epoch: [37]  [ 690/1349]  eta: 0:03:27  lr: 0.000090  min_lr: 0.000002  loss: 0.7989 (0.7807)  loss_scale: 131072.0000 (110206.7033)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
Epoch: [37]  [ 700/1349]  eta: 0:03:23  lr: 0.000090  min_lr: 0.000002  loss: 0.7988 (0.7807)  loss_scale: 131072.0000 (110504.3538)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [37]  [ 710/1349]  eta: 0:03:20  lr: 0.000090  min_lr: 0.000002  loss: 0.8006 (0.7799)  loss_scale: 131072.0000 (110793.6315)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
Epoch: [37]  [ 720/1349]  eta: 0:03:17  lr: 0.000090  min_lr: 0.000002  loss: 0.7203 (0.7788)  loss_scale: 131072.0000 (111074.8849)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
Epoch: [37]  [ 730/1349]  eta: 0:03:14  lr: 0.000090  min_lr: 0.000002  loss: 0.7475 (0.7786)  loss_scale: 131072.0000 (111348.4432)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
[2025-05-23 22:48:17,809] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 22:48:17,809] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 22:48:17,810] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 22:48:17,810] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [37]  [ 740/1349]  eta: 0:03:11  lr: 0.000089  min_lr: 0.000002  loss: 0.7640 (0.7783)  loss_scale: 131072.0000 (112499.0445)  weight_decay: 0.0500 (0.0500)  time: 0.3090  data: 0.0001  max mem: 41808
Epoch: [37]  [ 750/1349]  eta: 0:03:07  lr: 0.000089  min_lr: 0.000002  loss: 0.7794 (0.7782)  loss_scale: 262144.0000 (114491.6538)  weight_decay: 0.0500 (0.0500)  time: 0.3093  data: 0.0001  max mem: 41808
[2025-05-23 22:48:23,956] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 50669
[2025-05-23 22:48:23,956] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 22:48:23,956] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 50669
[2025-05-23 22:48:23,956] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 22:48:23,957] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [37]  [ 760/1349]  eta: 0:03:04  lr: 0.000089  min_lr: 0.000002  loss: 0.7849 (0.7777)  loss_scale: 262144.0000 (115570.7122)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
Epoch: [37]  [ 770/1349]  eta: 0:03:01  lr: 0.000089  min_lr: 0.000002  loss: 0.7849 (0.7774)  loss_scale: 131072.0000 (115771.7665)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [37]  [ 780/1349]  eta: 0:02:58  lr: 0.000089  min_lr: 0.000002  loss: 0.8094 (0.7784)  loss_scale: 131072.0000 (115967.6722)  weight_decay: 0.0500 (0.0500)  time: 0.3078  data: 0.0001  max mem: 41808
Epoch: [37]  [ 790/1349]  eta: 0:02:55  lr: 0.000089  min_lr: 0.000002  loss: 0.8094 (0.7790)  loss_scale: 131072.0000 (116158.6245)  weight_decay: 0.0500 (0.0500)  time: 0.3080  data: 0.0001  max mem: 41808
Epoch: [37]  [ 800/1349]  eta: 0:02:52  lr: 0.000089  min_lr: 0.000002  loss: 0.8730 (0.7803)  loss_scale: 131072.0000 (116344.8090)  weight_decay: 0.0500 (0.0500)  time: 0.3080  data: 0.0001  max mem: 41808
Epoch: [37]  [ 810/1349]  eta: 0:02:48  lr: 0.000089  min_lr: 0.000002  loss: 0.8376 (0.7805)  loss_scale: 131072.0000 (116526.4020)  weight_decay: 0.0500 (0.0500)  time: 0.3078  data: 0.0001  max mem: 41808
Epoch: [37]  [ 820/1349]  eta: 0:02:45  lr: 0.000089  min_lr: 0.000002  loss: 0.7966 (0.7801)  loss_scale: 131072.0000 (116703.5713)  weight_decay: 0.0500 (0.0500)  time: 0.3079  data: 0.0001  max mem: 41808
Epoch: [37]  [ 830/1349]  eta: 0:02:42  lr: 0.000089  min_lr: 0.000002  loss: 0.7446 (0.7802)  loss_scale: 131072.0000 (116876.4765)  weight_decay: 0.0500 (0.0500)  time: 0.3078  data: 0.0001  max mem: 41808
Epoch: [37]  [ 840/1349]  eta: 0:02:39  lr: 0.000088  min_lr: 0.000002  loss: 0.7824 (0.7800)  loss_scale: 131072.0000 (117045.2699)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
Epoch: [37]  [ 850/1349]  eta: 0:02:36  lr: 0.000088  min_lr: 0.000002  loss: 0.8289 (0.7810)  loss_scale: 131072.0000 (117210.0964)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [37]  [ 860/1349]  eta: 0:02:33  lr: 0.000088  min_lr: 0.000002  loss: 0.8538 (0.7814)  loss_scale: 131072.0000 (117371.0941)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [37]  [ 870/1349]  eta: 0:02:29  lr: 0.000088  min_lr: 0.000002  loss: 0.8345 (0.7813)  loss_scale: 131072.0000 (117528.3949)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [37]  [ 880/1349]  eta: 0:02:26  lr: 0.000088  min_lr: 0.000002  loss: 0.7282 (0.7813)  loss_scale: 131072.0000 (117682.1249)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
[2025-05-23 22:49:03,636] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 22:49:03,636] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 22:49:03,636] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 22:49:03,636] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 22:49:03,939] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 50799
[2025-05-23 22:49:03,939] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 50799
[2025-05-23 22:49:03,939] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 22:49:03,939] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 22:49:03,939] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [37]  [ 890/1349]  eta: 0:02:23  lr: 0.000088  min_lr: 0.000002  loss: 0.8013 (0.7816)  loss_scale: 131072.0000 (117979.5107)  weight_decay: 0.0500 (0.0500)  time: 0.3064  data: 0.0001  max mem: 41808
Epoch: [37]  [ 900/1349]  eta: 0:02:20  lr: 0.000088  min_lr: 0.000002  loss: 0.7666 (0.7813)  loss_scale: 131072.0000 (118124.8213)  weight_decay: 0.0500 (0.0500)  time: 0.3068  data: 0.0001  max mem: 41808
Epoch: [37]  [ 910/1349]  eta: 0:02:17  lr: 0.000088  min_lr: 0.000002  loss: 0.8297 (0.7824)  loss_scale: 131072.0000 (118266.9418)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
Epoch: [37]  [ 920/1349]  eta: 0:02:14  lr: 0.000088  min_lr: 0.000002  loss: 0.8492 (0.7828)  loss_scale: 131072.0000 (118405.9761)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
Epoch: [37]  [ 930/1349]  eta: 0:02:10  lr: 0.000088  min_lr: 0.000002  loss: 0.7674 (0.7825)  loss_scale: 131072.0000 (118542.0236)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [37]  [ 940/1349]  eta: 0:02:07  lr: 0.000088  min_lr: 0.000002  loss: 0.7308 (0.7820)  loss_scale: 131072.0000 (118675.1796)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [37]  [ 950/1349]  eta: 0:02:04  lr: 0.000087  min_lr: 0.000002  loss: 0.7824 (0.7825)  loss_scale: 131072.0000 (118805.5352)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0001  max mem: 41808
Epoch: [37]  [ 960/1349]  eta: 0:02:01  lr: 0.000087  min_lr: 0.000002  loss: 0.8174 (0.7827)  loss_scale: 131072.0000 (118933.1779)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [37]  [ 970/1349]  eta: 0:01:58  lr: 0.000087  min_lr: 0.000002  loss: 0.8174 (0.7826)  loss_scale: 131072.0000 (119058.1916)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
Epoch: [37]  [ 980/1349]  eta: 0:01:55  lr: 0.000087  min_lr: 0.000002  loss: 0.7803 (0.7824)  loss_scale: 131072.0000 (119180.6565)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
[2025-05-23 22:49:33,126] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 50894
[2025-05-23 22:49:33,126] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 50894
[2025-05-23 22:49:33,127] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 22:49:33,127] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2025-05-23 22:49:33,127] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [37]  [ 990/1349]  eta: 0:01:52  lr: 0.000087  min_lr: 0.000002  loss: 0.7888 (0.7824)  loss_scale: 65536.0000 (118639.3380)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
Epoch: [37]  [1000/1349]  eta: 0:01:48  lr: 0.000087  min_lr: 0.000002  loss: 0.7980 (0.7824)  loss_scale: 65536.0000 (118108.8352)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
Epoch: [37]  [1010/1349]  eta: 0:01:45  lr: 0.000087  min_lr: 0.000002  loss: 0.7817 (0.7818)  loss_scale: 65536.0000 (117588.8269)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0001  max mem: 41808
Epoch: [37]  [1020/1349]  eta: 0:01:42  lr: 0.000087  min_lr: 0.000002  loss: 0.7919 (0.7818)  loss_scale: 65536.0000 (117079.0049)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0001  max mem: 41808
Epoch: [37]  [1030/1349]  eta: 0:01:39  lr: 0.000087  min_lr: 0.000002  loss: 0.7959 (0.7823)  loss_scale: 65536.0000 (116579.0727)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [37]  [1040/1349]  eta: 0:01:36  lr: 0.000087  min_lr: 0.000002  loss: 0.8386 (0.7827)  loss_scale: 65536.0000 (116088.7454)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [37]  [1050/1349]  eta: 0:01:33  lr: 0.000086  min_lr: 0.000002  loss: 0.7604 (0.7820)  loss_scale: 65536.0000 (115607.7488)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [37]  [1060/1349]  eta: 0:01:30  lr: 0.000086  min_lr: 0.000002  loss: 0.6858 (0.7818)  loss_scale: 65536.0000 (115135.8190)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
Epoch: [37]  [1070/1349]  eta: 0:01:27  lr: 0.000086  min_lr: 0.000002  loss: 0.8110 (0.7818)  loss_scale: 65536.0000 (114672.7021)  weight_decay: 0.0500 (0.0500)  time: 0.3083  data: 0.0001  max mem: 41808
Epoch: [37]  [1080/1349]  eta: 0:01:23  lr: 0.000086  min_lr: 0.000002  loss: 0.7235 (0.7802)  loss_scale: 65536.0000 (114218.1536)  weight_decay: 0.0500 (0.0500)  time: 0.3089  data: 0.0001  max mem: 41808
[2025-05-23 22:50:05,426] [INFO] [logging.py:96:log_dist] [Rank 0] step=51000, skipped=313, lr=[2.045284865044601e-06, 2.045284865044601e-06, 2.727046486726135e-06, 2.727046486726135e-06, 3.636061982301513e-06, 3.636061982301513e-06, 4.848082643068684e-06, 4.848082643068684e-06, 6.464110190758246e-06, 6.464110190758246e-06, 8.618813587677661e-06, 8.618813587677661e-06, 1.149175145023688e-05, 1.149175145023688e-05, 1.5322335266982508e-05, 1.5322335266982508e-05, 2.0429780355976676e-05, 2.0429780355976676e-05, 2.7239707141302235e-05, 2.7239707141302235e-05, 3.6319609521736315e-05, 3.6319609521736315e-05, 4.8426146028981754e-05, 4.8426146028981754e-05, 6.4568194705309e-05, 6.4568194705309e-05, 8.609092627374534e-05, 8.609092627374534e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2025-05-23 22:50:05,427] [INFO] [timer.py:260:stop] epoch=0/micro_step=51000/global_step=51000, RunningAvgSamplesPerSec=207.96746579520743, CurrSamplesPerSec=214.92460297443904, MemAllocated=1.37GB, MaxMemAllocated=40.83GB
Epoch: [37]  [1090/1349]  eta: 0:01:20  lr: 0.000086  min_lr: 0.000002  loss: 0.7235 (0.7806)  loss_scale: 65536.0000 (113771.9377)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [37]  [1100/1349]  eta: 0:01:17  lr: 0.000086  min_lr: 0.000002  loss: 0.8094 (0.7804)  loss_scale: 65536.0000 (113333.8274)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
[2025-05-23 22:50:12,908] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 22:50:12,908] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 22:50:12,908] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2025-05-23 22:50:12,908] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [37]  [1110/1349]  eta: 0:01:14  lr: 0.000086  min_lr: 0.000002  loss: 0.7747 (0.7803)  loss_scale: 65536.0000 (112962.5923)  weight_decay: 0.0500 (0.0500)  time: 0.3130  data: 0.0001  max mem: 41808
Epoch: [37]  [1120/1349]  eta: 0:01:11  lr: 0.000086  min_lr: 0.000002  loss: 0.7583 (0.7796)  loss_scale: 131072.0000 (113124.1392)  weight_decay: 0.0500 (0.0500)  time: 0.3132  data: 0.0002  max mem: 41808
Epoch: [37]  [1130/1349]  eta: 0:01:08  lr: 0.000086  min_lr: 0.000002  loss: 0.7926 (0.7797)  loss_scale: 131072.0000 (113282.8294)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0002  max mem: 41808
Epoch: [37]  [1140/1349]  eta: 0:01:05  lr: 0.000086  min_lr: 0.000002  loss: 0.7379 (0.7790)  loss_scale: 131072.0000 (113438.7379)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
Epoch: [37]  [1150/1349]  eta: 0:01:02  lr: 0.000085  min_lr: 0.000002  loss: 0.7233 (0.7791)  loss_scale: 131072.0000 (113591.9374)  weight_decay: 0.0500 (0.0500)  time: 0.3067  data: 0.0001  max mem: 41808
Epoch: [37]  [1160/1349]  eta: 0:00:58  lr: 0.000085  min_lr: 0.000002  loss: 0.7730 (0.7787)  loss_scale: 131072.0000 (113742.4978)  weight_decay: 0.0500 (0.0500)  time: 0.3072  data: 0.0002  max mem: 41808
Epoch: [37]  [1170/1349]  eta: 0:00:55  lr: 0.000085  min_lr: 0.000002  loss: 0.8190 (0.7791)  loss_scale: 131072.0000 (113890.4868)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [37]  [1180/1349]  eta: 0:00:52  lr: 0.000085  min_lr: 0.000002  loss: 0.8514 (0.7799)  loss_scale: 131072.0000 (114035.9695)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [37]  [1190/1349]  eta: 0:00:49  lr: 0.000085  min_lr: 0.000002  loss: 0.8500 (0.7800)  loss_scale: 131072.0000 (114179.0092)  weight_decay: 0.0500 (0.0500)  time: 0.3075  data: 0.0001  max mem: 41808
Epoch: [37]  [1200/1349]  eta: 0:00:46  lr: 0.000085  min_lr: 0.000002  loss: 0.7234 (0.7792)  loss_scale: 131072.0000 (114319.6669)  weight_decay: 0.0500 (0.0500)  time: 0.3071  data: 0.0001  max mem: 41808
Epoch: [37]  [1210/1349]  eta: 0:00:43  lr: 0.000085  min_lr: 0.000002  loss: 0.7725 (0.7790)  loss_scale: 131072.0000 (114458.0017)  weight_decay: 0.0500 (0.0500)  time: 0.3073  data: 0.0001  max mem: 41808
Epoch: [37]  [1220/1349]  eta: 0:00:40  lr: 0.000085  min_lr: 0.000002  loss: 0.8360 (0.7794)  loss_scale: 131072.0000 (114594.0704)  weight_decay: 0.0500 (0.0500)  time: 0.3074  data: 0.0001  max mem: 41808
Epoch: [37]  [1230/1349]  eta: 0:00:37  lr: 0.000085  min_lr: 0.000002  loss: 0.8791 (0.7799)  loss_scale: 131072.0000 (114727.9285)  weight_decay: 0.0500 (0.0500)  time: 0.3076  data: 0.0001  max mem: 41808
[2025-05-23 22:50:52,264] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 22:50:52,264] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
[2025-05-23 22:50:52,264] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 128 iterations
[2025-05-23 22:50:52,265] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
Epoch: [37]  [1240/1349]  eta: 0:00:33  lr: 0.000085  min_lr: 0.000002  loss: 0.8587 (0.7803)  loss_scale: 131072.0000 (115176.4835)  weight_decay: 0.0500 (0.0500)  time: 0.3084  data: 0.0001  max mem: 41808
Epoch: [37]  [1250/1349]  eta: 0:00:30  lr: 0.000085  min_lr: 0.000002  loss: 0.8327 (0.7799)  loss_scale: 262144.0000 (116351.2838)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0001  max mem: 41808
[2025-05-23 22:50:58,400] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 51171
[2025-05-23 22:50:58,400] [INFO] [fused_optimizer.py:344:_update_scale] 
Grad overflow on iteration 51171
[2025-05-23 22:50:58,400] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 22:50:58,400] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
[2025-05-23 22:50:58,400] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
Epoch: [37]  [1260/1349]  eta: 0:00:27  lr: 0.000084  min_lr: 0.000002  loss: 0.7347 (0.7796)  loss_scale: 262144.0000 (117195.6225)  weight_decay: 0.0500 (0.0500)  time: 0.3059  data: 0.0001  max mem: 41808
Epoch: [37]  [1270/1349]  eta: 0:00:24  lr: 0.000084  min_lr: 0.000002  loss: 0.7897 (0.7798)  loss_scale: 131072.0000 (117304.7994)  weight_decay: 0.0500 (0.0500)  time: 0.3061  data: 0.0001  max mem: 41808
Epoch: [37]  [1280/1349]  eta: 0:00:21  lr: 0.000084  min_lr: 0.000002  loss: 0.7966 (0.7794)  loss_scale: 131072.0000 (117412.2717)  weight_decay: 0.0500 (0.0500)  time: 0.3070  data: 0.0001  max mem: 41808
Epoch: [37]  [1290/1349]  eta: 0:00:18  lr: 0.000084  min_lr: 0.000002  loss: 0.8467 (0.7800)  loss_scale: 131072.0000 (117518.0790)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
Epoch: [37]  [1300/1349]  eta: 0:00:15  lr: 0.000084  min_lr: 0.000002  loss: 0.8537 (0.7800)  loss_scale: 131072.0000 (117622.2598)  weight_decay: 0.0500 (0.0500)  time: 0.3069  data: 0.0001  max mem: 41808
Epoch: [37]  [1310/1349]  eta: 0:00:12  lr: 0.000084  min_lr: 0.000002  loss: 0.7834 (0.7801)  loss_scale: 131072.0000 (117724.8513)  weight_decay: 0.0500 (0.0500)  time: 0.3078  data: 0.0001  max mem: 41808
Epoch: [37]  [1320/1349]  eta: 0:00:09  lr: 0.000084  min_lr: 0.000002  loss: 0.7885 (0.7798)  loss_scale: 131072.0000 (117825.8895)  weight_decay: 0.0500 (0.0500)  time: 0.3077  data: 0.0002  max mem: 41808
Epoch: [37]  [1330/1349]  eta: 0:00:05  lr: 0.000084  min_lr: 0.000002  loss: 0.8134 (0.7803)  loss_scale: 131072.0000 (117925.4095)  weight_decay: 0.0500 (0.0500)  time: 0.3050  data: 0.0001  max mem: 41808
Epoch: [37]  [1340/1349]  eta: 0:00:02  lr: 0.000084  min_lr: 0.000002  loss: 0.8134 (0.7801)  loss_scale: 131072.0000 (118023.4452)  weight_decay: 0.0500 (0.0500)  time: 0.3024  data: 0.0001  max mem: 41808
Epoch: [37]  [1348/1349]  eta: 0:00:00  lr: 0.000084  min_lr: 0.000002  loss: 0.7835 (0.7801)  loss_scale: 131072.0000 (118100.8273)  weight_decay: 0.0500 (0.0500)  time: 0.3019  data: 0.0001  max mem: 41808
Epoch: [37] Total time: 0:06:59 (0.3112 s / it)
Averaged stats: lr: 0.000084  min_lr: 0.000002  loss: 0.7835 (0.7816)  loss_scale: 131072.0000 (118100.8273)  weight_decay: 0.0500 (0.0500)  total_time: 419.8610 (419.8572)
Val:  [  0/346]  eta: 1:24:42  loss: 3.2762 (3.2762)  acc1: 0.7812 (0.7812)  acc5: 87.5000 (87.5000)  time: 14.6881  data: 13.9369  max mem: 41808
Val:  [ 10/346]  eta: 0:12:11  loss: 0.1272 (0.6077)  acc1: 100.0000 (84.2330)  acc5: 100.0000 (98.2955)  time: 2.1784  data: 1.3513  max mem: 41808
Val:  [ 20/346]  eta: 0:08:19  loss: 0.1197 (0.4731)  acc1: 100.0000 (88.3557)  acc5: 100.0000 (98.9583)  time: 0.8748  data: 0.0465  max mem: 41808
Val:  [ 30/346]  eta: 0:06:40  loss: 0.1010 (0.3973)  acc1: 100.0000 (90.6754)  acc5: 100.0000 (99.2944)  time: 0.7663  data: 0.0003  max mem: 41808
Val:  [ 40/346]  eta: 0:05:47  loss: 0.1021 (0.4441)  acc1: 100.0000 (89.3483)  acc5: 100.0000 (99.1425)  time: 0.7195  data: 0.0003  max mem: 41808
Val:  [ 50/346]  eta: 0:05:15  loss: 0.1053 (0.3882)  acc1: 99.2188 (91.1152)  acc5: 100.0000 (99.2953)  time: 0.7514  data: 0.0003  max mem: 41808
Val:  [ 60/346]  eta: 0:04:51  loss: 0.1659 (0.3595)  acc1: 98.4375 (92.0210)  acc5: 100.0000 (99.3981)  time: 0.7757  data: 0.0081  max mem: 41808
Val:  [ 70/346]  eta: 0:04:36  loss: 0.1839 (0.3836)  acc1: 97.6562 (91.3732)  acc5: 100.0000 (99.3838)  time: 0.8381  data: 0.0798  max mem: 41808
Val:  [ 80/346]  eta: 0:04:22  loss: 0.2026 (0.3855)  acc1: 94.5312 (91.3580)  acc5: 100.0000 (99.3538)  time: 0.8981  data: 0.1450  max mem: 41808
